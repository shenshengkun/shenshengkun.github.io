<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>舒宇的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://shenshengkun.github.io/"/>
  <updated>2019-07-19T03:29:49.676Z</updated>
  <id>https://shenshengkun.github.io/</id>
  
  <author>
    <name>Shu Yu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k8s上用ceph-rbd存储</title>
    <link href="https://shenshengkun.github.io/posts/0ijfj445.html"/>
    <id>https://shenshengkun.github.io/posts/0ijfj445.html</id>
    <published>2019-07-19T02:10:01.000Z</published>
    <updated>2019-07-19T03:29:49.676Z</updated>
    
    <content type="html"><![CDATA[<p>k8s默认使用的本地存储，集群容灾性差，ceph作为开源的分布式存储系统，与openstack环境搭配使用，已经很多云计算公司运用于生产环境，可靠性得到验证。这里介绍一下在k8s环境下ceph如何使用. </p><p>Kubernetes支持后两种存储接口,支持的接入模式如下图: </p><p><img src="https://shenshengkun.github.io/images/k8s-ceph1.png" alt=""></p><h1 id="ceph端"><a href="#ceph端" class="headerlink" title="ceph端"></a>ceph端</h1><h2 id="新建pool"><a href="#新建pool" class="headerlink" title="新建pool"></a>新建pool</h2><p>新建一个pool pool_1包含90个pg </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create pool_1 90</span><br></pre></td></tr></table></figure><h2 id="RBD块设备"><a href="#RBD块设备" class="headerlink" title="RBD块设备"></a>RBD块设备</h2><p>在ceph集群中新建1个rbd块设备，lun1 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd create pool_1/lun1 --size 10G</span><br></pre></td></tr></table></figure><h2 id="ceph权限控制"><a href="#ceph权限控制" class="headerlink" title="ceph权限控制"></a>ceph权限控制</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">使用ceph-deploy --overwrite-conf admin部署的keyring权限太大，可以自己创建一个keyring client.rdb给块设备客户端node用</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ceph auth get-or-create client.rbd mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=pool_1&apos;  &gt; ceph.client.rbd.keyring</span><br><span class="line"></span><br><span class="line">k8s节点需要安装ceph</span><br><span class="line">yum install ceph-common</span><br><span class="line">echo &apos;rbd&apos; &gt; /etc/modules-load.d/rbd.conf</span><br><span class="line">modprobe rbd</span><br><span class="line">lsmod | grep rbd</span><br><span class="line">rbd                    83640  0 </span><br><span class="line">libceph               306625  1 rbd</span><br><span class="line"></span><br><span class="line">配置文件秘钥传到k8s上</span><br><span class="line">[root@ceph ceph]# scp ceph.client.rbd.keyring 192.168.6.102:/etc/ceph/</span><br><span class="line">root@192.168.6.102&apos;s password: </span><br><span class="line">ceph.client.rdb.keyring                                                                                                       100%   63     8.5KB/s   00:00    </span><br><span class="line">[root@ceph ceph]# scp ceph.conf 192.168.6.102:/etc/ceph/                </span><br><span class="line">root@192.168.6.102&apos;s password: </span><br><span class="line">ceph.conf                                                                                                                       100%  310    25.1KB/s   00:00    </span><br><span class="line">[root@ceph ceph]#</span><br></pre></td></tr></table></figure><h1 id="k8s的node上操作"><a href="#k8s的node上操作" class="headerlink" title="k8s的node上操作"></a>k8s的node上操作</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ceph]# ceph -s --name client.rdb</span><br><span class="line">  cluster:</span><br><span class="line">    id:     cbc04385-1cdf-4512-a3f5-a5b3e8686a05</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            application not enabled on 1 pool(s)</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph</span><br><span class="line">    mgr: ceph(active)</span><br><span class="line">    osd: 1 osds: 1 up, 1 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   1 pools, 90 pgs</span><br><span class="line">    objects: 5 objects, 709B</span><br><span class="line">    usage:   1.00GiB used, 19.0GiB / 20.0GiB avail</span><br><span class="line">    pgs:     90 active+clean</span><br><span class="line">    </span><br><span class="line">警告解决办法：</span><br><span class="line">ceph health detail</span><br><span class="line">ceph osd pool application enable pool_1 rbd</span><br></pre></td></tr></table></figure><h2 id="map设备"><a href="#map设备" class="headerlink" title="map设备"></a>map设备</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># rbd map pool_1/lun1 --name client.rbd</span><br><span class="line">rbd: sysfs write failed</span><br><span class="line">RBD image feature set mismatch. Try disabling features unsupported by the kernel with &quot;rbd feature disable&quot;.</span><br><span class="line">In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.</span><br><span class="line">rbd: map failed: (6) No such device or address</span><br><span class="line"></span><br><span class="line">解决办法：</span><br><span class="line">在ceph节点上</span><br><span class="line">rbd feature disable pool_1/lun1 exclusive-lock, object-map, fast-diff, deep-flatten</span><br></pre></td></tr></table></figure><h2 id="将块设备挂载在操作系统中进行格式化"><a href="#将块设备挂载在操作系统中进行格式化" class="headerlink" title="将块设备挂载在操作系统中进行格式化"></a>将块设备挂载在操作系统中进行格式化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rbd map pool_1/lun1 --name client.rbd</span><br><span class="line">mkfs.ext4 /dev/rbd0</span><br></pre></td></tr></table></figure><h2 id="创建pv、pvc"><a href="#创建pv、pvc" class="headerlink" title="创建pv、pvc"></a>创建pv、pvc</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对ceph.client.admin.keyring 的内容进行base64编码</span><br><span class="line">[root@node1 ceph]# ceph auth get-key client.rbd | base64</span><br><span class="line">QVFCTktERmRzeXpKQUJBQVVvVGVvWVYyamxhRi8zNU1hZ2R2dFE9PQ==</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">根据上面的输出，创建secret ceph-client-rbd</span><br><span class="line">[root@node1 ceph]# cat ceph-secret.yml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-client-rbd</span><br><span class="line">type: &quot;kubernetes.io/rbd&quot;  </span><br><span class="line">data:</span><br><span class="line">  key: QVFCTktERmRzeXpKQUJBQVVvVGVvWVYyamxhRi8zNU1hZ2R2dFE9PQ==</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">kubectl apply -f ceph-secret.yml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">创建pv，注意： 这里是user：rbd 而不是user: client.rbd</span><br><span class="line"></span><br><span class="line">[root@node1 ceph]# cat pv.yml </span><br><span class="line">kind: PersistentVolume</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-pool1-lun1</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 10Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce          </span><br><span class="line">  rbd:</span><br><span class="line">    fsType: ext4</span><br><span class="line">    image: lun1</span><br><span class="line">    monitors:</span><br><span class="line">      - &apos;192.168.6.101:6789&apos;</span><br><span class="line">    pool: pool_1</span><br><span class="line">    readOnly: false</span><br><span class="line">    secretRef:</span><br><span class="line">      name: ceph-client-rbd</span><br><span class="line">      namespace: default</span><br><span class="line">    user: rbd</span><br><span class="line">    </span><br><span class="line">[root@node1 ceph]# kubectl apply -f pv.yml </span><br><span class="line">persistentvolume/ceph-pool1-lun1 created</span><br><span class="line">[root@node1 ceph]# kubectl get pv</span><br><span class="line">NAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class="line">ceph-pool1-lun1   10Gi       RWO            Retain           Available           manual                  4s</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">创建pvc</span><br><span class="line"></span><br><span class="line">[root@node1 ceph]# cat pvc.yml </span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc1</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 10Gi</span><br><span class="line">      </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">[root@node1 ceph]# kubectl apply -f pvc.yml </span><br><span class="line">persistentvolumeclaim/pvc1 created</span><br><span class="line">[root@node1 ceph]# kubectl get pvc</span><br><span class="line">NAME   STATUS   VOLUME            CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1   Bound    ceph-pool1-lun1   10Gi       RWO            manual         7s</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;k8s默认使用的本地存储，集群容灾性差，ceph作为开源的分布式存储系统，与openstack环境搭配使用，已经很多云计算公司运用于生产环境，可靠性得到验证。这里介绍一下在k8s环境下ceph如何使用. &lt;/p&gt;
&lt;p&gt;Kubernetes支持后两种存储接口,支持的接入模式
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>ceph对象存储集群部署</title>
    <link href="https://shenshengkun.github.io/posts/ouy564tra.html"/>
    <id>https://shenshengkun.github.io/posts/ouy564tra.html</id>
    <published>2019-07-17T07:17:01.000Z</published>
    <updated>2019-07-17T07:38:00.175Z</updated>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.10.186   ceph1          admin、mon、mgr、osd、rgw</span><br><span class="line"></span><br><span class="line">192.168.10.187   ceph2          mon、mgr、osd、rgw </span><br><span class="line"></span><br><span class="line">192.168.10.188   ceph3          mon、mgr、osd、rgw</span><br></pre></td></tr></table></figure><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">[root@10dot186 ~]# vim /etc/hosts</span><br><span class="line">192.168.10.186   ceph1</span><br><span class="line">192.168.10.187   ceph2</span><br><span class="line">192.168.10.188   ceph3</span><br><span class="line"></span><br><span class="line">hostnamectl set-hostname ceph1</span><br><span class="line">hostnamectl set-hostname ceph2</span><br><span class="line">hostnamectl set-hostname ceph3</span><br><span class="line"></span><br><span class="line">ntpdate ntp1.aliyun.com</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id ceph1</span><br><span class="line">ssh-copy-id ceph2</span><br><span class="line">ssh-copy-id ceph3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ~]# vim /etc/yum.repos.d/ceph.repo</span><br><span class="line">[ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">priority=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">priority=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br><span class="line">yum makecache</span><br><span class="line">yum update -y</span><br><span class="line">yum install -y ceph-deploy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mkdir /etc/ceph &amp;&amp; cd /etc/ceph</span><br><span class="line">ceph-deploy new ceph1  ceph2 ceph3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">yum install -y python-setuptools</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在配置文件中增加：</span><br><span class="line">osd_pool_default_size = 3</span><br><span class="line">[mgr]</span><br><span class="line">mgr modules = dashboard</span><br><span class="line">[mon]</span><br><span class="line">mon allow pool delete = true</span><br></pre></td></tr></table></figure><h2 id="mon"><a href="#mon" class="headerlink" title="mon"></a>mon</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy install ceph1 ceph2 ceph3</span><br><span class="line"></span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     fcb2fa5e-481a-4494-9a27-374048f37113</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure><h2 id="mgr"><a href="#mgr" class="headerlink" title="mgr"></a>mgr</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mgr create ceph1 ceph2 ceph3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     fcb2fa5e-481a-4494-9a27-374048f37113</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active), standbys: ceph2, ceph3</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs: </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph mgr dump</span><br><span class="line">&#123;</span><br><span class="line">    &quot;epoch&quot;: 4,</span><br><span class="line">    &quot;active_gid&quot;: 4122,</span><br><span class="line">    &quot;active_name&quot;: &quot;ceph1&quot;,</span><br><span class="line">    &quot;active_addr&quot;: &quot;192.168.10.186:6800/22316&quot;,</span><br><span class="line">    &quot;available&quot;: true,</span><br><span class="line">    &quot;standbys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;gid&quot;: 4129,</span><br><span class="line">            &quot;name&quot;: &quot;ceph2&quot;,</span><br><span class="line">            &quot;available_modules&quot;: [</span><br><span class="line">                &quot;balancer&quot;,</span><br><span class="line">                &quot;dashboard&quot;,</span><br><span class="line">                &quot;influx&quot;,</span><br><span class="line">                &quot;localpool&quot;,</span><br><span class="line">                &quot;prometheus&quot;,</span><br><span class="line">                &quot;restful&quot;,</span><br><span class="line">                &quot;selftest&quot;,</span><br><span class="line">                &quot;status&quot;,</span><br><span class="line">                &quot;zabbix&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;gid&quot;: 4132,</span><br><span class="line">            &quot;name&quot;: &quot;ceph3&quot;,</span><br><span class="line">            &quot;available_modules&quot;: [</span><br><span class="line">                &quot;balancer&quot;,</span><br><span class="line">                &quot;dashboard&quot;,</span><br><span class="line">                &quot;influx&quot;,</span><br><span class="line">                &quot;localpool&quot;,</span><br><span class="line">                &quot;prometheus&quot;,</span><br><span class="line">                &quot;restful&quot;,</span><br><span class="line">                &quot;selftest&quot;,</span><br><span class="line">                &quot;status&quot;,</span><br><span class="line">                &quot;zabbix&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;modules&quot;: [</span><br><span class="line">        &quot;balancer&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;status&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;available_modules&quot;: [</span><br><span class="line">        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;influx&quot;,</span><br><span class="line">        &quot;localpool&quot;,</span><br><span class="line">        &quot;prometheus&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;selftest&quot;,</span><br><span class="line">        &quot;status&quot;,</span><br><span class="line">        &quot;zabbix&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;services&quot;: &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">[root@ceph1 ceph]# ceph mgr module enable dashboard</span><br><span class="line">[root@ceph1 ceph]# ceph mgr dump</span><br><span class="line">&#123;</span><br><span class="line">    &quot;epoch&quot;: 7,</span><br><span class="line">    &quot;active_gid&quot;: 4139,</span><br><span class="line">    &quot;active_name&quot;: &quot;ceph1&quot;,</span><br><span class="line">    &quot;active_addr&quot;: &quot;192.168.10.186:6800/22316&quot;,</span><br><span class="line">    &quot;available&quot;: true,</span><br><span class="line">    &quot;standbys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;gid&quot;: 4136,</span><br><span class="line">            &quot;name&quot;: &quot;ceph3&quot;,</span><br><span class="line">            &quot;available_modules&quot;: [</span><br><span class="line">                &quot;balancer&quot;,</span><br><span class="line">                &quot;dashboard&quot;,</span><br><span class="line">                &quot;influx&quot;,</span><br><span class="line">                &quot;localpool&quot;,</span><br><span class="line">                &quot;prometheus&quot;,</span><br><span class="line">                &quot;restful&quot;,</span><br><span class="line">                &quot;selftest&quot;,</span><br><span class="line">                &quot;status&quot;,</span><br><span class="line">                &quot;zabbix&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;gid&quot;: 4141,</span><br><span class="line">            &quot;name&quot;: &quot;ceph2&quot;,</span><br><span class="line">            &quot;available_modules&quot;: [</span><br><span class="line">                &quot;balancer&quot;,</span><br><span class="line">                &quot;dashboard&quot;,</span><br><span class="line">                &quot;influx&quot;,</span><br><span class="line">                &quot;localpool&quot;,</span><br><span class="line">                &quot;prometheus&quot;,</span><br><span class="line">                &quot;restful&quot;,</span><br><span class="line">                &quot;selftest&quot;,</span><br><span class="line">                &quot;status&quot;,</span><br><span class="line">                &quot;zabbix&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;modules&quot;: [</span><br><span class="line">        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;status&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;available_modules&quot;: [</span><br><span class="line">        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;influx&quot;,</span><br><span class="line">        &quot;localpool&quot;,</span><br><span class="line">        &quot;prometheus&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;selftest&quot;,</span><br><span class="line">        &quot;status&quot;,</span><br><span class="line">        &quot;zabbix&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;services&quot;: &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph config-key put mgr/dashboard/server_addr 192.168.6.101</span><br><span class="line">set mgr/dashboard/server_addr</span><br><span class="line">[root@ceph1 ceph]# ceph config-key put mgr/dashboard/server_port 7000</span><br><span class="line">set mgr/dashboard/server_port</span><br><span class="line">[root@ceph1 ~]# netstat -tulnp |grep 7000</span><br><span class="line">tcp        0      0 192.168.6.101:7000      0.0.0.0:*               LISTEN      19836/ceph-mgr</span><br></pre></td></tr></table></figure><p>这时看下danshboard图：</p><p><img src="https://shenshengkun.github.io/images/dash_cluster1.png" alt=""></p><h2 id="osd"><a href="#osd" class="headerlink" title="osd"></a>osd</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">每台机器做逻辑卷</span><br><span class="line">[root@ceph1 ceph]# pvcreate /dev/sdb</span><br><span class="line">  Physical volume &quot;/dev/sdb&quot; successfully created.</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# vgcreate data_vg1 /dev/sdb</span><br><span class="line">  Volume group &quot;data_vg1&quot; successfully created</span><br><span class="line">  </span><br><span class="line">[root@ceph1 ceph]# lvcreate -n data_lv1 -L 99g data_vg1   </span><br><span class="line">  Logical volume &quot;data_lv1&quot; created.</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ceph-deploy osd create ceph1 --data data_vg1/data_lv1</span><br><span class="line">ceph-deploy osd create ceph2 --data data_vg1/data_lv1</span><br><span class="line">ceph-deploy osd create ceph3 --data data_vg1/data_lv1</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     fcb2fa5e-481a-4494-9a27-374048f37113</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active), standbys: ceph3, ceph2</span><br><span class="line">    osd: 3 osds: 3 up, 3 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   3.01GiB used, 294GiB / 297GiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure><p>这时看下danshboard图：</p><p><img src="https://shenshengkun.github.io/images/dash_cluster2.png" alt=""></p><h2 id="rgw集群"><a href="#rgw集群" class="headerlink" title="rgw集群"></a>rgw集群</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy install --rgw ceph1 ceph2 ceph3</span><br><span class="line"></span><br><span class="line">ceph-deploy admin ceph1 ceph2 ceph3</span><br><span class="line"></span><br><span class="line">ceph-deploy rgw create ceph1 ceph2 ceph3</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     fcb2fa5e-481a-4494-9a27-374048f37113</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active), standbys: ceph3, ceph2</span><br><span class="line">    osd: 3 osds: 3 up, 3 in</span><br><span class="line">    rgw: 3 daemons active</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   4 pools, 32 pgs</span><br><span class="line">    objects: 191 objects, 3.08KiB</span><br><span class="line">    usage:   3.01GiB used, 294GiB / 297GiB avail</span><br><span class="line">    pgs:     32 active+clean</span><br></pre></td></tr></table></figure><p>这时看下danshboard图：</p><p><img src="https://shenshengkun.github.io/images/dash_cluster3.png" alt=""></p><h2 id="NGINX代理"><a href="#NGINX代理" class="headerlink" title="NGINX代理"></a>NGINX代理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">安装这里就不介绍了</span><br><span class="line"></span><br><span class="line">[root@ceph1 conf.d]# cat cephcloud.dev.goago.cn.conf </span><br><span class="line">        upstream cephcloud.dev.goago.cn  &#123;</span><br><span class="line">        server  192.168.10.186:7480;</span><br><span class="line">        server  192.168.10.187:7480;</span><br><span class="line">        server  192.168.10.188:7480;        </span><br><span class="line">        &#125;</span><br><span class="line">        server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  cephcloud.dev.goago.cn;</span><br><span class="line">        location / &#123;</span><br><span class="line">                        proxy_intercept_errors on;</span><br><span class="line">                        access_log /var/log/nginx/cephcloud_log;</span><br><span class="line">                        proxy_pass http://cephcloud.dev.goago.cn;</span><br><span class="line">                        proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">                        proxy_set_header Host $host;</span><br><span class="line">                        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">                        proxy_set_header Request_Uri $request_uri;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h2 id="s3和swift"><a href="#s3和swift" class="headerlink" title="s3和swift"></a>s3和swift</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">具体安装这里不叙述了，可以看我上篇文章</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">New settings:</span><br><span class="line">  Access Key: M954JYYAOBES65B7UNEZ</span><br><span class="line">  Secret Key: 11MZu3N9vB4S4C4N8U2Ywgkhxro3Xi6K9HPyRQ9v</span><br><span class="line">  Default Region: US</span><br><span class="line">  S3 Endpoint: cephcloud.dev.goago.cn</span><br><span class="line">  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.cephcloud.dev.goago.cn bucket</span><br><span class="line">  Encryption password: 123456</span><br><span class="line">  Path to GPG program: /usr/bin/gpg</span><br><span class="line">  Use HTTPS protocol: False</span><br><span class="line">  HTTP Proxy server name: </span><br><span class="line">  HTTP Proxy server port: 0</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;集群架构&quot;&gt;&lt;a href=&quot;#集群架构&quot; class=&quot;headerlink&quot; title=&quot;集群架构&quot;&gt;&lt;/a&gt;集群架构&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pr
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>ceph-luminous-bluestore</title>
    <link href="https://shenshengkun.github.io/posts/78fhjj54.html"/>
    <id>https://shenshengkun.github.io/posts/78fhjj54.html</id>
    <published>2019-07-16T09:10:01.000Z</published>
    <updated>2019-07-16T06:04:17.585Z</updated>
    
    <content type="html"><![CDATA[<h1 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ceph后端支持多种存储引擎，以插件化的形式来进行管理使用，目前支持filestore，kvstore，memstore以及bluestore</span><br><span class="line"></span><br><span class="line">1）Firestore存在的问题是：</span><br><span class="line">在写数据前需要先写journal，会有一倍的写放大；</span><br><span class="line">若是另外配备SSD盘给journal使用又增加额外的成本；</span><br><span class="line">filestore一开始只是对于SATA/SAS这一类机械盘进行设计的，没有专门针对SSD这一类的Flash介质盘做考虑。</span><br><span class="line"></span><br><span class="line">2）而Bluestore的优势在于：</span><br><span class="line">减少写放大；</span><br><span class="line">针对FLASH介质盘做优化；</span><br><span class="line">直接管理裸盘，进一步减少文件系统部分的开销。</span><br></pre></td></tr></table></figure><h1 id="Bluestore原理说明"><a href="#Bluestore原理说明" class="headerlink" title="Bluestore原理说明"></a>Bluestore原理说明</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对象可以直接存放在裸盘上，不需要任何文件系统接口。</span><br><span class="line">BlueStore 直接使用一个原始分区，ceph对象将直接写在块设备上，不再需要任何的文件系统；</span><br><span class="line">和osd一起进来的元数据将存储在 一个 名为 RocksDB 的键值对 数据库；</span><br></pre></td></tr></table></figure><h2 id="各层意义"><a href="#各层意义" class="headerlink" title="各层意义"></a>各层意义</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RocksDB ：存储 WAL 日志和元数据（omap）</span><br><span class="line">BlueRocksEnv: 与RocksDB 交互的接口</span><br><span class="line">BlueFS： 一个类似文件系统的 mini C++，使 rocksdb 生效，ENv 接口（存储 RocksDB 日志和 sst 文件）；</span><br><span class="line">因为rocksdb 一般跑在一个文件系统的上层，所以创建了 BlueFS。</span><br></pre></td></tr></table></figure><h2 id="RocksDB-存放的数据类型"><a href="#RocksDB-存放的数据类型" class="headerlink" title="RocksDB 存放的数据类型"></a>RocksDB 存放的数据类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">对象的元数据</span><br><span class="line">write-ahead 日志</span><br><span class="line">ceph omap  数据</span><br><span class="line">allocator metadata(元数据分配器)：决定数据存放位置；此功能可插拔</span><br></pre></td></tr></table></figure><h2 id="默认BlueStore模型"><a href="#默认BlueStore模型" class="headerlink" title="默认BlueStore模型"></a>默认BlueStore模型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个小分区（XFS或者ext4）,包括ceph files </span><br><span class="line">（init system descriptor,status,id,fsid,keyring 等）和RocksDB 文件</span><br><span class="line">第二个分区是一个原始分区</span><br></pre></td></tr></table></figure><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">每一部分都可以存放在不同的磁盘中，RocksDB WAL 和 DB 可以存放在不同的磁盘或者小分区中</span><br></pre></td></tr></table></figure><h1 id="添加osd"><a href="#添加osd" class="headerlink" title="添加osd"></a>添加osd</h1><h2 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h2><p>由于Luminous里默认使用Bluestore，可以直接操作裸盘,data和block-db会使用lv。综合成本及性能，我们把block.db使用ssd的分区，osd仍然使用sas，block.wal不指定. 这里vdb作为osd盘，vdc作为block-db盘</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">首先ssh到各个存储节点，block.db使用的ssd分区,这里node1举例：</span><br><span class="line"># ssh node1</span><br><span class="line"># pvcreate /dev/vdb  # 创建pv, 这里使用的整块磁盘(与后面的分区对比),  pvs 查看pv列表</span><br><span class="line">Physical volume &quot;/dev/vdb&quot; successfully created.</span><br><span class="line"># vgcreate data_vg1 /dev/vdb  # 创建vg,  vgs查看vg列表</span><br><span class="line">Volume group &quot;data_vg1&quot; successfully created</span><br><span class="line"># lvcreate -n data_lv1 -L 1020.00m data_vg1 #创建lv,lvs查看lv列表, -n指定lv名称, -L指定lv的大小,需要小于或者等于vg的VSize</span><br><span class="line">Logical volume &quot;data_lv1&quot; created.</span><br><span class="line"></span><br><span class="line">---------------------------------------------</span><br><span class="line">生产环境一块ssd磁盘会对应多块osd，所以这里也需要把ssd多个分区</span><br><span class="line"># parted /dev/vdc </span><br><span class="line">(parted) mklabel gpt                                                      </span><br><span class="line">(parted) mkpart primary 0% 25%   #因为测试，这里只做了一个占据磁盘25%容量的分区，实际情况根据osd数目划分相应的分区数</span><br><span class="line">(parted) quit</span><br><span class="line"># pvcreate /dev/vdc1  # 创建pv, 这里使用的是磁盘分区, pvs 查看pv列表</span><br><span class="line">Physical volume &quot;/dev/vdc1&quot; successfully created.</span><br><span class="line"># vgcreate block_db_vg1 /dev/vdc1  # 创建vg,  vgs查看vg列表</span><br><span class="line">Volume group &quot;block_db_vg1&quot; successfully created</span><br><span class="line"># lvcreate -n block_db_lv1 -L 1020.00m block_db_vg1  # 创建lv, lvs查看lv列表, -L指定lv的大小，需要小于或者等于 vg的VSize</span><br><span class="line">Logical volume &quot;block_db_lv1&quot; created.</span><br><span class="line"></span><br><span class="line">---------------------------------------------</span><br><span class="line"># 不需要加--bluestore 参数，默认就是使用bluestore方式，data_vg1/data_lv1 是数据盘，block_db_vg1/block_db_lv1是block-db</span><br><span class="line">管理节点执行：</span><br><span class="line">ceph-deploy --overwrite-conf osd create node1 --data data_vg1/data_lv1 --block-db block_db_vg1/block_db_lv1</span><br><span class="line">ceph-deploy --overwrite-conf osd create node2 --data data_vg1/data_lv1 --block-db block_db_vg1/block_db_lv1</span><br></pre></td></tr></table></figure><h2 id="案例2"><a href="#案例2" class="headerlink" title="案例2"></a>案例2</h2><p>创建具有3个逻辑卷的OSD（模拟不同类型的存储介质） </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#pvcreate /dev/sdb</span><br><span class="line">  Physical volume &quot;/dev/sdb&quot; successfully created.</span><br><span class="line">#vgcreate  ceph-pool /dev/sdb</span><br><span class="line">  Volume group &quot;ceph-pool&quot; successfully created</span><br><span class="line">#lvcreate -n osd0.wal -L 1G ceph-pool</span><br><span class="line">  Logical volume &quot;osd0.wal&quot; created.</span><br><span class="line"># lvcreate -n osd0.db -L 1G ceph-pool</span><br><span class="line">  Logical volume &quot;osd0.db&quot; created.</span><br><span class="line"># lvcreate -n osd0 -l 100%FREE ceph-pool</span><br><span class="line">  Logical volume &quot;osd0&quot; created.</span><br></pre></td></tr></table></figure><p>完成逻辑卷的创建后我们就可以创建 OSD 了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy osd create \</span><br><span class="line">    --data ceph-pool/osd0 \</span><br><span class="line">    --block-db ceph-pool/osd0.db \</span><br><span class="line">    --block-wal ceph-pool/osd0.wal \</span><br><span class="line">    --bluestore node1</span><br></pre></td></tr></table></figure><h2 id="wal-amp-db-的大小问题"><a href="#wal-amp-db-的大小问题" class="headerlink" title="wal&amp; db 的大小问题"></a>wal&amp; db 的大小问题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在 ceph bluestore 的情况下，wal 是 RocksDB 的write-ahead log, 相当于之前的 journal 数据，db 是 RocksDB 的metadata 信息。在磁盘选择原则是 block.wal &gt; block.db &gt; block。当然所有的数据也可以放到同一块盘上。</span><br><span class="line">默认情况下， wal 和 db 的大小分别是 512 MB 和 1GB，现在没有一个好的理论值，好像和 ceph 本身承载的数据类型有关系。</span><br><span class="line">值得注意的是，如果所有的数据都在单块盘上，那是没有必要指定 wal &amp;db 的大小的。如果 wal &amp; db 是在不同的盘上，由于 wal/db 一般都会分的比较小，是有满的可能性的。如果满了，这些数据会迁移到下一个快的盘上(wal - db - main)。所以最少不会因为数据满了，而造成无法写入。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;对比&quot;&gt;&lt;a href=&quot;#对比&quot; class=&quot;headerlink&quot; title=&quot;对比&quot;&gt;&lt;/a&gt;对比&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span 
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>ceph的pg算法</title>
    <link href="https://shenshengkun.github.io/posts/fd22jkk6.html"/>
    <id>https://shenshengkun.github.io/posts/fd22jkk6.html</id>
    <published>2019-07-11T09:10:01.000Z</published>
    <updated>2019-07-12T09:49:15.127Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PG介绍"><a href="#PG介绍" class="headerlink" title="PG介绍"></a>PG介绍</h1><p>PG, Placement Groups。CRUSH先将数据分解成一组对象，然后根据对象名称、复制级别和系统中的PG数等信息执行散列操作，再将结果生成PG ID。可以将PG看做一个逻辑容器，这个容器包含多个对象，同时这个逻辑对象映射之多个OSD上。<br>如果没有PG，在成千上万个OSD上管理和跟踪数百万计的对象的复制和传播是相当困难的。没有PG这一层，管理海量的对象所消耗的计算资源也是不可想象的。建议每个OSD上配置50~100个PG。</p><h1 id="计算PG数"><a href="#计算PG数" class="headerlink" title="计算PG数"></a>计算PG数</h1><p>官方推荐如下：</p><p><img src="https://shenshengkun.github.io/images/pg1.png" alt=""></p><p>Ceph集群中的PG总数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PG总数 = (OSD总数 * 100) / 最大副本数</span><br></pre></td></tr></table></figure><p>结果必须舍入到最接近的2的N次方幂的值。</p><p>Ceph集群中每个pool中的PG总数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">存储池PG总数 = (OSD总数 * 100 / 最大副本数) / 池数</span><br></pre></td></tr></table></figure><p>平衡每个存储池中的PG数和每个OSD中的PG数对于降低OSD的方差、避免速度缓慢的恢复再平衡进程是相当重要的。</p><h1 id="修改PG和PGP"><a href="#修改PG和PGP" class="headerlink" title="修改PG和PGP"></a>修改PG和PGP</h1><p>PGP是为了实现定位而设置的PG，它的值应该和PG的总数(即pg_num)保持一致。对于Ceph的一个pool而言，如果增加pg_num，还应该调整pgp_num为同样的值，这样集群才可以开始再平衡。<br>参数pg_num定义了PG的数量，PG映射至OSD。当任意pool的PG数增加时，PG依然保持和源OSD的映射。直至目前，Ceph还未开始再平衡。此时，增加pgp_num的值，PG才开始从源OSD迁移至其他的OSD，正式开始再平衡。PGP，Placement Groups of Placement。</p><p>获取现有的PG数和PGP数值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool get data pg_num</span><br><span class="line"></span><br><span class="line">ceph osd pool get data pgp_num</span><br></pre></td></tr></table></figure><p>检查存储池的副本数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd dump|grep -i size</span><br></pre></td></tr></table></figure><p>计算pg_num和pgp_num</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># pg_num calculation</span><br><span class="line">pg_num = (num_osds * 100) / num_copies</span><br><span class="line">num_up = pow(2, int(log(pg_num,2) + 0.5))</span><br><span class="line">num_down = pow(2, int(log(pg_num,2)))</span><br><span class="line">if abs(pg_num - num_up) &lt;= abs(pg_num - num_down):</span><br><span class="line">    pg_num = num_up</span><br><span class="line">else:</span><br><span class="line">    pg_num = num_down</span><br><span class="line">pgp_num = pg_num</span><br></pre></td></tr></table></figure><p>修改存储池的PG和PGP</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set data pg_num </span><br><span class="line"></span><br><span class="line">ceph osd pool set data pgp_num</span><br></pre></td></tr></table></figure><p>例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool ls</span><br><span class="line">ceph osd pool set .rgw.root pg_num 16</span><br><span class="line">ceph osd pool set .rgw.root pgp_num 16</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;PG介绍&quot;&gt;&lt;a href=&quot;#PG介绍&quot; class=&quot;headerlink&quot; title=&quot;PG介绍&quot;&gt;&lt;/a&gt;PG介绍&lt;/h1&gt;&lt;p&gt;PG, Placement Groups。CRUSH先将数据分解成一组对象，然后根据对象名称、复制级别和系统中的PG数等信息
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>ceph对象存储</title>
    <link href="https://shenshengkun.github.io/posts/dld987c6.html"/>
    <id>https://shenshengkun.github.io/posts/dld987c6.html</id>
    <published>2019-07-11T04:10:01.000Z</published>
    <updated>2019-07-12T08:50:59.533Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ceph-RGW简介"><a href="#Ceph-RGW简介" class="headerlink" title="Ceph RGW简介"></a>Ceph RGW简介</h1><p>Ceph RGW(即RADOS Gateway)是Ceph对象存储网关服务，是基于LIBRADOS接口封装实现的FastCGI服务，对外提供存储和管理对象数据的Restful API。 对象存储适用于图片、视频等各类文件的上传下载，可以设置相应的访问权限。目前Ceph RGW兼容常见的对象存储API，例如兼容绝大部分Amazon S3 API，兼容OpenStack Swift API。</p><h1 id="部署-RGW-服务"><a href="#部署-RGW-服务" class="headerlink" title="部署 RGW 服务"></a>部署 RGW 服务</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph-deploy install --rgw ceph1</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy install --rgw ceph1</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  testing                       : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa3faca5e60&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  dev_commit                    : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_mds                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  stable                        : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  adjust_repos                  : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function install at 0x7fa3fbb955f0&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_mgr                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_all                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  repo                          : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  host                          : [&apos;ceph1&apos;]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_rgw                   : True</span><br></pre></td></tr></table></figure><p>将配置文件、密钥文件同步到 ceph1：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph-deploy admin ceph1</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy admin ceph1</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe0e152d3b0&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  client                        : [&apos;ceph1&apos;]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function admin at 0x7fe0e1dc0230&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph1</span><br><span class="line">[ceph1][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph1][DEBUG ] detect machine type</span><br><span class="line">[ceph1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br></pre></td></tr></table></figure><p>启动一个RGW服务 ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">先将ceph.conf加一个参数配置</span><br><span class="line">[root@ceph1 ceph]# vim ceph.conf </span><br><span class="line">[global]</span><br><span class="line">fsid = cde3244e-89e0-4630-84d5-bf08c0e33b24</span><br><span class="line">mon_initial_members = ceph1</span><br><span class="line">mon_host = 192.168.6.101</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br><span class="line">osd_pool_default_size = 2</span><br><span class="line">[mgr]</span><br><span class="line">mgr modules = dashboard</span><br><span class="line">[mon]</span><br><span class="line">mon allow pool delete = true   ####有这个配置，生成的pool才可以被删除</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]#  ceph-deploy rgw create ceph1</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy rgw create ceph1</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  rgw                           : [(&apos;ceph1&apos;, &apos;rgw.ceph1&apos;)]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : create</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fda85404ab8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function rgw at 0x7fda85a53050&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts ceph1:rgw.ceph1</span><br><span class="line">[ceph1][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph1][DEBUG ] detect machine type</span><br><span class="line">[ceph_deploy.rgw][INFO  ] Distro info: CentOS Linux 7.6.1810 Core</span><br><span class="line">[ceph_deploy.rgw][DEBUG ] remote host will use systemd</span><br><span class="line">[ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to ceph1</span><br><span class="line">[ceph1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph1][DEBUG ] create path recursively if it doesn&apos;t exist</span><br><span class="line">[ceph1][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create client.rgw.ceph1 osd allow rwx mon allow rw -o /var/lib/ceph/radosgw/ceph-rgw.ceph1/keyring</span><br><span class="line">[ceph1][INFO  ] Running command: systemctl enable ceph-radosgw@rgw.ceph1</span><br><span class="line">[ceph1][INFO  ] Running command: systemctl start ceph-radosgw@rgw.ceph1</span><br><span class="line">[ceph1][INFO  ] Running command: systemctl enable ceph.target</span><br><span class="line">[ceph_deploy.rgw][INFO  ] The Ceph Object Gateway (RGW) is now running on host ceph1 and default port 7480</span><br><span class="line"></span><br><span class="line">验证：</span><br><span class="line">[root@ceph1 ceph]# systemctl status ceph-radosgw@rgw.ceph1</span><br><span class="line">● ceph-radosgw@rgw.ceph1.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Thu 2019-07-11 15:03:24 CST; 9s ago</span><br><span class="line"> Main PID: 21057 (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@rgw.ceph1.service</span><br><span class="line">           └─21057 /usr/bin/radosgw -f --cluster ceph --name client.rgw.ceph1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Jul 11 15:03:24 ceph1 systemd[1]: Started Ceph rados gateway.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     cde3244e-89e0-4630-84d5-bf08c0e33b24</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph1</span><br><span class="line">    mgr: ceph1(active)</span><br><span class="line">    osd: 2 osds: 2 up, 2 in</span><br><span class="line">    rgw: 1 daemon active</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   4 pools, 32 pgs</span><br><span class="line">    objects: 187 objects, 1.09KiB</span><br><span class="line">    usage:   2.01GiB used, 30.0GiB / 32.0GiB avail</span><br><span class="line">    pgs:     32 active+clean</span><br></pre></td></tr></table></figure><p>这时看下danshboard图：</p><p><img src="https://shenshengkun.github.io/images/ceph_dash2.png" alt=""></p><h1 id="使用亚马逊-s3-客户端进行访问"><a href="#使用亚马逊-s3-客户端进行访问" class="headerlink" title="使用亚马逊 s3 客户端进行访问"></a>使用亚马逊 s3 客户端进行访问</h1><h2 id="用户"><a href="#用户" class="headerlink" title="用户"></a>用户</h2><p>创建用户 ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# radosgw-admin user create --uid=&quot;radosgw&quot; --display-name=&quot;First User&quot;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;user_id&quot;: &quot;radosgw&quot;,</span><br><span class="line">    &quot;display_name&quot;: &quot;First User&quot;,</span><br><span class="line">    &quot;email&quot;: &quot;&quot;,</span><br><span class="line">    &quot;suspended&quot;: 0,</span><br><span class="line">    &quot;max_buckets&quot;: 1000,</span><br><span class="line">    &quot;auid&quot;: 0,</span><br><span class="line">    &quot;subusers&quot;: [],</span><br><span class="line">    &quot;keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;swift_keys&quot;: [],</span><br><span class="line">    &quot;caps&quot;: [],</span><br><span class="line">    &quot;op_mask&quot;: &quot;read, write, delete&quot;,</span><br><span class="line">    &quot;default_placement&quot;: &quot;&quot;,</span><br><span class="line">    &quot;placement_tags&quot;: [],</span><br><span class="line">    &quot;bucket_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;user_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;temp_url_keys&quot;: [],</span><br><span class="line">    &quot;type&quot;: &quot;rgw&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个是后续需要的账户信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br></pre></td></tr></table></figure><p>授权用户，允许 radosgw 读写 users 信息： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@ceph1 ceph]# radosgw-admin caps add --uid=radosgw --caps=&quot;users=*&quot;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;user_id&quot;: &quot;radosgw&quot;,</span><br><span class="line">    &quot;display_name&quot;: &quot;First User&quot;,</span><br><span class="line">    &quot;email&quot;: &quot;&quot;,</span><br><span class="line">    &quot;suspended&quot;: 0,</span><br><span class="line">    &quot;max_buckets&quot;: 1000,</span><br><span class="line">    &quot;auid&quot;: 0,</span><br><span class="line">    &quot;subusers&quot;: [],</span><br><span class="line">    &quot;keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;swift_keys&quot;: [],</span><br><span class="line">    &quot;caps&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;users&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;op_mask&quot;: &quot;read, write, delete&quot;,</span><br><span class="line">    &quot;default_placement&quot;: &quot;&quot;,</span><br><span class="line">    &quot;placement_tags&quot;: [],</span><br><span class="line">    &quot;bucket_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;user_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;temp_url_keys&quot;: [],</span><br><span class="line">    &quot;type&quot;: &quot;rgw&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>允许 radosgw 读写所有的usage信息： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]#  radosgw-admin caps add --uid=radosgw --caps=&quot;usage=read,write&quot;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;user_id&quot;: &quot;radosgw&quot;,</span><br><span class="line">    &quot;display_name&quot;: &quot;First User&quot;,</span><br><span class="line">    &quot;email&quot;: &quot;&quot;,</span><br><span class="line">    &quot;suspended&quot;: 0,</span><br><span class="line">    &quot;max_buckets&quot;: 1000,</span><br><span class="line">    &quot;auid&quot;: 0,</span><br><span class="line">    &quot;subusers&quot;: [],</span><br><span class="line">    &quot;keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;swift_keys&quot;: [],</span><br><span class="line">    &quot;caps&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;usage&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;users&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;op_mask&quot;: &quot;read, write, delete&quot;,</span><br><span class="line">    &quot;default_placement&quot;: &quot;&quot;,</span><br><span class="line">    &quot;placement_tags&quot;: [],</span><br><span class="line">    &quot;bucket_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;user_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;temp_url_keys&quot;: [],</span><br><span class="line">    &quot;type&quot;: &quot;rgw&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>创建子用户，做为后面 swift 客户端访问时使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]#  radosgw-admin subuser create --uid=radosgw --subuser=radosgw:swift --access=full</span><br><span class="line">&#123;</span><br><span class="line">    &quot;user_id&quot;: &quot;radosgw&quot;,</span><br><span class="line">    &quot;display_name&quot;: &quot;First User&quot;,</span><br><span class="line">    &quot;email&quot;: &quot;&quot;,</span><br><span class="line">    &quot;suspended&quot;: 0,</span><br><span class="line">    &quot;max_buckets&quot;: 1000,</span><br><span class="line">    &quot;auid&quot;: 0,</span><br><span class="line">    &quot;subusers&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;id&quot;: &quot;radosgw:swift&quot;,</span><br><span class="line">            &quot;permissions&quot;: &quot;full-control&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;swift_keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw:swift&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;A3GDj2yjkGJahkCM6YJS4QKQlGz2zd65GXvCkiwV&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;caps&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;usage&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;users&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;op_mask&quot;: &quot;read, write, delete&quot;,</span><br><span class="line">    &quot;default_placement&quot;: &quot;&quot;,</span><br><span class="line">    &quot;placement_tags&quot;: [],</span><br><span class="line">    &quot;bucket_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;user_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;temp_url_keys&quot;: [],</span><br><span class="line">    &quot;type&quot;: &quot;rgw&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>创建密钥 ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# radosgw-admin key create --subuser=radosgw:swift --key-type=swift --gen-secret</span><br><span class="line">&#123;</span><br><span class="line">    &quot;user_id&quot;: &quot;radosgw&quot;,</span><br><span class="line">    &quot;display_name&quot;: &quot;First User&quot;,</span><br><span class="line">    &quot;email&quot;: &quot;&quot;,</span><br><span class="line">    &quot;suspended&quot;: 0,</span><br><span class="line">    &quot;max_buckets&quot;: 1000,</span><br><span class="line">    &quot;auid&quot;: 0,</span><br><span class="line">    &quot;subusers&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;id&quot;: &quot;radosgw:swift&quot;,</span><br><span class="line">            &quot;permissions&quot;: &quot;full-control&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;swift_keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw:swift&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;caps&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;usage&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;users&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;op_mask&quot;: &quot;read, write, delete&quot;,</span><br><span class="line">    &quot;default_placement&quot;: &quot;&quot;,</span><br><span class="line">    &quot;placement_tags&quot;: [],</span><br><span class="line">    &quot;bucket_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;user_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;temp_url_keys&quot;: [],</span><br><span class="line">    &quot;type&quot;: &quot;rgw&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="安装-s3-客户端软件"><a href="#安装-s3-客户端软件" class="headerlink" title="安装 s3 客户端软件"></a>安装 s3 客户端软件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# yum -y install s3cmd.noarch</span><br></pre></td></tr></table></figure><h2 id="对-s3-进行配置"><a href="#对-s3-进行配置" class="headerlink" title="对 s3 进行配置"></a>对 s3 进行配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# s3cmd --configure</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">New settings:</span><br><span class="line">  Access Key: CQE7E6ZDVA74KVJ0077A</span><br><span class="line">  Secret Key: wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8</span><br><span class="line">  Default Region: US</span><br><span class="line">  S3 Endpoint: 192.168.6.101:7480</span><br><span class="line">  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.192.168.6.101:7480 bucket</span><br><span class="line">  Encryption password: 123456</span><br><span class="line">  Path to GPG program: /usr/bin/gpg</span><br><span class="line">  Use HTTPS protocol: False</span><br><span class="line">  HTTP Proxy server name: </span><br><span class="line">  HTTP Proxy server port: 0</span><br><span class="line"></span><br><span class="line">Test access with supplied credentials? [Y/n] y</span><br><span class="line">Please wait, attempting to list all buckets...</span><br><span class="line">Success. Your access key and secret key worked fine :-)</span><br><span class="line"></span><br><span class="line">Now verifying that encryption works...</span><br><span class="line">Success. Encryption and decryption worked fine :-)</span><br><span class="line"></span><br><span class="line">Save settings? [y/N] y</span><br><span class="line">Configuration saved to &apos;/root/.s3cfg&apos;</span><br></pre></td></tr></table></figure><p>格式是这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Default Region [US]:                        #这里一定不要修改，否则后面会报错</span><br><span class="line">S3 Endpoint [s3.amazonaws.com]: 192.168.6.101:7480</span><br><span class="line">DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket)s.192.168.20.148:7480 bucket     #相当于百度网盘的创建文件夹，这里是固定格式</span><br><span class="line">Path to GPG program [/usr/bin/gpg]:                 #保持默认</span><br><span class="line">Use HTTPS protocol [Yes]: no                    #这里写 no ，因为没有提供 https 端口</span><br><span class="line">HTTP Proxy server name:                       #这里不用写，因为没有代理</span><br><span class="line">Test access with supplied credentials? [Y/n] y</span><br><span class="line">Save settings? [y/N] y</span><br></pre></td></tr></table></figure><p>由于我没把端口改成80，所以需要带端口访问的，后续可以nginx代理</p><h2 id="创建存储数据的-bucket"><a href="#创建存储数据的-bucket" class="headerlink" title="创建存储数据的 bucket"></a>创建存储数据的 bucket</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# s3cmd mb s3://cephdir</span><br><span class="line">[root@ceph1 ~]# s3cmd put /etc/hosts s3://ceph_dir</span><br><span class="line">upload: &apos;/etc/hosts&apos; -&gt; &apos;s3://ceph_dir/hosts&apos;  [1 of 1]</span><br><span class="line"> 200 of 200   100% in    1s   133.14 B/s  done</span><br><span class="line"> </span><br><span class="line"> [root@ceph1 ~]# s3cmd ls s3://ceph_dir</span><br><span class="line">2019-07-11 08:41       200   s3://ceph_dir/hosts</span><br></pre></td></tr></table></figure><p> s3 的测试脚本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-f ~]# yum -y install python-boto</span><br><span class="line">[root@ceph-f ~]# vim s3test.py</span><br><span class="line">import boto.s3.connection</span><br><span class="line">access_key = &apos;N6ALEK0KS0ISYCIM5JBG&apos;</span><br><span class="line">secret_key = &apos;qK9hrpX2uwna4elPP1VsuErmAHBw3So40fE2K4yM&apos;</span><br><span class="line">conn = boto.connect_s3(</span><br><span class="line">         aws_access_key_id=access_key,         </span><br><span class="line">         aws_secret_access_key=secret_key,         </span><br><span class="line">         host=&apos;ceph1&apos;, port=7480,         </span><br><span class="line">         is_secure=False, calling_format=boto.s3.connection.OrdinaryCallingFormat(),        </span><br><span class="line">         )</span><br><span class="line">bucket = conn.create_bucket(&apos;xxx_yyy&apos;)</span><br><span class="line">for bucket in conn.get_all_buckets():</span><br><span class="line">     print &quot;&#123;name&#125; &#123;created&#125;&quot;.format(         </span><br><span class="line">     name=bucket.name,         </span><br><span class="line">     created=bucket.creation_date,     </span><br><span class="line">     )</span><br></pre></td></tr></table></figure><p>在使用时，请替换自己的 access_key、secret_key、主机名和端口</p><h1 id="swift-接口测试"><a href="#swift-接口测试" class="headerlink" title="swift 接口测试"></a>swift 接口测试</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install python-setuptools</span><br><span class="line">easy_install pip</span><br><span class="line">pip install --upgrade setuptools</span><br><span class="line">pip install --upgrade python-swiftclient</span><br></pre></td></tr></table></figure><p>命令行访问 ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# swift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh list</span><br><span class="line">ceph_dir</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">swift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh post sy-yt</span><br><span class="line"></span><br><span class="line">swift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh post sy_yt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ~]# swift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh list</span><br><span class="line">ceph_dir</span><br><span class="line">sy-yt</span><br><span class="line">sy_yt</span><br></pre></td></tr></table></figure><p>这里提供 swift 的测试脚本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-f ~]# vim swift.py</span><br><span class="line">import swiftclient</span><br><span class="line">user = &apos;radosgw:swift&apos;</span><br><span class="line">key = &apos;CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh&apos;</span><br><span class="line"></span><br><span class="line">conn = swiftclient.Connection(</span><br><span class="line">         user=user,         </span><br><span class="line">         key=key,         </span><br><span class="line">         authurl=&apos;http://192.168.6.101:7480/auth/v1.0&apos;,</span><br><span class="line">         )</span><br><span class="line">for container in conn.get_account()[1]:</span><br><span class="line">         print container[&apos;name&apos;]</span><br></pre></td></tr></table></figure><p>在使用时，请替换自己的 access_key、secret_key、authurl</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Ceph-RGW简介&quot;&gt;&lt;a href=&quot;#Ceph-RGW简介&quot; class=&quot;headerlink&quot; title=&quot;Ceph RGW简介&quot;&gt;&lt;/a&gt;Ceph RGW简介&lt;/h1&gt;&lt;p&gt;Ceph RGW(即RADOS Gateway)是Ceph对象存储网关服务，
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>ceph简单搭建</title>
    <link href="https://shenshengkun.github.io/posts/ldfl554c.html"/>
    <id>https://shenshengkun.github.io/posts/ldfl554c.html</id>
    <published>2019-07-11T03:10:01.000Z</published>
    <updated>2019-07-17T03:00:05.048Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ceph介绍"><a href="#ceph介绍" class="headerlink" title="ceph介绍"></a>ceph介绍</h1><h2 id="Ceph基础介绍"><a href="#Ceph基础介绍" class="headerlink" title="Ceph基础介绍"></a>Ceph基础介绍</h2><p>​    Ceph是一个可靠地、自动重均衡、自动恢复的分布式存储系统，根据场景划分可以将Ceph分为三大块，分别是对象存储、块设备存储和文件系统服务。在虚拟化领域里，比较常用到的是Ceph的块设备存储，比如在OpenStack项目里，Ceph的块设备存储可以对接OpenStack的cinder后端存储、Glance的镜像存储和虚拟机的数据存储，比较直观的是Ceph集群可以提供一个raw格式的块存储来作为虚拟机实例的硬盘。</p><p>​    Ceph相比其它存储的优势点在于它不单单是存储，同时还充分利用了存储节点上的计算能力，在存储每一个数据时，都会通过计算得出该数据存储的位置，尽量将数据分布均衡，同时由于Ceph的良好设计，采用了CRUSH算法、HASH环等方法，使得它不存在传统的单点故障的问题，且随着规模的扩大性能并不会受到影响。</p><h2 id="Ceph的核心组件"><a href="#Ceph的核心组件" class="headerlink" title="Ceph的核心组件"></a>Ceph的核心组件</h2><p>​    Ceph的核心组件包括Ceph OSD、Ceph Monitor和Ceph MDS。</p><p>  Ceph OSD：OSD的英文全称是Object Storage Device，它的主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其它OSD间进行心跳检查等，并将一些变化情况上报给Ceph Monitor。一般情况下一块硬盘对应一个OSD，由OSD来对硬盘存储进行管理，当然一个分区也可以成为一个OSD。</p><p>  Ceph OSD的架构实现由物理磁盘驱动器、Linux文件系统和Ceph OSD服务组成，对于Ceph OSD Deamon而言，Linux文件系统显性的支持了其拓展性，一般Linux文件系统有好几种，比如有BTRFS、XFS、Ext4等，BTRFS虽然有很多优点特性，但现在还没达到生产环境所需的稳定性，一般比较推荐使用XFS。</p><p>  伴随OSD的还有一个概念叫做Journal盘，一般写数据到Ceph集群时，都是先将数据写入到Journal盘中，然后每隔一段时间比如5秒再将Journal盘中的数据刷新到文件系统中。一般为了使读写时延更小，Journal盘都是采用SSD，一般分配10G以上，当然分配多点那是更好，Ceph中引入Journal盘的概念是因为Journal允许Ceph OSD功能很快做小的写操作；一个随机写入首先写入在上一个连续类型的journal，然后刷新到文件系统，这给了文件系统足够的时间来合并写入磁盘，一般情况下使用SSD作为OSD的journal可以有效缓冲突发负载。</p><p>  Ceph Monitor：由该英文名字我们可以知道它是一个监视器，负责监视Ceph集群，维护Ceph集群的健康状态，同时维护着Ceph集群中的各种Map图，比如OSD Map、Monitor Map、PG Map和CRUSH Map，这些Map统称为Cluster Map，Cluster Map是RADOS的关键数据结构，管理集群中的所有成员、关系、属性等信息以及数据的分发，比如当用户需要存储数据到Ceph集群时，OSD需要先通过Monitor获取最新的Map图，然后根据Map图和object id等计算出数据最终存储的位置。</p><p>  Ceph MDS：全称是Ceph MetaData Server，主要保存的文件系统服务的元数据，但对象存储和块存储设备是不需要使用该服务的。</p><p>  查看各种Map的信息可以通过如下命令：ceph osd(mon、pg) dump</p><h1 id="ceph-deploy安装ceph"><a href="#ceph-deploy安装ceph" class="headerlink" title="ceph-deploy安装ceph"></a>ceph-deploy安装ceph</h1><h2 id="基本环境"><a href="#基本环境" class="headerlink" title="基本环境"></a>基本环境</h2><table><thead><tr><th><strong>192.168.6.101</strong></th><th><strong>ceph1</strong></th></tr></thead><tbody><tr><td><strong>192.168.6.102</strong></td><td><strong>ceph2</strong></td></tr></tbody></table><p>俩台机器都挂俩块盘，一块系统盘，一块osd</p><p>配hosts：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# vim /etc/hosts</span><br><span class="line">192.168.6.101   ceph1</span><br><span class="line">192.168.6.102   ceph2</span><br></pre></td></tr></table></figure><p>时间同步：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate ntp1.aliyun.com</span><br></pre></td></tr></table></figure><p>允许无密码SSH登录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在ceph1上执行</span><br><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id ceph1</span><br><span class="line">ssh-copy-id ceph2</span><br></pre></td></tr></table></figure><p>配置主机名：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname ceph1</span><br><span class="line">hostnamectl set-hostname ceph2</span><br></pre></td></tr></table></figure><h2 id="配置ceph-repo"><a href="#配置ceph-repo" class="headerlink" title="配置ceph.repo"></a>配置ceph.repo</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# vim /etc/yum.repos.d/ceph.repo</span><br><span class="line">[ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">priority=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">priority=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br></pre></td></tr></table></figure><p> 安装ceph-deploy：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br><span class="line">yum makecache</span><br><span class="line">yum update -y</span><br><span class="line">yum install -y ceph-deploy</span><br></pre></td></tr></table></figure><h1 id="创建一个-Ceph-存储集群，它有一个-Monitor-和两个-OSD-守护进程"><a href="#创建一个-Ceph-存储集群，它有一个-Monitor-和两个-OSD-守护进程" class="headerlink" title="创建一个 Ceph 存储集群，它有一个 Monitor 和两个 OSD 守护进程"></a>创建一个 Ceph 存储集群，它有一个 Monitor 和两个 OSD 守护进程</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/ceph &amp;&amp; cd /etc/ceph</span><br><span class="line">ceph-deploy new ceph1     ###配置</span><br><span class="line"></span><br><span class="line">一般会遇到个报错：</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/bin/ceph-deploy&quot;, line 18, in &lt;module&gt;</span><br><span class="line">    from ceph_deploy.cli import main</span><br><span class="line">  File &quot;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    import pkg_resources</span><br><span class="line">ImportError: No module named pkg_resources</span><br><span class="line"></span><br><span class="line">解决：</span><br><span class="line">yum install -y python-setuptools</span><br></pre></td></tr></table></figure><p>在ceph.conf中追加以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 存储集群副本个数</span><br><span class="line"></span><br><span class="line">osd_pool_default_size = 2</span><br></pre></td></tr></table></figure><p>管理节点和osd节点都需要安装ceph 集群：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy install ceph1 ceph2</span><br></pre></td></tr></table></figure><p>配置MON初始化:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure><p>查看ceph集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     cde3244e-89e0-4630-84d5-bf08c0e33b24</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph1</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure><h2 id="开启监控模块"><a href="#开启监控模块" class="headerlink" title="开启监控模块"></a>开启监控模块</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mgr create ceph1</span><br></pre></td></tr></table></figure><p>在<code>/etc/ceph/ceph.conf</code>中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mgr]</span><br><span class="line">mgr modules = dashboard</span><br></pre></td></tr></table></figure><p>查看集群支持的模块</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph mgr dump   </span><br><span class="line">[root@ceph1 ceph]# ceph mgr module enable dashboard   #启用dashboard模块</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph mgr dump</span><br><span class="line">&#123;</span><br><span class="line">    &quot;epoch&quot;: 3,</span><br><span class="line">    &quot;active_gid&quot;: 4110,</span><br><span class="line">    &quot;active_name&quot;: &quot;ceph1&quot;,</span><br><span class="line">    &quot;active_addr&quot;: &quot;192.168.6.101:6800/6619&quot;,</span><br><span class="line">    &quot;available&quot;: true,</span><br><span class="line">    &quot;standbys&quot;: [],</span><br><span class="line">    &quot;modules&quot;: [        &quot;balancer&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;status&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;available_modules&quot;: [        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;influx&quot;,</span><br><span class="line">        &quot;localpool&quot;,</span><br><span class="line">        &quot;prometheus&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;selftest&quot;,</span><br><span class="line">        &quot;status&quot;,</span><br><span class="line">        &quot;zabbix&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;services&quot;: &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">[root@ceph1 ceph]# ceph mgr module enable dashboard</span><br><span class="line">[root@ceph1 ceph]# ceph mgr dump</span><br><span class="line">&#123;</span><br><span class="line">    &quot;epoch&quot;: 6,</span><br><span class="line">    &quot;active_gid&quot;: 4114,</span><br><span class="line">    &quot;active_name&quot;: &quot;ceph1&quot;,</span><br><span class="line">    &quot;active_addr&quot;: &quot;192.168.6.101:6800/6619&quot;,</span><br><span class="line">    &quot;available&quot;: true,</span><br><span class="line">    &quot;standbys&quot;: [],</span><br><span class="line">    &quot;modules&quot;: [        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;status&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;available_modules&quot;: [        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;influx&quot;,</span><br><span class="line">        &quot;localpool&quot;,</span><br><span class="line">        &quot;prometheus&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;selftest&quot;,</span><br><span class="line">        &quot;status&quot;,</span><br><span class="line">        &quot;zabbix&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;services&quot;: &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     cde3244e-89e0-4630-84d5-bf08c0e33b24</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph1</span><br><span class="line">    mgr: ceph1(active)</span><br><span class="line">    osd: 2 osds: 2 up, 2 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   2.00GiB used, 30.0GiB / 32.0GiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure><p>设置<code>dashboard</code>的<code>ip</code>和端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-node1 ceph]# ceph config-key put mgr/dashboard/server_addr 192.168.6.101</span><br><span class="line">set mgr/dashboard/server_addr</span><br><span class="line">[root@ceph-node1 ceph]# ceph config-key put mgr/dashboard/server_port 7000</span><br><span class="line">set mgr/dashboard/server_port</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# netstat -tulnp |grep 7000</span><br><span class="line">tcp        0      0 192.168.6.101:7000      0.0.0.0:*               LISTEN      19836/ceph-mgr</span><br></pre></td></tr></table></figure><h2 id="创建osd"><a href="#创建osd" class="headerlink" title="创建osd"></a>创建osd</h2><p>删除磁盘数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph-deploy disk zap ceph1 /dev/sdb</span><br><span class="line">[root@ceph1 ceph]# ceph-deploy disk zap ceph2 /dev/sdb</span><br></pre></td></tr></table></figure><p>创建osd(一共俩个)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph-deploy osd create ceph1 --data /dev/sdb</span><br><span class="line">[root@ceph1 ceph]# ceph-deploy osd create ceph2 --data /dev/sdb</span><br></pre></td></tr></table></figure><p> ceph秘钥拷贝（主节点执行）及修改密钥权限<br> 用 ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 monitor 地址和 ceph.client.admin.keyring 了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph-deploy admin ceph1 ceph2</span><br></pre></td></tr></table></figure><p>修改密钥权限（所有节点上执行）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph] # chmod +r /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">[root@ceph2] # chmod +r /etc/ceph/ceph.client.admin.keyring</span><br></pre></td></tr></table></figure><p>这时看下danshboard图：</p><p><img src="https://shenshengkun.github.io/images/ceph_dash1.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;ceph介绍&quot;&gt;&lt;a href=&quot;#ceph介绍&quot; class=&quot;headerlink&quot; title=&quot;ceph介绍&quot;&gt;&lt;/a&gt;ceph介绍&lt;/h1&gt;&lt;h2 id=&quot;Ceph基础介绍&quot;&gt;&lt;a href=&quot;#Ceph基础介绍&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>Kubernetes Pod无法删除</title>
    <link href="https://shenshengkun.github.io/posts/dk7569vg.html"/>
    <id>https://shenshengkun.github.io/posts/dk7569vg.html</id>
    <published>2019-07-04T05:53:01.000Z</published>
    <updated>2019-07-05T06:59:46.817Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题发现"><a href="#问题发现" class="headerlink" title="问题发现"></a>问题发现</h1><p>在node节点断电 重启后，发现有的pod节点状态不正常，之前的回收策略也都做了，就调研下是什么原因导致的</p><p>pod一直处于Terminated: ExitCode 状态</p><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><h2 id="直接删除，无法飘移"><a href="#直接删除，无法飘移" class="headerlink" title="直接删除，无法飘移"></a>直接删除，无法飘移</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod &lt;podname&gt; --namespace=&lt;namspacer&gt; --grace-period=0 --force</span><br><span class="line">发现pod无法漂移</span><br><span class="line">docker ps -a查看对应docker容器的状态，发现这两个Pod的docker容器处于Dead状态。使用docker rm &lt;container id&gt;，提示Device is Busy，无法删除。</span><br></pre></td></tr></table></figure><h2 id="现象是由于systemd服务PrivateTmp-true引起"><a href="#现象是由于systemd服务PrivateTmp-true引起" class="headerlink" title="现象是由于systemd服务PrivateTmp=true引起"></a>现象是由于systemd服务<code>PrivateTmp=true</code>引起</h2><p>最根本的方法是，当机器加入时，在 <code>docker.service</code> 中加上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">MountFlags=slave</span><br></pre></td></tr></table></figure><h1 id="关于Systemd的MountFlags"><a href="#关于Systemd的MountFlags" class="headerlink" title="关于Systemd的MountFlags"></a>关于Systemd的MountFlags</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MountFlags: 配置Systemd服务的Mount Namespace配置。会影响服务进程上下文中挂载点的信息，即服务是否会继承主机上已有的挂载点，以及如果服务运行时执行了挂载或卸载设备的操作，是否会真实地在主机上产生效果。可选值为shared、slave和private</span><br><span class="line"></span><br><span class="line">shared：服务与主机共用一个Mount Namespace，会继承主机挂载点，服务挂载或卸载设备时会真实地反映到主机上</span><br><span class="line"></span><br><span class="line">slave：服务使用独立的Mount Namespace，会继承主机挂载点，但服务对挂载点的操作只在自己的Namespace内生效，不会反映到主机上</span><br><span class="line"></span><br><span class="line">private: 服务使用独立的Mount Namespace，在启动时没有任何挂载点，服务对挂载点的操作也不会反映到主机上</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;问题发现&quot;&gt;&lt;a href=&quot;#问题发现&quot; class=&quot;headerlink&quot; title=&quot;问题发现&quot;&gt;&lt;/a&gt;问题发现&lt;/h1&gt;&lt;p&gt;在node节点断电 重启后，发现有的pod节点状态不正常，之前的回收策略也都做了，就调研下是什么原因导致的&lt;/p&gt;
&lt;p&gt;p
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>tomcat8导致文件权限访问不到</title>
    <link href="https://shenshengkun.github.io/posts/kfkd454f.html"/>
    <id>https://shenshengkun.github.io/posts/kfkd454f.html</id>
    <published>2019-07-01T03:09:10.000Z</published>
    <updated>2019-07-01T03:19:37.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>之前在tomcat 7下文件上传后访问一直没问题，现在tomcat版本升到8.5，在测试文件http上传时，发现所传文件无法通过nginx访问了：报错 403 forbidden </p><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><p>看下系统的umask</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/profile后发现</span><br><span class="line"></span><br><span class="line">if [ $UID -gt 199 ] &amp;&amp; [ &quot;`/usr/bin/id -gn`&quot; = &quot;`/usr/bin/id -un`&quot; ]; then</span><br><span class="line">    umask 002</span><br><span class="line">else</span><br><span class="line">    umask 022</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">022是没问题的</span><br></pre></td></tr></table></figure><p>看下tomcat的catlina.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if [ -z &quot;$UMASK&quot; ]; then</span><br><span class="line">UMASK=&quot;0027&quot;</span><br><span class="line">fi</span><br><span class="line">umask $UMASK</span><br><span class="line"></span><br><span class="line">tomcat8改成0027了，把这个改成0022就好了</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h1&gt;&lt;p&gt;之前在tomcat 7下文件上传后访问一直没问题，现在tomcat版本升到8.5，在测试文件http上传时，发现所传文件无法通过nginx访
      
    
    </summary>
    
      <category term="中间件" scheme="https://shenshengkun.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Kubelet状态更新机制</title>
    <link href="https://shenshengkun.github.io/posts/dfkdaa65.html"/>
    <id>https://shenshengkun.github.io/posts/dfkdaa65.html</id>
    <published>2019-06-18T06:14:01.000Z</published>
    <updated>2019-06-20T06:18:22.061Z</updated>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>当 Kubernetes 中 Node 节点出现状态异常的情况下，节点上的 Pod 会被重新调度到其他节点上去，但是有的时候我们会发现节点 Down 掉以后，Pod 并不会立即触发重新调度，这实际上就是和 Kubelet 的状态更新机制密切相关的，Kubernetes 提供了一些参数配置来触发重新调度到嗯时间，下面我们来分析下 Kubelet 状态更新的基本流程。</p><ol><li>kubelet 自身会定期更新状态到 apiserver，通过参数<code>--node-status-update-frequency</code>指定上报频率，默认是 10s 上报一次。</li><li>kube-controller-manager 会每隔<code>--node-monitor-period</code>时间去检查 kubelet 的状态，默认是 5s。</li><li>当 node 失联一段时间后，kubernetes 判定 node 为 <code>notready</code> 状态，这段时长通过<code>--node-monitor-grace-period</code>参数配置，默认 40s。</li><li>当 node 失联一段时间后，kubernetes 判定 node 为 <code>unhealthy</code> 状态，这段时长通过<code>--node-startup-grace-period</code>参数配置，默认 1m0s。</li><li>当 node 失联一段时间后，kubernetes 开始删除原 node 上的 pod，这段时长是通过<code>--pod-eviction-timeout</code>参数配置，默认 5m0s。</li></ol><blockquote><p>kube-controller-manager 和 kubelet 是异步工作的，这意味着延迟可能包括任何的网络延迟、apiserver 的延迟、etcd 延迟，一个节点上的负载引起的延迟等等。因此，如果<code>--node-status-update-frequency</code>设置为5s，那么实际上 etcd 中的数据变化会需要 6-7s，甚至更长时间。</p></blockquote><p>Kubelet在更新状态失败时，会进行<code>nodeStatusUpdateRetry</code>次重试，默认为 5 次。</p><p>Kubelet 会在函数<code>tryUpdateNodeStatus</code>中尝试进行状态更新。Kubelet 使用了 Golang 中的<code>http.Client()</code>方法，但是没有指定超时时间，因此，如果 API Server 过载时，当建立 TCP 连接时可能会出现一些故障。</p><p>因此，在<code>nodeStatusUpdateRetry</code> * <code>--node-status-update-frequency</code>时间后才会更新一次节点状态。</p><p>同时，Kubernetes 的 controller manager 将尝试每<code>--node-monitor-period</code>时间周期内检查<code>nodeStatusUpdateRetry</code>次。在<code>--node-monitor-grace-period</code>之后，会认为节点 unhealthy，然后会在<code>--pod-eviction-timeout</code>后删除 Pod。</p><p>kube proxy 有一个 watcher API，一旦 Pod 被驱逐了，kube proxy 将会通知更新节点的 iptables 规则，将 Pod 从 Service 的 Endpoints 中移除，这样就不会访问到来自故障节点的 Pod 了。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>对于这些参数的配置，需要根据不通的集群规模场景来进行配置。</p><h3 id="社区默认的配置"><a href="#社区默认的配置" class="headerlink" title="社区默认的配置"></a>社区默认的配置</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>10s</td></tr><tr><td>–node-monitor-period</td><td>5s</td></tr><tr><td>–node-monitor-grace-period</td><td>40s</td></tr><tr><td>–pod-eviction-timeout</td><td>5m</td></tr></tbody></table><h3 id="快速更新和快速响应"><a href="#快速更新和快速响应" class="headerlink" title="快速更新和快速响应"></a>快速更新和快速响应</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>4s</td></tr><tr><td>–node-monitor-period</td><td>2s</td></tr><tr><td>–node-monitor-grace-period</td><td>20s</td></tr><tr><td>–pod-eviction-timeout</td><td>30s</td></tr></tbody></table><p>在这种情况下，Pod 将在 50s 被驱逐，因为该节点在 20s 后被视为Down掉了，<code>--pod-eviction-timeout</code>在 30s 之后发生，但是，这种情况会给 etcd 产生很大的开销，因为每个节点都会尝试每 2s 更新一次状态。</p><p>如果环境有1000个节点，那么每分钟将有15000次节点更新操作，这可能需要大型 etcd 容器甚至是 etcd 的专用节点。</p><blockquote><p>如果我们计算尝试次数，则除法将给出5，但实际上每次尝试的 nodeStatusUpdateRetry 尝试将从3到5。 由于所有组件的延迟，尝试总次数将在15到25之间变化。</p></blockquote><h3 id="中等更新和平均响应"><a href="#中等更新和平均响应" class="headerlink" title="中等更新和平均响应"></a>中等更新和平均响应</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>20s</td></tr><tr><td>–node-monitor-period</td><td>5s</td></tr><tr><td>–node-monitor-grace-period</td><td>2m</td></tr><tr><td>–pod-eviction-timeout</td><td>1m</td></tr></tbody></table><p>这种场景下会 20s 更新一次 node 状态，controller manager 认为 node 状态不正常之前，会有 2m<em>60⁄20</em>5=30 次的 node 状态更新，Node 状态为 down 之后 1m，就会触发驱逐操作。</p><p>如果有 1000 个节点，1分钟之内就会有 60s/20s*1000=3000 次的节点状态更新操作。</p><h3 id="低更新和慢响应"><a href="#低更新和慢响应" class="headerlink" title="低更新和慢响应"></a>低更新和慢响应</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>1m</td></tr><tr><td>–node-monitor-period</td><td>5s</td></tr><tr><td>–node-monitor-grace-period</td><td>5m</td></tr><tr><td>–pod-eviction-timeout</td><td>1m</td></tr></tbody></table><p>Kubelet 将会 1m 更新一次节点的状态，在认为不健康之后会有 5m/1m*5=25 次重试更新的机会。Node为不健康的时候，1m 之后 pod开始被驱逐。</p><p>可以有不同的组合，例如快速更新和慢反应以满足特定情况。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;当 Kubernetes 中 Node 节点出现状态异常的情况下，节点上的 Pod 会被重新调度到其他节点上去，但是有的时候我们会发现节点 
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>istio架构与技术</title>
    <link href="https://shenshengkun.github.io/posts/5a4fga4h.html"/>
    <id>https://shenshengkun.github.io/posts/5a4fga4h.html</id>
    <published>2019-06-18T02:10:01.000Z</published>
    <updated>2019-06-21T01:27:54.328Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>使用云平台可以为组织提供丰富的好处。然而，不可否认的是，采用云可能会给 DevOps 团队带来压力。开发人员必须使用微服务以满足应用的可移植性，同时运营商管理了极其庞大的混合和多云部署。Istio 允许您连接、保护、控制和观测服务。</p><p>在较高的层次上，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。它是一个完全开源的服务网格，可以透明地分层到现有的分布式应用程序上。它也是一个平台，包括允许它集成到任何日志记录平台、遥测或策略系统的 API。Istio 的多样化功能集使您能够成功高效地运行分布式微服务架构，并提供保护、连接和监控微服务的统一方法。</p><h1 id="Service-Mesh"><a href="#Service-Mesh" class="headerlink" title="Service Mesh"></a>Service Mesh</h1><p>• 治理能力独立（Sidecar）</p><p>• 应用程序无感知 </p><p>• 服务通信的基础设施层</p><p><img src="https://shenshengkun.github.io/images/istio1.png" alt=""></p><h1 id="为什么要使用-Istio？"><a href="#为什么要使用-Istio？" class="headerlink" title="为什么要使用 Istio？"></a>为什么要使用 Istio？</h1><p>Istio 提供一种简单的方式来为已部署的服务建立网络，该网络具有负载均衡、服务间认证、监控等功能，只需要对服务的代码进行一点或不需要做任何改动。想要让服务支持 Istio，只需要在您的环境中部署一个特殊的 sidecar 代理，使用 Istio 控制平面功能配置和管理代理，拦截微服务之间的所有网络通信：</p><ul><li>HTTP、gRPC、WebSocket 和 TCP 流量的自动负载均衡。</li><li>通过丰富的路由规则、重试、故障转移和故障注入，可以对流量行为进行细粒度控制。</li><li>可插入的策略层和配置 API，支持访问控制、速率限制和配额。</li><li>对出入集群入口和出口中所有流量的自动度量指标、日志记录和追踪。</li><li>通过强大的基于身份的验证和授权，在集群中实现安全的服务间通信。</li></ul><p>Istio 旨在实现可扩展性，满足各种部署需求。</p><p><img src="https://shenshengkun.github.io/images/istio2.png" alt=""></p><h1 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h1><p>Istio 在服务网络中统一提供了许多关键功能：</p><h2 id="流量管理"><a href="#流量管理" class="headerlink" title="流量管理"></a>流量管理</h2><p>通过简单的规则配置和流量路由，您可以控制服务之间的流量和 API 调用。Istio 简化了断路器、超时和重试等服务级别属性的配置，并且可以轻松设置 A/B测试、金丝雀部署和基于百分比的流量分割的分阶段部署等重要任务。</p><p>通过更好地了解您的流量和开箱即用的故障恢复功能，您可以在问题出现之前先发现问题，使调用更可靠，并且使您的网络更加强大——无论您面临什么条件。</p><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><p>Istio 的安全功能使开发人员可以专注于应用程序级别的安全性。Istio 提供底层安全通信信道，并大规模管理服务通信的认证、授权和加密。使用Istio，服务通信在默认情况下是安全的，它允许您跨多种协议和运行时一致地实施策略——所有这些都很少或根本不需要应用程序更改。</p><p>虽然 Istio 与平台无关，但将其与 Kubernetes（或基础架构）网络策略结合使用，其优势会更大，包括在网络和应用层保护 pod 间或服务间通信的能力。</p><h2 id="可观察性"><a href="#可观察性" class="headerlink" title="可观察性"></a>可观察性</h2><p>Istio 强大的追踪、监控和日志记录可让您深入了解服务网格部署。通过 Istio 的监控功能，可以真正了解服务性能如何影响上游和下游的功能，而其自定义仪表板可以提供对所有服务性能的可视性，并让您了解该性能如何影响您的其他进程。</p><p>Istio 的 Mixer 组件负责策略控制和遥测收集。它提供后端抽象和中介，将 Istio 的其余部分与各个基础架构后端的实现细节隔离开来，并为运维提供对网格和基础架构后端之间所有交互的细粒度控制。</p><p>所有这些功能可以让您可以更有效地设置、监控和实施服务上的 SLO。当然，最重要的是，您可以快速有效地检测和修复问题。</p><h2 id="平台支持"><a href="#平台支持" class="headerlink" title="平台支持"></a>平台支持</h2><p>Istio 是独立于平台的，旨在运行在各种环境中，包括跨云、内部部署、Kubernetes、Mesos 等。您可以在 Kubernetes 上部署 Istio 或具有 Consul 的 Nomad 上部署。Istio 目前支持：</p><ul><li>在 Kubernetes 上部署的服务</li><li>使用 Consul 注册的服务</li><li>在虚拟机上部署的服务</li></ul><h2 id="集成和定制"><a href="#集成和定制" class="headerlink" title="集成和定制"></a>集成和定制</h2><p>策略执行组件可以扩展和定制，以便与现有的 ACL、日志、监控、配额、审计等方案集成。</p><p><img src="https://shenshengkun.github.io/images/istio3.png" alt=""></p><h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p>Istio 服务网格逻辑上分为<strong>数据平面</strong>和<strong>控制平面</strong>。</p><ul><li><strong>数据平面</strong>由一组以 sidecar 方式部署的智能代理（<a href="https://www.envoyproxy.io/" target="_blank" rel="noopener">Envoy</a>）组成。这些代理可以调节和控制微服务及 <a href="https://istio.io/zh/docs/concepts/policies-and-telemetry/" target="_blank" rel="noopener">Mixer</a> 之间所有的网络通信。</li><li><strong>控制平面</strong>负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。</li></ul><p>下图显示了构成每个面板的不同组件：</p><p><img src="https://shenshengkun.github.io/images/istio4.png" alt=""></p><h2 id="Envoy"><a href="#Envoy" class="headerlink" title="Envoy"></a>Envoy</h2><p>Istio 使用 Envoy代理的扩展版本，Envoy 是以 C++ 开发的高性能代理，用于调解服务网格中所有服务的所有入站和出站流量。Envoy 的许多内置功能被 Istio 发扬光大，例如：</p><ul><li>动态服务发现</li><li>负载均衡</li><li>TLS 终止</li><li>HTTP/2 &amp; gRPC 代理</li><li>熔断器</li><li>健康检查、基于百分比流量拆分的灰度发布</li><li>故障注入</li><li>丰富的度量指标</li></ul><p>Envoy 被部署为 <strong>sidecar</strong>，和对应服务在同一个 Kubernetes pod 中。这允许 Istio 将大量关于流量行为的信号作为属性提取出来，而这些属性又可以在 Mixer 中用于执行策略决策，并发送给监控系统，以提供整个网格行为的信息。</p><p>Sidecar 代理模型还可以将 Istio 的功能添加到现有部署中，而无需重新构建或重写代码。可以阅读更多来了解为什么我们在设计目标中选择这种方式。</p><h2 id="Mixer"><a href="#Mixer" class="headerlink" title="Mixer"></a>Mixer</h2><p>Mixer是一个独立于平台的组件，负责在服务网格上执行访问控制和使用策略，并从 Envoy 代理和其他服务收集遥测数据。代理提取请求级属性，发送到 Mixer 进行评估</p><p>Mixer 中包括一个灵活的插件模型，使其能够接入到各种主机环境和基础设施后端，从这些细节中抽象出 Envoy 代理和 Istio 管理的服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以配一些监控</span><br></pre></td></tr></table></figure><p><img src="https://shenshengkun.github.io/images/istio5.png" alt=""></p><h2 id="Pilot"><a href="#Pilot" class="headerlink" title="Pilot"></a>Pilot</h2><p>Pilot为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。它将控制流量行为的高级路由规则转换为特定于 Envoy 的配置，并在运行时将它们传播到 sidecar。</p><p>Pilot 将平台特定的服务发现机制抽象化并将其合成为符合 Envoy 数据平面 API的任何 sidecar 都可以使用的标准格式。这种松散耦合使得 Istio 能够在多种环境下运行（例如，Kubernetes、Consul、Nomad），同时保持用于流量管理的相同操作界面。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">k8s来说的话，比如说pod这样的元数据，传给Envoy</span><br></pre></td></tr></table></figure><h2 id="Citadel"><a href="#Citadel" class="headerlink" title="Citadel"></a>Citadel</h2><p>Citadel通过内置身份和凭证管理赋能强大的服务间和最终用户身份验证。可用于升级服务网格中未加密的流量，并为运维人员提供基于服务标识而不是网络控制的强制执行策略的能力。从 0.5 版本开始，Istio 支持基于角色的访问控制，以控制谁可以访问您的服务，而不是基于不稳定的三层或四层网络标识。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">证书安全管理中心，证书生成以及下发</span><br></pre></td></tr></table></figure><h2 id="Galley"><a href="#Galley" class="headerlink" title="Galley"></a>Galley</h2><p>Galley 将担任 Istio 的配置验证，获取配置，处理和分配组件的任务。它负责将其余的 Istio 组件与从底层平台（例如 Kubernetes）获取用户配置的细节中隔离开来。</p><h1 id="和k8s结合"><a href="#和k8s结合" class="headerlink" title="和k8s结合"></a>和k8s结合</h1><p><img src="https://shenshengkun.github.io/images/istio6.png" alt=""></p><h1 id="在-Helm-和-Tiller-的环境中使用-helm-install-命令进行安装"><a href="#在-Helm-和-Tiller-的环境中使用-helm-install-命令进行安装" class="headerlink" title="在 Helm 和 Tiller 的环境中使用 helm install 命令进行安装"></a>在 Helm 和 Tiller 的环境中使用 <code>helm install</code> 命令进行安装</h1><h2 id="下载istio"><a href="#下载istio" class="headerlink" title="下载istio"></a>下载istio</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.1.9 sh -</span><br><span class="line">mv istio-1.1.9 /opt/k8s/work/</span><br><span class="line">cd /opt/k8s/work/</span><br><span class="line">cd istio-1.1.9/</span><br><span class="line">export PATH=$PWD/bin:$PATH</span><br><span class="line">helm repo add istio.io https://storage.googleapis.com/istio-release/releases/1.1.9/charts/</span><br></pre></td></tr></table></figure><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">helm install install/kubernetes/helm/istio-init --name istio-init --namespace istio-system</span><br><span class="line"></span><br><span class="line">kubectl get crds | grep &apos;istio.io\|certmanager.k8s.io&apos; | wc -l</span><br><span class="line">53</span><br><span class="line"></span><br><span class="line">helm install install/kubernetes/helm/istio --name istio --namespace istio-system</span><br><span class="line"></span><br><span class="line">[root@node1 istio-1.1.9]# kubectl get pods -n istio-system</span><br><span class="line">NAME                                      READY   STATUS      RESTARTS   AGE</span><br><span class="line">istio-citadel-b6d6889c4-96fwx             1/1     Running     0          44h</span><br><span class="line">istio-galley-654c696595-2rbr9             1/1     Running     0          44h</span><br><span class="line">istio-ingressgateway-6b47b76cc6-2rxbq     1/1     Running     0          44h</span><br><span class="line">istio-init-crd-10-smj28                   0/1     Completed   0          44h</span><br><span class="line">istio-init-crd-11-wktdb                   0/1     Completed   0          44h</span><br><span class="line">istio-pilot-5c99cfc94-g7t84               2/2     Running     0          44h</span><br><span class="line">istio-policy-6c5795449-tkzmp              2/2     Running     3          44h</span><br><span class="line">istio-sidecar-injector-79c88d56cf-lmv9j   1/1     Running     0          44h</span><br><span class="line">istio-telemetry-64f99d84c7-ksjmh          2/2     Running     2          44h</span><br><span class="line">prometheus-d8d46c5b5-sdljw                1/1     Running     0          44h</span><br></pre></td></tr></table></figure><h2 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm delete --purge istio</span><br><span class="line">helm delete --purge istio-init</span><br><span class="line">kubectl delete -f install/kubernetes/helm/istio-init/files</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h1&gt;&lt;p&gt;使用云平台可以为组织提供丰富的好处。然而，不可否认的是，采用云可能会给 DevOps 团队带来压力。开发人员必须使用微服务以满足应用的可移植
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-helm</title>
    <link href="https://shenshengkun.github.io/posts/dkgg3h4f.html"/>
    <id>https://shenshengkun.github.io/posts/dkgg3h4f.html</id>
    <published>2019-06-17T10:20:01.000Z</published>
    <updated>2019-06-17T03:07:27.417Z</updated>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p><code>Helm</code>这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套测试环境，对于单体服务，部署一套测试环境我相信还是非常快的，但是对于微服务架构的应用，要部署一套新的环境，就有点折磨人了，微服务越多、你就会越绝望的。虽然我们线上和测试环境已经都迁移到了<code>kubernetes</code>环境，但是每个微服务也得维护一套<code>yaml</code>文件，而且每个环境下的配置文件也不太一样，部署一套新的环境成本是真的很高。如果我们能使用类似于<code>yum</code>的工具来安装我们的应用的话是不是就很爽歪歪了啊？<code>Helm</code>就相当于<code>kubernetes</code>环境下的<code>yum</code>包管理工具。 </p><h1 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h1><p>做为 Kubernetes 的一个包管理工具，<code>Helm</code>具有如下功能：</p><ul><li>创建新的 chart</li><li>chart 打包成 tgz 格式</li><li>上传 chart 到 chart 仓库或从仓库中下载 chart</li><li>在<code>Kubernetes</code>集群中安装或卸载 chart</li><li>管理用<code>Helm</code>安装的 chart 的发布周期</li></ul><h1 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h1><p>Helm 有三个重要概念：</p><ul><li>chart：包含了创建<code>Kubernetes</code>的一个应用实例的必要信息</li><li>config：包含了应用发布配置信息</li><li>release：是一个 chart 及其配置的一个运行实例</li></ul><h1 id="Helm组件"><a href="#Helm组件" class="headerlink" title="Helm组件"></a>Helm组件</h1><p>Helm 有以下两个组成部分：</p><p><code>Helm Client</code> 是用户命令行工具，其主要负责如下：</p><ul><li>本地 chart 开发</li><li>仓库管理</li><li>与 Tiller sever 交互</li><li>发送预安装的 chart</li><li>查询 release 信息</li><li>要求升级或卸载已存在的 release</li></ul><p><code>Tiller Server</code>是一个部署在<code>Kubernetes</code>集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下：</p><ul><li>监听来自 Helm client 的请求</li><li>通过 chart 及其配置构建一次发布</li><li>安装 chart 到<code>Kubernetes</code>集群，并跟踪随后的发布</li><li>通过与<code>Kubernetes</code>交互升级或卸载 chart</li><li>简单的说，client 管理 charts，而 server 管理发布 release</li></ul><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>我们可以在<a href="https://github.com/kubernetes/helm/releases" target="_blank" rel="noopener">Helm Realese</a>页面下载二进制文件，这里下载的2.14.1版本，解压后将可执行文件<code>helm</code>拷贝到<code>/usr/local/bin</code>目录下即可，这样<code>Helm</code>客户端就在这台机器上安装完成了。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br><span class="line"></span><br><span class="line">另外还需要在每个node节点安装yum install socat -y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]# helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.14.1&quot;, GitCommit:&quot;5270352a09c7e8b6e8c9593002a73535276507c0&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.14.1&quot;, GitCommit:&quot;5270352a09c7e8b6e8c9593002a73535276507c0&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">自Kubernetes 1.6版本开始，API Server启用了RBAC授权。而目前的Tiller部署没有定义授权的ServiceAccount，这会导致访问API Server时被拒绝。我们可以采用如下方法，明确为Tiller部署添加授权。</span><br><span class="line"></span><br><span class="line">kubectl create serviceaccount --namespace kube-system tiller</span><br><span class="line">kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span><br><span class="line">kubectl patch deploy --namespace kube-system tiller-deploy -p &apos;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&apos;</span><br><span class="line">kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default</span><br></pre></td></tr></table></figure><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>我们现在了尝试创建一个 Chart： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]# helm create hello-helm</span><br><span class="line">Creating hello-helm</span><br><span class="line">[root@node1 helm]# ls</span><br><span class="line">hello-helm  helm-v2.14.1-linux-amd64.tar.gz  linux-amd64</span><br><span class="line">[root@node1 helm]# helm install ./hello-helm</span><br><span class="line">NAME:   virulent-wolverine</span><br><span class="line">LAST DEPLOYED: Mon Jun 17 10:56:39 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                           READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">virulent-wolverine-hello-helm  0/1    0           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                            READY  STATUS             RESTARTS  AGE</span><br><span class="line">virulent-wolverine-hello-helm-6f54d6f866-d5t7v  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                           TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">virulent-wolverine-hello-helm  ClusterIP  10.254.123.130  &lt;none&gt;       80/TCP   1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export POD_NAME=$(kubectl get pods --namespace default -l &quot;app.kubernetes.io/name=hello-helm,app.kubernetes.io/instance=virulent-wolverine&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">  echo &quot;Visit http://127.0.0.1:8080 to use your application&quot;</span><br><span class="line">  kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><p>然后我们根据提示执行下面的命令： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export POD_NAME=$(kubectl get pods --namespace default -l &quot;app.kubernetes.io/name=hello-helm,app.kubernetes.io/instance=virulent-wolverine&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><p>访问：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# curl 127.0.0.1:8080</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>查看<code>release</code>： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# helm list</span><br><span class="line">NAME                    REVISION        UPDATED                         STATUS          CHART                   APP VERSION     NAMESPACE</span><br><span class="line">virulent-wolverine      1               Mon Jun 17 10:56:39 2019        DEPLOYED        hello-helm-0.1.0        1.0             default</span><br></pre></td></tr></table></figure><p>打包<code>chart</code>： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm package hello-helm</span><br></pre></td></tr></table></figure><p>删除：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# helm delete virulent-wolverine</span><br><span class="line">release &quot;virulent-wolverine&quot; deleted</span><br><span class="line">[root@node1 ~]# helm list</span><br><span class="line">[root@node1 ~]# helm list --all</span><br><span class="line">NAME                    REVISION        UPDATED                         STATUS  CHART                   APP VERSION     NAMESPACE</span><br><span class="line">virulent-wolverine      1               Mon Jun 17 10:56:39 2019        DELETED hello-helm-0.1.0        1.0             default</span><br></pre></td></tr></table></figure><p>彻底删除：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# helm delete virulent-wolverine --purge</span><br><span class="line">release &quot;virulent-wolverine&quot; deleted</span><br><span class="line">[root@node1 ~]# helm list --all</span><br><span class="line">[root@node1 ~]#</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;&lt;code&gt;Helm&lt;/code&gt;这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-dashboard</title>
    <link href="https://shenshengkun.github.io/posts/c2556dcf.html"/>
    <id>https://shenshengkun.github.io/posts/c2556dcf.html</id>
    <published>2019-06-10T08:16:01.000Z</published>
    <updated>2019-06-12T08:39:46.837Z</updated>
    
    <content type="html"><![CDATA[<h1 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/kubernetes/cluster/addons/dashboard</span><br></pre></td></tr></table></figure><p>修改 service 定义，指定端口类型为 NodePort，这样外界可以通过地址 NodeIP:NodePort 访问 dashboard；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cat dashboard-service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort # 增加这一行</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  ports:</span><br><span class="line">  - port: 443</span><br><span class="line">    targetPort: 8443</span><br></pre></td></tr></table></figure><p>修改镜像地址mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1在dashboard-controller.yaml中</p><h1 id="执行所有定义文件"><a href="#执行所有定义文件" class="headerlink" title="执行所有定义文件"></a>执行所有定义文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f  .</span><br></pre></td></tr></table></figure><h2 id="查看分配的-NodePort"><a href="#查看分配的-NodePort" class="headerlink" title="查看分配的 NodePort"></a>查看分配的 NodePort</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl get deployment kubernetes-dashboard  -n kube-system</span><br><span class="line"></span><br><span class="line">NAME                   READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line"></span><br><span class="line">kubernetes-dashboard   1/1     1            1           23h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 dashboard]# kubectl --namespace kube-system get pods -o wide</span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE     IP             NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-8854569d4-g2hth                 1/1     Running   4          5d22h   172.30.40.2    node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard-7848d45466-6pm2q   1/1     Running   0          23h     172.30.200.3   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-server-5f7cf7659-59swk          1/1     Running   0          2d5h    172.30.40.3    node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 dashboard]# kubectl get services kubernetes-dashboard -n kube-system</span><br><span class="line">NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kubernetes-dashboard   NodePort   10.254.234.85   &lt;none&gt;        443:32681/TCP   23h</span><br></pre></td></tr></table></figure><h1 id="访问-dashboard"><a href="#访问-dashboard" class="headerlink" title="访问 dashboard"></a>访问 dashboard</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://192.168.6.101:32681</span><br></pre></td></tr></table></figure><h2 id="创建登录-Dashboard-的-token-和-kubeconfig-配置文件"><a href="#创建登录-Dashboard-的-token-和-kubeconfig-配置文件" class="headerlink" title="创建登录 Dashboard 的 token 和 kubeconfig 配置文件"></a>创建登录 Dashboard 的 token 和 kubeconfig 配置文件</h2><p>dashboard 默认只支持 token 认证（不支持 client 证书认证），所以如果使用 Kubeconfig 文件，需要将 token 写入到该文件。</p><h3 id="创建登录-token"><a href="#创建登录-token" class="headerlink" title="创建登录 token"></a>创建登录 token</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl create sa dashboard-admin -n kube-system</span><br><span class="line">kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin</span><br><span class="line">ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk &apos;&#123;print $1&#125;&apos;)</span><br><span class="line">DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system $&#123;ADMIN_SECRET&#125; | grep -E &apos;^token&apos; | awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">echo $&#123;DASHBOARD_LOGIN_TOKEN&#125;</span><br></pre></td></tr></table></figure><p>使用输出的 token 登录 Dashboard。</p><h3 id="创建使用-token-的-KubeConfig-文件"><a href="#创建使用-token-的-KubeConfig-文件" class="headerlink" title="创建使用 token 的 KubeConfig 文件"></a>创建使用 token 的 KubeConfig 文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line"># 设置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置客户端认证参数，使用上面创建的 Token</span><br><span class="line">kubectl config set-credentials dashboard_user \</span><br><span class="line">  --token=$&#123;DASHBOARD_LOGIN_TOKEN&#125; \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置上下文参数</span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=dashboard_user \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置默认上下文</span><br><span class="line">kubectl config use-context default --kubeconfig=dashboard.kubeconfig</span><br></pre></td></tr></table></figure><p>用生成的 dashboard.kubeconfig 登录 Dashboard。</p><h1 id="为kubernetes-dashboard访问用户添加权限控制"><a href="#为kubernetes-dashboard访问用户添加权限控制" class="headerlink" title="为kubernetes dashboard访问用户添加权限控制"></a>为kubernetes dashboard访问用户添加权限控制</h1><h2 id="Role"><a href="#Role" class="headerlink" title="Role"></a>Role</h2><p><code>Role</code>表示是一组规则权限，只能累加，<code>Role</code>可以定义在一个<code>namespace</code>中，只能用于授予对单个命名空间中的资源访问的权限。比如我们新建一个对默认命名空间中<code>Pods</code>具有访问权限的角色：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: default</span><br><span class="line">  name: pod-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br></pre></td></tr></table></figure><h2 id="ClusterRole"><a href="#ClusterRole" class="headerlink" title="ClusterRole"></a>ClusterRole</h2><p><code>ClusterRole</code>具有与<code>Role</code>相同的权限角色控制能力，不同的是<code>ClusterRole</code>是集群级别的，可以用于:</p><ul><li>集群级别的资源控制(例如 node 访问权限)</li><li>非资源型 endpoints(例如 /healthz 访问)</li><li>所有命名空间资源控制(例如 pods)</li></ul><p>比如我们要创建一个授权某个特定命名空间或全部命名空间(取决于绑定方式)访问<strong>secrets</strong>的集群角色：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  # &quot;namespace&quot; omitted since ClusterRoles are not namespaced</span><br><span class="line">  name: secret-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;secrets&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br></pre></td></tr></table></figure><h2 id="RoleBinding和ClusterRoleBinding"><a href="#RoleBinding和ClusterRoleBinding" class="headerlink" title="RoleBinding和ClusterRoleBinding"></a>RoleBinding和ClusterRoleBinding</h2><p><code>RoloBinding</code>可以将角色中定义的权限授予用户或用户组，<code>RoleBinding</code>包含一组权限列表(<code>subjects</code>)，权限列表中包含有不同形式的待授予权限资源类型(users、groups、service accounts)，<code>RoleBinding</code>适用于某个命名空间内授权，而 <code>ClusterRoleBinding</code>适用于集群范围内的授权。</p><p>比如我们将默认命名空间的<code>pod-reader</code>角色授予用户jane，这样以后该用户在默认命名空间中将具有<code>pod-reader</code>的权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># This role binding allows &quot;jane&quot; to read pods in the &quot;default&quot; namespace.</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-pods</span><br><span class="line">  namespace: default</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: jane</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: pod-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p><code>RoleBinding</code>同样可以引用<code>ClusterRole</code>来对当前 namespace 内用户、用户组或 ServiceAccount 进行授权，这种操作允许集群管理员在整个集群内定义一些通用的 ClusterRole，然后在不同的 namespace 中使用 RoleBinding 来引用</p><p>例如，以下 RoleBinding 引用了一个 ClusterRole，这个 ClusterRole 具有整个集群内对 secrets 的访问权限；但是其授权用户 dave 只能访问 development 空间中的 secrets(因为 RoleBinding 定义在 development 命名空间)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># This role binding allows &quot;dave&quot; to read secrets in the &quot;development&quot; namespace.</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-secrets</span><br><span class="line">  namespace: development # This only grants permissions within the &quot;development&quot; namespace.</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: dave</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: secret-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>最后，使用 ClusterRoleBinding 可以对整个集群中的所有命名空间资源权限进行授权；以下 ClusterRoleBinding 样例展示了授权 manager 组内所有用户在全部命名空间中对 secrets 进行访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># This cluster role binding allows anyone in the &quot;manager&quot; group to read secrets in any namespace.</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-secrets-global</span><br><span class="line">subjects:</span><br><span class="line">- kind: Group</span><br><span class="line">  name: manager</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: secret-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><ul><li>新增一个新的用户sy</li><li>该用户只能对命名空间<code>kube-system</code>下面的<code>pods</code>和<code>deployments</code>进行管理</li></ul><p>第一步新建一个<code>ServiceAccount</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl create sa sy -n kube-system</span><br><span class="line">serviceaccount/sy created</span><br></pre></td></tr></table></figure><p>然后我们新建一个角色<strong>role-sy</strong>：(role.yaml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  name: role-sy</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line">- apiGroups: [&quot;extensions&quot;, &quot;apps&quot;]</span><br><span class="line">  resources: [&quot;deployments&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]</span><br></pre></td></tr></table></figure><p>注意上面的<code>rules</code>规则：管理<code>pods</code>和<code>deployments</code>的权限。</p><p>然后我们创建一个角色绑定，将上面的角色<code>role-sy绑定到**sy**的</code>ServiceAccount`上：(role-bind.yaml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: role-bind-sy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: sy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: role-sy</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>分别执行上面两个<code>yaml</code>文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl create -f role.yaml</span><br><span class="line">role.rbac.authorization.k8s.io/role-sy created</span><br><span class="line">[root@node1 dashboard]# kubectl create -f role-bind.yaml </span><br><span class="line">rolebinding.rbac.authorization.k8s.io/role-bind-sy created</span><br></pre></td></tr></table></figure><p>接下来该怎么做？和前面一样的，我们只需要拿到sy这个<code>ServiceAccount</code>的<code>token</code>就可以登录<code>Dashboard</code>了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl get secret -n kube-system |grep sy</span><br><span class="line">sy-token-5cmnl                                   kubernetes.io/service-account-token   3      3m2s</span><br><span class="line"></span><br><span class="line">[root@node1 dashboard]# kubectl get secret sy-token-5cmnl  -o jsonpath=&#123;.data.token&#125; -n kube-system |base64 -d</span><br><span class="line"># 会生成一串很长的base64后的字符串</span><br></pre></td></tr></table></figure><p>这样就可以控制权限了，需要将登录地址改为namespace=kube-system</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;修改配置文件&quot;&gt;&lt;a href=&quot;#修改配置文件&quot; class=&quot;headerlink&quot; title=&quot;修改配置文件&quot;&gt;&lt;/a&gt;修改配置文件&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gut
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-metrics-server</title>
    <link href="https://shenshengkun.github.io/posts/d3554aa2.html"/>
    <id>https://shenshengkun.github.io/posts/d3554aa2.html</id>
    <published>2019-06-10T03:16:01.000Z</published>
    <updated>2019-06-10T03:22:59.745Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。</p><p>从 Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。</p><p>替代方案如下：</p><ol><li>用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server；</li><li>通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator；</li><li>事件传输：使用第三方工具来传输、归档 kubernetes events；</li></ol><p>Kubernetes Dashboard 还不支持 metrics-server（PR：<a href="https://github.com/kubernetes/dashboard/pull/3504" target="_blank" rel="noopener">#3504</a>），如果使用 metrics-server 替代 Heapster，将无法在 dashboard 中以图形展示 Pod 的内存和 CPU 情况，需要通过 Prometheus、Grafana 等监控方案来弥补。</p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/</span><br><span class="line">git clone https://github.com/kubernetes-incubator/metrics-server.git</span><br><span class="line">cd metrics-server/deploy/1.8+/</span><br><span class="line">ls</span><br><span class="line">aggregated-metrics-reader.yaml  auth-delegator.yaml  auth-reader.yaml  metrics-apiservice.yaml  metrics-server-deployment.yaml  metrics-server-service.yaml  resource-reader.yaml</span><br></pre></td></tr></table></figure><h2 id="修改-metrics-server-deployment-yaml-文件，为-metrics-server-添加三个命令行参数："><a href="#修改-metrics-server-deployment-yaml-文件，为-metrics-server-添加三个命令行参数：" class="headerlink" title="修改 metrics-server-deployment.yaml 文件，为 metrics-server 添加三个命令行参数："></a>修改 <code>metrics-server-deployment.yaml</code> 文件，为 metrics-server 添加三个命令行参数：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: metrics-server</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: metrics-server</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: metrics-server</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: metrics-server</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: metrics-server</span><br><span class="line">      volumes:</span><br><span class="line">      # mount in tmp so we can safely use from-scratch images and/or read-only containers</span><br><span class="line">      - name: tmp-dir</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      containers:</span><br><span class="line">      - name: metrics-server</span><br><span class="line">        image: mirrorgooglecontainers/metrics-server-amd64:v0.3.3</span><br><span class="line">        command:</span><br><span class="line">        - /metrics-server</span><br><span class="line">        - --metric-resolution=30s</span><br><span class="line">        - --requestheader-allowed-names=aggregator</span><br><span class="line">        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: tmp-dir</span><br><span class="line">          mountPath: /tmp</span><br></pre></td></tr></table></figure><ul><li>–metric-resolution=30s：从 kubelet 采集数据的周期；</li><li>–requestheader-allowed-names=aggregator：允许请求 metrics-server API 的用户名，该名称与 kube-apiserver 的 <code>--proxy-client-cert-file</code> 指定的证书 CN 一致；</li><li>–kubelet-preferred-address-types：优先使用 InternalIP 来访问 kubelet，这样可以避免节点名称<strong>没有 DNS 解析</strong>记录时，通过节点名称调用节点 kubelet API 失败的情况（未配置时默认的情况）；</li></ul><h2 id="修改apiserver参数："><a href="#修改apiserver参数：" class="headerlink" title="修改apiserver参数："></a>修改apiserver参数：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/systemd/system</span><br><span class="line"></span><br><span class="line">vim kube-apiserver.service</span><br><span class="line"></span><br><span class="line">--requestheader-allowed-names=&quot;aggregator&quot;</span><br></pre></td></tr></table></figure><p>重启apiserver</p><h2 id="部署-metrics-server："><a href="#部署-metrics-server：" class="headerlink" title="部署 metrics-server："></a>部署 metrics-server：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/metrics-server/deploy/1.8+/</span><br><span class="line">kubectl create -f .</span><br></pre></td></tr></table></figure><h2 id="使用-kubectl-top-命令查看集群节点资源使用情况"><a href="#使用-kubectl-top-命令查看集群节点资源使用情况" class="headerlink" title="使用 kubectl top 命令查看集群节点资源使用情况"></a>使用 kubectl top 命令查看集群节点资源使用情况</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 1.8+]# kubectl top node</span><br><span class="line">NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   </span><br><span class="line">node1   117m         5%     2217Mi          57%       </span><br><span class="line">node2   147m         7%     2680Mi          69%</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-coredns</title>
    <link href="https://shenshengkun.github.io/posts/77sa4nc2.html"/>
    <id>https://shenshengkun.github.io/posts/77sa4nc2.html</id>
    <published>2019-06-06T04:32:01.000Z</published>
    <updated>2019-06-21T12:01:56.403Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>1.11后CoreDNS 已取代 Kube DNS 作为集群服务发现元件,由于 Kubernetes 需要让 Pod 与 Pod 之间能夠互相通信,然而要能够通信需要知道彼此的 IP 才行,而这种做法通常是通过 Kubernetes API 来获取,但是 Pod IP 会因为生命周期变化而改变,因此这种做法无法弹性使用,且还会增加 API Server 负担,基于此问题 Kubernetes 提供了 DNS 服务来作为查询,让 Pod 能夠以 Service 名称作为域名来查询 IP 位址,因此使用者就再不需要关心实际 Pod IP,而 DNS 也会根据 Pod 变化更新资源记录(Record resources)</p><p>CoreDNS 是由 CNCF 维护的开源 DNS 方案,该方案前身是 SkyDNS,其采用了 Caddy 的一部分来开发伺服器框架,使其能够建立一套快速灵活的 DNS,而 CoreDNS 每个功能都可以被当作成一個插件的中介软体,如 Log、Cache、Kubernetes 等功能,甚至能够将源记录存储在 Redis、Etcd 中</p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>coredns 目录是 <code>cluster/addons/dns</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/kubernetes/cluster/addons/dns/coredns</span><br><span class="line">cp coredns.yaml.base coredns.yaml</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">sed -i -e &quot;s/__PILLAR__DNS__DOMAIN__/$&#123;CLUSTER_DNS_DOMAIN&#125;/&quot; -e &quot;s/__PILLAR__DNS__SERVER__/$&#123;CLUSTER_DNS_SVC_IP&#125;/&quot; coredns.yaml</span><br><span class="line"></span><br><span class="line">还需要将镜像修改下，coredns/coredns:1.3.1</span><br></pre></td></tr></table></figure><h2 id="创建-coredns"><a href="#创建-coredns" class="headerlink" title="创建 coredns"></a>创建 coredns</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f coredns.yaml</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cat&lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: busybox</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: busybox</span><br><span class="line">    image: busybox:1.28.3</span><br><span class="line">    command:</span><br><span class="line">      - sleep</span><br><span class="line">      - &quot;3600&quot;</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>创建成功后，我们进行检查</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod</span><br><span class="line">NAME      READY   STATUS    RESTARTS   AGE</span><br><span class="line">busybox   1/1     Running   0          4s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 coredns]# kubectl exec -ti busybox -- nslookup kubernetes</span><br><span class="line">Server:    10.254.0.2</span><br><span class="line">Address 1: 10.254.0.2 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      kubernetes</span><br><span class="line">Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 ~]# kubectl exec -ti busybox ping kubernetes.default.svc.cluster.local</span><br><span class="line">PING kubernetes.default.svc.cluster.local (10.254.0.1): 56 data bytes</span><br><span class="line">64 bytes from 10.254.0.1: seq=0 ttl=64 time=0.099 ms</span><br><span class="line">^C</span><br><span class="line">--- kubernetes.default.svc.cluster.local ping statistics ---</span><br><span class="line">1 packets transmitted, 1 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.099/0.099/0.099 ms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：用my-svc.my-namespace.svc.cluster.local的方式可以访问服务</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;1.11后CoreDNS 已取代 Kube DNS 作为集群服务发现元件,由于 Kubernetes 需要让 Pod 与 Pod 之间能夠互
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s角色</title>
    <link href="https://shenshengkun.github.io/posts/65aa44f.html"/>
    <id>https://shenshengkun.github.io/posts/65aa44f.html</id>
    <published>2019-06-06T02:01:01.000Z</published>
    <updated>2019-07-08T07:42:22.780Z</updated>
    
    <content type="html"><![CDATA[<h1 id="查看node节点"><a href="#查看node节点" class="headerlink" title="查看node节点"></a>查看node节点</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 work]# kubectl get nodes</span><br><span class="line">NAME    STATUS   ROLES    AGE   VERSION</span><br><span class="line">node1   Ready    &lt;none&gt;   41h   v1.14.2</span><br><span class="line">node2   Ready    &lt;none&gt;   41h   v1.14.2</span><br></pre></td></tr></table></figure><h1 id="设置集群角色"><a href="#设置集群角色" class="headerlink" title="设置集群角色"></a>设置集群角色</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 设置 node1 为 master 角色</span><br><span class="line"></span><br><span class="line">kubectl label nodes node1 node-role.kubernetes.io/master=</span><br><span class="line"></span><br><span class="line"># 设置 node2 为 node 角色</span><br><span class="line"></span><br><span class="line">kubectl label nodes node2 node-role.kubernetes.io/node=</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 ~]# kubectl get nodes</span><br><span class="line">NAME    STATUS   ROLES    AGE   VERSION</span><br><span class="line">node1   Ready    master   42h   v1.14.2</span><br><span class="line">node2   Ready    node     42h   v1.14.2</span><br></pre></td></tr></table></figure><h1 id="设置taint"><a href="#设置taint" class="headerlink" title="设置taint"></a>设置taint</h1><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node [node] key=value[effect]   </span><br><span class="line">     其中[effect] 可取值: [ NoSchedule | PreferNoSchedule | NoExecute ]</span><br><span class="line">      NoSchedule: 一定不能被调度</span><br><span class="line">      PreferNoSchedule: 尽量不要调度</span><br><span class="line">      NoExecute: 不仅不会调度, 还会驱逐Node上已有的Pod</span><br></pre></td></tr></table></figure><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl taint nodes node1 node-role.kubernetes.io/master=:NoExecute</span><br><span class="line">node/node1 tainted</span><br><span class="line">[root@node1 ~]# kubectl get pods</span><br><span class="line">NAME             READY   STATUS        RESTARTS   AGE</span><br><span class="line">nginx-ds-kztdz   1/1     Running       0          18h</span><br><span class="line">nginx-ds-vbjh9   0/1     Terminating   0          18h</span><br><span class="line">[root@node1 ~]# kubectl get pods</span><br><span class="line">NAME             READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-ds-kztdz   1/1     Running   0          18h</span><br></pre></td></tr></table></figure><h2 id="查看taint"><a href="#查看taint" class="headerlink" title="查看taint"></a>查看taint</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl describe node node1</span><br><span class="line">Name:               node1</span><br><span class="line">Roles:              master</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/arch=amd64</span><br><span class="line">                    kubernetes.io/hostname=node1</span><br><span class="line">                    kubernetes.io/os=linux</span><br><span class="line">                    node-role.kubernetes.io/master=</span><br><span class="line">Annotations:        node.alpha.kubernetes.io/ttl: 0</span><br><span class="line">                    volumes.kubernetes.io/controller-managed-attach-detach: true</span><br><span class="line">CreationTimestamp:  Tue, 04 Jun 2019 15:28:56 +0800</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoExecute</span><br><span class="line">                    node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">Unschedulable:      false</span><br><span class="line">Conditions:</span><br><span class="line">  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message</span><br><span class="line">  ----             ------  -----------------                 ------------------                ------                       -------</span><br><span class="line">  MemoryPressure   False   Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available</span><br><span class="line">  DiskPressure     False   Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure</span><br><span class="line">  PIDPressure      False   Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available</span><br><span class="line">  Ready            True    Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletReady                 kubelet is posting ready status</span><br><span class="line">Addresses:</span><br><span class="line">  InternalIP:  192.168.6.101</span><br></pre></td></tr></table></figure><h2 id="删除taint"><a href="#删除taint" class="headerlink" title="删除taint"></a>删除taint</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl taint nodes node1 node-role.kubernetes.io/master-</span><br><span class="line">node/node1 untainted</span><br></pre></td></tr></table></figure><h1 id="RBAC"><a href="#RBAC" class="headerlink" title="RBAC"></a>RBAC</h1><p><code>Kubernetes</code>有一个很基本的特性就是它的<a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" rel="noopener">所有资源对象都是模型化的 API 对象</a>，允许执行 CRUD(Create、Read、Update、Delete)操作(也就是我们常说的增、删、改、查操作)，比如下面的这下资源：</p><ul><li>Pods</li><li>ConfigMaps</li><li>Deployments</li><li>Nodes</li><li>Secrets</li><li>Namespaces</li></ul><p>上面这些资源对象的可能存在的操作有：</p><ul><li>create</li><li>get</li><li>delete</li><li>list</li><li>update</li><li>edit</li><li>watch</li><li>exec</li></ul><p>在更上层，这些资源和 API Group 进行关联，比如<code>Pods</code>属于 Core API Group，而<code>Deployements</code>属于 apps API Group，要在<code>Kubernetes</code>中进行<code>RBAC</code>的管理，除了上面的这些资源和操作以外，我们还需要另外的一些对象：</p><ul><li>Rule：规则，规则是一组属于不同 API Group 资源上的一组操作的集合</li><li>Role 和 ClusterRole：角色和集群角色，这两个对象都包含上面的 Rules 元素，二者的区别在于，在 Role 中，定义的规则只适用于单个命名空间，也就是和 namespace 关联的，而 ClusterRole 是集群范围内的，因此定义的规则不受命名空间的约束。另外 Role 和 ClusterRole 在<code>Kubernetes</code>中都被定义为集群内部的 API 资源，和我们前面学习过的 Pod、ConfigMap 这些类似，都是我们集群的资源对象，所以同样的可以使用我们前面的<code>kubectl</code>相关的命令来进行操作</li><li>Subject：主题，对应在集群中尝试操作的对象，集群中定义了3种类型的主题资源：<ul><li>User Account：用户，这是有外部独立服务进行管理的，管理员进行私钥的分配，用户可以使用 KeyStone或者 Goolge 帐号，甚至一个用户名和密码的文件列表也可以。对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部的 API 来进行管理</li><li>Group：组，这是用来关联多个账户的，集群中有一些默认创建的组，比如cluster-admin</li><li>Service Account：服务帐号，通过<code>Kubernetes</code> API 来管理的一些用户帐号，和 namespace 进行关联的，适用于集群内部运行的应用程序，需要通过 API 来完成权限认证，所以在集群内部进行权限操作，我们都需要使用到 ServiceAccount，这也是我们这节课的重点</li></ul></li><li>RoleBinding 和 ClusterRoleBinding：角色绑定和集群角色绑定，简单来说就是把声明的 Subject 和我们的 Role 进行绑定的过程(给某个用户绑定上操作的权限)，二者的区别也是作用范围的区别：RoleBinding 只会影响到当前 namespace 下面的资源操作权限，而 ClusterRoleBinding 会影响到所有的 namespace。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;查看node节点&quot;&gt;&lt;a href=&quot;#查看node节点&quot; class=&quot;headerlink&quot; title=&quot;查看node节点&quot;&gt;&lt;/a&gt;查看node节点&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td cl
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-node节点</title>
    <link href="https://shenshengkun.github.io/posts/44qq5gb2.html"/>
    <id>https://shenshengkun.github.io/posts/44qq5gb2.html</id>
    <published>2019-06-05T07:33:01.000Z</published>
    <updated>2019-06-25T07:37:30.394Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;yum install -y epel-release&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;yum install -y conntrack ipvsadm ntp ntpdate ipset jq iptables curl sysstat libseccomp &amp;&amp; modprobe ip_vs &quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h1 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h1><h2 id="下载和分发-docker-二进制文件"><a href="#下载和分发-docker-二进制文件" class="headerlink" title="下载和分发 docker 二进制文件"></a>下载和分发 docker 二进制文件</h2><p>到 <a href="https://download.docker.com/linux/static/stable/x86_64/" target="_blank" rel="noopener">docker 下载页面</a> 下载最新发布包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">wget https://download.docker.com/linux/static/stable/x86_64/docker-18.09.6.tgz</span><br><span class="line">tar -xvf docker-18.09.6.tgz</span><br></pre></td></tr></table></figure><p>分发二进制文件到所有 worker 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp docker/*  root@$&#123;node_ip&#125;:/opt/k8s/bin/</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-systemd-unit-文件"><a href="#创建和分发-systemd-unit-文件" class="headerlink" title="创建和分发 systemd unit 文件"></a>创建和分发 systemd unit 文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; docker.service &lt;&lt;&quot;EOF&quot;</span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Application Container Engine</span><br><span class="line">Documentation=http://docs.docker.io</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=##DOCKER_DIR##</span><br><span class="line">Environment=&quot;PATH=/opt/k8s/bin:/bin:/sbin:/usr/bin:/usr/sbin&quot;</span><br><span class="line">EnvironmentFile=-/run/flannel/docker</span><br><span class="line">ExecStart=/opt/k8s/bin/dockerd $DOCKER_NETWORK_OPTIONS</span><br><span class="line">ExecReload=/bin/kill -s HUP $MAINPID</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发 systemd unit 文件到所有 worker 机器:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">sed -i -e &quot;s|##DOCKER_DIR##|$&#123;DOCKER_DIR&#125;|&quot; docker.service</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp docker.service root@$&#123;node_ip&#125;:/etc/systemd/system/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="配置和分发-docker-配置文件"><a href="#配置和分发-docker-配置文件" class="headerlink" title="配置和分发 docker 配置文件"></a>配置和分发 docker 配置文件</h2><p>使用国内的仓库镜像服务器以加快 pull image 的速度，同时增加下载的并发数 (需要重启 dockerd 生效)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; docker-daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;],</span><br><span class="line">    &quot;insecure-registries&quot;: [&quot;docker02:35000&quot;],</span><br><span class="line">    &quot;max-concurrent-downloads&quot;: 20,</span><br><span class="line">    &quot;live-restore&quot;: true,</span><br><span class="line">    &quot;max-concurrent-uploads&quot;: 10,</span><br><span class="line">    &quot;debug&quot;: true,</span><br><span class="line">    &quot;data-root&quot;: &quot;$&#123;DOCKER_DIR&#125;/data&quot;,</span><br><span class="line">    &quot;exec-root&quot;: &quot;$&#123;DOCKER_DIR&#125;/exec&quot;,</span><br><span class="line">    &quot;log-opts&quot;: &#123;</span><br><span class="line">      &quot;max-size&quot;: &quot;100m&quot;,</span><br><span class="line">      &quot;max-file&quot;: &quot;5&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发 docker 配置文件到所有 worker 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p  /etc/docker/ $&#123;DOCKER_DIR&#125;/&#123;data,exec&#125;&quot;</span><br><span class="line">    scp docker-daemon.json root@$&#123;node_ip&#125;:/etc/docker/daemon.json</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="启动-docker-服务"><a href="#启动-docker-服务" class="headerlink" title="启动 docker 服务"></a>启动 docker 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="检查服务运行状态"><a href="#检查服务运行状态" class="headerlink" title="检查服务运行状态"></a>检查服务运行状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status docker|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code></p><h1 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h1><h2 id="创建-kubelet-bootstrap-kubeconfig-文件"><a href="#创建-kubelet-bootstrap-kubeconfig-文件" class="headerlink" title="创建 kubelet bootstrap kubeconfig 文件"></a>创建 kubelet bootstrap kubeconfig 文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_name in $&#123;NODE_NAMES[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;</span><br><span class="line"></span><br><span class="line">    # 创建 token</span><br><span class="line">    export BOOTSTRAP_TOKEN=$(kubeadm token create \</span><br><span class="line">      --description kubelet-bootstrap-token \</span><br><span class="line">      --groups system:bootstrappers:$&#123;node_name&#125; \</span><br><span class="line">      --kubeconfig ~/.kube/config)</span><br><span class="line"></span><br><span class="line">    # 设置集群参数</span><br><span class="line">    kubectl config set-cluster kubernetes \</span><br><span class="line">      --certificate-authority=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">      --embed-certs=true \</span><br><span class="line">      --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig</span><br><span class="line"></span><br><span class="line">    # 设置客户端认证参数</span><br><span class="line">    kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">      --token=$&#123;BOOTSTRAP_TOKEN&#125; \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig</span><br><span class="line"></span><br><span class="line">    # 设置上下文参数</span><br><span class="line">    kubectl config set-context default \</span><br><span class="line">      --cluster=kubernetes \</span><br><span class="line">      --user=kubelet-bootstrap \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig</span><br><span class="line"></span><br><span class="line">    # 设置默认上下文</span><br><span class="line">    kubectl config use-context default --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>向 kubeconfig 写入的是 token，bootstrap 结束后 kube-controller-manager 为 kubelet 创建 client 和 server 证书；</li></ul><p>查看 kubeadm 为各节点创建的 token： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubeadm token list --kubeconfig ~/.kube/config</span><br><span class="line">TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION               EXTRA GROUPS</span><br><span class="line">kp5seh.klhbcowm40rkaoh1   &lt;invalid&gt;   2019-06-05T15:24:51+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node1</span><br><span class="line">u2zt2n.3tqw704a4ndqdj1k   &lt;invalid&gt;   2019-06-05T15:24:51+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node2</span><br><span class="line">[root@node1 ~]#</span><br></pre></td></tr></table></figure><h2 id="分发-bootstrap-kubeconfig-文件到所有-worker-节点"><a href="#分发-bootstrap-kubeconfig-文件到所有-worker-节点" class="headerlink" title="分发 bootstrap kubeconfig 文件到所有 worker 节点"></a>分发 bootstrap kubeconfig 文件到所有 worker 节点</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_name in $&#123;NODE_NAMES[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;</span><br><span class="line">    scp kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig root@$&#123;node_name&#125;:/etc/kubernetes/kubelet-bootstrap.kubeconfig</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kubelet-参数配置文件"><a href="#创建和分发-kubelet-参数配置文件" class="headerlink" title="创建和分发 kubelet 参数配置文件"></a>创建和分发 kubelet 参数配置文件</h2><p>创建 kubelet 参数配置文件模板（可配置项参考<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/config/types.go" target="_blank" rel="noopener">代码中注释</a> ）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kubelet-config.yaml.template &lt;&lt;EOF</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">address: &quot;##NODE_IP##&quot;</span><br><span class="line">staticPodPath: &quot;&quot;</span><br><span class="line">syncFrequency: 1m</span><br><span class="line">fileCheckFrequency: 20s</span><br><span class="line">httpCheckFrequency: 20s</span><br><span class="line">staticPodURL: &quot;&quot;</span><br><span class="line">port: 10250</span><br><span class="line">readOnlyPort: 0</span><br><span class="line">rotateCertificates: true</span><br><span class="line">serverTLSBootstrap: true</span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: false</span><br><span class="line">  webhook:</span><br><span class="line">    enabled: true</span><br><span class="line">  x509:</span><br><span class="line">    clientCAFile: &quot;/etc/kubernetes/cert/ca.pem&quot;</span><br><span class="line">authorization:</span><br><span class="line">  mode: Webhook</span><br><span class="line">registryPullQPS: 0</span><br><span class="line">registryBurst: 20</span><br><span class="line">eventRecordQPS: 0</span><br><span class="line">eventBurst: 20</span><br><span class="line">enableDebuggingHandlers: true</span><br><span class="line">enableContentionProfiling: true</span><br><span class="line">healthzPort: 10248</span><br><span class="line">healthzBindAddress: &quot;##NODE_IP##&quot;</span><br><span class="line">clusterDomain: &quot;$&#123;CLUSTER_DNS_DOMAIN&#125;&quot;</span><br><span class="line">clusterDNS:</span><br><span class="line">  - &quot;$&#123;CLUSTER_DNS_SVC_IP&#125;&quot;</span><br><span class="line">nodeStatusUpdateFrequency: 10s</span><br><span class="line">nodeStatusReportFrequency: 1m</span><br><span class="line">imageMinimumGCAge: 2m</span><br><span class="line">imageGCHighThresholdPercent: 85</span><br><span class="line">imageGCLowThresholdPercent: 80</span><br><span class="line">volumeStatsAggPeriod: 1m</span><br><span class="line">kubeletCgroups: &quot;&quot;</span><br><span class="line">systemCgroups: &quot;&quot;</span><br><span class="line">cgroupRoot: &quot;&quot;</span><br><span class="line">cgroupsPerQOS: true</span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">runtimeRequestTimeout: 10m</span><br><span class="line">hairpinMode: promiscuous-bridge</span><br><span class="line">maxPods: 220</span><br><span class="line">podCIDR: &quot;$&#123;CLUSTER_CIDR&#125;&quot;</span><br><span class="line">podPidsLimit: -1</span><br><span class="line">resolvConf: /etc/resolv.conf</span><br><span class="line">maxOpenFiles: 1000000</span><br><span class="line">kubeAPIQPS: 1000</span><br><span class="line">kubeAPIBurst: 2000</span><br><span class="line">serializeImagePulls: false</span><br><span class="line">evictionHard:</span><br><span class="line">  memory.available:  &quot;100Mi&quot;</span><br><span class="line">nodefs.available:  &quot;10%&quot;</span><br><span class="line">nodefs.inodesFree: &quot;5%&quot;</span><br><span class="line">imagefs.available: &quot;15%&quot;</span><br><span class="line">evictionSoft: &#123;&#125;</span><br><span class="line">enableControllerAttachDetach: true</span><br><span class="line">failSwapOn: true</span><br><span class="line">containerLogMaxSize: 20Mi</span><br><span class="line">containerLogMaxFiles: 10</span><br><span class="line">systemReserved: &#123;&#125;</span><br><span class="line">kubeReserved: &#123;&#125;</span><br><span class="line">systemReservedCgroup: &quot;&quot;</span><br><span class="line">kubeReservedCgroup: &quot;&quot;</span><br><span class="line">enforceNodeAllocatable: [&quot;pods&quot;]</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>address：kubelet 安全端口（https，10250）监听的地址，不能为 127.0.0.1，否则 kube-apiserver、heapster 等不能调用 kubelet 的 API；</li><li>readOnlyPort=0：关闭只读端口(默认 10255)，等效为未指定；</li><li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li><li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTP 证书认证；</li><li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li><li>对于未通过 x509 证书和 webhook 认证的请求(kube-apiserver 或其他客户端)，将被拒绝，提示 Unauthorized；</li><li>authroization.mode=Webhook：kubelet 使用 SubjectAccessReview API 查询 kube-apiserver 某 user、group 是否具有操作资源的权限(RBAC)；</li><li>featureGates.RotateKubeletClientCertificate、featureGates.RotateKubeletServerCertificate：自动 rotate 证书，证书的有效期取决于 kube-controller-manager 的 –experimental-cluster-signing-duration 参数；</li><li>需要 root 账户运行；</li></ul><p>为各节点创建和分发 kubelet 配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do </span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    sed -e &quot;s/##NODE_IP##/$&#123;node_ip&#125;/&quot; kubelet-config.yaml.template &gt; kubelet-config-$&#123;node_ip&#125;.yaml.template</span><br><span class="line">    scp kubelet-config-$&#123;node_ip&#125;.yaml.template root@$&#123;node_ip&#125;:/etc/kubernetes/kubelet-config.yaml</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kubelet-systemd-unit-文件"><a href="#创建和分发-kubelet-systemd-unit-文件" class="headerlink" title="创建和分发 kubelet systemd unit 文件"></a>创建和分发 kubelet systemd unit 文件</h2><p>创建 kubelet systemd unit 文件模板：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kubelet.service.template &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=$&#123;K8S_DIR&#125;/kubelet</span><br><span class="line">ExecStart=/opt/k8s/bin/kubelet \\</span><br><span class="line">  --allow-privileged=true \\</span><br><span class="line">  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \\</span><br><span class="line">  --cert-dir=/etc/kubernetes/cert \\</span><br><span class="line">  --cni-conf-dir=/etc/cni/net.d \\</span><br><span class="line">  --container-runtime=docker \\</span><br><span class="line">  --container-runtime-endpoint=unix:///var/run/dockershim.sock \\</span><br><span class="line">  --root-dir=$&#123;K8S_DIR&#125;/kubelet \\</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\</span><br><span class="line">  --config=/etc/kubernetes/kubelet-config.yaml \\</span><br><span class="line">  --hostname-override=##NODE_NAME## \\</span><br><span class="line">  --pod-infra-container-image=registry.cn-beijing.aliyuncs.com/k8s_images/pause-amd64:3.1 \\</span><br><span class="line">  --image-pull-progress-deadline=15m \\</span><br><span class="line">  --volume-plugin-dir=$&#123;K8S_DIR&#125;/kubelet/kubelet-plugins/volume/exec/ \\</span><br><span class="line">  --logtostderr=true \\</span><br><span class="line">  --v=2</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>如果设置了 <code>--hostname-override</code> 选项，则 <code>kube-proxy</code> 也需要设置该选项，否则会出现找不到 Node 的情况；</li><li><code>--bootstrap-kubeconfig</code>：指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；</li><li>K8S approve kubelet 的 csr 请求后，在 <code>--cert-dir</code> 目录创建证书和私钥文件，然后写入 <code>--kubeconfig</code> 文件；</li><li><code>--pod-infra-container-image</code> 不使用 redhat 的 <code>pod-infrastructure:latest</code> 镜像，它不能回收容器的僵尸；</li></ul><p>为各节点创建和分发 kubelet systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_name in $&#123;NODE_NAMES[@]&#125;</span><br><span class="line">  do </span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;node_name&#125;/&quot; kubelet.service.template &gt; kubelet-$&#123;node_name&#125;.service</span><br><span class="line">    scp kubelet-$&#123;node_name&#125;.service root@$&#123;node_name&#125;:/etc/systemd/system/kubelet.service</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="Bootstrap-Token-Auth-和授予权限"><a href="#Bootstrap-Token-Auth-和授予权限" class="headerlink" title="Bootstrap Token Auth 和授予权限"></a>Bootstrap Token Auth 和授予权限</h2><p>kubelet 启动时查找 <code>--kubeletconfig</code> 参数对应的文件是否存在，如果不存在则使用 <code>--bootstrap-kubeconfig</code> 指定的 kubeconfig 文件向 kube-apiserver 发送证书签名请求 (CSR)。</p><p>kube-apiserver 收到 CSR 请求后，对其中的 Token 进行认证，认证通过后将请求的 user 设置为 <code>system:bootstrap:&lt;Token ID&gt;</code>，group 设置为 <code>system:bootstrappers</code>，这一过程称为 Bootstrap Token Auth。</p><p>如果说kubelet启动失败的话：</p><p>创建一个 clusterrolebinding，将 group system:bootstrappers 和 clusterrole system:node-bootstrapper 绑定：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers</span><br></pre></td></tr></table></figure><h2 id="启动-kubelet-服务"><a href="#启动-kubelet-服务" class="headerlink" title="启动 kubelet 服务"></a>启动 kubelet 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kubelet/kubelet-plugins/volume/exec/&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;/usr/sbin/swapoff -a&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>kubelet 启动后使用 –bootstrap-kubeconfig 向 kube-apiserver 发送 CSR 请求，当这个 CSR 被 approve 后，kube-controller-manager 为 kubelet 创建 TLS 客户端证书、私钥和 –kubeletconfig 文件。</p><p>注意：kube-controller-manager 需要配置 <code>--cluster-signing-cert-file</code> 和 <code>--cluster-signing-key-file</code>参数，才会为 TLS Bootstrap 创建证书和私钥。</p><h2 id="自动-approve-CSR-请求"><a href="#自动-approve-CSR-请求" class="headerlink" title="自动 approve CSR 请求"></a>自动 approve CSR 请求</h2><p>创建三个 ClusterRoleBinding，分别用于自动 approve client、renew client、renew server 证书：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; csr-crb.yaml &lt;&lt;EOF</span><br><span class="line"> # Approve all CSRs for the group &quot;system:bootstrappers&quot;</span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: auto-approve-csrs-for-group</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:bootstrappers</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: system:certificates.k8s.io:certificatesigningrequests:nodeclient</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line"> # To let a node of the group &quot;system:nodes&quot; renew its own credentials</span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: node-client-cert-renewal</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:nodes</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><br><span class="line"># serving cert matching its client cert.</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: approve-node-server-renewal-csr</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;certificates.k8s.io&quot;]</span><br><span class="line">  resources: [&quot;certificatesigningrequests/selfnodeserver&quot;]</span><br><span class="line">  verbs: [&quot;create&quot;]</span><br><span class="line">---</span><br><span class="line"> # To let a node of the group &quot;system:nodes&quot; renew its own server credentials</span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: node-server-cert-renewal</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:nodes</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: approve-node-server-renewal-csr</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">EOF</span><br><span class="line">kubectl apply -f csr-crb.yaml</span><br></pre></td></tr></table></figure><ul><li>auto-approve-csrs-for-group：自动 approve node 的第一次 CSR； 注意第一次 CSR 时，请求的 Group 为 system:bootstrappers；</li><li>node-client-cert-renewal：自动 approve node 后续过期的 client 证书，自动生成的证书 Group 为 system:nodes;</li><li>node-server-cert-renewal：自动 approve node 后续过期的 server 证书，自动生成的证书 Group 为 system:nodes;</li></ul><h2 id="手动-approve-server-cert-csr"><a href="#手动-approve-server-cert-csr" class="headerlink" title="手动 approve server cert csr"></a>手动 approve server cert csr</h2><p>基于<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#kubelet-configuration" target="_blank" rel="noopener">安全性考虑</a>，CSR approving controllers 不会自动 approve kubelet server 证书签名请求，需要手动 approve：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get csr</span><br><span class="line">NAME        AGE     REQUESTOR                    CONDITION</span><br><span class="line">csr-5f4vh   9m25s   system:bootstrap:82jfrm      Approved,Issued</span><br><span class="line">csr-5r7j7   6m11s   system:node:zhangjun-k8s03   Pending</span><br><span class="line">csr-5rw7s   9m23s   system:bootstrap:b1f7np      Approved,Issued</span><br><span class="line">csr-9snww   8m3s    system:bootstrap:82jfrm      Approved,Issued</span><br><span class="line">csr-c7z56   6m12s   system:node:zhangjun-k8s02   Pending</span><br><span class="line">csr-j55lh   6m12s   system:node:zhangjun-k8s01   Pending</span><br><span class="line">csr-m29fm   9m25s   system:bootstrap:3gzd53      Approved,Issued</span><br><span class="line">csr-rc8w7   8m3s    system:bootstrap:3gzd53      Approved,Issued</span><br><span class="line">csr-vd52r   8m2s    system:bootstrap:b1f7np      Approved,Issued</span><br><span class="line"></span><br><span class="line">$ kubectl certificate approve csr-5r7j7</span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-5r7j7 approved</span><br><span class="line"></span><br><span class="line">$ kubectl certificate approve csr-c7z56</span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-c7z56 approved</span><br><span class="line"></span><br><span class="line">$ kubectl certificate approve csr-j55lh</span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-j55lh approved</span><br></pre></td></tr></table></figure><h2 id="kubelet-提供的-API-接口"><a href="#kubelet-提供的-API-接口" class="headerlink" title="kubelet 提供的 API 接口"></a>kubelet 提供的 API 接口</h2><ul><li>10248: healthz http 服务；</li><li>10250: https 服务，访问该端口时需要认证和授权（即使访问 /healthz 也需要）；</li><li>未开启只读端口 10255；</li><li>从 K8S v1.10 开始，去除了 <code>--cadvisor-port</code> 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp; API。</li></ul><p>例如执行 <code>kubectl exec -it nginx-ds-5rmws -- sh</code> 命令时，kube-apiserver 会向 kubelet 发送如下请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">POST /exec/default/nginx-ds-5rmws/my-nginx?command=sh&amp;input=1&amp;output=1&amp;tty=1</span><br></pre></td></tr></table></figure><p>kubelet 接收 10250 端口的 https 请求，可以访问如下资源：</p><ul><li>/pods、/runningpods</li><li>/metrics、/metrics/cadvisor、/metrics/probes</li><li>/spec</li><li>/stats、/stats/container</li><li>/logs</li><li>/run/、/exec/, /attach/, /portForward/, /containerLogs/</li></ul><p>详情参考：<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3</a></p><p>由于关闭了匿名认证，同时开启了 webhook 授权，所有访问 10250 端口 https API 的请求都需要被认证和授权。</p><p>预定义的 ClusterRole system:kubelet-api-admin 授予访问 kubelet 所有 API 的权限(kube-apiserver 使用的 kubernetes 证书 User 授予了该权限)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe clusterrole system:kubelet-api-admin</span><br><span class="line">Name:         system:kubelet-api-admin</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate=true</span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources      Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------      -----------------  --------------  -----</span><br><span class="line">  nodes          []                 []              [get list watch proxy]</span><br><span class="line">  nodes/log      []                 []              [*]</span><br><span class="line">  nodes/metrics  []                 []              [*]</span><br><span class="line">  nodes/proxy    []                 []              [*]</span><br><span class="line">  nodes/spec     []                 []              [*]</span><br><span class="line">  nodes/stats    []                 []              [*]</span><br></pre></td></tr></table></figure><h2 id="kubelet-api-认证和授权"><a href="#kubelet-api-认证和授权" class="headerlink" title="kubelet api 认证和授权"></a>kubelet api 认证和授权</h2><p>kubelet 配置了如下认证参数：</p><ul><li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li><li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTPs 证书认证；</li><li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li></ul><p>同时配置了如下授权参数：</p><ul><li>authroization.mode=Webhook：开启 RBAC 授权；</li></ul><p>kubelet 收到请求后，使用 clientCAFile 对证书签名进行认证，或者查询 bearer token 是否有效。如果两者都没通过，则拒绝请求，提示 Unauthorized：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ curl -s --cacert /etc/kubernetes/cert/ca.pem https://192.168.6.101:10250/metrics</span><br><span class="line">Unauthorized</span><br><span class="line"></span><br><span class="line">$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer 123456&quot; https://192.168.6.101:10250/metrics</span><br><span class="line">Unauthorized</span><br></pre></td></tr></table></figure><p>通过认证后，kubelet 使用 SubjectAccessReview API 向 kube-apiserver 发送请求，查询证书或 token 对应的 user、group 是否有操作资源的权限(RBAC)；</p><h3 id="证书认证和授权"><a href="#证书认证和授权" class="headerlink" title="证书认证和授权"></a>证书认证和授权</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ # 权限不足的证书；</span><br><span class="line">$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /etc/kubernetes/cert/kube-controller-manager.pem --key /etc/kubernetes/cert/kube-controller-manager-key.pem https://192.168.6.101:10250/metrics</span><br><span class="line">Forbidden (user=system:kube-controller-manager, verb=get, resource=nodes, subresource=metrics)</span><br><span class="line"></span><br><span class="line">$ # 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；</span><br><span class="line">$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://192.168.6.101:10250/metrics|head</span><br><span class="line"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span><br><span class="line"># TYPE apiserver_audit_event_total counter</span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span><br><span class="line"># TYPE apiserver_audit_requests_rejected_total counter</span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span><br><span class="line"># TYPE apiserver_client_certificate_expiration_seconds histogram</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;0&quot;&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;1800&quot;&#125; 0</span><br></pre></td></tr></table></figure><ul><li><code>--cacert</code>、<code>--cert</code>、<code>--key</code> 的参数值必须是文件路径，如上面的 <code>./admin.pem</code> 不能省略 <code>./</code>，否则返回 <code>401 Unauthorized</code>；</li></ul><h3 id="bear-token-认证和授权"><a href="#bear-token-认证和授权" class="headerlink" title="bear token 认证和授权"></a>bear token 认证和授权</h3><p>创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">kubectl create sa kubelet-api-test</span><br><span class="line">kubectl create clusterrolebinding kubelet-api-test --clusterrole=system:kubelet-api-admin --serviceaccount=default:kubelet-api-test</span><br><span class="line">SECRET=$(kubectl get secrets | grep kubelet-api-test | awk &apos;&#123;print $1&#125;&apos;)</span><br><span class="line">TOKEN=$(kubectl describe secret $&#123;SECRET&#125; | grep -E &apos;^token&apos; | awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">echo $&#123;TOKEN&#125;</span><br><span class="line">$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer $&#123;TOKEN&#125;&quot; https://192.168.6.101:10250/metrics|head</span><br><span class="line"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span><br><span class="line"># TYPE apiserver_audit_event_total counter</span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span><br><span class="line"># TYPE apiserver_audit_requests_rejected_total counter</span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span><br><span class="line"># TYPE apiserver_client_certificate_expiration_seconds histogram</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;0&quot;&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;1800&quot;&#125; 0</span><br></pre></td></tr></table></figure><h1 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h1><h2 id="创建-kube-proxy-证书"><a href="#创建-kube-proxy-证书" class="headerlink" title="创建 kube-proxy 证书"></a>创建 kube-proxy 证书</h2><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-proxy-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;system:kube-proxy&quot;,</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>CN：指定该证书的 User 为 <code>system:kube-proxy</code>；</li><li>预定义的 RoleBinding <code>system:node-proxier</code> 将User <code>system:kube-proxy</code> 与 Role <code>system:node-proxier</code> 绑定，该 Role 授予了调用 <code>kube-apiserver</code> Proxy 相关 API 的权限；</li><li>该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空；</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy</span><br><span class="line">ls kube-proxy*</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kubeconfig-文件"><a href="#创建和分发-kubeconfig-文件" class="headerlink" title="创建和分发 kubeconfig 文件"></a>创建和分发 kubeconfig 文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/opt/k8s/work/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials kube-proxy \</span><br><span class="line">  --client-certificate=kube-proxy.pem \</span><br><span class="line">  --client-key=kube-proxy-key.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kube-proxy \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure><ul><li><code>--embed-certs=true</code>：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl-proxy.kubeconfig 文件中(不加时，写入的是证书文件路径)；</li></ul><p>分发 kubeconfig 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_name in $&#123;NODE_NAMES[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;</span><br><span class="line">    scp kube-proxy.kubeconfig root@$&#123;node_name&#125;:/etc/kubernetes/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-kube-proxy-配置文件"><a href="#创建-kube-proxy-配置文件" class="headerlink" title="创建 kube-proxy 配置文件"></a>创建 kube-proxy 配置文件</h2><p>从 v1.10 开始，kube-proxy <strong>部分参数</strong>可以配置文件中配置。可以使用 <code>--write-config-to</code> 选项生成该配置文件，或者参考 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.14/pkg/proxy/apis/config/types.go" target="_blank" rel="noopener">源代码的注释</a>。</p><p>创建 kube-proxy config 文件模板：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-proxy-config.yaml.template &lt;&lt;EOF</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">clientConnection:</span><br><span class="line">  burst: 200</span><br><span class="line">  kubeconfig: &quot;/etc/kubernetes/kube-proxy.kubeconfig&quot;</span><br><span class="line">  qps: 100</span><br><span class="line">bindAddress: ##NODE_IP##</span><br><span class="line">healthzBindAddress: ##NODE_IP##:10256</span><br><span class="line">metricsBindAddress: ##NODE_IP##:10249</span><br><span class="line">enableProfiling: true</span><br><span class="line">clusterCIDR: $&#123;CLUSTER_CIDR&#125;</span><br><span class="line">hostnameOverride: ##NODE_NAME##</span><br><span class="line">mode: &quot;ipvs&quot;</span><br><span class="line">portRange: &quot;&quot;</span><br><span class="line">kubeProxyIPTablesConfiguration:</span><br><span class="line">  masqueradeAll: false</span><br><span class="line">kubeProxyIPVSConfiguration:</span><br><span class="line">  scheduler: rr</span><br><span class="line">  excludeCIDRs: []</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>bindAddress</code>: 监听地址；</li><li><code>clientConnection.kubeconfig</code>: 连接 apiserver 的 kubeconfig 文件；</li><li><code>clusterCIDR</code>: kube-proxy 根据 <code>--cluster-cidr</code> 判断集群内部和外部流量，指定 <code>--cluster-cidr</code> 或 <code>--masquerade-all</code> 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；</li><li><code>hostnameOverride</code>: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则；</li><li><code>mode</code>: 使用 ipvs 模式；</li></ul><p>为各节点创建和分发 kube-proxy 配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 3; i++ ))</span><br><span class="line">  do </span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;NODE_NAMES[i]&#125;&quot;</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-proxy-config.yaml.template &gt; kube-proxy-config-$&#123;NODE_NAMES[i]&#125;.yaml.template</span><br><span class="line">    scp kube-proxy-config-$&#123;NODE_NAMES[i]&#125;.yaml.template root@$&#123;NODE_NAMES[i]&#125;:/etc/kubernetes/kube-proxy-config.yaml</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kube-proxy-systemd-unit-文件"><a href="#创建和分发-kube-proxy-systemd-unit-文件" class="headerlink" title="创建和分发 kube-proxy systemd unit 文件"></a>创建和分发 kube-proxy systemd unit 文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kube-proxy.service &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kube-Proxy Server</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=$&#123;K8S_DIR&#125;/kube-proxy</span><br><span class="line">ExecStart=/opt/k8s/bin/kube-proxy \\</span><br><span class="line">  --config=/etc/kubernetes/kube-proxy-config.yaml \\</span><br><span class="line">  --logtostderr=true \\</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发 kube-proxy systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_name in $&#123;NODE_NAMES[@]&#125;</span><br><span class="line">  do </span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;</span><br><span class="line">    scp kube-proxy.service root@$&#123;node_name&#125;:/etc/systemd/system/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="启动-kube-proxy-服务"><a href="#启动-kube-proxy-服务" class="headerlink" title="启动 kube-proxy 服务"></a>启动 kube-proxy 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-proxy&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;modprobe ip_vs_rr&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>启动服务前必须先创建工作目录；</li></ul><h2 id="检查启动结果"><a href="#检查启动结果" class="headerlink" title="检查启动结果"></a>检查启动结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-proxy|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code></p><h2 id="查看-ipvs-路由规则"><a href="#查看-ipvs-路由规则" class="headerlink" title="查看 ipvs 路由规则"></a>查看 ipvs 路由规则</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;/usr/sbin/ipvsadm -ln&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;安装依赖包&quot;&gt;&lt;a href=&quot;#安装依赖包&quot; class=&quot;headerlink&quot; title=&quot;安装依赖包&quot;&gt;&lt;/a&gt;安装依赖包&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-controller、schedule</title>
    <link href="https://shenshengkun.github.io/posts/544ccaa2.html"/>
    <id>https://shenshengkun.github.io/posts/544ccaa2.html</id>
    <published>2019-06-05T07:16:01.000Z</published>
    <updated>2019-06-05T08:52:13.176Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kube-controller-manager-集群"><a href="#kube-controller-manager-集群" class="headerlink" title="kube-controller-manager 集群"></a>kube-controller-manager 集群</h1><h2 id="创建-kube-controller-manager-证书和私钥"><a href="#创建-kube-controller-manager-证书和私钥" class="headerlink" title="创建 kube-controller-manager 证书和私钥"></a>创建 kube-controller-manager 证书和私钥</h2><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-controller-manager-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">      &quot;127.0.0.1&quot;,</span><br><span class="line">      &quot;192.168.6.101&quot;,</span><br><span class="line">      &quot;192.168.6.102&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">        &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">        &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">        &quot;O&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class="line">        &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>hosts 列表包含<strong>所有</strong> kube-controller-manager 节点 IP；</li><li>CN 和 O 均为 <code>system:kube-controller-manager</code>，kubernetes 内置的 ClusterRoleBindings <code>system:kube-controller-manager</code> 赋予 kube-controller-manager 工作所需的权限。</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager</span><br></pre></td></tr></table></figure><p>将生成的证书和私钥分发到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-controller-manager*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kubeconfig-文件"><a href="#创建和分发-kubeconfig-文件" class="headerlink" title="创建和分发 kubeconfig 文件"></a>创建和分发 kubeconfig 文件</h2><p>kube-controller-manager 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-controller-manager 证书：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/opt/k8s/work/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials system:kube-controller-manager \</span><br><span class="line">  --client-certificate=kube-controller-manager.pem \</span><br><span class="line">  --client-key=kube-controller-manager-key.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-context system:kube-controller-manager \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=system:kube-controller-manager \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig</span><br></pre></td></tr></table></figure><p>分发 kubeconfig 到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-controller-manager.kubeconfig root@$&#123;node_ip&#125;:/etc/kubernetes/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-kube-controller-manager-systemd-unit-模板文件"><a href="#创建-kube-controller-manager-systemd-unit-模板文件" class="headerlink" title="创建 kube-controller-manager systemd unit 模板文件"></a>创建 kube-controller-manager systemd unit 模板文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kube-controller-manager.service.template &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=$&#123;K8S_DIR&#125;/kube-controller-manager</span><br><span class="line">ExecStart=/opt/k8s/bin/kube-controller-manager \\</span><br><span class="line">  --profiling \\</span><br><span class="line">  --cluster-name=kubernetes \\</span><br><span class="line">  --controllers=*,bootstrapsigner,tokencleaner \\</span><br><span class="line">  --kube-api-qps=1000 \\</span><br><span class="line">  --kube-api-burst=2000 \\</span><br><span class="line">  --leader-elect \\</span><br><span class="line">  --use-service-account-credentials\\</span><br><span class="line">  --concurrent-service-syncs=2 \\</span><br><span class="line">  --bind-address=##NODE_IP## \\</span><br><span class="line">  --secure-port=10252 \\</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/cert/kube-controller-manager.pem \\</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/cert/kube-controller-manager-key.pem \\</span><br><span class="line">  --port=0 \\</span><br><span class="line">  --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\</span><br><span class="line">  --client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --requestheader-allowed-names=&quot;&quot; \\</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \\</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \\</span><br><span class="line">  --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\</span><br><span class="line">  --cluster-signing-cert-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --cluster-signing-key-file=/etc/kubernetes/cert/ca-key.pem \\</span><br><span class="line">  --experimental-cluster-signing-duration=8760h \\</span><br><span class="line">  --horizontal-pod-autoscaler-sync-period=10s \\</span><br><span class="line">  --concurrent-deployment-syncs=10 \\</span><br><span class="line">  --concurrent-gc-syncs=30 \\</span><br><span class="line">  --node-cidr-mask-size=24 \\</span><br><span class="line">  --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\</span><br><span class="line">  --pod-eviction-timeout=6m \\</span><br><span class="line">  --terminated-pod-gc-threshold=10000 \\</span><br><span class="line">  --root-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --service-account-private-key-file=/etc/kubernetes/cert/ca-key.pem \\</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\</span><br><span class="line">  --logtostderr=true \\</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>--port=0</code>：关闭监听非安全端口（http），同时 <code>--address</code> 参数无效，<code>--bind-address</code> 参数有效；</li><li><code>--secure-port=10252</code>、<code>--bind-address=0.0.0.0</code>: 在所有网络接口监听 10252 端口的 https /metrics 请求；</li><li><code>--kubeconfig</code>：指定 kubeconfig 文件路径，kube-controller-manager 使用它连接和验证 kube-apiserver；</li><li><code>--authentication-kubeconfig</code> 和 <code>--authorization-kubeconfig</code>：kube-controller-manager 使用它连接 apiserver，对 client 的请求进行认证和授权。<code>kube-controller-manager</code> 不再使用 <code>--tls-ca-file</code>对请求 https metrics 的 Client 证书进行校验。如果没有配置这两个 kubeconfig 参数，则 client 连接 kube-controller-manager https 端口的请求会被拒绝(提示权限不足)。</li><li><code>--cluster-signing-*-file</code>：签名 TLS Bootstrap 创建的证书；</li><li><code>--experimental-cluster-signing-duration</code>：指定 TLS Bootstrap 证书的有效期；</li><li><code>--root-ca-file</code>：放置到容器 ServiceAccount 中的 CA 证书，用来对 kube-apiserver 的证书进行校验；</li><li><code>--service-account-private-key-file</code>：签名 ServiceAccount 中 Token 的私钥文件，必须和 kube-apiserver 的 <code>--service-account-key-file</code> 指定的公钥文件配对使用；</li><li><code>--service-cluster-ip-range</code> ：指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致；</li><li><code>--leader-elect=true</code>：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li><li><code>--controllers=*,bootstrapsigner,tokencleaner</code>：启用的控制器列表，tokencleaner 用于自动清理过期的 Bootstrap token；</li><li><code>--horizontal-pod-autoscaler-*</code>：custom metrics 相关参数，支持 autoscaling/v2alpha1；</li><li><code>--tls-cert-file</code>、<code>--tls-private-key-file</code>：使用 https 输出 metrics 时使用的 Server 证书和秘钥；</li><li><code>--use-service-account-credentials=true</code>: kube-controller-manager 中各 controller 使用 serviceaccount 访问 kube-apiserver；</li></ul><h2 id="创建和分发-kube-controller-mananger-systemd-unit-文件"><a href="#创建和分发-kube-controller-mananger-systemd-unit-文件" class="headerlink" title="创建和分发 kube-controller-mananger systemd unit 文件"></a>创建和分发 kube-controller-mananger systemd unit 文件</h2><p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 2; i++ ))</span><br><span class="line">  do</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-controller-manager.service.template &gt; kube-controller-manager-$&#123;NODE_IPS[i]&#125;.service </span><br><span class="line">  done</span><br><span class="line">ls kube-controller-manager*.service</span><br></pre></td></tr></table></figure><ul><li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li></ul><p>分发到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-controller-manager-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-controller-manager.service</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>文件重命名为 kube-controller-manager.service;</li></ul><h2 id="启动-kube-controller-manager-服务"><a href="#启动-kube-controller-manager-服务" class="headerlink" title="启动 kube-controller-manager 服务"></a>启动 kube-controller-manager 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-controller-manager&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl restart kube-controller-manager&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>启动服务前必须先创建工作目录；</li></ul><h2 id="检查服务运行状态"><a href="#检查服务运行状态" class="headerlink" title="检查服务运行状态"></a>检查服务运行状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-controller-manager|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -u kube-controller-manager</span><br></pre></td></tr></table></figure><h2 id="查看当前的-leader"><a href="#查看当前的-leader" class="headerlink" title="查看当前的 leader"></a>查看当前的 leader</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    control-plane.alpha.kubernetes.io/leader: &apos;&#123;&quot;holderIdentity&quot;:&quot;node1_3e3a8815-8698-11e9-87d5-005056b16e40&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2019-06-04T07:14:15Z&quot;,&quot;renewTime&quot;:&quot;2019-06-05T07:22:40Z&quot;,&quot;leaderTransitions&quot;:2&#125;&apos;</span><br><span class="line">  creationTimestamp: &quot;2019-06-04T07:00:39Z&quot;</span><br><span class="line">  name: kube-controller-manager</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: &quot;124731&quot;</span><br><span class="line">  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager</span><br><span class="line">  uid: 75e30eec-8696-11e9-b371-005056b1d2de</span><br></pre></td></tr></table></figure><h1 id="kube-scheduler-集群"><a href="#kube-scheduler-集群" class="headerlink" title="kube-scheduler 集群"></a>kube-scheduler 集群</h1><h2 id="创建-kube-scheduler-证书和私钥"><a href="#创建-kube-scheduler-证书和私钥" class="headerlink" title="创建 kube-scheduler 证书和私钥"></a>创建 kube-scheduler 证书和私钥</h2><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-scheduler-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;system:kube-scheduler&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">      &quot;127.0.0.1&quot;,</span><br><span class="line">      &quot;192.168.6.101&quot;,</span><br><span class="line">      &quot;192.168.6.102&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">        &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">        &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">        &quot;O&quot;: &quot;system:kube-scheduler&quot;,</span><br><span class="line">        &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>hosts 列表包含<strong>所有</strong> kube-scheduler 节点 IP；</li><li>CN 和 O 均为 <code>system:kube-scheduler</code>，kubernetes 内置的 ClusterRoleBindings <code>system:kube-scheduler</code> 将赋予 kube-scheduler 工作所需的权限；</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler</span><br><span class="line">ls kube-scheduler*pem</span><br></pre></td></tr></table></figure><p>将生成的证书和私钥分发到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-scheduler*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kubeconfig-文件-1"><a href="#创建和分发-kubeconfig-文件-1" class="headerlink" title="创建和分发 kubeconfig 文件"></a>创建和分发 kubeconfig 文件</h2><p>kube-scheduler 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-scheduler 证书：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/opt/k8s/work/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials system:kube-scheduler \</span><br><span class="line">  --client-certificate=kube-scheduler.pem \</span><br><span class="line">  --client-key=kube-scheduler-key.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-context system:kube-scheduler \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=system:kube-scheduler \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig</span><br></pre></td></tr></table></figure><p>分发 kubeconfig 到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-scheduler.kubeconfig root@$&#123;node_ip&#125;:/etc/kubernetes/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-kube-scheduler-配置文件"><a href="#创建-kube-scheduler-配置文件" class="headerlink" title="创建 kube-scheduler 配置文件"></a>创建 kube-scheduler 配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt;kube-scheduler.yaml.template &lt;&lt;EOF</span><br><span class="line">apiVersion: kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeSchedulerConfiguration</span><br><span class="line">bindTimeoutSeconds: 600</span><br><span class="line">clientConnection:</span><br><span class="line">  burst: 200</span><br><span class="line">  kubeconfig: &quot;/etc/kubernetes/kube-scheduler.kubeconfig&quot;</span><br><span class="line">  qps: 100</span><br><span class="line">enableContentionProfiling: false</span><br><span class="line">enableProfiling: true</span><br><span class="line">hardPodAffinitySymmetricWeight: 1</span><br><span class="line">healthzBindAddress: ##NODE_IP##:10251</span><br><span class="line">leaderElection:</span><br><span class="line">  leaderElect: true</span><br><span class="line">metricsBindAddress: ##NODE_IP##:10251</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>--kubeconfig</code>：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；</li><li><code>--leader-elect=true</code>：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li></ul><p>替换模板文件中的变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 3; i++ ))</span><br><span class="line">  do</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-scheduler.yaml.template &gt; kube-scheduler-$&#123;NODE_IPS[i]&#125;.yaml</span><br><span class="line">  done</span><br><span class="line">ls kube-scheduler*.yaml</span><br></pre></td></tr></table></figure><ul><li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li></ul><p>分发 kube-scheduler 配置文件到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-scheduler-$&#123;node_ip&#125;.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/kube-scheduler.yaml</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>重命名为 kube-scheduler.yaml;</li></ul><h2 id="创建-kube-scheduler-systemd-unit-模板文件"><a href="#创建-kube-scheduler-systemd-unit-模板文件" class="headerlink" title="创建 kube-scheduler systemd unit 模板文件"></a>创建 kube-scheduler systemd unit 模板文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-scheduler.service.template &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Scheduler</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=$&#123;K8S_DIR&#125;/kube-scheduler</span><br><span class="line">ExecStart=/opt/k8s/bin/kube-scheduler \\</span><br><span class="line">  --config=/etc/kubernetes/kube-scheduler.yaml \\</span><br><span class="line">  --bind-address=##NODE_IP## \\</span><br><span class="line">  --secure-port=10259 \\</span><br><span class="line">  --port=0 \\</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/cert/kube-scheduler.pem \\</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/cert/kube-scheduler-key.pem \\</span><br><span class="line">  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\</span><br><span class="line">  --client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --requestheader-allowed-names=&quot;&quot; \\</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \\</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \\</span><br><span class="line">  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\</span><br><span class="line">  --logtostderr=true \\</span><br><span class="line">  --v=2</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h2 id="为各节点创建和分发-kube-scheduler-systemd-unit-文件"><a href="#为各节点创建和分发-kube-scheduler-systemd-unit-文件" class="headerlink" title="为各节点创建和分发 kube-scheduler systemd unit 文件"></a>为各节点创建和分发 kube-scheduler systemd unit 文件</h2><p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 2; i++ ))</span><br><span class="line">  do</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-scheduler.service.template &gt; kube-scheduler-$&#123;NODE_IPS[i]&#125;.service </span><br><span class="line">  done</span><br><span class="line">ls kube-scheduler*.service</span><br></pre></td></tr></table></figure><p>分发 systemd unit 文件到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-scheduler-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-scheduler.service</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>重命名为 kube-scheduler.service；</li></ul><h2 id="启动-kube-scheduler-服务"><a href="#启动-kube-scheduler-服务" class="headerlink" title="启动 kube-scheduler 服务"></a>启动 kube-scheduler 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-scheduler&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl restart kube-scheduler&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>启动服务前必须先创建工作目录；</li></ul><h2 id="检查服务运行状态-1"><a href="#检查服务运行状态-1" class="headerlink" title="检查服务运行状态"></a>检查服务运行状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-scheduler|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code></p><h2 id="查看当前的-leader-1"><a href="#查看当前的-leader-1" class="headerlink" title="查看当前的 leader"></a>查看当前的 leader</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    control-plane.alpha.kubernetes.io/leader: &apos;&#123;&quot;holderIdentity&quot;:&quot;node1_b23eda23-8698-11e9-b281-005056b16e40&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2019-06-04T07:17:00Z&quot;,&quot;renewTime&quot;:&quot;2019-06-05T07:31:12Z&quot;,&quot;leaderTransitions&quot;:1&#125;&apos;</span><br><span class="line">  creationTimestamp: &quot;2019-06-04T07:07:02Z&quot;</span><br><span class="line">  name: kube-scheduler</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: &quot;125460&quot;</span><br><span class="line">  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler</span><br><span class="line">  uid: 5a3888a1-8697-11e9-b371-005056b1d2de</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;kube-controller-manager-集群&quot;&gt;&lt;a href=&quot;#kube-controller-manager-集群&quot; class=&quot;headerlink&quot; title=&quot;kube-controller-manager 集群&quot;&gt;&lt;/a&gt;kube-con
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-apiserver集群</title>
    <link href="https://shenshengkun.github.io/posts/863q77b5.html"/>
    <id>https://shenshengkun.github.io/posts/863q77b5.html</id>
    <published>2019-06-05T06:54:01.000Z</published>
    <updated>2019-06-05T08:52:01.693Z</updated>
    
    <content type="html"><![CDATA[<h1 id="nginx代理"><a href="#nginx代理" class="headerlink" title="nginx代理"></a>nginx代理</h1><h2 id="基于-nginx-代理的-kube-apiserver-高可用方案"><a href="#基于-nginx-代理的-kube-apiserver-高可用方案" class="headerlink" title="基于 nginx 代理的 kube-apiserver 高可用方案"></a>基于 nginx 代理的 kube-apiserver 高可用方案</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- 控制节点的 kube-controller-manager、kube-scheduler 是多实例部署，所以只要有一个实例正常，就可以保证高可用；</span><br><span class="line">- 集群内的 Pod 使用 K8S 服务域名 kubernetes 访问 kube-apiserver， kube-dns 会自动解析出多个 kube-apiserver 节点的 IP，所以也是高可用的；</span><br><span class="line">- 在每个节点起一个 nginx 进程，后端对接多个 apiserver 实例，nginx 对它们做健康检查和负载均衡；</span><br><span class="line">- kubelet、kube-proxy、controller-manager、scheduler 通过本地的 nginx（监听 127.0.0.1）访问 kube-apiserver，从而实现 kube-apiserver 的高可用；</span><br></pre></td></tr></table></figure><h2 id="下载和编译-nginx"><a href="#下载和编译-nginx" class="headerlink" title="下载和编译 nginx"></a>下载和编译 nginx</h2><p>下载源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">wget http://nginx.org/download/nginx-1.15.3.tar.gz</span><br><span class="line">tar -xzvf nginx-1.15.3.tar.gz</span><br></pre></td></tr></table></figure><p>配置编译参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/nginx-1.15.3</span><br><span class="line">mkdir nginx-prefix</span><br><span class="line">./configure --with-stream --without-http --prefix=$(pwd)/nginx-prefix --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module</span><br></pre></td></tr></table></figure><ul><li><code>--with-stream</code>：开启 4 层透明转发(TCP Proxy)功能；</li><li><code>--without-xxx</code>：关闭所有其他功能，这样生成的动态链接二进制程序依赖最小；</li></ul><p>编译和安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/nginx-1.15.3</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure><h2 id="安装和部署-nginx"><a href="#安装和部署-nginx" class="headerlink" title="安装和部署 nginx"></a>安装和部署 nginx</h2><p>创建目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    mkdir -p /opt/k8s/kube-nginx/&#123;conf,logs,sbin&#125;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>拷贝二进制程序：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp /opt/k8s/work/nginx-1.15.3/nginx-prefix/sbin/nginx  root@$&#123;node_ip&#125;:/opt/k8s/kube-nginx/sbin/kube-nginx</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod a+x /opt/k8s/kube-nginx/sbin/*&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /opt/k8s/kube-nginx/&#123;conf,logs,sbin&#125;&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>重命名二进制文件为 kube-nginx；</li></ul><p>配置 nginx，开启 4 层透明转发功能：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-nginx.conf &lt;&lt;EOF</span><br><span class="line">worker_processes 1;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stream &#123;</span><br><span class="line">    upstream backend &#123;</span><br><span class="line">        hash $remote_addr consistent;</span><br><span class="line">        server 192.168.6.101:6443        max_fails=3 fail_timeout=30s;</span><br><span class="line">        server 192.168.6.102:6443        max_fails=3 fail_timeout=30s;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen 127.0.0.1:8443;</span><br><span class="line">        proxy_connect_timeout 1s;</span><br><span class="line">        proxy_pass backend;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-nginx.conf  root@$&#123;node_ip&#125;:/opt/k8s/kube-nginx/conf/kube-nginx.conf</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="配置-systemd-unit-文件，启动服务"><a href="#配置-systemd-unit-文件，启动服务" class="headerlink" title="配置 systemd unit 文件，启动服务"></a>配置 systemd unit 文件，启动服务</h2><p>配置 kube-nginx systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-nginx.service &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=kube-apiserver nginx proxy</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=forking</span><br><span class="line">ExecStartPre=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -t</span><br><span class="line">ExecStart=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx</span><br><span class="line">ExecReload=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -s reload</span><br><span class="line">PrivateTmp=true</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-nginx.service  root@$&#123;node_ip&#125;:/etc/systemd/system/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>启动 kube-nginx 服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-nginx &amp;&amp; systemctl restart kube-nginx&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h1 id="apiserver集群"><a href="#apiserver集群" class="headerlink" title="apiserver集群"></a>apiserver集群</h1><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="下载最新版本二进制文件"><a href="#下载最新版本二进制文件" class="headerlink" title="下载最新版本二进制文件"></a>下载最新版本二进制文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">wget https://dl.k8s.io/v1.14.2/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">tar -xzvf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">cd kubernetes</span><br><span class="line">tar -xzvf  kubernetes-src.tar.gz</span><br></pre></td></tr></table></figure><p>将二进制文件拷贝到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kubernetes/server/bin/&#123;apiextensions-apiserver,cloud-controller-manager,kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,kubeadm,kubectl,kubelet,mounter&#125; root@$&#123;node_ip&#125;:/opt/k8s/bin/</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-kubernetes-证书和私钥"><a href="#创建-kubernetes-证书和私钥" class="headerlink" title="创建 kubernetes 证书和私钥"></a>创建 kubernetes 证书和私钥</h2><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kubernetes-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">  &quot;hosts&quot;: [</span><br><span class="line">    &quot;127.0.0.1&quot;,</span><br><span class="line">    &quot;192.168.6.101&quot;,</span><br><span class="line">    &quot;192.168.6.102&quot;,</span><br><span class="line">    &quot;$&#123;CLUSTER_KUBERNETES_SVC_IP&#125;&quot;,</span><br><span class="line">    &quot;kubernetes&quot;,</span><br><span class="line">    &quot;kubernetes.default&quot;,</span><br><span class="line">    &quot;kubernetes.default.svc&quot;,</span><br><span class="line">    &quot;kubernetes.default.svc.cluster&quot;,</span><br><span class="line">    &quot;kubernetes.default.svc.cluster.local.&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>hosts 字段指定授权使用该证书的 <strong>IP 和域名列表</strong>，这里列出了 master 节点 IP、kubernetes 服务的 IP 和域名；</li><li>kubernetes 服务 IP 是 apiserver 自动创建的，一般是 <code>--service-cluster-ip-range</code> 参数指定的网段的<strong>第一个IP</strong>，后续可以通过下面命令获取：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get svc kubernetes</span><br><span class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.254.0.1   &lt;none&gt;        443/TCP   24h</span><br></pre></td></tr></table></figure><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes</span><br><span class="line">ls kubernetes*pem</span><br></pre></td></tr></table></figure><p>将生成的证书和私钥文件拷贝到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/kubernetes/cert&quot;</span><br><span class="line">    scp kubernetes*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建加密配置文件"><a href="#创建加密配置文件" class="headerlink" title="创建加密配置文件"></a>创建加密配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; encryption-config.yaml &lt;&lt;EOF</span><br><span class="line">kind: EncryptionConfig</span><br><span class="line">apiVersion: v1</span><br><span class="line">resources:</span><br><span class="line">  - resources:</span><br><span class="line">      - secrets</span><br><span class="line">    providers:</span><br><span class="line">      - aescbc:</span><br><span class="line">          keys:</span><br><span class="line">            - name: key1</span><br><span class="line">              secret: $&#123;ENCRYPTION_KEY&#125;</span><br><span class="line">      - identity: &#123;&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>将加密配置文件拷贝到 master 节点的 <code>/etc/kubernetes</code> 目录下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp encryption-config.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建审计策略文件"><a href="#创建审计策略文件" class="headerlink" title="创建审计策略文件"></a>创建审计策略文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; audit-policy.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: audit.k8s.io/v1beta1</span><br><span class="line">kind: Policy</span><br><span class="line">rules:</span><br><span class="line">  # The following requests were manually identified as high-volume and low-risk, so drop them.</span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - endpoints</span><br><span class="line">          - services</span><br><span class="line">          - services/status</span><br><span class="line">    users:</span><br><span class="line">      - &apos;system:kube-proxy&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - watch</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - nodes</span><br><span class="line">          - nodes/status</span><br><span class="line">    userGroups:</span><br><span class="line">      - &apos;system:nodes&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    namespaces:</span><br><span class="line">      - kube-system</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - endpoints</span><br><span class="line">    users:</span><br><span class="line">      - &apos;system:kube-controller-manager&apos;</span><br><span class="line">      - &apos;system:kube-scheduler&apos;</span><br><span class="line">      - &apos;system:serviceaccount:kube-system:endpoint-controller&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - update</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - namespaces</span><br><span class="line">          - namespaces/status</span><br><span class="line">          - namespaces/finalize</span><br><span class="line">    users:</span><br><span class="line">      - &apos;system:apiserver&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">  # Don&apos;t log HPA fetching metrics.</span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">    users:</span><br><span class="line">      - &apos;system:kube-controller-manager&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line"></span><br><span class="line">  # Don&apos;t log these read-only URLs.</span><br><span class="line">  - level: None</span><br><span class="line">    nonResourceURLs:</span><br><span class="line">      - &apos;/healthz*&apos;</span><br><span class="line">      - /version</span><br><span class="line">      - &apos;/swagger*&apos;</span><br><span class="line"></span><br><span class="line">  # Don&apos;t log events requests.</span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - events</span><br><span class="line"></span><br><span class="line">  # node and pod status calls from nodes are high-volume and can be large, don&apos;t log responses for expected updates from nodes</span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - nodes/status</span><br><span class="line">          - pods/status</span><br><span class="line">    users:</span><br><span class="line">      - kubelet</span><br><span class="line">      - &apos;system:node-problem-detector&apos;</span><br><span class="line">      - &apos;system:serviceaccount:kube-system:node-problem-detector&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - nodes/status</span><br><span class="line">          - pods/status</span><br><span class="line">    userGroups:</span><br><span class="line">      - &apos;system:nodes&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">  # deletecollection calls can be large, don&apos;t log responses for expected namespace deletions</span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    users:</span><br><span class="line">      - &apos;system:serviceaccount:kube-system:namespace-controller&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - deletecollection</span><br><span class="line"></span><br><span class="line">  # Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data,</span><br><span class="line">  # so only log at the Metadata level.</span><br><span class="line">  - level: Metadata</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - secrets</span><br><span class="line">          - configmaps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">        resources:</span><br><span class="line">          - tokenreviews</span><br><span class="line">  # Get repsonses can be large; skip them.</span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">      - group: admissionregistration.k8s.io</span><br><span class="line">      - group: apiextensions.k8s.io</span><br><span class="line">      - group: apiregistration.k8s.io</span><br><span class="line">      - group: apps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">      - group: authorization.k8s.io</span><br><span class="line">      - group: autoscaling</span><br><span class="line">      - group: batch</span><br><span class="line">      - group: certificates.k8s.io</span><br><span class="line">      - group: extensions</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">      - group: networking.k8s.io</span><br><span class="line">      - group: policy</span><br><span class="line">      - group: rbac.authorization.k8s.io</span><br><span class="line">      - group: scheduling.k8s.io</span><br><span class="line">      - group: settings.k8s.io</span><br><span class="line">      - group: storage.k8s.io</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line"></span><br><span class="line">  # Default level for known APIs</span><br><span class="line">  - level: RequestResponse</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">      - group: admissionregistration.k8s.io</span><br><span class="line">      - group: apiextensions.k8s.io</span><br><span class="line">      - group: apiregistration.k8s.io</span><br><span class="line">      - group: apps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">      - group: authorization.k8s.io</span><br><span class="line">      - group: autoscaling</span><br><span class="line">      - group: batch</span><br><span class="line">      - group: certificates.k8s.io</span><br><span class="line">      - group: extensions</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">      - group: networking.k8s.io</span><br><span class="line">      - group: policy</span><br><span class="line">      - group: rbac.authorization.k8s.io</span><br><span class="line">      - group: scheduling.k8s.io</span><br><span class="line">      - group: settings.k8s.io</span><br><span class="line">      - group: storage.k8s.io</span><br><span class="line"></span><br><span class="line">  # Default level for all other requests.</span><br><span class="line">  - level: Metadata</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发审计策略文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp audit-policy.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/audit-policy.yaml</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建后续访问-metrics-server-使用的证书"><a href="#创建后续访问-metrics-server-使用的证书" class="headerlink" title="创建后续访问 metrics-server 使用的证书"></a>创建后续访问 metrics-server 使用的证书</h2><p>创建证书签名请求:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; proxy-client-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;aggregator&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>CN 名称为 aggregator，需要与 metrics-server 的 <code>--requestheader-allowed-names</code> 参数配置一致，否则访问会被 metrics-server 拒绝；</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  -ca-key=/etc/kubernetes/cert/ca-key.pem  \</span><br><span class="line">  -config=/etc/kubernetes/cert/ca-config.json  \</span><br><span class="line">  -profile=kubernetes proxy-client-csr.json | cfssljson -bare proxy-client</span><br><span class="line">ls proxy-client*.pem</span><br></pre></td></tr></table></figure><p>将生成的证书和私钥文件拷贝到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp proxy-client*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-kube-apiserver-systemd-unit-模板文件"><a href="#创建-kube-apiserver-systemd-unit-模板文件" class="headerlink" title="创建 kube-apiserver systemd unit 模板文件"></a>创建 kube-apiserver systemd unit 模板文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kube-apiserver.service.template &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Server</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=$&#123;K8S_DIR&#125;/kube-apiserver</span><br><span class="line">ExecStart=/opt/k8s/bin/kube-apiserver \\</span><br><span class="line">  --advertise-address=##NODE_IP## \\</span><br><span class="line">  --default-not-ready-toleration-seconds=360 \\</span><br><span class="line">  --default-unreachable-toleration-seconds=360 \\</span><br><span class="line">  --feature-gates=DynamicAuditing=true \\</span><br><span class="line">  --max-mutating-requests-inflight=2000 \\</span><br><span class="line">  --max-requests-inflight=4000 \\</span><br><span class="line">  --default-watch-cache-size=200 \\</span><br><span class="line">  --delete-collection-workers=2 \\</span><br><span class="line">  --encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\</span><br><span class="line">  --etcd-cafile=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --etcd-certfile=/etc/kubernetes/cert/kubernetes.pem \\</span><br><span class="line">  --etcd-keyfile=/etc/kubernetes/cert/kubernetes-key.pem \\</span><br><span class="line">  --etcd-servers=$&#123;ETCD_ENDPOINTS&#125; \\</span><br><span class="line">  --bind-address=##NODE_IP## \\</span><br><span class="line">  --secure-port=6443 \\</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/cert/kubernetes.pem \\</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/cert/kubernetes-key.pem \\</span><br><span class="line">  --insecure-port=0 \\</span><br><span class="line">  --audit-dynamic-configuration \\</span><br><span class="line">  --audit-log-maxage=15 \\</span><br><span class="line">  --audit-log-maxbackup=3 \\</span><br><span class="line">  --audit-log-maxsize=100 \\</span><br><span class="line">  --audit-log-mode=batch \\</span><br><span class="line">  --audit-log-truncate-enabled \\</span><br><span class="line">  --audit-log-batch-buffer-size=20000 \\</span><br><span class="line">  --audit-log-batch-max-size=2 \\</span><br><span class="line">  --audit-log-path=$&#123;K8S_DIR&#125;/kube-apiserver/audit.log \\</span><br><span class="line">  --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\</span><br><span class="line">  --profiling \\</span><br><span class="line">  --anonymous-auth=false \\</span><br><span class="line">  --client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --enable-bootstrap-token-auth \\</span><br><span class="line">  --requestheader-allowed-names=&quot;&quot; \\</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \\</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \\</span><br><span class="line">  --service-account-key-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --authorization-mode=Node,RBAC \\</span><br><span class="line">  --runtime-config=api/all=true \\</span><br><span class="line">  --enable-admission-plugins=NodeRestriction \\</span><br><span class="line">  --allow-privileged=true \\</span><br><span class="line">  --apiserver-count=3 \\</span><br><span class="line">  --event-ttl=168h \\</span><br><span class="line">  --kubelet-certificate-authority=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --kubelet-client-certificate=/etc/kubernetes/cert/kubernetes.pem \\</span><br><span class="line">  --kubelet-client-key=/etc/kubernetes/cert/kubernetes-key.pem \\</span><br><span class="line">  --kubelet-https=true \\</span><br><span class="line">  --kubelet-timeout=10s \\</span><br><span class="line">  --proxy-client-cert-file=/etc/kubernetes/cert/proxy-client.pem \\</span><br><span class="line">  --proxy-client-key-file=/etc/kubernetes/cert/proxy-client-key.pem \\</span><br><span class="line">  --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\</span><br><span class="line">  --service-node-port-range=$&#123;NODE_PORT_RANGE&#125; \\</span><br><span class="line">  --logtostderr=true \\</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=10</span><br><span class="line">Type=notify</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>--advertise-address</code>：apiserver 对外通告的 IP（kubernetes 服务后端节点 IP）；</li><li><code>--default-*-toleration-seconds</code>：设置节点异常相关的阈值；</li><li><code>--max-*-requests-inflight</code>：请求相关的最大阈值；</li><li><code>--etcd-*</code>：访问 etcd 的证书和 etcd 服务器地址；</li><li><code>--experimental-encryption-provider-config</code>：指定用于加密 etcd 中 secret 的配置；</li><li><code>--bind-address</code>： https 监听的 IP，不能为 <code>127.0.0.1</code>，否则外界不能访问它的安全端口 6443；</li><li><code>--secret-port</code>：https 监听端口；</li><li><code>--insecure-port=0</code>：关闭监听 http 非安全端口(8080)；</li><li><code>--tls-*-file</code>：指定 apiserver 使用的证书、私钥和 CA 文件；</li><li><code>--audit-*</code>：配置审计策略和审计日志文件相关的参数；</li><li><code>--client-ca-file</code>：验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy 等)请求所带的证书；</li><li><code>--enable-bootstrap-token-auth</code>：启用 kubelet bootstrap 的 token 认证；</li><li><code>--requestheader-*</code>：kube-apiserver 的 aggregator layer 相关的配置参数，proxy-client &amp; HPA 需要使用；</li><li><code>--requestheader-client-ca-file</code>：用于签名 <code>--proxy-client-cert-file</code> 和 <code>--proxy-client-key-file</code> 指定的证书；在启用了 metric aggregator 时使用；</li><li>如果 <code>--requestheader-allowed-names</code> 不为空，则<code>--proxy-client-cert-file</code> 证书的 CN 必须位于 allowed-names 中，默认为 aggregator;</li><li><code>--service-account-key-file</code>：签名 ServiceAccount Token 的公钥文件，kube-controller-manager 的 <code>--service-account-private-key-file</code> 指定私钥文件，两者配对使用；</li><li><code>--runtime-config=api/all=true</code>： 启用所有版本的 APIs，如 autoscaling/v2alpha1；</li><li><code>--authorization-mode=Node,RBAC</code>、<code>--anonymous-auth=false</code>： 开启 Node 和 RBAC 授权模式，拒绝未授权的请求；</li><li><code>--enable-admission-plugins</code>：启用一些默认关闭的 plugins；</li><li><code>--allow-privileged</code>：运行执行 privileged 权限的容器；</li><li><code>--apiserver-count=3</code>：指定 apiserver 实例的数量；</li><li><code>--event-ttl</code>：指定 events 的保存时间；</li><li><code>--kubelet-*</code>：如果指定，则使用 https 访问 kubelet APIs；需要为证书对应的用户(上面 kubernetes*.pem 证书的用户为 kubernetes) 用户定义 RBAC 规则，否则访问 kubelet API 时提示未授权；</li><li><code>--proxy-client-*</code>：apiserver 访问 metrics-server 使用的证书；</li><li><code>--service-cluster-ip-range</code>： 指定 Service Cluster IP 地址段；</li><li><code>--service-node-port-range</code>： 指定 NodePort 的端口范围；</li></ul><p>如果 kube-apiserver 机器<strong>没有</strong>运行 kube-proxy，则还需要添加 <code>--enable-aggregator-routing=true</code> 参数；</p><p>关于 <code>--requestheader-XXX</code> 相关参数，参考：</p><ul><li><a href="https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md</a></li><li><a href="https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/" target="_blank" rel="noopener">https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/</a></li></ul><p>注意：requestheader-client-ca-file 指定的 CA 证书，必须具有 client auth and server auth；</p><h2 id="为各节点创建和分发-kube-apiserver-systemd-unit-文件"><a href="#为各节点创建和分发-kube-apiserver-systemd-unit-文件" class="headerlink" title="为各节点创建和分发 kube-apiserver systemd unit 文件"></a>为各节点创建和分发 kube-apiserver systemd unit 文件</h2><p>替换模板文件中的变量，为各节点生成 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 2; i++ ))</span><br><span class="line">  do</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-apiserver.service.template &gt; kube-apiserver-$&#123;NODE_IPS[i]&#125;.service </span><br><span class="line">  done</span><br><span class="line">ls kube-apiserver*.service</span><br></pre></td></tr></table></figure><ul><li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li></ul><p>分发生成的 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-apiserver-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-apiserver.service</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>文件重命名为 kube-apiserver.service;</li></ul><h2 id="启动-kube-apiserver-服务"><a href="#启动-kube-apiserver-服务" class="headerlink" title="启动 kube-apiserver 服务"></a>启动 kube-apiserver 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-apiserver&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl restart kube-apiserver&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>启动服务前必须先创建工作目录；</li></ul><h2 id="检查-kube-apiserver-运行状态"><a href="#检查-kube-apiserver-运行状态" class="headerlink" title="检查 kube-apiserver 运行状态"></a>检查 kube-apiserver 运行状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-apiserver |grep &apos;Active:&apos;&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code></p><h2 id="打印-kube-apiserver-写入-etcd-的数据"><a href="#打印-kube-apiserver-写入-etcd-的数据" class="headerlink" title="打印 kube-apiserver 写入 etcd 的数据"></a>打印 kube-apiserver 写入 etcd 的数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">ETCDCTL_API=3 etcdctl \</span><br><span class="line">    --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">    --cacert=/opt/k8s/work/ca.pem \</span><br><span class="line">    --cert=/opt/k8s/work/etcd.pem \</span><br><span class="line">    --key=/opt/k8s/work/etcd-key.pem \</span><br><span class="line">    get /registry/ --prefix --keys-only</span><br></pre></td></tr></table></figure><h2 id="授予-kube-apiserver-访问-kubelet-API-的权限"><a href="#授予-kube-apiserver-访问-kubelet-API-的权限" class="headerlink" title="授予 kube-apiserver 访问 kubelet API 的权限"></a>授予 kube-apiserver 访问 kubelet API 的权限</h2><p>在执行 kubectl exec、run、logs 等命令时，apiserver 会将请求转发到 kubelet 的 https 端口。这里定义 RBAC 规则，授权 apiserver 使用的证书（kubernetes.pem）用户名（CN：kuberntes）访问 kubelet API 的权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;nginx代理&quot;&gt;&lt;a href=&quot;#nginx代理&quot; class=&quot;headerlink&quot; title=&quot;nginx代理&quot;&gt;&lt;/a&gt;nginx代理&lt;/h1&gt;&lt;h2 id=&quot;基于-nginx-代理的-kube-apiserver-高可用方案&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-flannel网络、kubectl工具</title>
    <link href="https://shenshengkun.github.io/posts/66ae7fg23.html"/>
    <id>https://shenshengkun.github.io/posts/66ae7fg23.html</id>
    <published>2019-06-05T06:30:01.000Z</published>
    <updated>2019-06-05T08:51:51.368Z</updated>
    
    <content type="html"><![CDATA[<h1 id="flannel网络"><a href="#flannel网络" class="headerlink" title="flannel网络"></a>flannel网络</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Flannel是CoreOS团队针对Kubernetes设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。 </p><h3 id="flannel在k8s工作"><a href="#flannel在k8s工作" class="headerlink" title="flannel在k8s工作"></a>flannel在k8s工作</h3><p>kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472（<strong>需要开放该端口</strong>，如公有云 AWS 等）。</p><p>flanneld 第一次启动时，从 etcd 获取配置的 Pod 网段信息，为本节点分配一个未使用的地址段，然后创建 <code>flannedl.1</code> 网络接口（也可能是其它名称，如 flannel1 等）。</p><p>flannel 将分配给自己的 Pod 网段信息写入 <code>/run/flannel/docker</code> 文件，docker 后续使用这个文件中的环境变量设置 <code>docker0</code> 网桥，从而从这个地址段为本节点的所有 Pod 容器分配 IP。</p><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h3 id="下载和分发-flanneld-二进制文件"><a href="#下载和分发-flanneld-二进制文件" class="headerlink" title="下载和分发 flanneld 二进制文件"></a>下载和分发 flanneld 二进制文件</h3><p>从 flannel 的 <a href="https://github.com/coreos/flannel/releases" target="_blank" rel="noopener">release 页面</a> 下载最新版本的安装包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">mkdir flannel</span><br><span class="line">wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz</span><br><span class="line">tar -xzvf flannel-v0.11.0-linux-amd64.tar.gz -C flannel</span><br></pre></td></tr></table></figure><p>分发二进制文件到集群所有节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp flannel/&#123;flanneld,mk-docker-opts.sh&#125; root@$&#123;node_ip&#125;:/opt/k8s/bin/</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h3 id="创建-flannel-证书和私钥"><a href="#创建-flannel-证书和私钥" class="headerlink" title="创建 flannel 证书和私钥"></a>创建 flannel 证书和私钥</h3><p>flanneld 从 etcd 集群存取网段分配信息，而 etcd 集群启用了双向 x509 证书认证，所以需要为 flanneld 生成证书和私钥。</p><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; flanneld-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;flanneld&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld</span><br><span class="line">ls flanneld*pem</span><br></pre></td></tr></table></figure><p>将生成的证书和私钥分发到<strong>所有节点</strong>（master 和 worker）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/flanneld/cert&quot;</span><br><span class="line">    scp flanneld*.pem root@$&#123;node_ip&#125;:/etc/flanneld/cert</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h3 id="在etcd-写入集群-Pod-网段信息"><a href="#在etcd-写入集群-Pod-网段信息" class="headerlink" title="在etcd 写入集群 Pod 网段信息"></a>在etcd 写入集群 Pod 网段信息</h3><p>注意：本步骤<strong>只需执行一次</strong>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">etcdctl \</span><br><span class="line">  --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">  --ca-file=/opt/k8s/work/ca.pem \</span><br><span class="line">  --cert-file=/opt/k8s/work/flanneld.pem \</span><br><span class="line">  --key-file=/opt/k8s/work/flanneld-key.pem \</span><br><span class="line">  mk $&#123;FLANNEL_ETCD_PREFIX&#125;/config &apos;&#123;&quot;Network&quot;:&quot;&apos;$&#123;CLUSTER_CIDR&#125;&apos;&quot;, &quot;SubnetLen&quot;: 21, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;&apos;</span><br></pre></td></tr></table></figure><ul><li>flanneld <strong>当前版本 (v0.11.0) 不支持 etcd v3</strong>，故使用 etcd v2 API 写入配置 key 和网段数据；</li><li>写入的 Pod 网段 <code>${CLUSTER_CIDR}</code> 地址段（如 /16）必须小于 <code>SubnetLen</code>，必须与 <code>kube-controller-manager</code> 的 <code>--cluster-cidr</code> 参数值一致；</li></ul><h3 id="创建-flanneld-的-systemd"><a href="#创建-flanneld-的-systemd" class="headerlink" title="创建 flanneld 的 systemd"></a>创建 flanneld 的 systemd</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; flanneld.service &lt;&lt; EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Flanneld overlay address etcd agent</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line">After=etcd.service</span><br><span class="line">Before=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">ExecStart=/opt/k8s/bin/flanneld \\</span><br><span class="line">  -etcd-cafile=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  -etcd-certfile=/etc/flanneld/cert/flanneld.pem \\</span><br><span class="line">  -etcd-keyfile=/etc/flanneld/cert/flanneld-key.pem \\</span><br><span class="line">  -etcd-endpoints=$&#123;ETCD_ENDPOINTS&#125; \\</span><br><span class="line">  -etcd-prefix=$&#123;FLANNEL_ETCD_PREFIX&#125; \\</span><br><span class="line">  -iface=$&#123;IFACE&#125; \\</span><br><span class="line">  -ip-masq</span><br><span class="line">ExecStartPost=/opt/k8s/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">RequiredBy=docker.service</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>mk-docker-opts.sh</code> 脚本将分配给 flanneld 的 Pod 子网段信息写入 <code>/run/flannel/docker</code> 文件，后续 docker 启动时使用这个文件中的环境变量配置 docker0 网桥；</li><li>flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 <code>-iface</code> 参数指定通信接口;</li><li>flanneld 运行时需要 root 权限；</li><li><code>-ip-masq</code>: flanneld 为访问 Pod 网络外的流量设置 SNAT 规则，同时将传递给 Docker 的变量 <code>--ip-masq</code>（<code>/run/flannel/docker</code> 文件中）设置为 false，这样 Docker 将不再创建 SNAT 规则； Docker 的 <code>--ip-masq</code> 为 true 时，创建的 SNAT 规则比较“暴力”：将所有本节点 Pod 发起的、访问非 docker0 接口的请求做 SNAT，这样访问其他节点 Pod 的请求来源 IP 会被设置为 flannel.1 接口的 IP，导致目的 Pod 看不到真实的来源 Pod IP。 flanneld 创建的 SNAT 规则比较温和，只对访问非 Pod 网段的请求做 SNAT。</li></ul><h3 id="分发-flanneld-systemd-unit"><a href="#分发-flanneld-systemd-unit" class="headerlink" title="分发 flanneld systemd unit"></a>分发 flanneld systemd unit</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp flanneld.service root@$&#123;node_ip&#125;:/etc/systemd/system/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h3 id="启动-flanneld-服务"><a href="#启动-flanneld-服务" class="headerlink" title="启动 flanneld 服务"></a>启动 flanneld 服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h3 id="检查启动结果"><a href="#检查启动结果" class="headerlink" title="检查启动结果"></a>检查启动结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status flanneld|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code></p><h3 id="检查分配给各-flanneld-的-Pod-网段信息"><a href="#检查分配给各-flanneld-的-Pod-网段信息" class="headerlink" title="检查分配给各 flanneld 的 Pod 网段信息"></a>检查分配给各 flanneld 的 Pod 网段信息</h3><p>查看集群 Pod 网段(/16)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">etcdctl \</span><br><span class="line">  --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">  --ca-file=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/cert/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/cert/flanneld-key.pem \</span><br><span class="line">  get $&#123;FLANNEL_ETCD_PREFIX&#125;/config</span><br></pre></td></tr></table></figure><p>输出：</p><p><code>{&quot;Network&quot;:&quot;172.30.0.0/16&quot;, &quot;SubnetLen&quot;: 21, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;}}</code></p><p>查看已分配的 Pod 子网段列表(/24):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">etcdctl \</span><br><span class="line">  --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">  --ca-file=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/cert/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/cert/flanneld-key.pem \</span><br><span class="line">  ls $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets</span><br></pre></td></tr></table></figure><p>输出（结果视部署情况而定）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# etcdctl \</span><br><span class="line">&gt;   --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">&gt;   --ca-file=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">&gt;   --cert-file=/etc/flanneld/cert/flanneld.pem \</span><br><span class="line">&gt;   --key-file=/etc/flanneld/cert/flanneld-key.pem \</span><br><span class="line">&gt;   ls $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets</span><br><span class="line">/kubernetes/network/subnets/172.30.168.0-21</span><br><span class="line">/kubernetes/network/subnets/172.30.48.0-21</span><br></pre></td></tr></table></figure><p>查看某一 Pod 网段对应的节点 IP 和 flannel 接口地址:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">etcdctl \</span><br><span class="line">  --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">  --ca-file=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/cert/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/cert/flanneld-key.pem \</span><br><span class="line">  get $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets/172.30.168.0-21</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# etcdctl \</span><br><span class="line">&gt;   --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">&gt;   --ca-file=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">&gt;   --cert-file=/etc/flanneld/cert/flanneld.pem \</span><br><span class="line">&gt;   --key-file=/etc/flanneld/cert/flanneld-key.pem \</span><br><span class="line">&gt;   get $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets/172.30.168.0-21</span><br><span class="line">&#123;&quot;PublicIP&quot;:&quot;192.168.6.101&quot;,&quot;BackendType&quot;:&quot;vxlan&quot;,&quot;BackendData&quot;:&#123;&quot;VtepMAC&quot;:&quot;62:58:f9:a2:16:73&quot;&#125;&#125;</span><br><span class="line">[root@node1 ~]#</span><br></pre></td></tr></table></figure><h3 id="验证各节点能通过-Pod-网段互通"><a href="#验证各节点能通过-Pod-网段互通" class="headerlink" title="验证各节点能通过 Pod 网段互通"></a>验证各节点能通过 Pod 网段互通</h3><p>在<strong>各节点上部署</strong> flannel 后，检查是否创建了 flannel 接口(名称可能为 flannel0、flannel.0、flannel.1 等)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh $&#123;node_ip&#125; &quot;/usr/sbin/ip addr show flannel.1|grep -w inet&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">&gt;   do</span><br><span class="line">&gt;     echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">&gt;     ssh $&#123;node_ip&#125; &quot;/usr/sbin/ip addr show flannel.1|grep -w inet&quot;</span><br><span class="line">&gt;   done</span><br><span class="line">&gt;&gt;&gt; 192.168.6.101</span><br><span class="line">    inet 172.30.168.0/32 scope global flannel.1</span><br><span class="line">&gt;&gt;&gt; 192.168.6.102</span><br><span class="line">    inet 172.30.48.0/32 scope global flannel.1</span><br></pre></td></tr></table></figure><p>在各节点上 ping 所有 flannel 接口 IP，确保能通 </p><h1 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h1><h2 id="下载和分发-kubectl-二进制文件"><a href="#下载和分发-kubectl-二进制文件" class="headerlink" title="下载和分发 kubectl 二进制文件"></a>下载和分发 kubectl 二进制文件</h2><p>下载和解压：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">wget https://dl.k8s.io/v1.14.2/kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">tar -xzvf kubernetes-client-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>分发到所有使用 kubectl 的节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kubernetes/client/bin/kubectl root@$&#123;node_ip&#125;:/opt/k8s/bin/</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-admin-证书和私钥"><a href="#创建-admin-证书和私钥" class="headerlink" title="创建 admin 证书和私钥"></a>创建 admin 证书和私钥</h2><p>kubectl 与 apiserver https 安全端口通信，apiserver 对提供的证书进行认证和授权。</p><p>kubectl 作为集群的管理工具，需要被授予最高权限，这里创建具有<strong>最高权限</strong>的 admin 证书。</p><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; admin-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;admin&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;system:masters&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>O 为 <code>system:masters</code>，kube-apiserver 收到该证书后将请求的 Group 设置为 system:masters；</li><li>预定义的 ClusterRoleBinding <code>cluster-admin</code> 将 Group <code>system:masters</code> 与 Role <code>cluster-admin</code> 绑定，该 Role 授予<strong>所有 API</strong>的权限；</li><li>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes admin-csr.json | cfssljson -bare admin</span><br></pre></td></tr></table></figure><h2 id="创建-kubeconfig-文件"><a href="#创建-kubeconfig-文件" class="headerlink" title="创建 kubeconfig 文件"></a>创建 kubeconfig 文件</h2><p>kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line"></span><br><span class="line"># 设置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/opt/k8s/work/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置客户端认证参数</span><br><span class="line">kubectl config set-credentials admin \</span><br><span class="line">  --client-certificate=/opt/k8s/work/admin.pem \</span><br><span class="line">  --client-key=/opt/k8s/work/admin-key.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置上下文参数</span><br><span class="line">kubectl config set-context kubernetes \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=admin \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置默认上下文</span><br><span class="line">kubectl config use-context kubernetes --kubeconfig=kubectl.kubeconfig</span><br></pre></td></tr></table></figure><ul><li><code>--certificate-authority</code>：验证 kube-apiserver 证书的根证书；</li><li><code>--client-certificate</code>、<code>--client-key</code>：刚生成的 <code>admin</code> 证书和私钥，连接 kube-apiserver 时使用；</li><li><code>--embed-certs=true</code>：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(不加时，写入的是证书文件路径，后续拷贝 kubeconfig 到其它机器时，还需要单独拷贝证书文件，不方便。)；</li></ul><h2 id="分发-kubeconfig-文件"><a href="#分发-kubeconfig-文件" class="headerlink" title="分发 kubeconfig 文件"></a>分发 kubeconfig 文件</h2><p>分发到所有使用 <code>kubectl</code> 命令的节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p ~/.kube&quot;</span><br><span class="line">    scp kubectl.kubeconfig root@$&#123;node_ip&#125;:~/.kube/config</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>保存的文件名为 <code>~/.kube/config</code>；</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;flannel网络&quot;&gt;&lt;a href=&quot;#flannel网络&quot; class=&quot;headerlink&quot; title=&quot;flannel网络&quot;&gt;&lt;/a&gt;flannel网络&lt;/h1&gt;&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-etcd集群</title>
    <link href="https://shenshengkun.github.io/posts/7dqa4nb2.html"/>
    <id>https://shenshengkun.github.io/posts/7dqa4nb2.html</id>
    <published>2019-06-04T08:30:01.000Z</published>
    <updated>2019-06-05T08:51:39.964Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>ETCD 是一个高可用的分布式键值数据库，可用于服务发现。ETCD 采用 raft 一致性算法，基于 Go 语言实现。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">特点</span><br><span class="line"></span><br><span class="line">简单：安装配置使用简单，提供 HTTP API </span><br><span class="line"></span><br><span class="line">安全：支持 SSL 证书 </span><br><span class="line"></span><br><span class="line">可靠：采用 raft 算法，实现分布式系统数据的可用性和一致性</span><br></pre></td></tr></table></figure><p>kubernetes 使用 etcd 存储所有运行数据 </p><h1 id="下载和分发-etcd-二进制文件"><a href="#下载和分发-etcd-二进制文件" class="headerlink" title="下载和分发 etcd 二进制文件"></a>下载和分发 etcd 二进制文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">wget https://github.com/coreos/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gz</span><br><span class="line">tar -xvf etcd-v3.3.13-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>分发二进制文件到集群所有节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp etcd-v3.3.13-linux-amd64/etcd* root@$&#123;node_ip&#125;:/opt/k8s/bin</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h1 id="创建-etcd-证书和私钥"><a href="#创建-etcd-证书和私钥" class="headerlink" title="创建 etcd 证书和私钥"></a>创建 etcd 证书和私钥</h1><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; etcd-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;etcd&quot;,</span><br><span class="line">  &quot;hosts&quot;: [</span><br><span class="line">    &quot;127.0.0.1&quot;,</span><br><span class="line">    &quot;192.168.6.101&quot;,</span><br><span class="line">    &quot;192.168.6.102&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">    -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">    -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">    -profile=kubernetes etcd-csr.json | cfssljson -bare etcd</span><br><span class="line">ls etcd*pem</span><br></pre></td></tr></table></figure><p>分发生成的证书和私钥到各 etcd 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/etcd/cert&quot;</span><br><span class="line">    scp etcd*.pem root@$&#123;node_ip&#125;:/etc/etcd/cert/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h1 id="创建-etcd-的-systemd"><a href="#创建-etcd-的-systemd" class="headerlink" title="创建 etcd 的 systemd"></a>创建 etcd 的 systemd</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; etcd.service.template &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line">Documentation=https://github.com/coreos</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=$&#123;ETCD_DATA_DIR&#125;</span><br><span class="line">ExecStart=/opt/k8s/bin/etcd \\</span><br><span class="line">  --data-dir=$&#123;ETCD_DATA_DIR&#125; \\</span><br><span class="line">  --wal-dir=$&#123;ETCD_WAL_DIR&#125; \\</span><br><span class="line">  --name=##NODE_NAME## \\</span><br><span class="line">  --cert-file=/etc/etcd/cert/etcd.pem \\</span><br><span class="line">  --key-file=/etc/etcd/cert/etcd-key.pem \\</span><br><span class="line">  --trusted-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --peer-cert-file=/etc/etcd/cert/etcd.pem \\</span><br><span class="line">  --peer-key-file=/etc/etcd/cert/etcd-key.pem \\</span><br><span class="line">  --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --peer-client-cert-auth \\</span><br><span class="line">  --client-cert-auth \\</span><br><span class="line">  --listen-peer-urls=https://##NODE_IP##:2380 \\</span><br><span class="line">  --initial-advertise-peer-urls=https://##NODE_IP##:2380 \\</span><br><span class="line">  --listen-client-urls=https://##NODE_IP##:2379,http://127.0.0.1:2379 \\</span><br><span class="line">  --advertise-client-urls=https://##NODE_IP##:2379 \\</span><br><span class="line">  --initial-cluster-token=etcd-cluster-0 \\</span><br><span class="line">  --initial-cluster=$&#123;ETCD_NODES&#125; \\</span><br><span class="line">  --initial-cluster-state=new \\</span><br><span class="line">  --auto-compaction-mode=periodic \\</span><br><span class="line">  --auto-compaction-retention=1 \\</span><br><span class="line">  --max-request-bytes=33554432 \\</span><br><span class="line">  --quota-backend-bytes=6442450944 \\</span><br><span class="line">  --heartbeat-interval=250 \\</span><br><span class="line">  --election-timeout=2000</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>WorkingDirectory</code>、<code>--data-dir</code>：指定工作目录和数据目录为 <code>${ETCD_DATA_DIR}</code>，需在启动服务前创建这个目录；</li><li><code>--wal-dir</code>：指定 wal 目录，为了提高性能，一般使用 SSD 或者和 <code>--data-dir</code> 不同的磁盘；</li><li><code>--name</code>：指定节点名称，当 <code>--initial-cluster-state</code> 值为 <code>new</code> 时，<code>--name</code> 的参数值必须位于 <code>--initial-cluster</code> 列表中；</li><li><code>--cert-file</code>、<code>--key-file</code>：etcd server 与 client 通信时使用的证书和私钥；</li><li><code>--trusted-ca-file</code>：签名 client 证书的 CA 证书，用于验证 client 证书；</li><li><code>--peer-cert-file</code>、<code>--peer-key-file</code>：etcd 与 peer 通信使用的证书和私钥；</li><li><code>--peer-trusted-ca-file</code>：签名 peer 证书的 CA 证书，用于验证 peer 证书；</li></ul><p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 2; i++ ))</span><br><span class="line">  do</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; etcd.service.template &gt; etcd-$&#123;NODE_IPS[i]&#125;.service </span><br><span class="line">  done</span><br><span class="line">ls *.service</span><br></pre></td></tr></table></figure><p>分发生成的 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp etcd-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/etcd.service</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h1 id="启动-etcd-服务"><a href="#启动-etcd-服务" class="headerlink" title="启动 etcd 服务"></a>启动 etcd 服务</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;ETCD_DATA_DIR&#125; $&#123;ETCD_WAL_DIR&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl restart etcd &quot; &amp;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="检查结果"><a href="#检查结果" class="headerlink" title="检查结果"></a>检查结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status etcd|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -u etcd</span><br></pre></td></tr></table></figure><h2 id="验证服务状态"><a href="#验证服务状态" class="headerlink" title="验证服务状态"></a>验证服务状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ETCDCTL_API=3 /opt/k8s/bin/etcdctl \</span><br><span class="line">    --endpoints=https://$&#123;node_ip&#125;:2379 \</span><br><span class="line">    --cacert=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">    --cert=/etc/etcd/cert/etcd.pem \</span><br><span class="line">    --key=/etc/etcd/cert/etcd-key.pem endpoint health</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="结果显示"><a href="#结果显示" class="headerlink" title="结果显示"></a>结果显示</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">&gt;   do</span><br><span class="line">&gt;     echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">&gt;     ETCDCTL_API=3 /opt/k8s/bin/etcdctl \</span><br><span class="line">&gt;     --endpoints=https://$&#123;node_ip&#125;:2379 \</span><br><span class="line">&gt;     --cacert=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">&gt;     --cert=/etc/etcd/cert/etcd.pem \</span><br><span class="line">&gt;     --key=/etc/etcd/cert/etcd-key.pem endpoint health</span><br><span class="line">&gt;   done</span><br><span class="line">&gt;&gt;&gt; 192.168.6.101</span><br><span class="line">https://192.168.6.101:2379 is healthy: successfully committed proposal: took = 2.45561ms</span><br><span class="line">&gt;&gt;&gt; 192.168.6.102</span><br><span class="line">https://192.168.6.102:2379 is healthy: successfully committed proposal: took = 3.898134ms</span><br></pre></td></tr></table></figure><h2 id="查看当前的-leader"><a href="#查看当前的-leader" class="headerlink" title="查看当前的 leader"></a>查看当前的 leader</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">ETCDCTL_API=3 /opt/k8s/bin/etcdctl \</span><br><span class="line">  -w table --cacert=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --cert=/etc/etcd/cert/etcd.pem \</span><br><span class="line">  --key=/etc/etcd/cert/etcd-key.pem \</span><br><span class="line">  --endpoints=$&#123;ETCD_ENDPOINTS&#125; endpoint status</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# ETCDCTL_API=3 /opt/k8s/bin/etcdctl \</span><br><span class="line">&gt;   -w table --cacert=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">&gt;   --cert=/etc/etcd/cert/etcd.pem \</span><br><span class="line">&gt;   --key=/etc/etcd/cert/etcd-key.pem \</span><br><span class="line">&gt;   --endpoints=$&#123;ETCD_ENDPOINTS&#125; endpoint status</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">| https://192.168.6.101:2379 | 77fff77a6e7d24c5 |  3.3.13 |  864 kB |      true |         8 |      41721 |</span><br><span class="line">| https://192.168.6.102:2379 |  e82e7402173c61e |  3.3.13 |  872 kB |     false |         8 |      41721 |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br></pre></td></tr></table></figure><p>可以看到6.101为leader</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;ETCD 是一个高可用的分布式键值数据库，可用于服务发现。ETCD 采用 raft 一致性算法，基于 Go 语言实现。&lt;/p&gt;
&lt;figur
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
</feed>
