<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>舒宇的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://shenshengkun.github.io/"/>
  <updated>2019-07-30T08:56:19.753Z</updated>
  <id>https://shenshengkun.github.io/</id>
  
  <author>
    <name>Shu Yu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k8s部署jenkins动态slave</title>
    <link href="https://shenshengkun.github.io/posts/dkkd644a.html"/>
    <id>https://shenshengkun.github.io/posts/dkkd644a.html</id>
    <published>2019-07-29T07:10:01.000Z</published>
    <updated>2019-07-30T08:56:19.753Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kubernetes-环境下面使用-Jenkins-有什么好处"><a href="#Kubernetes-环境下面使用-Jenkins-有什么好处" class="headerlink" title="Kubernetes 环境下面使用 Jenkins 有什么好处"></a>Kubernetes 环境下面使用 Jenkins 有什么好处</h1><p>我们知道持续构建与发布是我们日常工作中必不可少的一个步骤，目前大多公司都采用 Jenkins 集群来搭建符合需求的 CI/CD 流程，然而传统的 Jenkins Slave 一主多从方式会存在一些痛点，比如：</p><ul><li>主 Master 发生单点故障时，整个流程都不可用了</li><li>每个 Slave 的配置环境不一样，来完成不同语言的编译打包等操作，但是这些差异化的配置导致管理起来非常不方便，维护起来也是比较费劲</li><li>资源分配不均衡，有的 Slave 要运行的 job 出现排队等待，而有的 Slave 处于空闲状态</li><li>资源有浪费，每台 Slave 可能是物理机或者虚拟机，当 Slave 处于空闲状态时，也不会完全释放掉资源。</li></ul><p>正因为上面的这些种种痛点，我们渴望一种更高效更可靠的方式来完成这个 CI/CD 流程，而 Docker 虚拟化容器技术能很好的解决这个痛点，又特别是在 Kubernetes 集群环境下面能够更好来解决上面的问题。</p><p>这种方式的工作流程大致为：当 Jenkins Master 接受到 Build 请求时，会根据配置的 Label 动态创建一个运行在 Pod 中的 Jenkins Slave 并注册到 Master 上，当运行完 Job 后，这个 Slave 会被注销并且这个 Pod 也会自动删除，恢复到最初状态。</p><p>那么我们使用这种方式带来了哪些好处呢？</p><ul><li><strong>服务高可用</strong>，当 Jenkins Master 出现故障时，Kubernetes 会自动创建一个新的 Jenkins Master 容器，并且将 Volume 分配给新创建的容器，保证数据不丢失，从而达到集群服务高可用。</li><li><strong>动态伸缩</strong>，合理使用资源，每次运行 Job 时，会自动创建一个 Jenkins Slave，Job 完成后，Slave 自动注销并删除容器，资源自动释放，而且 Kubernetes 会根据每个资源的使用情况，动态分配 Slave 到空闲的节点上创建，降低出现因某节点资源利用率高，还排队等待在该节点的情况。</li><li><strong>扩展性好</strong>，当 Kubernetes 集群的资源严重不足而导致 Job 排队等待时，可以很容易的添加一个 Kubernetes Node 到集群中，从而实现扩展。</li></ul><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h2 id="创建serviceAccount"><a href="#创建serviceAccount" class="headerlink" title="创建serviceAccount"></a>创建serviceAccount</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 jenkins]# cat rbac.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins2</span><br><span class="line">  namespace: default</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins2</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;extensions&quot;, &quot;apps&quot;]</span><br><span class="line">    resources: [&quot;deployments&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;delete&quot;, &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;patch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;services&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;delete&quot;, &quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;patch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;pods&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;pods/exec&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;pods/log&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;secrets&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins2</span><br><span class="line">  namespace: default</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: jenkins2</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: jenkins2</span><br><span class="line">    namespace: default</span><br></pre></td></tr></table></figure><h2 id="jenkins"><a href="#jenkins" class="headerlink" title="jenkins"></a>jenkins</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 jenkins]# cat jenkins.yaml </span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins2</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: jenkins2</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 10</span><br><span class="line">      serviceAccount: jenkins2</span><br><span class="line">      containers:</span><br><span class="line">      - name: jenkins</span><br><span class="line">        image: jenkins/jenkins:lts</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          name: web</span><br><span class="line">          protocol: TCP</span><br><span class="line">        - containerPort: 50000</span><br><span class="line">          name: agent</span><br><span class="line">          protocol: TCP</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 1Gi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 512Mi</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /login</span><br><span class="line">            port: 8080</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">          failureThreshold: 12</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /login</span><br><span class="line">            port: 8080</span><br><span class="line">          initialDelaySeconds: 60</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">          failureThreshold: 12</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: jenkinshome</span><br><span class="line">          subPath: jenkins2</span><br><span class="line">          mountPath: /var/jenkins_home</span><br><span class="line">        env:</span><br><span class="line">        - name: LIMITS_MEMORY</span><br><span class="line">          valueFrom:</span><br><span class="line">            resourceFieldRef:</span><br><span class="line">              resource: limits.memory</span><br><span class="line">              divisor: 1Mi</span><br><span class="line">        - name: JAVA_OPTS</span><br><span class="line">          value: -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 -Duser.timezone=Asia/Shanghai</span><br><span class="line">      securityContext:</span><br><span class="line">        fsGroup: 1000</span><br><span class="line">      volumes:</span><br><span class="line">      - name: jenkinshome</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: cephfs-pvc</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: jenkins2</span><br><span class="line">  namespace: default</span><br><span class="line">  labels:</span><br><span class="line">    app: jenkins2</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: jenkins2</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - name: web</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: web</span><br><span class="line">    nodePort: 30002</span><br><span class="line">  - name: agent</span><br><span class="line">    port: 50000</span><br><span class="line">    targetPort: agent</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 jenkins]# kubectl apply -f rbac.yaml </span><br><span class="line">serviceaccount/jenkins2 created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/jenkins2 created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/jenkins2 created</span><br><span class="line">[root@node1 jenkins]# kubectl create -f jenkins.yaml </span><br><span class="line">deployment.extensions/jenkins2 created</span><br><span class="line">service/jenkins2 created</span><br><span class="line"></span><br><span class="line">[root@node1 jenkins]# kubectl get pods</span><br><span class="line">NAME                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">busybox                     1/1     Running   291        12d</span><br><span class="line">jenkins2-5f76f7f8b5-cqwb4   1/1     Running   0          17m</span><br></pre></td></tr></table></figure><p>访问：</p><p><a href="http://192.168.6.102:30002" target="_blank" rel="noopener">http://192.168.6.102:30002</a></p><p>初始化的密码我们可以在 jenkins 的容器的日志中进行查看，也可以直接在cephfs的共享数据目录中查看： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 secrets]# cat /mnt/cephfs/jenkins2/secrets/initialAdminPassword </span><br><span class="line">fc5cf245b9924616b4d430a50ad413dc</span><br></pre></td></tr></table></figure><h1 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h1><p>第1步. 我们需要安装<strong>kubernetes plugin</strong>， 点击 Manage Jenkins -&gt; Manage Plugins -&gt; Available -&gt; Kubernetes勾选安装即可。 </p><p>第2步. 安装完毕后，点击 Manage Jenkins —&gt; Configure System —&gt; (拖到最下方)Add a new cloud —&gt; 选择 Kubernetes，然后填写 Kubernetes 和 Jenkins 配置信息。  </p><p><img src="https://shenshengkun.github.io/images/k8s_jenkins1.png" alt=""></p><p>第3步. 配置 Pod Template，其实就是配置 Jenkins Slave 运行的 Pod 模板，命名空间我们同样是用 kube-ops，Labels 这里也非常重要，对于后面执行 Job 的时候需要用到该值 </p><p><img src="https://shenshengkun.github.io/images/k8s_jenkins2.png" alt=""></p><p>另外需要注意我们这里需要在下面挂载两个主机目录，一个是 /var/run/docker.sock，该文件是用于 Pod 中的容器能够共享宿主机的 Docker，这就是大家说的 docker in docker 的方式，Docker 二进制文件我们已经打包到上面的镜像中了，另外一个目录下 /root/.kube 目录，我们将这个目录挂载到容器的 /home/jenkins/.kube 目录下面这是为了让我们能够在 Pod 的容器中能够使用 kubectl 工具来访问我们的 Kubernetes 集群，方便我们后面在 Slave Pod 部署 Kubernetes 应用。  </p><p>另外还有几个参数需要注意，如下图中的<strong>Time in minutes to retain slave when idle</strong>，这个参数表示的意思是当处于空闲状态的时候保留 Slave Pod 多长时间，这个参数最好我们保存默认就行了，如果你设置过大的话，Job 任务执行完成后，对应的 Slave Pod 就不会立即被销毁删除。  </p><p>Jenkins Slave Pod 中没有配置权限，所以需要配置上 ServiceAccount，在 Slave Pod 配置的地方点击下面的高级，添加上对应的 ServiceAccount ，jenkins2</p><p><img src="https://shenshengkun.github.io/images/k8s_jenkins3.png" alt=""></p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>Kubernetes 插件的配置工作完成了，接下来我们就来添加一个 Job 任务，看是否能够在 Slave Pod 中执行，任务执行完成后看 Pod 是否会被销毁。</p><p>在 Jenkins 首页点击<strong>create new jobs</strong>，创建一个测试的任务，输入任务名称，然后我们选择 Freestyle project 类型的任务： </p><p><img src="https://shenshengkun.github.io/images/k8s_jenkins4.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;测试 Kubernetes 动态生成 jenkins slave&quot;</span><br><span class="line">echo &quot;==============docker in docker===========&quot;</span><br><span class="line">docker info</span><br><span class="line"></span><br><span class="line">echo &quot;=============kubectl=============&quot;</span><br><span class="line">kubectl get pods</span><br></pre></td></tr></table></figure><p>我们直接在页面点击做成的 Build now 触发构建即可，然后观察 Kubernetes 集群中 Pod 的变化 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Kubernetes-环境下面使用-Jenkins-有什么好处&quot;&gt;&lt;a href=&quot;#Kubernetes-环境下面使用-Jenkins-有什么好处&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes 环境下面使用 Jenkins 有什么好
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>pxc集群部署</title>
    <link href="https://shenshengkun.github.io/posts/dlfl674a.html"/>
    <id>https://shenshengkun.github.io/posts/dlfl674a.html</id>
    <published>2019-07-25T02:49:10.000Z</published>
    <updated>2019-07-29T07:35:25.659Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pxc介绍"><a href="#pxc介绍" class="headerlink" title="pxc介绍"></a>pxc介绍</h1><p>​    galera产品是以galera cluster方式为mysql提高高可用集群解决方案的。galera cluster就是集成了galera插件的mysql集群。galera replication是codership提供的mysql数据同步方案，具有高可用性，方便扩展，并且可以实现多个mysql节点间的数据同步复制与读写，可保障数据库的服务高可用及数据强一致性。 </p><p>​    PXC属于一套近乎完美的mysql高可用集群解决方案，相比那些比较传统的基于主从复制模式的集群架构MHA和MM+keepalived，galera cluster最突出特点就是解决了诟病已久的数据复制延迟问题，基本上可以达到实时同步。而且节点与节点之间，他们相互的关系是对等的。本身galera cluster也是一种多主架构。galera cluster最关注的是数据的一致性，对待事物的行为时，要么在所有节点上执行，要么都不执行，它的实现机制决定了它对待一致性的行为非常严格，这也能非常完美的保证MySQL集群的数据一致性； </p><p>​    对galera cluster的封装有两个，虽然名称不同，但实质都是一样的，使用的都是galera cluster。一个MySQL的创始人在自己全新的MariaDB上实现的MAriaDB cluster；一个是著名的MySQL服务和工具提供商percona实现的percona xtradb cluster，简称PXC 。</p><p>​    要搭建PXC架构至少需要3个mysql实例来组成一个集群，三个实例之间不是主从模式，而是各自为主，所以三者是对等关系，不分从属，这就叫multi-master架构。客户端写入和读取数据时，连接哪个实例都是一样的。读取到的数据时相同的，写入任意一个实例之后，集群自己会将新写入的数据同步到其他实例上，这种架构不共享任何数据，是一种高冗余架构。 </p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h2 id="部署准备工作"><a href="#部署准备工作" class="headerlink" title="部署准备工作"></a>部署准备工作</h2><table><thead><tr><th>主机名</th><th>IP</th></tr></thead><tbody><tr><td>pxc1</td><td>192.168.6.201   192.168.6.200</td></tr><tr><td>pxc2</td><td>192.168.6.202</td></tr><tr><td>pxc3</td><td>192.168.6.203</td></tr></tbody></table><p>关防火墙、selinux、ntp时间同步</p><h2 id="安装pxc"><a href="#安装pxc" class="headerlink" title="安装pxc"></a>安装pxc</h2><h3 id="rpm安装"><a href="#rpm安装" class="headerlink" title="rpm安装"></a>rpm安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">##安装yum源</span><br><span class="line"> yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpm</span><br><span class="line"> ##定位到pxc57的仓库</span><br><span class="line">  percona-release setup pxc57</span><br><span class="line"> #解压</span><br><span class="line"> tar vxf Percona-XtraBackup-2.4.14-ref675d4-el7-x86_64-bundle.tar</span><br><span class="line"> tar vxf Percona-XtraDB-Cluster-5.7.25-31.35-r463-el7-x86_64-bundle.tar</span><br><span class="line"> ##安装rpm包用yum localinstall 命令率先从本地安装，并根据本地rpm包会在线寻找未安装的依赖(pxc官网下载的安装包会依赖其他包)</span><br><span class="line"> yum localinstall *.rpm</span><br></pre></td></tr></table></figure><h3 id="分别在三个节点创建数据存储目录"><a href="#分别在三个节点创建数据存储目录" class="headerlink" title="分别在三个节点创建数据存储目录"></a>分别在三个节点创建数据存储目录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /data/local/percona-xtradb-cluster/data</span><br><span class="line">chown -R mysql:mysql /data/local/percona-xtradb-cluster/data</span><br><span class="line">mkdir -p /data/local/percona-xtradb-cluster/run</span><br><span class="line">chown -R mysql:mysql /data/local/percona-xtradb-cluster/run</span><br><span class="line">mkdir -p /data/logs/mysql</span><br><span class="line">chown -R mysql:mysql /data/logs/mysql</span><br></pre></td></tr></table></figure><h3 id="修改-etc-my-cnf配置文件，其他两台节点需要修改server-id和wsrep-node-address"><a href="#修改-etc-my-cnf配置文件，其他两台节点需要修改server-id和wsrep-node-address" class="headerlink" title="修改/etc/my.cnf配置文件，其他两台节点需要修改server_id和wsrep_node_address"></a>修改/etc/my.cnf配置文件，其他两台节点需要修改server_id和wsrep_node_address</h3><p>pxc1：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">[root@pxc1 ~]# cat /etc/my.cnf</span><br><span class="line">[client]</span><br><span class="line">port = 3306</span><br><span class="line">socket = /data/local/percona-xtradb-cluster/run/mysql.sock</span><br><span class="line">default-character-set = utf8mb4</span><br><span class="line">[mysqld]</span><br><span class="line">user =  mysql</span><br><span class="line">innodb_buffer_pool_size = 1024M</span><br><span class="line">character_set_server = utf8mb4</span><br><span class="line">datadir = /data/local/percona-xtradb-cluster/data</span><br><span class="line">port = 3306</span><br><span class="line">server_id = 1</span><br><span class="line">socket = /data/local/percona-xtradb-cluster/run/mysql.sock</span><br><span class="line">pid-file = /data/local/percona-xtradb-cluster/run/mysql.pid</span><br><span class="line">log-error = /data/logs/mysql/error.log</span><br><span class="line">log_warnings = 2</span><br><span class="line">slow_query_log_file = /data/logs/mysql/slow.log</span><br><span class="line">long_query_time = 2</span><br><span class="line">log_timestamps=SYSTEM</span><br><span class="line">lower_case_table_names = 1</span><br><span class="line">key_buffer_size = 1344M</span><br><span class="line">event_scheduler=ON</span><br><span class="line">sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES</span><br><span class="line">wsrep_provider=/usr/lib64/libgalera_smm.so</span><br><span class="line">wsrep_cluster_address=gcomm://192.168.6.201,192.168.6.202,192.168.6.203</span><br><span class="line">binlog_format=ROW</span><br><span class="line">pxc_strict_mode=PERMISSIVE</span><br><span class="line">max_connect_errors=1000</span><br><span class="line">max_allowed_packet = 1024M</span><br><span class="line">default_storage_engine=InnoDB</span><br><span class="line">#Innodb</span><br><span class="line">innodb_flush_method = O_DIRECT</span><br><span class="line">innodb_log_files_in_group = 5</span><br><span class="line">innodb_lock_wait_timeout = 50</span><br><span class="line">innodb_log_file_size = 1024M</span><br><span class="line">innodb_flush_log_at_trx_commit = 1</span><br><span class="line">innodb_file_per_table = 1</span><br><span class="line">innodb_thread_concurrency = 8</span><br><span class="line">innodb_buffer_pool_size = 5G</span><br><span class="line">innodb_read_io_threads = 24</span><br><span class="line">innodb_write_io_threads = 24</span><br><span class="line">log_bin_trust_function_creators=1</span><br><span class="line">innodb_autoinc_lock_mode=2</span><br><span class="line">innodb_locks_unsafe_for_binlog=1</span><br><span class="line"># CACHES AND LIMITS #</span><br><span class="line">tmp_table_size = 32M</span><br><span class="line">max_heap_table_size = 32M</span><br><span class="line">max_connections = 1000</span><br><span class="line">thread_cache_size = 50</span><br><span class="line">open_files_limit = 65535</span><br><span class="line">table_definition_cache = 4096</span><br><span class="line">table_open_cache = 5000</span><br><span class="line">#wsrep</span><br><span class="line">wsrep_retry_autocommit=1</span><br><span class="line">wsrep_auto_increment_control=1</span><br><span class="line">wsrep_node_name = pxc1</span><br><span class="line">wsrep_node_address=192.168.6.201</span><br><span class="line">wsrep_sst_method=xtrabackup-v2</span><br><span class="line">wsrep_cluster_name=pxc</span><br><span class="line">wsrep_sst_auth=&quot;sstuser:secret&quot;</span><br></pre></td></tr></table></figure><p>pxc2：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">[root@pxc2 ~]# cat /etc/my.cnf</span><br><span class="line">[client]</span><br><span class="line">port = 3306</span><br><span class="line">socket = /data/local/percona-xtradb-cluster/run/mysql.sock</span><br><span class="line">default-character-set = utf8mb4</span><br><span class="line">[mysqld]</span><br><span class="line">user =  mysql</span><br><span class="line">innodb_buffer_pool_size = 1024M</span><br><span class="line">character_set_server = utf8mb4</span><br><span class="line">datadir = /data/local/percona-xtradb-cluster/data</span><br><span class="line">port = 3306</span><br><span class="line">server_id = 2</span><br><span class="line">socket = /data/local/percona-xtradb-cluster/run/mysql.sock</span><br><span class="line">pid-file = /data/local/percona-xtradb-cluster/run/mysql.pid</span><br><span class="line">log-error = /data/logs/mysql/error.log</span><br><span class="line">log_warnings = 2</span><br><span class="line">slow_query_log_file = /data/logs/mysql/slow.log</span><br><span class="line">long_query_time = 2</span><br><span class="line">log_timestamps=SYSTEM</span><br><span class="line">lower_case_table_names = 1</span><br><span class="line">key_buffer_size = 1344M</span><br><span class="line">event_scheduler=ON</span><br><span class="line">sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES</span><br><span class="line">wsrep_provider=/usr/lib64/libgalera_smm.so</span><br><span class="line">wsrep_cluster_address=gcomm://192.168.6.201,192.168.6.202,192.168.6.203</span><br><span class="line">binlog_format=ROW</span><br><span class="line">pxc_strict_mode=PERMISSIVE</span><br><span class="line">max_connect_errors=1000</span><br><span class="line">max_allowed_packet = 1024M</span><br><span class="line">default_storage_engine=InnoDB</span><br><span class="line">#Innodb</span><br><span class="line">innodb_flush_method = O_DIRECT</span><br><span class="line">innodb_log_files_in_group = 5</span><br><span class="line">innodb_lock_wait_timeout = 50</span><br><span class="line">innodb_log_file_size = 1024M</span><br><span class="line">innodb_flush_log_at_trx_commit = 1</span><br><span class="line">innodb_file_per_table = 1</span><br><span class="line">innodb_thread_concurrency = 8</span><br><span class="line">innodb_buffer_pool_size = 5G</span><br><span class="line">innodb_read_io_threads = 24</span><br><span class="line">innodb_write_io_threads = 24</span><br><span class="line">log_bin_trust_function_creators=1</span><br><span class="line">innodb_autoinc_lock_mode=2</span><br><span class="line">innodb_locks_unsafe_for_binlog=1</span><br><span class="line"># CACHES AND LIMITS #</span><br><span class="line">tmp_table_size = 32M</span><br><span class="line">max_heap_table_size = 32M</span><br><span class="line">max_connections = 1000</span><br><span class="line">thread_cache_size = 50</span><br><span class="line">open_files_limit = 65535</span><br><span class="line">table_definition_cache = 4096</span><br><span class="line">table_open_cache = 5000</span><br><span class="line">#wsrep</span><br><span class="line">wsrep_retry_autocommit=1</span><br><span class="line">wsrep_auto_increment_control=1</span><br><span class="line">wsrep_node_name = pxc2</span><br><span class="line">wsrep_node_address=192.168.6.202</span><br><span class="line">wsrep_sst_method=xtrabackup-v2</span><br><span class="line">wsrep_cluster_name=pxc</span><br><span class="line">wsrep_sst_auth=&quot;sstuser:secret&quot;</span><br></pre></td></tr></table></figure><p>pxc3:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">[root@pxc3 ~]# cat /etc/my.cnf</span><br><span class="line">[client]</span><br><span class="line">port = 3306</span><br><span class="line">socket = /data/local/percona-xtradb-cluster/run/mysql.sock</span><br><span class="line">default-character-set = utf8mb4</span><br><span class="line">[mysqld]</span><br><span class="line">user =  mysql</span><br><span class="line">innodb_buffer_pool_size = 1024M</span><br><span class="line">character_set_server = utf8mb4</span><br><span class="line">datadir = /data/local/percona-xtradb-cluster/data</span><br><span class="line">port = 3306</span><br><span class="line">server_id = 3</span><br><span class="line">socket = /data/local/percona-xtradb-cluster/run/mysql.sock</span><br><span class="line">pid-file = /data/local/percona-xtradb-cluster/run/mysql.pid</span><br><span class="line">log-error = /data/logs/mysql/error.log</span><br><span class="line">log_warnings = 2</span><br><span class="line">slow_query_log_file = /data/logs/mysql/slow.log</span><br><span class="line">long_query_time = 2</span><br><span class="line">log_timestamps=SYSTEM</span><br><span class="line">lower_case_table_names = 1</span><br><span class="line">key_buffer_size = 1344M</span><br><span class="line">event_scheduler=ON</span><br><span class="line">sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES</span><br><span class="line">wsrep_provider=/usr/lib64/libgalera_smm.so</span><br><span class="line">wsrep_cluster_address=gcomm://192.168.6.201,192.168.6.202,192.168.6.203</span><br><span class="line">binlog_format=ROW</span><br><span class="line">pxc_strict_mode=PERMISSIVE</span><br><span class="line">max_connect_errors=1000</span><br><span class="line">max_allowed_packet = 1024M</span><br><span class="line">default_storage_engine=InnoDB</span><br><span class="line">#Innodb</span><br><span class="line">innodb_flush_method = O_DIRECT</span><br><span class="line">innodb_log_files_in_group = 5</span><br><span class="line">innodb_lock_wait_timeout = 50</span><br><span class="line">innodb_log_file_size = 1024M</span><br><span class="line">innodb_flush_log_at_trx_commit = 1</span><br><span class="line">innodb_file_per_table = 1</span><br><span class="line">innodb_thread_concurrency = 8</span><br><span class="line">innodb_buffer_pool_size = 5G</span><br><span class="line">innodb_read_io_threads = 24</span><br><span class="line">innodb_write_io_threads = 24</span><br><span class="line">log_bin_trust_function_creators=1</span><br><span class="line">innodb_autoinc_lock_mode=2</span><br><span class="line">innodb_locks_unsafe_for_binlog=1</span><br><span class="line"># CACHES AND LIMITS #</span><br><span class="line">tmp_table_size = 32M</span><br><span class="line">max_heap_table_size = 32M</span><br><span class="line">max_connections = 1000</span><br><span class="line">thread_cache_size = 50</span><br><span class="line">open_files_limit = 65535</span><br><span class="line">table_definition_cache = 4096</span><br><span class="line">table_open_cache = 5000</span><br><span class="line">#wsrep</span><br><span class="line">wsrep_retry_autocommit=1</span><br><span class="line">wsrep_auto_increment_control=1</span><br><span class="line">wsrep_node_name = pxc3</span><br><span class="line">wsrep_node_address=192.168.6.203</span><br><span class="line">wsrep_sst_method=xtrabackup-v2</span><br><span class="line">wsrep_cluster_name=pxc</span><br><span class="line">wsrep_sst_auth=&quot;sstuser:secret&quot;</span><br></pre></td></tr></table></figure><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>主节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service  mysql@bootstrap.service  start</span><br></pre></td></tr></table></figure><p>其他节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql start</span><br></pre></td></tr></table></figure><h3 id="启动第二个节点时遇到sst连接失败的问题"><a href="#启动第二个节点时遇到sst连接失败的问题" class="headerlink" title="启动第二个节点时遇到sst连接失败的问题"></a>启动第二个节点时遇到sst连接失败的问题</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">将wsrep.cnf 配置文件的wsrep_sst_method改为rsync,成功启动集群后停止集群，然后改回xtrabackup-v2,再启动集群节点。</span><br></pre></td></tr></table></figure><h3 id="修改密码"><a href="#修改密码" class="headerlink" title="修改密码"></a>修改密码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mysql5.7版本日志均在error.log 里面生成</span><br><span class="line">grep &quot;temporary password&quot;  /data/logs/mysql/error.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">使用改密码登陆MySQL，修改成自己想要的密码</span><br><span class="line">mysql&gt; alter user &apos;root&apos;@&apos;localhost&apos; idnetified by &apos;pxc&apos;;</span><br><span class="line"></span><br><span class="line">配置SST认证账号</span><br><span class="line">CREATE USER &apos;sstuser&apos;@&apos;192.168.%.%&apos; IDENTIFIED BY &apos;secret&apos;;</span><br><span class="line">GRANT RELOAD, LOCK TABLES, PROCESS, REPLICATION CLIENT ON *.* TO &apos;sstuser&apos;@&apos;192.168.%.%&apos;;</span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><h3 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h3><p>查看节点数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show global status like &apos;wsrep_cluster_size&apos;;</span><br></pre></td></tr></table></figure><p>查看集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show global status like &apos;wsrep%&apos;;</span><br></pre></td></tr></table></figure><p>查看当前节点状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show global status like &apos;wsrep_cluster_status&apos;;</span><br></pre></td></tr></table></figure><h1 id="安装keepalive"><a href="#安装keepalive" class="headerlink" title="安装keepalive"></a>安装keepalive</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">6.200为虚ip</span><br><span class="line">在192.168.6.201,6.202上</span><br><span class="line">yum -y install keepalived</span><br></pre></td></tr></table></figure><p>主：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@pxc1 ~]# cat /etc/keepalived/keepalived.conf </span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id LVS_mysql</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script Checkmysql &#123;</span><br><span class="line">    script &quot;curl -k http://192.168.7.201:3306&quot; # vip</span><br><span class="line">    interval 3</span><br><span class="line">    timeout 9</span><br><span class="line">    fall 2</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER</span><br><span class="line">    interface ens160 # 本地网卡名称</span><br><span class="line">    virtual_router_id 61</span><br><span class="line">    priority 120 # 权重,要唯一</span><br><span class="line">    advert_int 1</span><br><span class="line">    mcast_src_ip 192.168.7.201 # 本地IP</span><br><span class="line">    nopreempt</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass sqP05dQgMSlzrxHj</span><br><span class="line">    &#125;</span><br><span class="line">    unicast_peer &#123;</span><br><span class="line">        192.168.7.202</span><br><span class="line">        192.168.7.203</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.7.200/24 # VIP</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">        Checkmysql</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@pxc2 ~]# cat /etc/keepalived/keepalived.conf </span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id LVS_mysql</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script Checkmysql &#123;</span><br><span class="line">    script &quot;curl -k http://192.168.7.201:3306&quot; # vip</span><br><span class="line">    interval 3</span><br><span class="line">    timeout 9</span><br><span class="line">    fall 2</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state BACKUP</span><br><span class="line">    interface ens160 # 本地网卡名称</span><br><span class="line">    virtual_router_id 61</span><br><span class="line">    priority 80 # 权重,要唯一</span><br><span class="line">    advert_int 1</span><br><span class="line">    mcast_src_ip 192.168.7.202 # 本地IP</span><br><span class="line">    nopreempt</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass sqP05dQgMSlzrxHj</span><br><span class="line">    &#125;</span><br><span class="line">    unicast_peer &#123;</span><br><span class="line">        192.168.7.201</span><br><span class="line">        192.168.7.203</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.7.200/24 # VIP</span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">        Checkmysql</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>登录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@pxc1 ~]# mysql -usstuser -psecret --host 192.168.6.200</span><br><span class="line">mysql: [Warning] Using a password on the command line interface can be insecure.</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 1099</span><br><span class="line">Server version: 5.7.25-28-57 Percona XtraDB Cluster (GPL), Release rel28, Revision a2ef85f, WSREP version 31.35, wsrep_31.35</span><br><span class="line"></span><br><span class="line">Copyright (c) 2009-2019 Percona LLC and/or its affiliates</span><br><span class="line">Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;pxc介绍&quot;&gt;&lt;a href=&quot;#pxc介绍&quot; class=&quot;headerlink&quot; title=&quot;pxc介绍&quot;&gt;&lt;/a&gt;pxc介绍&lt;/h1&gt;&lt;p&gt;​    galera产品是以galera cluster方式为mysql提高高可用集群解决方案的。galera c
      
    
    </summary>
    
      <category term="中间件" scheme="https://shenshengkun.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s上用cephfs存储</title>
    <link href="https://shenshengkun.github.io/posts/0rfhdj22.html"/>
    <id>https://shenshengkun.github.io/posts/0rfhdj22.html</id>
    <published>2019-07-22T03:10:01.000Z</published>
    <updated>2019-07-29T07:01:10.799Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>k8s对ceph rbd模式不支持<strong>ReadWriteMany（RWX）</strong>,为了满足k8s的灵活性需求,采用支持多点挂载的cephfs工作模式 </p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="ceph端"><a href="#ceph端" class="headerlink" title="ceph端"></a>ceph端</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mds create ceph</span><br><span class="line"></span><br><span class="line">为cephfs创建存储池</span><br><span class="line">ceph osd pool create cephfs_data 8</span><br><span class="line">ceph osd pool create  cephfs_metadata 8</span><br><span class="line">ceph fs new cephfs cephfs_metadata    cephfs_data</span><br></pre></td></tr></table></figure><p>查看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ceph]# ceph  fs  ls</span><br><span class="line">name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</span><br><span class="line"></span><br><span class="line">[root@node1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     cbc04385-1cdf-4512-a3f5-a5b3e8686a05</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph</span><br><span class="line">    mgr: ceph(active)</span><br><span class="line">    mds: cephfs-1/1/1 up  &#123;0=ceph=up:active&#125;</span><br><span class="line">    osd: 1 osds: 1 up, 1 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   3 pools, 106 pgs</span><br><span class="line">    objects: 73 objects, 137MiB</span><br><span class="line">    usage:   3.82GiB used, 16.2GiB / 20.0GiB avail</span><br><span class="line">    pgs:     106 active+clean</span><br></pre></td></tr></table></figure><h2 id="k8s挂载ceph"><a href="#k8s挂载ceph" class="headerlink" title="k8s挂载ceph"></a>k8s挂载ceph</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ceph]# cat admin.secret </span><br><span class="line">AQA6EzBd5zAwIRAAgs+NyCjoAKuSrQKoFvQi9w==</span><br><span class="line"></span><br><span class="line">[root@node1 ceph]# mkdir /mnt/cephfs</span><br><span class="line"></span><br><span class="line">[root@node1 ceph]# mount -t ceph 192.168.6.101:6789:/ /mnt/cephfs -o name=admin,secretfile=/etc/ceph/admin.secret</span><br><span class="line"></span><br><span class="line">[root@node1 ceph]# df -h</span><br><span class="line">Filesystem            Size  Used Avail Use% Mounted on</span><br><span class="line">devtmpfs              2.0G     0  2.0G   0% /dev</span><br><span class="line">tmpfs                 2.0G     0  2.0G   0% /dev/shm</span><br><span class="line">tmpfs                 2.0G  198M  1.8G  10% /run</span><br><span class="line">tmpfs                 2.0G     0  2.0G   0% /sys/fs/cgroup</span><br><span class="line">/dev/mapper/cl-root    36G   12G   24G  34% /</span><br><span class="line">/dev/sda1            1014M  216M  799M  22% /boot</span><br><span class="line">tmpfs                 2.0G   12K  2.0G   1% /data/k8s/k8s/kubelet/pods/42d3eb88-aaba-11e9-b1ef-005056b1d2de/volumes/kubernetes.io~secret/coredns-token-qklgj</span><br><span class="line">overlay                36G   12G   24G  34% /data/k8s/docker/data/overlay2/f651aa6ff6070b5a3acb8ba0e8d810a1d699a7853e0d3df802851fa64fc2c029/merged</span><br><span class="line">shm                    64M     0   64M   0% /data/k8s/docker/data/containers/531654eae411cc84ed7ed3c2d5e60afa2bd68cca57cf821636369be5869f6f7d/mounts/shm</span><br><span class="line">tmpfs                 2.0G   12K  2.0G   1% /data/k8s/k8s/kubelet/pods/4eb99bd2-aaba-11e9-b1ef-005056b1d2de/volumes/kubernetes.io~secret/default-token-npj5x</span><br><span class="line">overlay                36G   12G   24G  34% /data/k8s/docker/data/overlay2/1707081196a127bbf0f46f57a9a6831c8f6d87a6f92dad5a578375d24eb41580/merged</span><br><span class="line">shm                    64M     0   64M   0% /data/k8s/docker/data/containers/b6b70ef43252b12414afc08562ceb4002467ff700acb01941dce73cf9e40d88a/mounts/shm</span><br><span class="line">overlay                36G   12G   24G  34% /data/k8s/docker/data/overlay2/3ccf15f58beb54a2b59ab2c5c421ac3a8f3f2d5896b5b12430a8f672783eeb84/merged</span><br><span class="line">tmpfs                 396M     0  396M   0% /run/user/0</span><br><span class="line">192.168.6.101:6789:/   20G  3.9G   17G  20% /mnt/cephfs</span><br></pre></td></tr></table></figure><h3 id="创建secret"><a href="#创建secret" class="headerlink" title="创建secret"></a>创建secret</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ceph]# cat cephfs-secret.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-secret</span><br><span class="line">data:</span><br><span class="line">  key: QVFBNkV6QmQ1ekF3SVJBQWdzK055Q2pvQUt1U3JRS29GdlFpOXc9PQ==</span><br></pre></td></tr></table></figure><h3 id="pv"><a href="#pv" class="headerlink" title="pv"></a>pv</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ceph]# cat cephfs-pv.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-pv</span><br><span class="line">  labels:</span><br><span class="line">    pv: cephfs-pv</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 1Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  cephfs:</span><br><span class="line">    monitors:</span><br><span class="line">      - 192.168.6.101:6789</span><br><span class="line">    user: admin</span><br><span class="line">    secretRef:</span><br><span class="line">      name: ceph-secret</span><br><span class="line">    readOnly: false</span><br><span class="line">  persistentVolumeReclaimPolicy: Delete</span><br></pre></td></tr></table></figure><h3 id="pvc"><a href="#pvc" class="headerlink" title="pvc"></a>pvc</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ceph]# cat cephfs-pvc.yaml </span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: cephfs-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      pv: cephfs-pv</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;k8s对ceph rbd模式不支持&lt;strong&gt;ReadWriteMany（RWX）&lt;/strong&gt;,为了满足k8s的灵活性需求,采用支
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>rbd无法map</title>
    <link href="https://shenshengkun.github.io/posts/dl9876la.html"/>
    <id>https://shenshengkun.github.io/posts/dl9876la.html</id>
    <published>2019-07-19T09:10:01.000Z</published>
    <updated>2019-07-22T08:32:24.811Z</updated>
    
    <content type="html"><![CDATA[<h1 id="默认开启了rbd的一些属性"><a href="#默认开启了rbd的一些属性" class="headerlink" title="默认开启了rbd的一些属性"></a>默认开启了rbd的一些属性</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# ceph --show-config|grep rbd|grep features</span><br><span class="line">rbd_default_features = 61</span><br></pre></td></tr></table></figure><p>RBD属性表： </p><p><img src="https://shenshengkun.github.io/images/ceph_rbd_shuxing.png" alt=""></p><p>61的意思是上面图中的bit码相加得到的值<br>对rbd进行内核的map操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# rbd map mytest</span><br><span class="line">rbd: sysfs write failed</span><br><span class="line">RBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable&quot;.</span><br><span class="line">In some cases useful info is found in syslog - try &quot;dmesg | tail&quot; or so.</span><br><span class="line">rbd: map failed: (6) No such device or address</span><br></pre></td></tr></table></figure><p>根据提示查询打印的信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# dmesg | tail</span><br><span class="line">[10440.462708] rbd: image mytest: image uses unsupported features: 0x3c</span><br></pre></td></tr></table></figure><p>这个地方提示的很清楚了，不支持的属性0x3c，0x3c是16进制的数值，换算成10进制是3*16+12=60<br>60的意思是不支持：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">32+16+8+4 = exclusive-lock, object-map, fast-diff, deep-flatten</span><br><span class="line"></span><br><span class="line">也就是不支持这些属性，现在动态关闭这些属性</span><br></pre></td></tr></table></figure><h1 id="关闭这些属性"><a href="#关闭这些属性" class="headerlink" title="关闭这些属性"></a>关闭这些属性</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd feature disable &#123;poolname&#125;/&#123;imagename&#125; &#123;feature&#125;</span><br></pre></td></tr></table></figure><p>如果不想动态的关闭，那么在创建rbd之前，在配置文件中设置这个参数即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rbd_default_features = 3</span><br><span class="line"></span><br><span class="line">关于属性支持的，目前到内核4.6仍然只支持</span><br><span class="line"></span><br><span class="line">layering,striping = 1 + 2</span><br><span class="line"></span><br><span class="line">这两个属性</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;默认开启了rbd的一些属性&quot;&gt;&lt;a href=&quot;#默认开启了rbd的一些属性&quot; class=&quot;headerlink&quot; title=&quot;默认开启了rbd的一些属性&quot;&gt;&lt;/a&gt;默认开启了rbd的一些属性&lt;/h1&gt;&lt;figure class=&quot;highlight plai
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s上用ceph-rbd存储</title>
    <link href="https://shenshengkun.github.io/posts/0ijfj445.html"/>
    <id>https://shenshengkun.github.io/posts/0ijfj445.html</id>
    <published>2019-07-19T02:10:01.000Z</published>
    <updated>2019-07-19T03:29:49.676Z</updated>
    
    <content type="html"><![CDATA[<p>k8s默认使用的本地存储，集群容灾性差，ceph作为开源的分布式存储系统，与openstack环境搭配使用，已经很多云计算公司运用于生产环境，可靠性得到验证。这里介绍一下在k8s环境下ceph如何使用. </p><p>Kubernetes支持后两种存储接口,支持的接入模式如下图: </p><p><img src="https://shenshengkun.github.io/images/k8s-ceph1.png" alt=""></p><h1 id="ceph端"><a href="#ceph端" class="headerlink" title="ceph端"></a>ceph端</h1><h2 id="新建pool"><a href="#新建pool" class="headerlink" title="新建pool"></a>新建pool</h2><p>新建一个pool pool_1包含90个pg </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create pool_1 90</span><br></pre></td></tr></table></figure><h2 id="RBD块设备"><a href="#RBD块设备" class="headerlink" title="RBD块设备"></a>RBD块设备</h2><p>在ceph集群中新建1个rbd块设备，lun1 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rbd create pool_1/lun1 --size 10G</span><br></pre></td></tr></table></figure><h2 id="ceph权限控制"><a href="#ceph权限控制" class="headerlink" title="ceph权限控制"></a>ceph权限控制</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">使用ceph-deploy --overwrite-conf admin部署的keyring权限太大，可以自己创建一个keyring client.rdb给块设备客户端node用</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ceph auth get-or-create client.rbd mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=pool_1&apos;  &gt; ceph.client.rbd.keyring</span><br><span class="line"></span><br><span class="line">k8s节点需要安装ceph</span><br><span class="line">yum install ceph-common</span><br><span class="line">echo &apos;rbd&apos; &gt; /etc/modules-load.d/rbd.conf</span><br><span class="line">modprobe rbd</span><br><span class="line">lsmod | grep rbd</span><br><span class="line">rbd                    83640  0 </span><br><span class="line">libceph               306625  1 rbd</span><br><span class="line"></span><br><span class="line">配置文件秘钥传到k8s上</span><br><span class="line">[root@ceph ceph]# scp ceph.client.rbd.keyring 192.168.6.102:/etc/ceph/</span><br><span class="line">root@192.168.6.102&apos;s password: </span><br><span class="line">ceph.client.rdb.keyring                                                                                                       100%   63     8.5KB/s   00:00    </span><br><span class="line">[root@ceph ceph]# scp ceph.conf 192.168.6.102:/etc/ceph/                </span><br><span class="line">root@192.168.6.102&apos;s password: </span><br><span class="line">ceph.conf                                                                                                                       100%  310    25.1KB/s   00:00    </span><br><span class="line">[root@ceph ceph]#</span><br></pre></td></tr></table></figure><h1 id="k8s的node上操作"><a href="#k8s的node上操作" class="headerlink" title="k8s的node上操作"></a>k8s的node上操作</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ceph]# ceph -s --name client.rdb</span><br><span class="line">  cluster:</span><br><span class="line">    id:     cbc04385-1cdf-4512-a3f5-a5b3e8686a05</span><br><span class="line">    health: HEALTH_WARN</span><br><span class="line">            application not enabled on 1 pool(s)</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph</span><br><span class="line">    mgr: ceph(active)</span><br><span class="line">    osd: 1 osds: 1 up, 1 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   1 pools, 90 pgs</span><br><span class="line">    objects: 5 objects, 709B</span><br><span class="line">    usage:   1.00GiB used, 19.0GiB / 20.0GiB avail</span><br><span class="line">    pgs:     90 active+clean</span><br><span class="line">    </span><br><span class="line">警告解决办法：</span><br><span class="line">ceph health detail</span><br><span class="line">ceph osd pool application enable pool_1 rbd</span><br></pre></td></tr></table></figure><h2 id="map设备"><a href="#map设备" class="headerlink" title="map设备"></a>map设备</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># rbd map pool_1/lun1 --name client.rbd</span><br><span class="line">rbd: sysfs write failed</span><br><span class="line">RBD image feature set mismatch. Try disabling features unsupported by the kernel with &quot;rbd feature disable&quot;.</span><br><span class="line">In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.</span><br><span class="line">rbd: map failed: (6) No such device or address</span><br><span class="line"></span><br><span class="line">解决办法：</span><br><span class="line">在ceph节点上</span><br><span class="line">rbd feature disable pool_1/lun1 exclusive-lock, object-map, fast-diff, deep-flatten</span><br></pre></td></tr></table></figure><h2 id="将块设备挂载在操作系统中进行格式化"><a href="#将块设备挂载在操作系统中进行格式化" class="headerlink" title="将块设备挂载在操作系统中进行格式化"></a>将块设备挂载在操作系统中进行格式化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rbd map pool_1/lun1 --name client.rbd</span><br><span class="line">mkfs.ext4 /dev/rbd0</span><br></pre></td></tr></table></figure><h2 id="创建pv、pvc"><a href="#创建pv、pvc" class="headerlink" title="创建pv、pvc"></a>创建pv、pvc</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对ceph.client.admin.keyring 的内容进行base64编码</span><br><span class="line">[root@node1 ceph]# ceph auth get-key client.rbd | base64</span><br><span class="line">QVFCTktERmRzeXpKQUJBQVVvVGVvWVYyamxhRi8zNU1hZ2R2dFE9PQ==</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">根据上面的输出，创建secret ceph-client-rbd</span><br><span class="line">[root@node1 ceph]# cat ceph-secret.yml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-client-rbd</span><br><span class="line">type: &quot;kubernetes.io/rbd&quot;  </span><br><span class="line">data:</span><br><span class="line">  key: QVFCTktERmRzeXpKQUJBQVVvVGVvWVYyamxhRi8zNU1hZ2R2dFE9PQ==</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">kubectl apply -f ceph-secret.yml</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">创建pv，注意： 这里是user：rbd 而不是user: client.rbd</span><br><span class="line"></span><br><span class="line">[root@node1 ceph]# cat pv.yml </span><br><span class="line">kind: PersistentVolume</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: ceph-pool1-lun1</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 10Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce          </span><br><span class="line">  rbd:</span><br><span class="line">    fsType: ext4</span><br><span class="line">    image: lun1</span><br><span class="line">    monitors:</span><br><span class="line">      - &apos;192.168.6.101:6789&apos;</span><br><span class="line">    pool: pool_1</span><br><span class="line">    readOnly: false</span><br><span class="line">    secretRef:</span><br><span class="line">      name: ceph-client-rbd</span><br><span class="line">      namespace: default</span><br><span class="line">    user: rbd</span><br><span class="line">    </span><br><span class="line">[root@node1 ceph]# kubectl apply -f pv.yml </span><br><span class="line">persistentvolume/ceph-pool1-lun1 created</span><br><span class="line">[root@node1 ceph]# kubectl get pv</span><br><span class="line">NAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class="line">ceph-pool1-lun1   10Gi       RWO            Retain           Available           manual                  4s</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">创建pvc</span><br><span class="line"></span><br><span class="line">[root@node1 ceph]# cat pvc.yml </span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: pvc1</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 10Gi</span><br><span class="line">      </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">[root@node1 ceph]# kubectl apply -f pvc.yml </span><br><span class="line">persistentvolumeclaim/pvc1 created</span><br><span class="line">[root@node1 ceph]# kubectl get pvc</span><br><span class="line">NAME   STATUS   VOLUME            CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1   Bound    ceph-pool1-lun1   10Gi       RWO            manual         7s</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;k8s默认使用的本地存储，集群容灾性差，ceph作为开源的分布式存储系统，与openstack环境搭配使用，已经很多云计算公司运用于生产环境，可靠性得到验证。这里介绍一下在k8s环境下ceph如何使用. &lt;/p&gt;
&lt;p&gt;Kubernetes支持后两种存储接口,支持的接入模式
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>ceph对象存储集群部署</title>
    <link href="https://shenshengkun.github.io/posts/ouy564tra.html"/>
    <id>https://shenshengkun.github.io/posts/ouy564tra.html</id>
    <published>2019-07-17T07:17:01.000Z</published>
    <updated>2019-07-17T07:38:00.175Z</updated>
    
    <content type="html"><![CDATA[<h1 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.10.186   ceph1          admin、mon、mgr、osd、rgw</span><br><span class="line"></span><br><span class="line">192.168.10.187   ceph2          mon、mgr、osd、rgw </span><br><span class="line"></span><br><span class="line">192.168.10.188   ceph3          mon、mgr、osd、rgw</span><br></pre></td></tr></table></figure><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">[root@10dot186 ~]# vim /etc/hosts</span><br><span class="line">192.168.10.186   ceph1</span><br><span class="line">192.168.10.187   ceph2</span><br><span class="line">192.168.10.188   ceph3</span><br><span class="line"></span><br><span class="line">hostnamectl set-hostname ceph1</span><br><span class="line">hostnamectl set-hostname ceph2</span><br><span class="line">hostnamectl set-hostname ceph3</span><br><span class="line"></span><br><span class="line">ntpdate ntp1.aliyun.com</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id ceph1</span><br><span class="line">ssh-copy-id ceph2</span><br><span class="line">ssh-copy-id ceph3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ~]# vim /etc/yum.repos.d/ceph.repo</span><br><span class="line">[ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">priority=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">priority=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br><span class="line">yum makecache</span><br><span class="line">yum update -y</span><br><span class="line">yum install -y ceph-deploy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mkdir /etc/ceph &amp;&amp; cd /etc/ceph</span><br><span class="line">ceph-deploy new ceph1  ceph2 ceph3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">yum install -y python-setuptools</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">在配置文件中增加：</span><br><span class="line">osd_pool_default_size = 3</span><br><span class="line">[mgr]</span><br><span class="line">mgr modules = dashboard</span><br><span class="line">[mon]</span><br><span class="line">mon allow pool delete = true</span><br></pre></td></tr></table></figure><h2 id="mon"><a href="#mon" class="headerlink" title="mon"></a>mon</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy install ceph1 ceph2 ceph3</span><br><span class="line"></span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     fcb2fa5e-481a-4494-9a27-374048f37113</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure><h2 id="mgr"><a href="#mgr" class="headerlink" title="mgr"></a>mgr</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mgr create ceph1 ceph2 ceph3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     fcb2fa5e-481a-4494-9a27-374048f37113</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active), standbys: ceph2, ceph3</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs: </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph mgr dump</span><br><span class="line">&#123;</span><br><span class="line">    &quot;epoch&quot;: 4,</span><br><span class="line">    &quot;active_gid&quot;: 4122,</span><br><span class="line">    &quot;active_name&quot;: &quot;ceph1&quot;,</span><br><span class="line">    &quot;active_addr&quot;: &quot;192.168.10.186:6800/22316&quot;,</span><br><span class="line">    &quot;available&quot;: true,</span><br><span class="line">    &quot;standbys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;gid&quot;: 4129,</span><br><span class="line">            &quot;name&quot;: &quot;ceph2&quot;,</span><br><span class="line">            &quot;available_modules&quot;: [</span><br><span class="line">                &quot;balancer&quot;,</span><br><span class="line">                &quot;dashboard&quot;,</span><br><span class="line">                &quot;influx&quot;,</span><br><span class="line">                &quot;localpool&quot;,</span><br><span class="line">                &quot;prometheus&quot;,</span><br><span class="line">                &quot;restful&quot;,</span><br><span class="line">                &quot;selftest&quot;,</span><br><span class="line">                &quot;status&quot;,</span><br><span class="line">                &quot;zabbix&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;gid&quot;: 4132,</span><br><span class="line">            &quot;name&quot;: &quot;ceph3&quot;,</span><br><span class="line">            &quot;available_modules&quot;: [</span><br><span class="line">                &quot;balancer&quot;,</span><br><span class="line">                &quot;dashboard&quot;,</span><br><span class="line">                &quot;influx&quot;,</span><br><span class="line">                &quot;localpool&quot;,</span><br><span class="line">                &quot;prometheus&quot;,</span><br><span class="line">                &quot;restful&quot;,</span><br><span class="line">                &quot;selftest&quot;,</span><br><span class="line">                &quot;status&quot;,</span><br><span class="line">                &quot;zabbix&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;modules&quot;: [</span><br><span class="line">        &quot;balancer&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;status&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;available_modules&quot;: [</span><br><span class="line">        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;influx&quot;,</span><br><span class="line">        &quot;localpool&quot;,</span><br><span class="line">        &quot;prometheus&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;selftest&quot;,</span><br><span class="line">        &quot;status&quot;,</span><br><span class="line">        &quot;zabbix&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;services&quot;: &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">[root@ceph1 ceph]# ceph mgr module enable dashboard</span><br><span class="line">[root@ceph1 ceph]# ceph mgr dump</span><br><span class="line">&#123;</span><br><span class="line">    &quot;epoch&quot;: 7,</span><br><span class="line">    &quot;active_gid&quot;: 4139,</span><br><span class="line">    &quot;active_name&quot;: &quot;ceph1&quot;,</span><br><span class="line">    &quot;active_addr&quot;: &quot;192.168.10.186:6800/22316&quot;,</span><br><span class="line">    &quot;available&quot;: true,</span><br><span class="line">    &quot;standbys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;gid&quot;: 4136,</span><br><span class="line">            &quot;name&quot;: &quot;ceph3&quot;,</span><br><span class="line">            &quot;available_modules&quot;: [</span><br><span class="line">                &quot;balancer&quot;,</span><br><span class="line">                &quot;dashboard&quot;,</span><br><span class="line">                &quot;influx&quot;,</span><br><span class="line">                &quot;localpool&quot;,</span><br><span class="line">                &quot;prometheus&quot;,</span><br><span class="line">                &quot;restful&quot;,</span><br><span class="line">                &quot;selftest&quot;,</span><br><span class="line">                &quot;status&quot;,</span><br><span class="line">                &quot;zabbix&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;gid&quot;: 4141,</span><br><span class="line">            &quot;name&quot;: &quot;ceph2&quot;,</span><br><span class="line">            &quot;available_modules&quot;: [</span><br><span class="line">                &quot;balancer&quot;,</span><br><span class="line">                &quot;dashboard&quot;,</span><br><span class="line">                &quot;influx&quot;,</span><br><span class="line">                &quot;localpool&quot;,</span><br><span class="line">                &quot;prometheus&quot;,</span><br><span class="line">                &quot;restful&quot;,</span><br><span class="line">                &quot;selftest&quot;,</span><br><span class="line">                &quot;status&quot;,</span><br><span class="line">                &quot;zabbix&quot;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;modules&quot;: [</span><br><span class="line">        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;status&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;available_modules&quot;: [</span><br><span class="line">        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;influx&quot;,</span><br><span class="line">        &quot;localpool&quot;,</span><br><span class="line">        &quot;prometheus&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;selftest&quot;,</span><br><span class="line">        &quot;status&quot;,</span><br><span class="line">        &quot;zabbix&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;services&quot;: &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph config-key put mgr/dashboard/server_addr 192.168.6.101</span><br><span class="line">set mgr/dashboard/server_addr</span><br><span class="line">[root@ceph1 ceph]# ceph config-key put mgr/dashboard/server_port 7000</span><br><span class="line">set mgr/dashboard/server_port</span><br><span class="line">[root@ceph1 ~]# netstat -tulnp |grep 7000</span><br><span class="line">tcp        0      0 192.168.6.101:7000      0.0.0.0:*               LISTEN      19836/ceph-mgr</span><br></pre></td></tr></table></figure><p>这时看下danshboard图：</p><p><img src="https://shenshengkun.github.io/images/dash_cluster1.png" alt=""></p><h2 id="osd"><a href="#osd" class="headerlink" title="osd"></a>osd</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">每台机器做逻辑卷</span><br><span class="line">[root@ceph1 ceph]# pvcreate /dev/sdb</span><br><span class="line">  Physical volume &quot;/dev/sdb&quot; successfully created.</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# vgcreate data_vg1 /dev/sdb</span><br><span class="line">  Volume group &quot;data_vg1&quot; successfully created</span><br><span class="line">  </span><br><span class="line">[root@ceph1 ceph]# lvcreate -n data_lv1 -L 99g data_vg1   </span><br><span class="line">  Logical volume &quot;data_lv1&quot; created.</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ceph-deploy osd create ceph1 --data data_vg1/data_lv1</span><br><span class="line">ceph-deploy osd create ceph2 --data data_vg1/data_lv1</span><br><span class="line">ceph-deploy osd create ceph3 --data data_vg1/data_lv1</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     fcb2fa5e-481a-4494-9a27-374048f37113</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active), standbys: ceph3, ceph2</span><br><span class="line">    osd: 3 osds: 3 up, 3 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   3.01GiB used, 294GiB / 297GiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure><p>这时看下danshboard图：</p><p><img src="https://shenshengkun.github.io/images/dash_cluster2.png" alt=""></p><h2 id="rgw集群"><a href="#rgw集群" class="headerlink" title="rgw集群"></a>rgw集群</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy install --rgw ceph1 ceph2 ceph3</span><br><span class="line"></span><br><span class="line">ceph-deploy admin ceph1 ceph2 ceph3</span><br><span class="line"></span><br><span class="line">ceph-deploy rgw create ceph1 ceph2 ceph3</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     fcb2fa5e-481a-4494-9a27-374048f37113</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 3 daemons, quorum ceph1,ceph2,ceph3</span><br><span class="line">    mgr: ceph1(active), standbys: ceph3, ceph2</span><br><span class="line">    osd: 3 osds: 3 up, 3 in</span><br><span class="line">    rgw: 3 daemons active</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   4 pools, 32 pgs</span><br><span class="line">    objects: 191 objects, 3.08KiB</span><br><span class="line">    usage:   3.01GiB used, 294GiB / 297GiB avail</span><br><span class="line">    pgs:     32 active+clean</span><br></pre></td></tr></table></figure><p>这时看下danshboard图：</p><p><img src="https://shenshengkun.github.io/images/dash_cluster3.png" alt=""></p><h2 id="NGINX代理"><a href="#NGINX代理" class="headerlink" title="NGINX代理"></a>NGINX代理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">安装这里就不介绍了</span><br><span class="line"></span><br><span class="line">[root@ceph1 conf.d]# cat cephcloud.dev.goago.cn.conf </span><br><span class="line">        upstream cephcloud.dev.goago.cn  &#123;</span><br><span class="line">        server  192.168.10.186:7480;</span><br><span class="line">        server  192.168.10.187:7480;</span><br><span class="line">        server  192.168.10.188:7480;        </span><br><span class="line">        &#125;</span><br><span class="line">        server &#123;</span><br><span class="line">        listen       80;</span><br><span class="line">        server_name  cephcloud.dev.goago.cn;</span><br><span class="line">        location / &#123;</span><br><span class="line">                        proxy_intercept_errors on;</span><br><span class="line">                        access_log /var/log/nginx/cephcloud_log;</span><br><span class="line">                        proxy_pass http://cephcloud.dev.goago.cn;</span><br><span class="line">                        proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">                        proxy_set_header Host $host;</span><br><span class="line">                        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">                        proxy_set_header Request_Uri $request_uri;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h2 id="s3和swift"><a href="#s3和swift" class="headerlink" title="s3和swift"></a>s3和swift</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">具体安装这里不叙述了，可以看我上篇文章</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">New settings:</span><br><span class="line">  Access Key: M954JYYAOBES65B7UNEZ</span><br><span class="line">  Secret Key: 11MZu3N9vB4S4C4N8U2Ywgkhxro3Xi6K9HPyRQ9v</span><br><span class="line">  Default Region: US</span><br><span class="line">  S3 Endpoint: cephcloud.dev.goago.cn</span><br><span class="line">  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.cephcloud.dev.goago.cn bucket</span><br><span class="line">  Encryption password: 123456</span><br><span class="line">  Path to GPG program: /usr/bin/gpg</span><br><span class="line">  Use HTTPS protocol: False</span><br><span class="line">  HTTP Proxy server name: </span><br><span class="line">  HTTP Proxy server port: 0</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;集群架构&quot;&gt;&lt;a href=&quot;#集群架构&quot; class=&quot;headerlink&quot; title=&quot;集群架构&quot;&gt;&lt;/a&gt;集群架构&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pr
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>ceph-luminous-bluestore</title>
    <link href="https://shenshengkun.github.io/posts/78fhjj54.html"/>
    <id>https://shenshengkun.github.io/posts/78fhjj54.html</id>
    <published>2019-07-16T09:10:01.000Z</published>
    <updated>2019-07-16T06:04:17.585Z</updated>
    
    <content type="html"><![CDATA[<h1 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ceph后端支持多种存储引擎，以插件化的形式来进行管理使用，目前支持filestore，kvstore，memstore以及bluestore</span><br><span class="line"></span><br><span class="line">1）Firestore存在的问题是：</span><br><span class="line">在写数据前需要先写journal，会有一倍的写放大；</span><br><span class="line">若是另外配备SSD盘给journal使用又增加额外的成本；</span><br><span class="line">filestore一开始只是对于SATA/SAS这一类机械盘进行设计的，没有专门针对SSD这一类的Flash介质盘做考虑。</span><br><span class="line"></span><br><span class="line">2）而Bluestore的优势在于：</span><br><span class="line">减少写放大；</span><br><span class="line">针对FLASH介质盘做优化；</span><br><span class="line">直接管理裸盘，进一步减少文件系统部分的开销。</span><br></pre></td></tr></table></figure><h1 id="Bluestore原理说明"><a href="#Bluestore原理说明" class="headerlink" title="Bluestore原理说明"></a>Bluestore原理说明</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对象可以直接存放在裸盘上，不需要任何文件系统接口。</span><br><span class="line">BlueStore 直接使用一个原始分区，ceph对象将直接写在块设备上，不再需要任何的文件系统；</span><br><span class="line">和osd一起进来的元数据将存储在 一个 名为 RocksDB 的键值对 数据库；</span><br></pre></td></tr></table></figure><h2 id="各层意义"><a href="#各层意义" class="headerlink" title="各层意义"></a>各层意义</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RocksDB ：存储 WAL 日志和元数据（omap）</span><br><span class="line">BlueRocksEnv: 与RocksDB 交互的接口</span><br><span class="line">BlueFS： 一个类似文件系统的 mini C++，使 rocksdb 生效，ENv 接口（存储 RocksDB 日志和 sst 文件）；</span><br><span class="line">因为rocksdb 一般跑在一个文件系统的上层，所以创建了 BlueFS。</span><br></pre></td></tr></table></figure><h2 id="RocksDB-存放的数据类型"><a href="#RocksDB-存放的数据类型" class="headerlink" title="RocksDB 存放的数据类型"></a>RocksDB 存放的数据类型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">对象的元数据</span><br><span class="line">write-ahead 日志</span><br><span class="line">ceph omap  数据</span><br><span class="line">allocator metadata(元数据分配器)：决定数据存放位置；此功能可插拔</span><br></pre></td></tr></table></figure><h2 id="默认BlueStore模型"><a href="#默认BlueStore模型" class="headerlink" title="默认BlueStore模型"></a>默认BlueStore模型</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">第一个小分区（XFS或者ext4）,包括ceph files </span><br><span class="line">（init system descriptor,status,id,fsid,keyring 等）和RocksDB 文件</span><br><span class="line">第二个分区是一个原始分区</span><br></pre></td></tr></table></figure><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">每一部分都可以存放在不同的磁盘中，RocksDB WAL 和 DB 可以存放在不同的磁盘或者小分区中</span><br></pre></td></tr></table></figure><h1 id="添加osd"><a href="#添加osd" class="headerlink" title="添加osd"></a>添加osd</h1><h2 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h2><p>由于Luminous里默认使用Bluestore，可以直接操作裸盘,data和block-db会使用lv。综合成本及性能，我们把block.db使用ssd的分区，osd仍然使用sas，block.wal不指定. 这里vdb作为osd盘，vdc作为block-db盘</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">首先ssh到各个存储节点，block.db使用的ssd分区,这里node1举例：</span><br><span class="line"># ssh node1</span><br><span class="line"># pvcreate /dev/vdb  # 创建pv, 这里使用的整块磁盘(与后面的分区对比),  pvs 查看pv列表</span><br><span class="line">Physical volume &quot;/dev/vdb&quot; successfully created.</span><br><span class="line"># vgcreate data_vg1 /dev/vdb  # 创建vg,  vgs查看vg列表</span><br><span class="line">Volume group &quot;data_vg1&quot; successfully created</span><br><span class="line"># lvcreate -n data_lv1 -L 1020.00m data_vg1 #创建lv,lvs查看lv列表, -n指定lv名称, -L指定lv的大小,需要小于或者等于vg的VSize</span><br><span class="line">Logical volume &quot;data_lv1&quot; created.</span><br><span class="line"></span><br><span class="line">---------------------------------------------</span><br><span class="line">生产环境一块ssd磁盘会对应多块osd，所以这里也需要把ssd多个分区</span><br><span class="line"># parted /dev/vdc </span><br><span class="line">(parted) mklabel gpt                                                      </span><br><span class="line">(parted) mkpart primary 0% 25%   #因为测试，这里只做了一个占据磁盘25%容量的分区，实际情况根据osd数目划分相应的分区数</span><br><span class="line">(parted) quit</span><br><span class="line"># pvcreate /dev/vdc1  # 创建pv, 这里使用的是磁盘分区, pvs 查看pv列表</span><br><span class="line">Physical volume &quot;/dev/vdc1&quot; successfully created.</span><br><span class="line"># vgcreate block_db_vg1 /dev/vdc1  # 创建vg,  vgs查看vg列表</span><br><span class="line">Volume group &quot;block_db_vg1&quot; successfully created</span><br><span class="line"># lvcreate -n block_db_lv1 -L 1020.00m block_db_vg1  # 创建lv, lvs查看lv列表, -L指定lv的大小，需要小于或者等于 vg的VSize</span><br><span class="line">Logical volume &quot;block_db_lv1&quot; created.</span><br><span class="line"></span><br><span class="line">---------------------------------------------</span><br><span class="line"># 不需要加--bluestore 参数，默认就是使用bluestore方式，data_vg1/data_lv1 是数据盘，block_db_vg1/block_db_lv1是block-db</span><br><span class="line">管理节点执行：</span><br><span class="line">ceph-deploy --overwrite-conf osd create node1 --data data_vg1/data_lv1 --block-db block_db_vg1/block_db_lv1</span><br><span class="line">ceph-deploy --overwrite-conf osd create node2 --data data_vg1/data_lv1 --block-db block_db_vg1/block_db_lv1</span><br></pre></td></tr></table></figure><h2 id="案例2"><a href="#案例2" class="headerlink" title="案例2"></a>案例2</h2><p>创建具有3个逻辑卷的OSD（模拟不同类型的存储介质） </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#pvcreate /dev/sdb</span><br><span class="line">  Physical volume &quot;/dev/sdb&quot; successfully created.</span><br><span class="line">#vgcreate  ceph-pool /dev/sdb</span><br><span class="line">  Volume group &quot;ceph-pool&quot; successfully created</span><br><span class="line">#lvcreate -n osd0.wal -L 1G ceph-pool</span><br><span class="line">  Logical volume &quot;osd0.wal&quot; created.</span><br><span class="line"># lvcreate -n osd0.db -L 1G ceph-pool</span><br><span class="line">  Logical volume &quot;osd0.db&quot; created.</span><br><span class="line"># lvcreate -n osd0 -l 100%FREE ceph-pool</span><br><span class="line">  Logical volume &quot;osd0&quot; created.</span><br></pre></td></tr></table></figure><p>完成逻辑卷的创建后我们就可以创建 OSD 了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy osd create \</span><br><span class="line">    --data ceph-pool/osd0 \</span><br><span class="line">    --block-db ceph-pool/osd0.db \</span><br><span class="line">    --block-wal ceph-pool/osd0.wal \</span><br><span class="line">    --bluestore node1</span><br></pre></td></tr></table></figure><h2 id="wal-amp-db-的大小问题"><a href="#wal-amp-db-的大小问题" class="headerlink" title="wal&amp; db 的大小问题"></a>wal&amp; db 的大小问题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在 ceph bluestore 的情况下，wal 是 RocksDB 的write-ahead log, 相当于之前的 journal 数据，db 是 RocksDB 的metadata 信息。在磁盘选择原则是 block.wal &gt; block.db &gt; block。当然所有的数据也可以放到同一块盘上。</span><br><span class="line">默认情况下， wal 和 db 的大小分别是 512 MB 和 1GB，现在没有一个好的理论值，好像和 ceph 本身承载的数据类型有关系。</span><br><span class="line">值得注意的是，如果所有的数据都在单块盘上，那是没有必要指定 wal &amp;db 的大小的。如果 wal &amp; db 是在不同的盘上，由于 wal/db 一般都会分的比较小，是有满的可能性的。如果满了，这些数据会迁移到下一个快的盘上(wal - db - main)。所以最少不会因为数据满了，而造成无法写入。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;对比&quot;&gt;&lt;a href=&quot;#对比&quot; class=&quot;headerlink&quot; title=&quot;对比&quot;&gt;&lt;/a&gt;对比&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span 
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>ceph的pg算法</title>
    <link href="https://shenshengkun.github.io/posts/fd22jkk6.html"/>
    <id>https://shenshengkun.github.io/posts/fd22jkk6.html</id>
    <published>2019-07-11T09:10:01.000Z</published>
    <updated>2019-07-12T09:49:15.127Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PG介绍"><a href="#PG介绍" class="headerlink" title="PG介绍"></a>PG介绍</h1><p>PG, Placement Groups。CRUSH先将数据分解成一组对象，然后根据对象名称、复制级别和系统中的PG数等信息执行散列操作，再将结果生成PG ID。可以将PG看做一个逻辑容器，这个容器包含多个对象，同时这个逻辑对象映射之多个OSD上。<br>如果没有PG，在成千上万个OSD上管理和跟踪数百万计的对象的复制和传播是相当困难的。没有PG这一层，管理海量的对象所消耗的计算资源也是不可想象的。建议每个OSD上配置50~100个PG。</p><h1 id="计算PG数"><a href="#计算PG数" class="headerlink" title="计算PG数"></a>计算PG数</h1><p>官方推荐如下：</p><p><img src="https://shenshengkun.github.io/images/pg1.png" alt=""></p><p>Ceph集群中的PG总数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PG总数 = (OSD总数 * 100) / 最大副本数</span><br></pre></td></tr></table></figure><p>结果必须舍入到最接近的2的N次方幂的值。</p><p>Ceph集群中每个pool中的PG总数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">存储池PG总数 = (OSD总数 * 100 / 最大副本数) / 池数</span><br></pre></td></tr></table></figure><p>平衡每个存储池中的PG数和每个OSD中的PG数对于降低OSD的方差、避免速度缓慢的恢复再平衡进程是相当重要的。</p><h1 id="修改PG和PGP"><a href="#修改PG和PGP" class="headerlink" title="修改PG和PGP"></a>修改PG和PGP</h1><p>PGP是为了实现定位而设置的PG，它的值应该和PG的总数(即pg_num)保持一致。对于Ceph的一个pool而言，如果增加pg_num，还应该调整pgp_num为同样的值，这样集群才可以开始再平衡。<br>参数pg_num定义了PG的数量，PG映射至OSD。当任意pool的PG数增加时，PG依然保持和源OSD的映射。直至目前，Ceph还未开始再平衡。此时，增加pgp_num的值，PG才开始从源OSD迁移至其他的OSD，正式开始再平衡。PGP，Placement Groups of Placement。</p><p>获取现有的PG数和PGP数值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool get data pg_num</span><br><span class="line"></span><br><span class="line">ceph osd pool get data pgp_num</span><br></pre></td></tr></table></figure><p>检查存储池的副本数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd dump|grep -i size</span><br></pre></td></tr></table></figure><p>计算pg_num和pgp_num</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># pg_num calculation</span><br><span class="line">pg_num = (num_osds * 100) / num_copies</span><br><span class="line">num_up = pow(2, int(log(pg_num,2) + 0.5))</span><br><span class="line">num_down = pow(2, int(log(pg_num,2)))</span><br><span class="line">if abs(pg_num - num_up) &lt;= abs(pg_num - num_down):</span><br><span class="line">    pg_num = num_up</span><br><span class="line">else:</span><br><span class="line">    pg_num = num_down</span><br><span class="line">pgp_num = pg_num</span><br></pre></td></tr></table></figure><p>修改存储池的PG和PGP</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set data pg_num </span><br><span class="line"></span><br><span class="line">ceph osd pool set data pgp_num</span><br></pre></td></tr></table></figure><p>例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool ls</span><br><span class="line">ceph osd pool set .rgw.root pg_num 16</span><br><span class="line">ceph osd pool set .rgw.root pgp_num 16</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;PG介绍&quot;&gt;&lt;a href=&quot;#PG介绍&quot; class=&quot;headerlink&quot; title=&quot;PG介绍&quot;&gt;&lt;/a&gt;PG介绍&lt;/h1&gt;&lt;p&gt;PG, Placement Groups。CRUSH先将数据分解成一组对象，然后根据对象名称、复制级别和系统中的PG数等信息
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>ceph组件的状态</title>
    <link href="https://shenshengkun.github.io/posts/dsl5646k.html"/>
    <id>https://shenshengkun.github.io/posts/dsl5646k.html</id>
    <published>2019-07-11T05:10:01.000Z</published>
    <updated>2019-07-19T06:11:06.640Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ceph-整体状态查看"><a href="#Ceph-整体状态查看" class="headerlink" title="Ceph 整体状态查看"></a>Ceph 整体状态查看</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph -s  #ceph状态是否正常，及配置运行状态</span><br><span class="line">ceph -w  #实时查看数据写入情况</span><br><span class="line">ceph health detail #如果集群有问题，会详细列出具体的pg或者osd</span><br></pre></td></tr></table></figure><h1 id="mon"><a href="#mon" class="headerlink" title="mon"></a>mon</h1><h2 id="mon相关状态"><a href="#mon相关状态" class="headerlink" title="mon相关状态"></a>mon相关状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph quorum_status -f json-pretty</span><br></pre></td></tr></table></figure><h2 id="client-无法链接mon的可能原因"><a href="#client-无法链接mon的可能原因" class="headerlink" title="client 无法链接mon的可能原因"></a>client 无法链接mon的可能原因</h2><ol><li>连通性和防火墙规则。在MON主机上修改允许TCP 端口6789的访问。</li><li>磁盘空间。每个MON主机上必须有超过5%的空闲磁盘空间使MON和levelDB数据库正常工作。</li><li>MON没有工作或者离开选举，检查如上命令输出结果中的quorum_status和mon_status或者ceph -s 的输出来确定失败的MON进程，尝试重启或者部署一个新的来替代它。</li></ol><h2 id="MON-状态表"><a href="#MON-状态表" class="headerlink" title="MON 状态表"></a>MON 状态表</h2><table><thead><tr><th>状态</th><th>说明</th></tr></thead><tbody><tr><td>probing</td><td>正在探测态。这意味着MON正在寻找其他的MON。当MON启动时， MON尝试找在monmap定义的其他剩余的MON。在多MON的集群中，直到MON找到足够多的MON构建法定选举人数之前，它一直在这个状态。这意味着如果3个MON中的2个挂掉，剩余的1个MON将一直在probing状态，直到启动其他的MON中的1个为止。</td></tr><tr><td>electing</td><td>正在选举态。这意味着MON正在选举中。这应该很快完成，但有时也会卡在正这，这通常是因为MON主机节点间的时钟偏移导致的.</td></tr><tr><td>synchronizing</td><td>正在同步态。这意味着MON为了加入到法定人数中和集群中其他的MON正在同步.</td></tr><tr><td>leader或peon</td><td>领导态或员工态。这不应该出现。然而有机会出现，一般和时钟偏移有很大关系</td></tr></tbody></table><h3 id="时钟偏移警告"><a href="#时钟偏移警告" class="headerlink" title="时钟偏移警告"></a>时钟偏移警告</h3><p>MON可能被MON节点之间的重要的时钟偏移激烈的影响。这经常会转变为没有明显原因的诡异的行为。为了避免这种问题，应该在MON节点上运行一个时间同步的工具。默认最大容忍的时钟偏移为0.05s，虽然可以修改，但不建议修改，这是官方开发和QA认可的值。私自未经测试修改虽然无数据丢失风险，可能会对MON集群和总体集群健康导致意外的作用。 如果遇到这个告警，同步时钟，在MON上运行NTP的客户端会有帮助。如果经常遇到这个问题，可能是因为使用了远端的NTP服务器，请考虑在内网部署NTP服务器。</p><h1 id="OSD"><a href="#OSD" class="headerlink" title="OSD"></a>OSD</h1><h2 id="OSD-状态表"><a href="#OSD-状态表" class="headerlink" title="OSD 状态表"></a>OSD 状态表</h2><table><thead><tr><th>状态</th><th>说明</th></tr></thead><tbody><tr><td>up</td><td>osd启动</td></tr><tr><td>down</td><td>osd停止</td></tr><tr><td>in</td><td>osd在集群中</td></tr><tr><td>out</td><td>osd不在集群中，默认OSD down 超过300s,Ceph会标记为out，会触发重新平衡操作</td></tr><tr><td>up &amp; in</td><td>说明该OSD正常运行，且已经承载至少一个PG的数据。这是一个OSD的标准工作状态</td></tr><tr><td>up &amp; out</td><td>说明该OSD正常运行，但并未承载任何PG，其中也没有数据。一个新的OSD刚刚被加入Ceph集群后，便会处于这一状态。而一个出现故障的OSD被修复后，重新加入Ceph集群时，也是处于这一状态</td></tr><tr><td>down &amp; in</td><td>说明该OSD发生异常，但仍然承载着至少一个PG，其中仍然存储着数据。这种状态下的OSD刚刚被发现存在异常，可能仍能恢复正常，也可能会彻底无法工作</td></tr><tr><td>down &amp; out</td><td>说明该OSD已经彻底发生故障，且已经不再承载任何PG</td></tr></tbody></table><h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><ol><li>硬盘失败。可以通过系统日志或SMART活动确认。有些有缺陷的硬盘因为密集的有时限的错误修复活动变的很慢。</li><li>网络连接问题。可以使用ping、iperf等普通网络工具进行调试。</li><li>OSD文件存储的磁盘空间不足。 磁盘到85%将会触发HEALTH_WARN告警。磁盘到95%会触发HEALTH_ERR告警，OSD为了避免填满磁盘会停止。</li><li>超过系统资源限制。系统内存应该足够主机上所有的OSD进程，打开文件句柄数和最大线程数也应该是足够的。<br>OSD进程处理心跳的限制导致进程自杀。默认的处理和通信超时不足以执行IO饥饿型的操作，尤其是失败后的恢复。这经常会导致OSD闪断的现象出现。</li></ol><h2 id="暂时关闭pg重新平衡"><a href="#暂时关闭pg重新平衡" class="headerlink" title="暂时关闭pg重新平衡"></a>暂时关闭pg重新平衡</h2><p>在维护操作或解决问题时，不希望在停止一些OSD后，超时的OSD被标记为out后，CRUSH算法自动进行重新平衡操作。需要执行集群关闭out检测命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd set noout</span><br></pre></td></tr></table></figure><p>这样在停止的OSD中的PG会变为降级态。当维护操作完成后，需要先启动停止的OSD，再恢复默认设置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd unset noout</span><br></pre></td></tr></table></figure><h2 id="老-慢-请求"><a href="#老-慢-请求" class="headerlink" title="老/慢 请求"></a>老/慢 请求</h2><p>如果一个OSD服务进程很慢地响应请求。它会产生一个请求耗时过久超过30秒的警告提示信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;date&#125; &#123;osd.num&#125; [WRN] 1 slow requests, 1 included below; oldest blocked for &gt; 30.005692 secs</span><br><span class="line">&#123;date&#125; &#123;osd.num&#125;  [WRN] slow request 30.005692 seconds old, received at &#123;date-time&#125;: osd_op(client.4240.0:8 benchmark_data_ceph-1_39426_object7 [write 0~4194304] 0.69848840) v4 currently waiting for subops from [610]</span><br></pre></td></tr></table></figure><ol><li>可能的原因和修复方法包括：</li><li>硬盘故障（检查dmesg的输出信息）；替换为正常的硬盘</li><li>内核文件系统bug（检查dmesg的输出信息确）；升级内核</li><li>集群负载过高（检查系统负载、iostat等）；机器扩容，尝试降低系统负载</li><li>ceph-osd服务进程的的bug；升级ceph或重启OSD</li></ol><h2 id="OSD-闪断"><a href="#OSD-闪断" class="headerlink" title="OSD 闪断"></a>OSD 闪断</h2><p>OSD重启或恢复中后，OSD在peering状态一直闪断。因为IO密集型的任务导致影响心跳检测异常，你可以暂时为集群通过打开nodown noup选项。执行命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd set nodown </span><br><span class="line">ceph osd set noup  </span><br><span class="line">ceph osd set noout</span><br></pre></td></tr></table></figure><p>当集群健康稳定后，执行如下命令恢复默认值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd unset nodown  </span><br><span class="line">ceph osd unset noup  </span><br><span class="line">ceph osd unset noout</span><br></pre></td></tr></table></figure><h2 id="确认磁盘损坏"><a href="#确认磁盘损坏" class="headerlink" title="确认磁盘损坏"></a>确认磁盘损坏</h2><p>检查日志，执行如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dmesg | egrep sd[a­]</span><br><span class="line">eg:</span><br><span class="line">dmesg |egrep sda</span><br><span class="line"></span><br><span class="line">通过smartctl提取信息和执行测试检查可疑的设备:</span><br><span class="line">smartctl a /dev/sda</span><br><span class="line"></span><br><span class="line">通过观察响应时间确认。任何磁盘持续显示不常见的值可能会失败：</span><br><span class="line">iostat x /dev/sda</span><br></pre></td></tr></table></figure><p>###替换osd数据磁盘<br>当集群规模比较大，磁盘出硬件故障是一个常态。为了维持集群规模稳定，必须及时的修复因硬盘故障停止的OSD。 因为Ceph采用了多个副本的策略，一般情况下，不需要恢复坏掉硬盘的数据。用一个新硬盘初始化一个OSD即可。操作步骤如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">两种情况：</span><br><span class="line">a. 如果磁盘坏掉osd会标记为down，默认300秒osd会被标记为out，数据会开始迁移。所以我们替换osd数据磁盘，确保数据迁移完成，集群状态是ok。</span><br><span class="line">b. 如果磁盘将要损坏，但还没有坏，仍然是up&amp;in的，则需要先把该osd 设置为out: ceph osd out osd.0,这样集群会把osd.0的数据 rebalancing and copying到其他机器上去。直到整个集群编程active+clean，再进行后续操作</span><br><span class="line"></span><br><span class="line">1. 关闭 osd.0的进程</span><br><span class="line">systemctl stop ceph-osd@0</span><br><span class="line"></span><br><span class="line">2. 删除旧osd信息(osd.0为例)：</span><br><span class="line">ceph osd crush remove osd.0</span><br><span class="line">ceph auth del osd.0</span><br><span class="line">ceph osd rm 0</span><br><span class="line"></span><br><span class="line">3. 创建新osd</span><br><span class="line">a. ceph osd create #会自动生成uuid和osd-number</span><br><span class="line">b. ssh &#123;new_osd_host&#125;</span><br><span class="line">c. sudo mkdir /var/lib/ceph/osd/ceph-&#123;osd-number&#125;  #上一步生成的osd-number</span><br><span class="line">d. 分区 通过parted把osd的磁盘分区为一个分区</span><br><span class="line">e. sudo mkfs -t xfs /dev/&#123;drive&#125; # 上一步分区</span><br><span class="line">f. sudo mount /dev/&#123;sdx&#125; /var/lib/ceph/osd/ceph-&#123;osd-number&#125;</span><br><span class="line">g. ceph-osd -i &#123;osd-number&#125; --mkfs --mkkey   # 初始化osd数据目录</span><br><span class="line">目录必须为空</span><br><span class="line">h. ceph auth add osd.&#123;osd-number&#125; osd &apos;allow *&apos; mon &apos;allow rwx&apos; -i /var/lib/ceph/osd/ceph-&#123;osd-number&#125;/keyring #注册认证key</span><br><span class="line">i. ceph osd crush add osd.&#123;osd-number&#125;# 添加osd到crush map，则该osd可以接受数据了，这个时候osd的状态为 down &amp; in。ceph osd crush add osd.0 1.0 host=bj-yh-ceph-node2</span><br><span class="line">j. systemctl start ceph-osd@&#123;osd-number&#125; # 启动osd进程，数据会rebalancing and migrating 到新的osd上</span><br></pre></td></tr></table></figure><h2 id="替换ssd日志磁盘"><a href="#替换ssd日志磁盘" class="headerlink" title="替换ssd日志磁盘"></a>替换ssd日志磁盘</h2><p>由于我们使用过程中，一块ssd分4个区，给4个osd使用，所以如果ssd日志磁盘坏掉，需要给对应的osd都要操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1. 设置OSD状态为noout，防止数据重新平衡</span><br><span class="line">ceph osd set noout</span><br><span class="line"></span><br><span class="line">2. 停止osd进程</span><br><span class="line">ssh &#123;ssd所在节点&#125;</span><br><span class="line">systemctl stop ceph-osd@x  #ssd所对应的osds</span><br><span class="line"></span><br><span class="line">3. 日志数据落盘到数据盘</span><br><span class="line">ceph-osd -i &#123;osd-number&#125; --flush-journal #该ssd日志分区所对应的所有osd-number</span><br><span class="line"></span><br><span class="line">4. 删除日志链接</span><br><span class="line">rm -rf /var/lib/ceph/osd/&#123;osd-number&#125;/journal # #该ssd日志分区所对应的所有osd-number</span><br><span class="line"></span><br><span class="line">5. 创建日志链接</span><br><span class="line">ln -s /dev/disk/by-partuuid/&#123;uuid&#125; /var/lib/ceph/osd/ceph-&#123;osd-number&#125;/journal # 注意别把使用中的分区给绑定错了</span><br><span class="line">chown ceph:ceph /var/lib/ceph/osd/ceph-&#123;osd-number&#125;/journal</span><br><span class="line">echo &#123;uuid&#125; &gt; /var/lib/ceph/osd/ceph-&#123;osd-number&#125;/journal_uuid  #前面/dev/disk/by-partuuid/&#123;uuid&#125; uuid</span><br><span class="line"></span><br><span class="line">6. 创建日志</span><br><span class="line">ceph-osd -i &#123;osd-number&#125; --mkjournal</span><br><span class="line"></span><br><span class="line">7. 启动osd进程</span><br><span class="line">systemctl start ceph-osd@&#123;osd-number&#125;</span><br><span class="line"></span><br><span class="line">如果所有osd进程都起来了</span><br><span class="line"></span><br><span class="line">8. 去除noout的标记</span><br><span class="line">ceph osd set noout</span><br></pre></td></tr></table></figure><h1 id="PG"><a href="#PG" class="headerlink" title="PG"></a>PG</h1><p>ceph health detail #正常会返回 ok</p><h2 id="PG-状态表"><a href="#PG-状态表" class="headerlink" title="PG 状态表"></a>PG 状态表</h2><p>正常是active+clean</p><table><thead><tr><th>状态</th><th>描述</th></tr></thead><tbody><tr><td>active</td><td>活跃状态。ceph将处理到达这个PG的读写请求</td></tr><tr><td>unactive</td><td>非活跃状态。该PG不能处理读写请求</td></tr><tr><td>clean</td><td>干净状态。Ceph复制PG内所有对象到设定正确的数目</td></tr><tr><td>unclean</td><td>非干净状态。PG不能从上一个失败中恢复</td></tr><tr><td>down</td><td>离线状态。有必需数据的副本挂掉，比如对象所在的3个副本的OSD挂掉，所以PG离线</td></tr><tr><td>degraded</td><td>降级状态。ceph有些对象的副本数目没有达到系统设置，一般是因为有OSD挂掉</td></tr><tr><td>inconsistent</td><td>不一致态。Ceph 清理和深度清理后检测到PG中的对象在副本存在不一致，例如对象的文件大小不一致或recovery结束后一个对象的副本丢失</td></tr><tr><td>peering</td><td>正在同步状态。PG正在执行同步处理</td></tr><tr><td>recovering</td><td>正在恢复状态。Ceph正在执行迁移或同步对象和他们的副本</td></tr><tr><td>incomplete</td><td>未完成状态。实际的副本数少于min_size。Ceph检测到PG正在丢失关于已经写操作的信息，或者没有任何健康的副本。如果遇到这种状态，尝试启动失败的OSD，这些OSD中可能包含需要的信息或者临时调整副本min_size的值到允许恢复。</td></tr><tr><td>stale</td><td>未刷新状态。PG状态没有被任何OSD更新，这说明所有存储这个PG的OSD可能down</td></tr><tr><td>backfilling</td><td>正在后台填充状态。 当一个新的OSD加入集群后，Ceph通过移动一些其他OSD上的PG到新的OSD来达到新的平衡；这个过程完成后，这个OSD可以处理客户端的IO请求。</td></tr><tr><td>remapped</td><td>重新映射状态。PG活动集任何的一个改变，数据发生从老活动集到新活动集的迁移。在迁移期间还是用老的活动集中的主OSD处理客户端请求，一旦迁移完成新活动集中的主OSD开始处理。</td></tr></tbody></table><h2 id="PG-长时间卡在一些状态"><a href="#PG-长时间卡在一些状态" class="headerlink" title="PG 长时间卡在一些状态"></a>PG 长时间卡在一些状态</h2><p>遇到失败后PG进入如 “degraded” 或 “peering”的状态是正常的。通常这些状态指示失败恢复处理过程中的正常继续。然而，一个PG长时间保持在其中一些状态可能是一个更大问题的提示。因此，MON当PG卡在一个非正常态时会警告。 我们特别地检查：</p><ol><li>inactive : PG太长时间不在active态，例如PG长时间不能处理读写请求，通常是peering的问题。</li><li>unclean : PG太长时间不在clean态，例如PG不能完成从上一个失败的恢复，通常是unfound objects导致。</li><li>stale : PG状态未被OSD更新，表示所有存储PG的OSD可能挂掉，一般启动相应的OSD进程即可。</li></ol><p>在MON节点执行如下命令，可以明确列出卡住的PG：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph pg dump_stuck stale</span><br><span class="line">ceph pg dump_stuck inactive</span><br><span class="line">ceph pg dump_stuck unclean</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Ceph清理和深度清理后到PG处于inconsistent态：</span><br><span class="line"></span><br><span class="line">清理操作被用来检查对象的可用性和健康状态。当集群没有IO密集型（例如恢复）的操作时PG被清理，已经执行清理操作再执行IO密集操作，然而清理操作继续。如果清理任务发现任何对象有损坏或者不匹配的数据（校验和检测），它将标记这个对象为不能使用并且需要手动介入和恢复。OSD执行写操作时计算校验和，Ceph并不能武断地决定副本中的哪个校验和是正确的。例如有3个副本的校验和，有1个不同，很容易猜出应该修复的错误副本（从其他副本恢复），但是当有3个不同的校验和或者一些比特错误，我们不能武断的说哪个是好的。这不是一个端到端的数据修正检查。</span><br><span class="line"></span><br><span class="line">手动修复损坏的pg</span><br><span class="line"></span><br><span class="line">1. 找到有不一致对象的PG， 执行如下命令 ceph pg dump | grep inconsistent 或者 ceph health detail</span><br><span class="line">2. 当主副本是正确数据时，执行修复命令。或者通过在OSD的硬盘上手动复制正确的文件覆盖掉错误的文件。 ceph pg repair &#123;pgnum&#125;</span><br><span class="line">   注意：如果主副本错误，应该使用手动修复，如果通过命令修复则会把主副本的错误数据复制到其他副本。</span><br></pre></td></tr></table></figure><h3 id="incomplete-PG"><a href="#incomplete-PG" class="headerlink" title="incomplete PG"></a>incomplete PG</h3><p>这个告警是因为实际副本数少于min_size。可能是由于PG对应的OSD挂掉导致。尝试启动挂掉的OSD。</p><h3 id="stale-PG"><a href="#stale-PG" class="headerlink" title="stale PG"></a>stale PG</h3><p>简单地重启受影响的OSD。当一个OSD不能映射它持有所有的对象时这个问题发生,执行如下操作：</p><ol><li><p>找到PG</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg dump_stuck stale</span><br></pre></td></tr></table></figure></li><li><p>映射pg，找到osd：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg map &#123;pgname&#125;</span><br></pre></td></tr></table></figure></li><li><p>重启上面的osd</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh &#123;osd-node&#125;</span><br><span class="line">systemctl restart ceph-osd@&#123;osd-number&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="peering-和-down-PG"><a href="#peering-和-down-PG" class="headerlink" title="peering 和 down PG"></a>peering 和 down PG</h3><ol><li><p>找到受影响的pg</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph health detail</span><br></pre></td></tr></table></figure></li><li><p>下面命令响应结果中的 [“recovery_state”][“blocked”]部分显示peering被停止 的原因,大多数的情况都是一些OSD挂掉。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph pg &#123;pgname&#125; query</span><br></pre></td></tr></table></figure></li><li><p>尝试重启上面挂掉的OSD，如果无法启动，应该为执行如下命令标记为lost，让恢复操作开始。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd lost &#123;osd-number&#125;</span><br></pre></td></tr></table></figure></li></ol><h3 id="unfound-objects"><a href="#unfound-objects" class="headerlink" title="unfound objects"></a>unfound objects</h3><p>在特殊的失败组合下Ceph会警告unfound objects。这意味着存储集群知道有些对象存在，但是却无法找到它的副本。下面的例子说明这是怎么发生的，有1个PG他映射的的OSD是 1和2：</p><ol><li><p>OSD 1挂掉</p></li><li><p>OSD 2单独处理一些请求</p></li><li><p>OSD 1运行</p></li><li><p>OSD 1和2重新peering，1上丢失的对象在队列中等待恢复</p></li><li><p>在新对象之前被复制之前，OSD2挂掉<br>现在OSD 1知道一些对象存在，但是没有这个副本活的OSD。 这种情况下，到这些对象的IO将被阻塞，集群希望失败的OSD快速地回来。这时假设返回一个IO错误给用户是适当的。<br>修复建议：</p></li><li><p>启动停止的osd</p></li><li><p>如果还无法恢复，你可能只有放弃丢失的对象。执行如下命令回滚或删除对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph pg  &#123;pgname&#125;  mark_unfound_lost revert|delete</span><br><span class="line">revert选项：回滚到对象的前一个版本</span><br><span class="line">delete选项：完全删除这个对象</span><br><span class="line">使用这个操作时注意，因为它可能是使预期存在这个对象的程序混乱。</span><br><span class="line">列出带有丢失对象的PG的名字：</span><br><span class="line">ceph pg &#123;pgname&#125; list_missing</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Ceph-整体状态查看&quot;&gt;&lt;a href=&quot;#Ceph-整体状态查看&quot; class=&quot;headerlink&quot; title=&quot;Ceph 整体状态查看&quot;&gt;&lt;/a&gt;Ceph 整体状态查看&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;tabl
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>ceph对象存储</title>
    <link href="https://shenshengkun.github.io/posts/dld987c6.html"/>
    <id>https://shenshengkun.github.io/posts/dld987c6.html</id>
    <published>2019-07-11T04:10:01.000Z</published>
    <updated>2019-07-12T08:50:59.533Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Ceph-RGW简介"><a href="#Ceph-RGW简介" class="headerlink" title="Ceph RGW简介"></a>Ceph RGW简介</h1><p>Ceph RGW(即RADOS Gateway)是Ceph对象存储网关服务，是基于LIBRADOS接口封装实现的FastCGI服务，对外提供存储和管理对象数据的Restful API。 对象存储适用于图片、视频等各类文件的上传下载，可以设置相应的访问权限。目前Ceph RGW兼容常见的对象存储API，例如兼容绝大部分Amazon S3 API，兼容OpenStack Swift API。</p><h1 id="部署-RGW-服务"><a href="#部署-RGW-服务" class="headerlink" title="部署 RGW 服务"></a>部署 RGW 服务</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph-deploy install --rgw ceph1</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy install --rgw ceph1</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  testing                       : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa3faca5e60&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  dev_commit                    : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_mds                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  stable                        : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  adjust_repos                  : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function install at 0x7fa3fbb955f0&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_mgr                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_all                   : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  repo                          : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  host                          : [&apos;ceph1&apos;]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  install_rgw                   : True</span><br></pre></td></tr></table></figure><p>将配置文件、密钥文件同步到 ceph1：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph-deploy admin ceph1</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy admin ceph1</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe0e152d3b0&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  client                        : [&apos;ceph1&apos;]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function admin at 0x7fe0e1dc0230&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph1</span><br><span class="line">[ceph1][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph1][DEBUG ] detect machine type</span><br><span class="line">[ceph1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br></pre></td></tr></table></figure><p>启动一个RGW服务 ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">先将ceph.conf加一个参数配置</span><br><span class="line">[root@ceph1 ceph]# vim ceph.conf </span><br><span class="line">[global]</span><br><span class="line">fsid = cde3244e-89e0-4630-84d5-bf08c0e33b24</span><br><span class="line">mon_initial_members = ceph1</span><br><span class="line">mon_host = 192.168.6.101</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br><span class="line">osd_pool_default_size = 2</span><br><span class="line">[mgr]</span><br><span class="line">mgr modules = dashboard</span><br><span class="line">[mon]</span><br><span class="line">mon allow pool delete = true   ####有这个配置，生成的pool才可以被删除</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]#  ceph-deploy rgw create ceph1</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /usr/bin/ceph-deploy rgw create ceph1</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  rgw                           : [(&apos;ceph1&apos;, &apos;rgw.ceph1&apos;)]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  subcommand                    : create</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fda85404ab8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;function rgw at 0x7fda85a53050&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts ceph1:rgw.ceph1</span><br><span class="line">[ceph1][DEBUG ] connected to host: ceph1 </span><br><span class="line">[ceph1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph1][DEBUG ] detect machine type</span><br><span class="line">[ceph_deploy.rgw][INFO  ] Distro info: CentOS Linux 7.6.1810 Core</span><br><span class="line">[ceph_deploy.rgw][DEBUG ] remote host will use systemd</span><br><span class="line">[ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to ceph1</span><br><span class="line">[ceph1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph1][DEBUG ] create path recursively if it doesn&apos;t exist</span><br><span class="line">[ceph1][INFO  ] Running command: ceph --cluster ceph --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create client.rgw.ceph1 osd allow rwx mon allow rw -o /var/lib/ceph/radosgw/ceph-rgw.ceph1/keyring</span><br><span class="line">[ceph1][INFO  ] Running command: systemctl enable ceph-radosgw@rgw.ceph1</span><br><span class="line">[ceph1][INFO  ] Running command: systemctl start ceph-radosgw@rgw.ceph1</span><br><span class="line">[ceph1][INFO  ] Running command: systemctl enable ceph.target</span><br><span class="line">[ceph_deploy.rgw][INFO  ] The Ceph Object Gateway (RGW) is now running on host ceph1 and default port 7480</span><br><span class="line"></span><br><span class="line">验证：</span><br><span class="line">[root@ceph1 ceph]# systemctl status ceph-radosgw@rgw.ceph1</span><br><span class="line">● ceph-radosgw@rgw.ceph1.service - Ceph rados gateway</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Thu 2019-07-11 15:03:24 CST; 9s ago</span><br><span class="line"> Main PID: 21057 (radosgw)</span><br><span class="line">   CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@rgw.ceph1.service</span><br><span class="line">           └─21057 /usr/bin/radosgw -f --cluster ceph --name client.rgw.ceph1 --setuser ceph --setgroup ceph</span><br><span class="line"></span><br><span class="line">Jul 11 15:03:24 ceph1 systemd[1]: Started Ceph rados gateway.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     cde3244e-89e0-4630-84d5-bf08c0e33b24</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph1</span><br><span class="line">    mgr: ceph1(active)</span><br><span class="line">    osd: 2 osds: 2 up, 2 in</span><br><span class="line">    rgw: 1 daemon active</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   4 pools, 32 pgs</span><br><span class="line">    objects: 187 objects, 1.09KiB</span><br><span class="line">    usage:   2.01GiB used, 30.0GiB / 32.0GiB avail</span><br><span class="line">    pgs:     32 active+clean</span><br></pre></td></tr></table></figure><p>这时看下danshboard图：</p><p><img src="https://shenshengkun.github.io/images/ceph_dash2.png" alt=""></p><h1 id="使用亚马逊-s3-客户端进行访问"><a href="#使用亚马逊-s3-客户端进行访问" class="headerlink" title="使用亚马逊 s3 客户端进行访问"></a>使用亚马逊 s3 客户端进行访问</h1><h2 id="用户"><a href="#用户" class="headerlink" title="用户"></a>用户</h2><p>创建用户 ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# radosgw-admin user create --uid=&quot;radosgw&quot; --display-name=&quot;First User&quot;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;user_id&quot;: &quot;radosgw&quot;,</span><br><span class="line">    &quot;display_name&quot;: &quot;First User&quot;,</span><br><span class="line">    &quot;email&quot;: &quot;&quot;,</span><br><span class="line">    &quot;suspended&quot;: 0,</span><br><span class="line">    &quot;max_buckets&quot;: 1000,</span><br><span class="line">    &quot;auid&quot;: 0,</span><br><span class="line">    &quot;subusers&quot;: [],</span><br><span class="line">    &quot;keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;swift_keys&quot;: [],</span><br><span class="line">    &quot;caps&quot;: [],</span><br><span class="line">    &quot;op_mask&quot;: &quot;read, write, delete&quot;,</span><br><span class="line">    &quot;default_placement&quot;: &quot;&quot;,</span><br><span class="line">    &quot;placement_tags&quot;: [],</span><br><span class="line">    &quot;bucket_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;user_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;temp_url_keys&quot;: [],</span><br><span class="line">    &quot;type&quot;: &quot;rgw&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个是后续需要的账户信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br></pre></td></tr></table></figure><p>授权用户，允许 radosgw 读写 users 信息： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@ceph1 ceph]# radosgw-admin caps add --uid=radosgw --caps=&quot;users=*&quot;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;user_id&quot;: &quot;radosgw&quot;,</span><br><span class="line">    &quot;display_name&quot;: &quot;First User&quot;,</span><br><span class="line">    &quot;email&quot;: &quot;&quot;,</span><br><span class="line">    &quot;suspended&quot;: 0,</span><br><span class="line">    &quot;max_buckets&quot;: 1000,</span><br><span class="line">    &quot;auid&quot;: 0,</span><br><span class="line">    &quot;subusers&quot;: [],</span><br><span class="line">    &quot;keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;swift_keys&quot;: [],</span><br><span class="line">    &quot;caps&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;users&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;op_mask&quot;: &quot;read, write, delete&quot;,</span><br><span class="line">    &quot;default_placement&quot;: &quot;&quot;,</span><br><span class="line">    &quot;placement_tags&quot;: [],</span><br><span class="line">    &quot;bucket_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;user_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;temp_url_keys&quot;: [],</span><br><span class="line">    &quot;type&quot;: &quot;rgw&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>允许 radosgw 读写所有的usage信息： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]#  radosgw-admin caps add --uid=radosgw --caps=&quot;usage=read,write&quot;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;user_id&quot;: &quot;radosgw&quot;,</span><br><span class="line">    &quot;display_name&quot;: &quot;First User&quot;,</span><br><span class="line">    &quot;email&quot;: &quot;&quot;,</span><br><span class="line">    &quot;suspended&quot;: 0,</span><br><span class="line">    &quot;max_buckets&quot;: 1000,</span><br><span class="line">    &quot;auid&quot;: 0,</span><br><span class="line">    &quot;subusers&quot;: [],</span><br><span class="line">    &quot;keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;swift_keys&quot;: [],</span><br><span class="line">    &quot;caps&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;usage&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;users&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;op_mask&quot;: &quot;read, write, delete&quot;,</span><br><span class="line">    &quot;default_placement&quot;: &quot;&quot;,</span><br><span class="line">    &quot;placement_tags&quot;: [],</span><br><span class="line">    &quot;bucket_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;user_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;temp_url_keys&quot;: [],</span><br><span class="line">    &quot;type&quot;: &quot;rgw&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>创建子用户，做为后面 swift 客户端访问时使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]#  radosgw-admin subuser create --uid=radosgw --subuser=radosgw:swift --access=full</span><br><span class="line">&#123;</span><br><span class="line">    &quot;user_id&quot;: &quot;radosgw&quot;,</span><br><span class="line">    &quot;display_name&quot;: &quot;First User&quot;,</span><br><span class="line">    &quot;email&quot;: &quot;&quot;,</span><br><span class="line">    &quot;suspended&quot;: 0,</span><br><span class="line">    &quot;max_buckets&quot;: 1000,</span><br><span class="line">    &quot;auid&quot;: 0,</span><br><span class="line">    &quot;subusers&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;id&quot;: &quot;radosgw:swift&quot;,</span><br><span class="line">            &quot;permissions&quot;: &quot;full-control&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;swift_keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw:swift&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;A3GDj2yjkGJahkCM6YJS4QKQlGz2zd65GXvCkiwV&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;caps&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;usage&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;users&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;op_mask&quot;: &quot;read, write, delete&quot;,</span><br><span class="line">    &quot;default_placement&quot;: &quot;&quot;,</span><br><span class="line">    &quot;placement_tags&quot;: [],</span><br><span class="line">    &quot;bucket_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;user_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;temp_url_keys&quot;: [],</span><br><span class="line">    &quot;type&quot;: &quot;rgw&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>创建密钥 ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# radosgw-admin key create --subuser=radosgw:swift --key-type=swift --gen-secret</span><br><span class="line">&#123;</span><br><span class="line">    &quot;user_id&quot;: &quot;radosgw&quot;,</span><br><span class="line">    &quot;display_name&quot;: &quot;First User&quot;,</span><br><span class="line">    &quot;email&quot;: &quot;&quot;,</span><br><span class="line">    &quot;suspended&quot;: 0,</span><br><span class="line">    &quot;max_buckets&quot;: 1000,</span><br><span class="line">    &quot;auid&quot;: 0,</span><br><span class="line">    &quot;subusers&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;id&quot;: &quot;radosgw:swift&quot;,</span><br><span class="line">            &quot;permissions&quot;: &quot;full-control&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw&quot;,</span><br><span class="line">            &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;swift_keys&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;user&quot;: &quot;radosgw:swift&quot;,</span><br><span class="line">            &quot;secret_key&quot;: &quot;CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;caps&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;usage&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;type&quot;: &quot;users&quot;,</span><br><span class="line">            &quot;perm&quot;: &quot;*&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;op_mask&quot;: &quot;read, write, delete&quot;,</span><br><span class="line">    &quot;default_placement&quot;: &quot;&quot;,</span><br><span class="line">    &quot;placement_tags&quot;: [],</span><br><span class="line">    &quot;bucket_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;user_quota&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: false,</span><br><span class="line">        &quot;check_on_raw&quot;: false,</span><br><span class="line">        &quot;max_size&quot;: -1,</span><br><span class="line">        &quot;max_size_kb&quot;: 0,</span><br><span class="line">        &quot;max_objects&quot;: -1</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;temp_url_keys&quot;: [],</span><br><span class="line">    &quot;type&quot;: &quot;rgw&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="安装-s3-客户端软件"><a href="#安装-s3-客户端软件" class="headerlink" title="安装 s3 客户端软件"></a>安装 s3 客户端软件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# yum -y install s3cmd.noarch</span><br></pre></td></tr></table></figure><h2 id="对-s3-进行配置"><a href="#对-s3-进行配置" class="headerlink" title="对 s3 进行配置"></a>对 s3 进行配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# s3cmd --configure</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">New settings:</span><br><span class="line">  Access Key: CQE7E6ZDVA74KVJ0077A</span><br><span class="line">  Secret Key: wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8</span><br><span class="line">  Default Region: US</span><br><span class="line">  S3 Endpoint: 192.168.6.101:7480</span><br><span class="line">  DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.192.168.6.101:7480 bucket</span><br><span class="line">  Encryption password: 123456</span><br><span class="line">  Path to GPG program: /usr/bin/gpg</span><br><span class="line">  Use HTTPS protocol: False</span><br><span class="line">  HTTP Proxy server name: </span><br><span class="line">  HTTP Proxy server port: 0</span><br><span class="line"></span><br><span class="line">Test access with supplied credentials? [Y/n] y</span><br><span class="line">Please wait, attempting to list all buckets...</span><br><span class="line">Success. Your access key and secret key worked fine :-)</span><br><span class="line"></span><br><span class="line">Now verifying that encryption works...</span><br><span class="line">Success. Encryption and decryption worked fine :-)</span><br><span class="line"></span><br><span class="line">Save settings? [y/N] y</span><br><span class="line">Configuration saved to &apos;/root/.s3cfg&apos;</span><br></pre></td></tr></table></figure><p>格式是这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Default Region [US]:                        #这里一定不要修改，否则后面会报错</span><br><span class="line">S3 Endpoint [s3.amazonaws.com]: 192.168.6.101:7480</span><br><span class="line">DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket)s.192.168.20.148:7480 bucket     #相当于百度网盘的创建文件夹，这里是固定格式</span><br><span class="line">Path to GPG program [/usr/bin/gpg]:                 #保持默认</span><br><span class="line">Use HTTPS protocol [Yes]: no                    #这里写 no ，因为没有提供 https 端口</span><br><span class="line">HTTP Proxy server name:                       #这里不用写，因为没有代理</span><br><span class="line">Test access with supplied credentials? [Y/n] y</span><br><span class="line">Save settings? [y/N] y</span><br></pre></td></tr></table></figure><p>由于我没把端口改成80，所以需要带端口访问的，后续可以nginx代理</p><h2 id="创建存储数据的-bucket"><a href="#创建存储数据的-bucket" class="headerlink" title="创建存储数据的 bucket"></a>创建存储数据的 bucket</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# s3cmd mb s3://cephdir</span><br><span class="line">[root@ceph1 ~]# s3cmd put /etc/hosts s3://ceph_dir</span><br><span class="line">upload: &apos;/etc/hosts&apos; -&gt; &apos;s3://ceph_dir/hosts&apos;  [1 of 1]</span><br><span class="line"> 200 of 200   100% in    1s   133.14 B/s  done</span><br><span class="line"> </span><br><span class="line"> [root@ceph1 ~]# s3cmd ls s3://ceph_dir</span><br><span class="line">2019-07-11 08:41       200   s3://ceph_dir/hosts</span><br></pre></td></tr></table></figure><p> s3 的测试脚本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-f ~]# yum -y install python-boto</span><br><span class="line">[root@ceph-f ~]# vim s3test.py</span><br><span class="line">import boto.s3.connection</span><br><span class="line">access_key = &apos;N6ALEK0KS0ISYCIM5JBG&apos;</span><br><span class="line">secret_key = &apos;qK9hrpX2uwna4elPP1VsuErmAHBw3So40fE2K4yM&apos;</span><br><span class="line">conn = boto.connect_s3(</span><br><span class="line">         aws_access_key_id=access_key,         </span><br><span class="line">         aws_secret_access_key=secret_key,         </span><br><span class="line">         host=&apos;ceph1&apos;, port=7480,         </span><br><span class="line">         is_secure=False, calling_format=boto.s3.connection.OrdinaryCallingFormat(),        </span><br><span class="line">         )</span><br><span class="line">bucket = conn.create_bucket(&apos;xxx_yyy&apos;)</span><br><span class="line">for bucket in conn.get_all_buckets():</span><br><span class="line">     print &quot;&#123;name&#125; &#123;created&#125;&quot;.format(         </span><br><span class="line">     name=bucket.name,         </span><br><span class="line">     created=bucket.creation_date,     </span><br><span class="line">     )</span><br></pre></td></tr></table></figure><p>在使用时，请替换自己的 access_key、secret_key、主机名和端口</p><h1 id="swift-接口测试"><a href="#swift-接口测试" class="headerlink" title="swift 接口测试"></a>swift 接口测试</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install python-setuptools</span><br><span class="line">easy_install pip</span><br><span class="line">pip install --upgrade setuptools</span><br><span class="line">pip install --upgrade python-swiftclient</span><br></pre></td></tr></table></figure><p>命令行访问 ：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# swift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh list</span><br><span class="line">ceph_dir</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">swift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh post sy-yt</span><br><span class="line"></span><br><span class="line">swift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh post sy_yt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ~]# swift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh list</span><br><span class="line">ceph_dir</span><br><span class="line">sy-yt</span><br><span class="line">sy_yt</span><br></pre></td></tr></table></figure><p>这里提供 swift 的测试脚本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-f ~]# vim swift.py</span><br><span class="line">import swiftclient</span><br><span class="line">user = &apos;radosgw:swift&apos;</span><br><span class="line">key = &apos;CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh&apos;</span><br><span class="line"></span><br><span class="line">conn = swiftclient.Connection(</span><br><span class="line">         user=user,         </span><br><span class="line">         key=key,         </span><br><span class="line">         authurl=&apos;http://192.168.6.101:7480/auth/v1.0&apos;,</span><br><span class="line">         )</span><br><span class="line">for container in conn.get_account()[1]:</span><br><span class="line">         print container[&apos;name&apos;]</span><br></pre></td></tr></table></figure><p>在使用时，请替换自己的 access_key、secret_key、authurl</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Ceph-RGW简介&quot;&gt;&lt;a href=&quot;#Ceph-RGW简介&quot; class=&quot;headerlink&quot; title=&quot;Ceph RGW简介&quot;&gt;&lt;/a&gt;Ceph RGW简介&lt;/h1&gt;&lt;p&gt;Ceph RGW(即RADOS Gateway)是Ceph对象存储网关服务，
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>ceph简单搭建</title>
    <link href="https://shenshengkun.github.io/posts/ldfl554c.html"/>
    <id>https://shenshengkun.github.io/posts/ldfl554c.html</id>
    <published>2019-07-11T03:10:01.000Z</published>
    <updated>2019-07-17T03:00:05.048Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ceph介绍"><a href="#ceph介绍" class="headerlink" title="ceph介绍"></a>ceph介绍</h1><h2 id="Ceph基础介绍"><a href="#Ceph基础介绍" class="headerlink" title="Ceph基础介绍"></a>Ceph基础介绍</h2><p>​    Ceph是一个可靠地、自动重均衡、自动恢复的分布式存储系统，根据场景划分可以将Ceph分为三大块，分别是对象存储、块设备存储和文件系统服务。在虚拟化领域里，比较常用到的是Ceph的块设备存储，比如在OpenStack项目里，Ceph的块设备存储可以对接OpenStack的cinder后端存储、Glance的镜像存储和虚拟机的数据存储，比较直观的是Ceph集群可以提供一个raw格式的块存储来作为虚拟机实例的硬盘。</p><p>​    Ceph相比其它存储的优势点在于它不单单是存储，同时还充分利用了存储节点上的计算能力，在存储每一个数据时，都会通过计算得出该数据存储的位置，尽量将数据分布均衡，同时由于Ceph的良好设计，采用了CRUSH算法、HASH环等方法，使得它不存在传统的单点故障的问题，且随着规模的扩大性能并不会受到影响。</p><h2 id="Ceph的核心组件"><a href="#Ceph的核心组件" class="headerlink" title="Ceph的核心组件"></a>Ceph的核心组件</h2><p>​    Ceph的核心组件包括Ceph OSD、Ceph Monitor和Ceph MDS。</p><p>  Ceph OSD：OSD的英文全称是Object Storage Device，它的主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其它OSD间进行心跳检查等，并将一些变化情况上报给Ceph Monitor。一般情况下一块硬盘对应一个OSD，由OSD来对硬盘存储进行管理，当然一个分区也可以成为一个OSD。</p><p>  Ceph OSD的架构实现由物理磁盘驱动器、Linux文件系统和Ceph OSD服务组成，对于Ceph OSD Deamon而言，Linux文件系统显性的支持了其拓展性，一般Linux文件系统有好几种，比如有BTRFS、XFS、Ext4等，BTRFS虽然有很多优点特性，但现在还没达到生产环境所需的稳定性，一般比较推荐使用XFS。</p><p>  伴随OSD的还有一个概念叫做Journal盘，一般写数据到Ceph集群时，都是先将数据写入到Journal盘中，然后每隔一段时间比如5秒再将Journal盘中的数据刷新到文件系统中。一般为了使读写时延更小，Journal盘都是采用SSD，一般分配10G以上，当然分配多点那是更好，Ceph中引入Journal盘的概念是因为Journal允许Ceph OSD功能很快做小的写操作；一个随机写入首先写入在上一个连续类型的journal，然后刷新到文件系统，这给了文件系统足够的时间来合并写入磁盘，一般情况下使用SSD作为OSD的journal可以有效缓冲突发负载。</p><p>  Ceph Monitor：由该英文名字我们可以知道它是一个监视器，负责监视Ceph集群，维护Ceph集群的健康状态，同时维护着Ceph集群中的各种Map图，比如OSD Map、Monitor Map、PG Map和CRUSH Map，这些Map统称为Cluster Map，Cluster Map是RADOS的关键数据结构，管理集群中的所有成员、关系、属性等信息以及数据的分发，比如当用户需要存储数据到Ceph集群时，OSD需要先通过Monitor获取最新的Map图，然后根据Map图和object id等计算出数据最终存储的位置。</p><p>  Ceph MDS：全称是Ceph MetaData Server，主要保存的文件系统服务的元数据，但对象存储和块存储设备是不需要使用该服务的。</p><p>  查看各种Map的信息可以通过如下命令：ceph osd(mon、pg) dump</p><h1 id="ceph-deploy安装ceph"><a href="#ceph-deploy安装ceph" class="headerlink" title="ceph-deploy安装ceph"></a>ceph-deploy安装ceph</h1><h2 id="基本环境"><a href="#基本环境" class="headerlink" title="基本环境"></a>基本环境</h2><table><thead><tr><th><strong>192.168.6.101</strong></th><th><strong>ceph1</strong></th></tr></thead><tbody><tr><td><strong>192.168.6.102</strong></td><td><strong>ceph2</strong></td></tr></tbody></table><p>俩台机器都挂俩块盘，一块系统盘，一块osd</p><p>配hosts：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# vim /etc/hosts</span><br><span class="line">192.168.6.101   ceph1</span><br><span class="line">192.168.6.102   ceph2</span><br></pre></td></tr></table></figure><p>时间同步：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate ntp1.aliyun.com</span><br></pre></td></tr></table></figure><p>允许无密码SSH登录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">在ceph1上执行</span><br><span class="line">ssh-keygen</span><br><span class="line">ssh-copy-id ceph1</span><br><span class="line">ssh-copy-id ceph2</span><br></pre></td></tr></table></figure><p>配置主机名：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname ceph1</span><br><span class="line">hostnamectl set-hostname ceph2</span><br></pre></td></tr></table></figure><h2 id="配置ceph-repo"><a href="#配置ceph-repo" class="headerlink" title="配置ceph.repo"></a>配置ceph.repo</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# vim /etc/yum.repos.d/ceph.repo</span><br><span class="line">[ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">priority=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">priority=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line">priority=1</span><br></pre></td></tr></table></figure><p> 安装ceph-deploy：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br><span class="line">yum makecache</span><br><span class="line">yum update -y</span><br><span class="line">yum install -y ceph-deploy</span><br></pre></td></tr></table></figure><h1 id="创建一个-Ceph-存储集群，它有一个-Monitor-和两个-OSD-守护进程"><a href="#创建一个-Ceph-存储集群，它有一个-Monitor-和两个-OSD-守护进程" class="headerlink" title="创建一个 Ceph 存储集群，它有一个 Monitor 和两个 OSD 守护进程"></a>创建一个 Ceph 存储集群，它有一个 Monitor 和两个 OSD 守护进程</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/ceph &amp;&amp; cd /etc/ceph</span><br><span class="line">ceph-deploy new ceph1     ###配置</span><br><span class="line"></span><br><span class="line">一般会遇到个报错：</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;/usr/bin/ceph-deploy&quot;, line 18, in &lt;module&gt;</span><br><span class="line">    from ceph_deploy.cli import main</span><br><span class="line">  File &quot;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    import pkg_resources</span><br><span class="line">ImportError: No module named pkg_resources</span><br><span class="line"></span><br><span class="line">解决：</span><br><span class="line">yum install -y python-setuptools</span><br></pre></td></tr></table></figure><p>在ceph.conf中追加以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 存储集群副本个数</span><br><span class="line"></span><br><span class="line">osd_pool_default_size = 2</span><br></pre></td></tr></table></figure><p>管理节点和osd节点都需要安装ceph 集群：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy install ceph1 ceph2</span><br></pre></td></tr></table></figure><p>配置MON初始化:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure><p>查看ceph集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     cde3244e-89e0-4630-84d5-bf08c0e33b24</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph1</span><br><span class="line">    mgr: no daemons active</span><br><span class="line">    osd: 0 osds: 0 up, 0 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   0B used, 0B / 0B avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure><h2 id="开启监控模块"><a href="#开启监控模块" class="headerlink" title="开启监控模块"></a>开启监控模块</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mgr create ceph1</span><br></pre></td></tr></table></figure><p>在<code>/etc/ceph/ceph.conf</code>中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mgr]</span><br><span class="line">mgr modules = dashboard</span><br></pre></td></tr></table></figure><p>查看集群支持的模块</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph mgr dump   </span><br><span class="line">[root@ceph1 ceph]# ceph mgr module enable dashboard   #启用dashboard模块</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph mgr dump</span><br><span class="line">&#123;</span><br><span class="line">    &quot;epoch&quot;: 3,</span><br><span class="line">    &quot;active_gid&quot;: 4110,</span><br><span class="line">    &quot;active_name&quot;: &quot;ceph1&quot;,</span><br><span class="line">    &quot;active_addr&quot;: &quot;192.168.6.101:6800/6619&quot;,</span><br><span class="line">    &quot;available&quot;: true,</span><br><span class="line">    &quot;standbys&quot;: [],</span><br><span class="line">    &quot;modules&quot;: [        &quot;balancer&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;status&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;available_modules&quot;: [        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;influx&quot;,</span><br><span class="line">        &quot;localpool&quot;,</span><br><span class="line">        &quot;prometheus&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;selftest&quot;,</span><br><span class="line">        &quot;status&quot;,</span><br><span class="line">        &quot;zabbix&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;services&quot;: &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">[root@ceph1 ceph]# ceph mgr module enable dashboard</span><br><span class="line">[root@ceph1 ceph]# ceph mgr dump</span><br><span class="line">&#123;</span><br><span class="line">    &quot;epoch&quot;: 6,</span><br><span class="line">    &quot;active_gid&quot;: 4114,</span><br><span class="line">    &quot;active_name&quot;: &quot;ceph1&quot;,</span><br><span class="line">    &quot;active_addr&quot;: &quot;192.168.6.101:6800/6619&quot;,</span><br><span class="line">    &quot;available&quot;: true,</span><br><span class="line">    &quot;standbys&quot;: [],</span><br><span class="line">    &quot;modules&quot;: [        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;status&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;available_modules&quot;: [        &quot;balancer&quot;,</span><br><span class="line">        &quot;dashboard&quot;,</span><br><span class="line">        &quot;influx&quot;,</span><br><span class="line">        &quot;localpool&quot;,</span><br><span class="line">        &quot;prometheus&quot;,</span><br><span class="line">        &quot;restful&quot;,</span><br><span class="line">        &quot;selftest&quot;,</span><br><span class="line">        &quot;status&quot;,</span><br><span class="line">        &quot;zabbix&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;services&quot;: &#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[root@ceph1 ceph]# ceph -s</span><br><span class="line">  cluster:</span><br><span class="line">    id:     cde3244e-89e0-4630-84d5-bf08c0e33b24</span><br><span class="line">    health: HEALTH_OK</span><br><span class="line"> </span><br><span class="line">  services:</span><br><span class="line">    mon: 1 daemons, quorum ceph1</span><br><span class="line">    mgr: ceph1(active)</span><br><span class="line">    osd: 2 osds: 2 up, 2 in</span><br><span class="line"> </span><br><span class="line">  data:</span><br><span class="line">    pools:   0 pools, 0 pgs</span><br><span class="line">    objects: 0 objects, 0B</span><br><span class="line">    usage:   2.00GiB used, 30.0GiB / 32.0GiB avail</span><br><span class="line">    pgs:</span><br></pre></td></tr></table></figure><p>设置<code>dashboard</code>的<code>ip</code>和端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-node1 ceph]# ceph config-key put mgr/dashboard/server_addr 192.168.6.101</span><br><span class="line">set mgr/dashboard/server_addr</span><br><span class="line">[root@ceph-node1 ceph]# ceph config-key put mgr/dashboard/server_port 7000</span><br><span class="line">set mgr/dashboard/server_port</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ~]# netstat -tulnp |grep 7000</span><br><span class="line">tcp        0      0 192.168.6.101:7000      0.0.0.0:*               LISTEN      19836/ceph-mgr</span><br></pre></td></tr></table></figure><h2 id="创建osd"><a href="#创建osd" class="headerlink" title="创建osd"></a>创建osd</h2><p>删除磁盘数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph-deploy disk zap ceph1 /dev/sdb</span><br><span class="line">[root@ceph1 ceph]# ceph-deploy disk zap ceph2 /dev/sdb</span><br></pre></td></tr></table></figure><p>创建osd(一共俩个)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph-deploy osd create ceph1 --data /dev/sdb</span><br><span class="line">[root@ceph1 ceph]# ceph-deploy osd create ceph2 --data /dev/sdb</span><br></pre></td></tr></table></figure><p> ceph秘钥拷贝（主节点执行）及修改密钥权限<br> 用 ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 monitor 地址和 ceph.client.admin.keyring 了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph]# ceph-deploy admin ceph1 ceph2</span><br></pre></td></tr></table></figure><p>修改密钥权限（所有节点上执行）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph1 ceph] # chmod +r /etc/ceph/ceph.client.admin.keyring</span><br><span class="line">[root@ceph2] # chmod +r /etc/ceph/ceph.client.admin.keyring</span><br></pre></td></tr></table></figure><p>这时看下danshboard图：</p><p><img src="https://shenshengkun.github.io/images/ceph_dash1.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;ceph介绍&quot;&gt;&lt;a href=&quot;#ceph介绍&quot; class=&quot;headerlink&quot; title=&quot;ceph介绍&quot;&gt;&lt;/a&gt;ceph介绍&lt;/h1&gt;&lt;h2 id=&quot;Ceph基础介绍&quot;&gt;&lt;a href=&quot;#Ceph基础介绍&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="ceph" scheme="https://shenshengkun.github.io/categories/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>Kubernetes Pod无法删除</title>
    <link href="https://shenshengkun.github.io/posts/dk7569vg.html"/>
    <id>https://shenshengkun.github.io/posts/dk7569vg.html</id>
    <published>2019-07-04T05:53:01.000Z</published>
    <updated>2019-07-05T06:59:46.817Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题发现"><a href="#问题发现" class="headerlink" title="问题发现"></a>问题发现</h1><p>在node节点断电 重启后，发现有的pod节点状态不正常，之前的回收策略也都做了，就调研下是什么原因导致的</p><p>pod一直处于Terminated: ExitCode 状态</p><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><h2 id="直接删除，无法飘移"><a href="#直接删除，无法飘移" class="headerlink" title="直接删除，无法飘移"></a>直接删除，无法飘移</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod &lt;podname&gt; --namespace=&lt;namspacer&gt; --grace-period=0 --force</span><br><span class="line">发现pod无法漂移</span><br><span class="line">docker ps -a查看对应docker容器的状态，发现这两个Pod的docker容器处于Dead状态。使用docker rm &lt;container id&gt;，提示Device is Busy，无法删除。</span><br></pre></td></tr></table></figure><h2 id="现象是由于systemd服务PrivateTmp-true引起"><a href="#现象是由于systemd服务PrivateTmp-true引起" class="headerlink" title="现象是由于systemd服务PrivateTmp=true引起"></a>现象是由于systemd服务<code>PrivateTmp=true</code>引起</h2><p>最根本的方法是，当机器加入时，在 <code>docker.service</code> 中加上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">MountFlags=slave</span><br></pre></td></tr></table></figure><h1 id="关于Systemd的MountFlags"><a href="#关于Systemd的MountFlags" class="headerlink" title="关于Systemd的MountFlags"></a>关于Systemd的MountFlags</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MountFlags: 配置Systemd服务的Mount Namespace配置。会影响服务进程上下文中挂载点的信息，即服务是否会继承主机上已有的挂载点，以及如果服务运行时执行了挂载或卸载设备的操作，是否会真实地在主机上产生效果。可选值为shared、slave和private</span><br><span class="line"></span><br><span class="line">shared：服务与主机共用一个Mount Namespace，会继承主机挂载点，服务挂载或卸载设备时会真实地反映到主机上</span><br><span class="line"></span><br><span class="line">slave：服务使用独立的Mount Namespace，会继承主机挂载点，但服务对挂载点的操作只在自己的Namespace内生效，不会反映到主机上</span><br><span class="line"></span><br><span class="line">private: 服务使用独立的Mount Namespace，在启动时没有任何挂载点，服务对挂载点的操作也不会反映到主机上</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;问题发现&quot;&gt;&lt;a href=&quot;#问题发现&quot; class=&quot;headerlink&quot; title=&quot;问题发现&quot;&gt;&lt;/a&gt;问题发现&lt;/h1&gt;&lt;p&gt;在node节点断电 重启后，发现有的pod节点状态不正常，之前的回收策略也都做了，就调研下是什么原因导致的&lt;/p&gt;
&lt;p&gt;p
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>tomcat8导致文件权限访问不到</title>
    <link href="https://shenshengkun.github.io/posts/kfkd454f.html"/>
    <id>https://shenshengkun.github.io/posts/kfkd454f.html</id>
    <published>2019-07-01T03:09:10.000Z</published>
    <updated>2019-07-01T03:19:37.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>之前在tomcat 7下文件上传后访问一直没问题，现在tomcat版本升到8.5，在测试文件http上传时，发现所传文件无法通过nginx访问了：报错 403 forbidden </p><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><p>看下系统的umask</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/profile后发现</span><br><span class="line"></span><br><span class="line">if [ $UID -gt 199 ] &amp;&amp; [ &quot;`/usr/bin/id -gn`&quot; = &quot;`/usr/bin/id -un`&quot; ]; then</span><br><span class="line">    umask 002</span><br><span class="line">else</span><br><span class="line">    umask 022</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">022是没问题的</span><br></pre></td></tr></table></figure><p>看下tomcat的catlina.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if [ -z &quot;$UMASK&quot; ]; then</span><br><span class="line">UMASK=&quot;0027&quot;</span><br><span class="line">fi</span><br><span class="line">umask $UMASK</span><br><span class="line"></span><br><span class="line">tomcat8改成0027了，把这个改成0022就好了</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h1&gt;&lt;p&gt;之前在tomcat 7下文件上传后访问一直没问题，现在tomcat版本升到8.5，在测试文件http上传时，发现所传文件无法通过nginx访
      
    
    </summary>
    
      <category term="中间件" scheme="https://shenshengkun.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Kubelet状态更新机制</title>
    <link href="https://shenshengkun.github.io/posts/dfkdaa65.html"/>
    <id>https://shenshengkun.github.io/posts/dfkdaa65.html</id>
    <published>2019-06-18T06:14:01.000Z</published>
    <updated>2019-06-20T06:18:22.061Z</updated>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>当 Kubernetes 中 Node 节点出现状态异常的情况下，节点上的 Pod 会被重新调度到其他节点上去，但是有的时候我们会发现节点 Down 掉以后，Pod 并不会立即触发重新调度，这实际上就是和 Kubelet 的状态更新机制密切相关的，Kubernetes 提供了一些参数配置来触发重新调度到嗯时间，下面我们来分析下 Kubelet 状态更新的基本流程。</p><ol><li>kubelet 自身会定期更新状态到 apiserver，通过参数<code>--node-status-update-frequency</code>指定上报频率，默认是 10s 上报一次。</li><li>kube-controller-manager 会每隔<code>--node-monitor-period</code>时间去检查 kubelet 的状态，默认是 5s。</li><li>当 node 失联一段时间后，kubernetes 判定 node 为 <code>notready</code> 状态，这段时长通过<code>--node-monitor-grace-period</code>参数配置，默认 40s。</li><li>当 node 失联一段时间后，kubernetes 判定 node 为 <code>unhealthy</code> 状态，这段时长通过<code>--node-startup-grace-period</code>参数配置，默认 1m0s。</li><li>当 node 失联一段时间后，kubernetes 开始删除原 node 上的 pod，这段时长是通过<code>--pod-eviction-timeout</code>参数配置，默认 5m0s。</li></ol><blockquote><p>kube-controller-manager 和 kubelet 是异步工作的，这意味着延迟可能包括任何的网络延迟、apiserver 的延迟、etcd 延迟，一个节点上的负载引起的延迟等等。因此，如果<code>--node-status-update-frequency</code>设置为5s，那么实际上 etcd 中的数据变化会需要 6-7s，甚至更长时间。</p></blockquote><p>Kubelet在更新状态失败时，会进行<code>nodeStatusUpdateRetry</code>次重试，默认为 5 次。</p><p>Kubelet 会在函数<code>tryUpdateNodeStatus</code>中尝试进行状态更新。Kubelet 使用了 Golang 中的<code>http.Client()</code>方法，但是没有指定超时时间，因此，如果 API Server 过载时，当建立 TCP 连接时可能会出现一些故障。</p><p>因此，在<code>nodeStatusUpdateRetry</code> * <code>--node-status-update-frequency</code>时间后才会更新一次节点状态。</p><p>同时，Kubernetes 的 controller manager 将尝试每<code>--node-monitor-period</code>时间周期内检查<code>nodeStatusUpdateRetry</code>次。在<code>--node-monitor-grace-period</code>之后，会认为节点 unhealthy，然后会在<code>--pod-eviction-timeout</code>后删除 Pod。</p><p>kube proxy 有一个 watcher API，一旦 Pod 被驱逐了，kube proxy 将会通知更新节点的 iptables 规则，将 Pod 从 Service 的 Endpoints 中移除，这样就不会访问到来自故障节点的 Pod 了。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>对于这些参数的配置，需要根据不通的集群规模场景来进行配置。</p><h3 id="社区默认的配置"><a href="#社区默认的配置" class="headerlink" title="社区默认的配置"></a>社区默认的配置</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>10s</td></tr><tr><td>–node-monitor-period</td><td>5s</td></tr><tr><td>–node-monitor-grace-period</td><td>40s</td></tr><tr><td>–pod-eviction-timeout</td><td>5m</td></tr></tbody></table><h3 id="快速更新和快速响应"><a href="#快速更新和快速响应" class="headerlink" title="快速更新和快速响应"></a>快速更新和快速响应</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>4s</td></tr><tr><td>–node-monitor-period</td><td>2s</td></tr><tr><td>–node-monitor-grace-period</td><td>20s</td></tr><tr><td>–pod-eviction-timeout</td><td>30s</td></tr></tbody></table><p>在这种情况下，Pod 将在 50s 被驱逐，因为该节点在 20s 后被视为Down掉了，<code>--pod-eviction-timeout</code>在 30s 之后发生，但是，这种情况会给 etcd 产生很大的开销，因为每个节点都会尝试每 2s 更新一次状态。</p><p>如果环境有1000个节点，那么每分钟将有15000次节点更新操作，这可能需要大型 etcd 容器甚至是 etcd 的专用节点。</p><blockquote><p>如果我们计算尝试次数，则除法将给出5，但实际上每次尝试的 nodeStatusUpdateRetry 尝试将从3到5。 由于所有组件的延迟，尝试总次数将在15到25之间变化。</p></blockquote><h3 id="中等更新和平均响应"><a href="#中等更新和平均响应" class="headerlink" title="中等更新和平均响应"></a>中等更新和平均响应</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>20s</td></tr><tr><td>–node-monitor-period</td><td>5s</td></tr><tr><td>–node-monitor-grace-period</td><td>2m</td></tr><tr><td>–pod-eviction-timeout</td><td>1m</td></tr></tbody></table><p>这种场景下会 20s 更新一次 node 状态，controller manager 认为 node 状态不正常之前，会有 2m<em>60⁄20</em>5=30 次的 node 状态更新，Node 状态为 down 之后 1m，就会触发驱逐操作。</p><p>如果有 1000 个节点，1分钟之内就会有 60s/20s*1000=3000 次的节点状态更新操作。</p><h3 id="低更新和慢响应"><a href="#低更新和慢响应" class="headerlink" title="低更新和慢响应"></a>低更新和慢响应</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>1m</td></tr><tr><td>–node-monitor-period</td><td>5s</td></tr><tr><td>–node-monitor-grace-period</td><td>5m</td></tr><tr><td>–pod-eviction-timeout</td><td>1m</td></tr></tbody></table><p>Kubelet 将会 1m 更新一次节点的状态，在认为不健康之后会有 5m/1m*5=25 次重试更新的机会。Node为不健康的时候，1m 之后 pod开始被驱逐。</p><p>可以有不同的组合，例如快速更新和慢反应以满足特定情况。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;当 Kubernetes 中 Node 节点出现状态异常的情况下，节点上的 Pod 会被重新调度到其他节点上去，但是有的时候我们会发现节点 
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>istio架构与技术</title>
    <link href="https://shenshengkun.github.io/posts/5a4fga4h.html"/>
    <id>https://shenshengkun.github.io/posts/5a4fga4h.html</id>
    <published>2019-06-18T02:10:01.000Z</published>
    <updated>2019-06-21T01:27:54.328Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>使用云平台可以为组织提供丰富的好处。然而，不可否认的是，采用云可能会给 DevOps 团队带来压力。开发人员必须使用微服务以满足应用的可移植性，同时运营商管理了极其庞大的混合和多云部署。Istio 允许您连接、保护、控制和观测服务。</p><p>在较高的层次上，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。它是一个完全开源的服务网格，可以透明地分层到现有的分布式应用程序上。它也是一个平台，包括允许它集成到任何日志记录平台、遥测或策略系统的 API。Istio 的多样化功能集使您能够成功高效地运行分布式微服务架构，并提供保护、连接和监控微服务的统一方法。</p><h1 id="Service-Mesh"><a href="#Service-Mesh" class="headerlink" title="Service Mesh"></a>Service Mesh</h1><p>• 治理能力独立（Sidecar）</p><p>• 应用程序无感知 </p><p>• 服务通信的基础设施层</p><p><img src="https://shenshengkun.github.io/images/istio1.png" alt=""></p><h1 id="为什么要使用-Istio？"><a href="#为什么要使用-Istio？" class="headerlink" title="为什么要使用 Istio？"></a>为什么要使用 Istio？</h1><p>Istio 提供一种简单的方式来为已部署的服务建立网络，该网络具有负载均衡、服务间认证、监控等功能，只需要对服务的代码进行一点或不需要做任何改动。想要让服务支持 Istio，只需要在您的环境中部署一个特殊的 sidecar 代理，使用 Istio 控制平面功能配置和管理代理，拦截微服务之间的所有网络通信：</p><ul><li>HTTP、gRPC、WebSocket 和 TCP 流量的自动负载均衡。</li><li>通过丰富的路由规则、重试、故障转移和故障注入，可以对流量行为进行细粒度控制。</li><li>可插入的策略层和配置 API，支持访问控制、速率限制和配额。</li><li>对出入集群入口和出口中所有流量的自动度量指标、日志记录和追踪。</li><li>通过强大的基于身份的验证和授权，在集群中实现安全的服务间通信。</li></ul><p>Istio 旨在实现可扩展性，满足各种部署需求。</p><p><img src="https://shenshengkun.github.io/images/istio2.png" alt=""></p><h1 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h1><p>Istio 在服务网络中统一提供了许多关键功能：</p><h2 id="流量管理"><a href="#流量管理" class="headerlink" title="流量管理"></a>流量管理</h2><p>通过简单的规则配置和流量路由，您可以控制服务之间的流量和 API 调用。Istio 简化了断路器、超时和重试等服务级别属性的配置，并且可以轻松设置 A/B测试、金丝雀部署和基于百分比的流量分割的分阶段部署等重要任务。</p><p>通过更好地了解您的流量和开箱即用的故障恢复功能，您可以在问题出现之前先发现问题，使调用更可靠，并且使您的网络更加强大——无论您面临什么条件。</p><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><p>Istio 的安全功能使开发人员可以专注于应用程序级别的安全性。Istio 提供底层安全通信信道，并大规模管理服务通信的认证、授权和加密。使用Istio，服务通信在默认情况下是安全的，它允许您跨多种协议和运行时一致地实施策略——所有这些都很少或根本不需要应用程序更改。</p><p>虽然 Istio 与平台无关，但将其与 Kubernetes（或基础架构）网络策略结合使用，其优势会更大，包括在网络和应用层保护 pod 间或服务间通信的能力。</p><h2 id="可观察性"><a href="#可观察性" class="headerlink" title="可观察性"></a>可观察性</h2><p>Istio 强大的追踪、监控和日志记录可让您深入了解服务网格部署。通过 Istio 的监控功能，可以真正了解服务性能如何影响上游和下游的功能，而其自定义仪表板可以提供对所有服务性能的可视性，并让您了解该性能如何影响您的其他进程。</p><p>Istio 的 Mixer 组件负责策略控制和遥测收集。它提供后端抽象和中介，将 Istio 的其余部分与各个基础架构后端的实现细节隔离开来，并为运维提供对网格和基础架构后端之间所有交互的细粒度控制。</p><p>所有这些功能可以让您可以更有效地设置、监控和实施服务上的 SLO。当然，最重要的是，您可以快速有效地检测和修复问题。</p><h2 id="平台支持"><a href="#平台支持" class="headerlink" title="平台支持"></a>平台支持</h2><p>Istio 是独立于平台的，旨在运行在各种环境中，包括跨云、内部部署、Kubernetes、Mesos 等。您可以在 Kubernetes 上部署 Istio 或具有 Consul 的 Nomad 上部署。Istio 目前支持：</p><ul><li>在 Kubernetes 上部署的服务</li><li>使用 Consul 注册的服务</li><li>在虚拟机上部署的服务</li></ul><h2 id="集成和定制"><a href="#集成和定制" class="headerlink" title="集成和定制"></a>集成和定制</h2><p>策略执行组件可以扩展和定制，以便与现有的 ACL、日志、监控、配额、审计等方案集成。</p><p><img src="https://shenshengkun.github.io/images/istio3.png" alt=""></p><h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p>Istio 服务网格逻辑上分为<strong>数据平面</strong>和<strong>控制平面</strong>。</p><ul><li><strong>数据平面</strong>由一组以 sidecar 方式部署的智能代理（<a href="https://www.envoyproxy.io/" target="_blank" rel="noopener">Envoy</a>）组成。这些代理可以调节和控制微服务及 <a href="https://istio.io/zh/docs/concepts/policies-and-telemetry/" target="_blank" rel="noopener">Mixer</a> 之间所有的网络通信。</li><li><strong>控制平面</strong>负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。</li></ul><p>下图显示了构成每个面板的不同组件：</p><p><img src="https://shenshengkun.github.io/images/istio4.png" alt=""></p><h2 id="Envoy"><a href="#Envoy" class="headerlink" title="Envoy"></a>Envoy</h2><p>Istio 使用 Envoy代理的扩展版本，Envoy 是以 C++ 开发的高性能代理，用于调解服务网格中所有服务的所有入站和出站流量。Envoy 的许多内置功能被 Istio 发扬光大，例如：</p><ul><li>动态服务发现</li><li>负载均衡</li><li>TLS 终止</li><li>HTTP/2 &amp; gRPC 代理</li><li>熔断器</li><li>健康检查、基于百分比流量拆分的灰度发布</li><li>故障注入</li><li>丰富的度量指标</li></ul><p>Envoy 被部署为 <strong>sidecar</strong>，和对应服务在同一个 Kubernetes pod 中。这允许 Istio 将大量关于流量行为的信号作为属性提取出来，而这些属性又可以在 Mixer 中用于执行策略决策，并发送给监控系统，以提供整个网格行为的信息。</p><p>Sidecar 代理模型还可以将 Istio 的功能添加到现有部署中，而无需重新构建或重写代码。可以阅读更多来了解为什么我们在设计目标中选择这种方式。</p><h2 id="Mixer"><a href="#Mixer" class="headerlink" title="Mixer"></a>Mixer</h2><p>Mixer是一个独立于平台的组件，负责在服务网格上执行访问控制和使用策略，并从 Envoy 代理和其他服务收集遥测数据。代理提取请求级属性，发送到 Mixer 进行评估</p><p>Mixer 中包括一个灵活的插件模型，使其能够接入到各种主机环境和基础设施后端，从这些细节中抽象出 Envoy 代理和 Istio 管理的服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以配一些监控</span><br></pre></td></tr></table></figure><p><img src="https://shenshengkun.github.io/images/istio5.png" alt=""></p><h2 id="Pilot"><a href="#Pilot" class="headerlink" title="Pilot"></a>Pilot</h2><p>Pilot为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。它将控制流量行为的高级路由规则转换为特定于 Envoy 的配置，并在运行时将它们传播到 sidecar。</p><p>Pilot 将平台特定的服务发现机制抽象化并将其合成为符合 Envoy 数据平面 API的任何 sidecar 都可以使用的标准格式。这种松散耦合使得 Istio 能够在多种环境下运行（例如，Kubernetes、Consul、Nomad），同时保持用于流量管理的相同操作界面。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">k8s来说的话，比如说pod这样的元数据，传给Envoy</span><br></pre></td></tr></table></figure><h2 id="Citadel"><a href="#Citadel" class="headerlink" title="Citadel"></a>Citadel</h2><p>Citadel通过内置身份和凭证管理赋能强大的服务间和最终用户身份验证。可用于升级服务网格中未加密的流量，并为运维人员提供基于服务标识而不是网络控制的强制执行策略的能力。从 0.5 版本开始，Istio 支持基于角色的访问控制，以控制谁可以访问您的服务，而不是基于不稳定的三层或四层网络标识。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">证书安全管理中心，证书生成以及下发</span><br></pre></td></tr></table></figure><h2 id="Galley"><a href="#Galley" class="headerlink" title="Galley"></a>Galley</h2><p>Galley 将担任 Istio 的配置验证，获取配置，处理和分配组件的任务。它负责将其余的 Istio 组件与从底层平台（例如 Kubernetes）获取用户配置的细节中隔离开来。</p><h1 id="和k8s结合"><a href="#和k8s结合" class="headerlink" title="和k8s结合"></a>和k8s结合</h1><p><img src="https://shenshengkun.github.io/images/istio6.png" alt=""></p><h1 id="在-Helm-和-Tiller-的环境中使用-helm-install-命令进行安装"><a href="#在-Helm-和-Tiller-的环境中使用-helm-install-命令进行安装" class="headerlink" title="在 Helm 和 Tiller 的环境中使用 helm install 命令进行安装"></a>在 Helm 和 Tiller 的环境中使用 <code>helm install</code> 命令进行安装</h1><h2 id="下载istio"><a href="#下载istio" class="headerlink" title="下载istio"></a>下载istio</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.1.9 sh -</span><br><span class="line">mv istio-1.1.9 /opt/k8s/work/</span><br><span class="line">cd /opt/k8s/work/</span><br><span class="line">cd istio-1.1.9/</span><br><span class="line">export PATH=$PWD/bin:$PATH</span><br><span class="line">helm repo add istio.io https://storage.googleapis.com/istio-release/releases/1.1.9/charts/</span><br></pre></td></tr></table></figure><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">helm install install/kubernetes/helm/istio-init --name istio-init --namespace istio-system</span><br><span class="line"></span><br><span class="line">kubectl get crds | grep &apos;istio.io\|certmanager.k8s.io&apos; | wc -l</span><br><span class="line">53</span><br><span class="line"></span><br><span class="line">helm install install/kubernetes/helm/istio --name istio --namespace istio-system</span><br><span class="line"></span><br><span class="line">[root@node1 istio-1.1.9]# kubectl get pods -n istio-system</span><br><span class="line">NAME                                      READY   STATUS      RESTARTS   AGE</span><br><span class="line">istio-citadel-b6d6889c4-96fwx             1/1     Running     0          44h</span><br><span class="line">istio-galley-654c696595-2rbr9             1/1     Running     0          44h</span><br><span class="line">istio-ingressgateway-6b47b76cc6-2rxbq     1/1     Running     0          44h</span><br><span class="line">istio-init-crd-10-smj28                   0/1     Completed   0          44h</span><br><span class="line">istio-init-crd-11-wktdb                   0/1     Completed   0          44h</span><br><span class="line">istio-pilot-5c99cfc94-g7t84               2/2     Running     0          44h</span><br><span class="line">istio-policy-6c5795449-tkzmp              2/2     Running     3          44h</span><br><span class="line">istio-sidecar-injector-79c88d56cf-lmv9j   1/1     Running     0          44h</span><br><span class="line">istio-telemetry-64f99d84c7-ksjmh          2/2     Running     2          44h</span><br><span class="line">prometheus-d8d46c5b5-sdljw                1/1     Running     0          44h</span><br></pre></td></tr></table></figure><h2 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm delete --purge istio</span><br><span class="line">helm delete --purge istio-init</span><br><span class="line">kubectl delete -f install/kubernetes/helm/istio-init/files</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h1&gt;&lt;p&gt;使用云平台可以为组织提供丰富的好处。然而，不可否认的是，采用云可能会给 DevOps 团队带来压力。开发人员必须使用微服务以满足应用的可移植
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-helm</title>
    <link href="https://shenshengkun.github.io/posts/dkgg3h4f.html"/>
    <id>https://shenshengkun.github.io/posts/dkgg3h4f.html</id>
    <published>2019-06-17T10:20:01.000Z</published>
    <updated>2019-06-17T03:07:27.417Z</updated>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p><code>Helm</code>这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套测试环境，对于单体服务，部署一套测试环境我相信还是非常快的，但是对于微服务架构的应用，要部署一套新的环境，就有点折磨人了，微服务越多、你就会越绝望的。虽然我们线上和测试环境已经都迁移到了<code>kubernetes</code>环境，但是每个微服务也得维护一套<code>yaml</code>文件，而且每个环境下的配置文件也不太一样，部署一套新的环境成本是真的很高。如果我们能使用类似于<code>yum</code>的工具来安装我们的应用的话是不是就很爽歪歪了啊？<code>Helm</code>就相当于<code>kubernetes</code>环境下的<code>yum</code>包管理工具。 </p><h1 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h1><p>做为 Kubernetes 的一个包管理工具，<code>Helm</code>具有如下功能：</p><ul><li>创建新的 chart</li><li>chart 打包成 tgz 格式</li><li>上传 chart 到 chart 仓库或从仓库中下载 chart</li><li>在<code>Kubernetes</code>集群中安装或卸载 chart</li><li>管理用<code>Helm</code>安装的 chart 的发布周期</li></ul><h1 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h1><p>Helm 有三个重要概念：</p><ul><li>chart：包含了创建<code>Kubernetes</code>的一个应用实例的必要信息</li><li>config：包含了应用发布配置信息</li><li>release：是一个 chart 及其配置的一个运行实例</li></ul><h1 id="Helm组件"><a href="#Helm组件" class="headerlink" title="Helm组件"></a>Helm组件</h1><p>Helm 有以下两个组成部分：</p><p><code>Helm Client</code> 是用户命令行工具，其主要负责如下：</p><ul><li>本地 chart 开发</li><li>仓库管理</li><li>与 Tiller sever 交互</li><li>发送预安装的 chart</li><li>查询 release 信息</li><li>要求升级或卸载已存在的 release</li></ul><p><code>Tiller Server</code>是一个部署在<code>Kubernetes</code>集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下：</p><ul><li>监听来自 Helm client 的请求</li><li>通过 chart 及其配置构建一次发布</li><li>安装 chart 到<code>Kubernetes</code>集群，并跟踪随后的发布</li><li>通过与<code>Kubernetes</code>交互升级或卸载 chart</li><li>简单的说，client 管理 charts，而 server 管理发布 release</li></ul><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>我们可以在<a href="https://github.com/kubernetes/helm/releases" target="_blank" rel="noopener">Helm Realese</a>页面下载二进制文件，这里下载的2.14.1版本，解压后将可执行文件<code>helm</code>拷贝到<code>/usr/local/bin</code>目录下即可，这样<code>Helm</code>客户端就在这台机器上安装完成了。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br><span class="line"></span><br><span class="line">另外还需要在每个node节点安装yum install socat -y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]# helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.14.1&quot;, GitCommit:&quot;5270352a09c7e8b6e8c9593002a73535276507c0&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.14.1&quot;, GitCommit:&quot;5270352a09c7e8b6e8c9593002a73535276507c0&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">自Kubernetes 1.6版本开始，API Server启用了RBAC授权。而目前的Tiller部署没有定义授权的ServiceAccount，这会导致访问API Server时被拒绝。我们可以采用如下方法，明确为Tiller部署添加授权。</span><br><span class="line"></span><br><span class="line">kubectl create serviceaccount --namespace kube-system tiller</span><br><span class="line">kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span><br><span class="line">kubectl patch deploy --namespace kube-system tiller-deploy -p &apos;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&apos;</span><br><span class="line">kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default</span><br></pre></td></tr></table></figure><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>我们现在了尝试创建一个 Chart： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]# helm create hello-helm</span><br><span class="line">Creating hello-helm</span><br><span class="line">[root@node1 helm]# ls</span><br><span class="line">hello-helm  helm-v2.14.1-linux-amd64.tar.gz  linux-amd64</span><br><span class="line">[root@node1 helm]# helm install ./hello-helm</span><br><span class="line">NAME:   virulent-wolverine</span><br><span class="line">LAST DEPLOYED: Mon Jun 17 10:56:39 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                           READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">virulent-wolverine-hello-helm  0/1    0           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                            READY  STATUS             RESTARTS  AGE</span><br><span class="line">virulent-wolverine-hello-helm-6f54d6f866-d5t7v  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                           TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">virulent-wolverine-hello-helm  ClusterIP  10.254.123.130  &lt;none&gt;       80/TCP   1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export POD_NAME=$(kubectl get pods --namespace default -l &quot;app.kubernetes.io/name=hello-helm,app.kubernetes.io/instance=virulent-wolverine&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">  echo &quot;Visit http://127.0.0.1:8080 to use your application&quot;</span><br><span class="line">  kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><p>然后我们根据提示执行下面的命令： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export POD_NAME=$(kubectl get pods --namespace default -l &quot;app.kubernetes.io/name=hello-helm,app.kubernetes.io/instance=virulent-wolverine&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><p>访问：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# curl 127.0.0.1:8080</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>查看<code>release</code>： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# helm list</span><br><span class="line">NAME                    REVISION        UPDATED                         STATUS          CHART                   APP VERSION     NAMESPACE</span><br><span class="line">virulent-wolverine      1               Mon Jun 17 10:56:39 2019        DEPLOYED        hello-helm-0.1.0        1.0             default</span><br></pre></td></tr></table></figure><p>打包<code>chart</code>： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm package hello-helm</span><br></pre></td></tr></table></figure><p>删除：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# helm delete virulent-wolverine</span><br><span class="line">release &quot;virulent-wolverine&quot; deleted</span><br><span class="line">[root@node1 ~]# helm list</span><br><span class="line">[root@node1 ~]# helm list --all</span><br><span class="line">NAME                    REVISION        UPDATED                         STATUS  CHART                   APP VERSION     NAMESPACE</span><br><span class="line">virulent-wolverine      1               Mon Jun 17 10:56:39 2019        DELETED hello-helm-0.1.0        1.0             default</span><br></pre></td></tr></table></figure><p>彻底删除：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# helm delete virulent-wolverine --purge</span><br><span class="line">release &quot;virulent-wolverine&quot; deleted</span><br><span class="line">[root@node1 ~]# helm list --all</span><br><span class="line">[root@node1 ~]#</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;&lt;code&gt;Helm&lt;/code&gt;这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-dashboard</title>
    <link href="https://shenshengkun.github.io/posts/c2556dcf.html"/>
    <id>https://shenshengkun.github.io/posts/c2556dcf.html</id>
    <published>2019-06-10T08:16:01.000Z</published>
    <updated>2019-06-12T08:39:46.837Z</updated>
    
    <content type="html"><![CDATA[<h1 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/kubernetes/cluster/addons/dashboard</span><br></pre></td></tr></table></figure><p>修改 service 定义，指定端口类型为 NodePort，这样外界可以通过地址 NodeIP:NodePort 访问 dashboard；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cat dashboard-service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort # 增加这一行</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  ports:</span><br><span class="line">  - port: 443</span><br><span class="line">    targetPort: 8443</span><br></pre></td></tr></table></figure><p>修改镜像地址mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1在dashboard-controller.yaml中</p><h1 id="执行所有定义文件"><a href="#执行所有定义文件" class="headerlink" title="执行所有定义文件"></a>执行所有定义文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f  .</span><br></pre></td></tr></table></figure><h2 id="查看分配的-NodePort"><a href="#查看分配的-NodePort" class="headerlink" title="查看分配的 NodePort"></a>查看分配的 NodePort</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl get deployment kubernetes-dashboard  -n kube-system</span><br><span class="line"></span><br><span class="line">NAME                   READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line"></span><br><span class="line">kubernetes-dashboard   1/1     1            1           23h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 dashboard]# kubectl --namespace kube-system get pods -o wide</span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE     IP             NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-8854569d4-g2hth                 1/1     Running   4          5d22h   172.30.40.2    node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard-7848d45466-6pm2q   1/1     Running   0          23h     172.30.200.3   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-server-5f7cf7659-59swk          1/1     Running   0          2d5h    172.30.40.3    node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 dashboard]# kubectl get services kubernetes-dashboard -n kube-system</span><br><span class="line">NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kubernetes-dashboard   NodePort   10.254.234.85   &lt;none&gt;        443:32681/TCP   23h</span><br></pre></td></tr></table></figure><h1 id="访问-dashboard"><a href="#访问-dashboard" class="headerlink" title="访问 dashboard"></a>访问 dashboard</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://192.168.6.101:32681</span><br></pre></td></tr></table></figure><h2 id="创建登录-Dashboard-的-token-和-kubeconfig-配置文件"><a href="#创建登录-Dashboard-的-token-和-kubeconfig-配置文件" class="headerlink" title="创建登录 Dashboard 的 token 和 kubeconfig 配置文件"></a>创建登录 Dashboard 的 token 和 kubeconfig 配置文件</h2><p>dashboard 默认只支持 token 认证（不支持 client 证书认证），所以如果使用 Kubeconfig 文件，需要将 token 写入到该文件。</p><h3 id="创建登录-token"><a href="#创建登录-token" class="headerlink" title="创建登录 token"></a>创建登录 token</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl create sa dashboard-admin -n kube-system</span><br><span class="line">kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin</span><br><span class="line">ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk &apos;&#123;print $1&#125;&apos;)</span><br><span class="line">DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system $&#123;ADMIN_SECRET&#125; | grep -E &apos;^token&apos; | awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">echo $&#123;DASHBOARD_LOGIN_TOKEN&#125;</span><br></pre></td></tr></table></figure><p>使用输出的 token 登录 Dashboard。</p><h3 id="创建使用-token-的-KubeConfig-文件"><a href="#创建使用-token-的-KubeConfig-文件" class="headerlink" title="创建使用 token 的 KubeConfig 文件"></a>创建使用 token 的 KubeConfig 文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line"># 设置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置客户端认证参数，使用上面创建的 Token</span><br><span class="line">kubectl config set-credentials dashboard_user \</span><br><span class="line">  --token=$&#123;DASHBOARD_LOGIN_TOKEN&#125; \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置上下文参数</span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=dashboard_user \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置默认上下文</span><br><span class="line">kubectl config use-context default --kubeconfig=dashboard.kubeconfig</span><br></pre></td></tr></table></figure><p>用生成的 dashboard.kubeconfig 登录 Dashboard。</p><h1 id="为kubernetes-dashboard访问用户添加权限控制"><a href="#为kubernetes-dashboard访问用户添加权限控制" class="headerlink" title="为kubernetes dashboard访问用户添加权限控制"></a>为kubernetes dashboard访问用户添加权限控制</h1><h2 id="Role"><a href="#Role" class="headerlink" title="Role"></a>Role</h2><p><code>Role</code>表示是一组规则权限，只能累加，<code>Role</code>可以定义在一个<code>namespace</code>中，只能用于授予对单个命名空间中的资源访问的权限。比如我们新建一个对默认命名空间中<code>Pods</code>具有访问权限的角色：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: default</span><br><span class="line">  name: pod-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br></pre></td></tr></table></figure><h2 id="ClusterRole"><a href="#ClusterRole" class="headerlink" title="ClusterRole"></a>ClusterRole</h2><p><code>ClusterRole</code>具有与<code>Role</code>相同的权限角色控制能力，不同的是<code>ClusterRole</code>是集群级别的，可以用于:</p><ul><li>集群级别的资源控制(例如 node 访问权限)</li><li>非资源型 endpoints(例如 /healthz 访问)</li><li>所有命名空间资源控制(例如 pods)</li></ul><p>比如我们要创建一个授权某个特定命名空间或全部命名空间(取决于绑定方式)访问<strong>secrets</strong>的集群角色：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  # &quot;namespace&quot; omitted since ClusterRoles are not namespaced</span><br><span class="line">  name: secret-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;secrets&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br></pre></td></tr></table></figure><h2 id="RoleBinding和ClusterRoleBinding"><a href="#RoleBinding和ClusterRoleBinding" class="headerlink" title="RoleBinding和ClusterRoleBinding"></a>RoleBinding和ClusterRoleBinding</h2><p><code>RoloBinding</code>可以将角色中定义的权限授予用户或用户组，<code>RoleBinding</code>包含一组权限列表(<code>subjects</code>)，权限列表中包含有不同形式的待授予权限资源类型(users、groups、service accounts)，<code>RoleBinding</code>适用于某个命名空间内授权，而 <code>ClusterRoleBinding</code>适用于集群范围内的授权。</p><p>比如我们将默认命名空间的<code>pod-reader</code>角色授予用户jane，这样以后该用户在默认命名空间中将具有<code>pod-reader</code>的权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># This role binding allows &quot;jane&quot; to read pods in the &quot;default&quot; namespace.</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-pods</span><br><span class="line">  namespace: default</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: jane</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: pod-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p><code>RoleBinding</code>同样可以引用<code>ClusterRole</code>来对当前 namespace 内用户、用户组或 ServiceAccount 进行授权，这种操作允许集群管理员在整个集群内定义一些通用的 ClusterRole，然后在不同的 namespace 中使用 RoleBinding 来引用</p><p>例如，以下 RoleBinding 引用了一个 ClusterRole，这个 ClusterRole 具有整个集群内对 secrets 的访问权限；但是其授权用户 dave 只能访问 development 空间中的 secrets(因为 RoleBinding 定义在 development 命名空间)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># This role binding allows &quot;dave&quot; to read secrets in the &quot;development&quot; namespace.</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-secrets</span><br><span class="line">  namespace: development # This only grants permissions within the &quot;development&quot; namespace.</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: dave</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: secret-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>最后，使用 ClusterRoleBinding 可以对整个集群中的所有命名空间资源权限进行授权；以下 ClusterRoleBinding 样例展示了授权 manager 组内所有用户在全部命名空间中对 secrets 进行访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># This cluster role binding allows anyone in the &quot;manager&quot; group to read secrets in any namespace.</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-secrets-global</span><br><span class="line">subjects:</span><br><span class="line">- kind: Group</span><br><span class="line">  name: manager</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: secret-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><ul><li>新增一个新的用户sy</li><li>该用户只能对命名空间<code>kube-system</code>下面的<code>pods</code>和<code>deployments</code>进行管理</li></ul><p>第一步新建一个<code>ServiceAccount</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl create sa sy -n kube-system</span><br><span class="line">serviceaccount/sy created</span><br></pre></td></tr></table></figure><p>然后我们新建一个角色<strong>role-sy</strong>：(role.yaml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  name: role-sy</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line">- apiGroups: [&quot;extensions&quot;, &quot;apps&quot;]</span><br><span class="line">  resources: [&quot;deployments&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]</span><br></pre></td></tr></table></figure><p>注意上面的<code>rules</code>规则：管理<code>pods</code>和<code>deployments</code>的权限。</p><p>然后我们创建一个角色绑定，将上面的角色<code>role-sy绑定到**sy**的</code>ServiceAccount`上：(role-bind.yaml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: role-bind-sy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: sy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: role-sy</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>分别执行上面两个<code>yaml</code>文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl create -f role.yaml</span><br><span class="line">role.rbac.authorization.k8s.io/role-sy created</span><br><span class="line">[root@node1 dashboard]# kubectl create -f role-bind.yaml </span><br><span class="line">rolebinding.rbac.authorization.k8s.io/role-bind-sy created</span><br></pre></td></tr></table></figure><p>接下来该怎么做？和前面一样的，我们只需要拿到sy这个<code>ServiceAccount</code>的<code>token</code>就可以登录<code>Dashboard</code>了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl get secret -n kube-system |grep sy</span><br><span class="line">sy-token-5cmnl                                   kubernetes.io/service-account-token   3      3m2s</span><br><span class="line"></span><br><span class="line">[root@node1 dashboard]# kubectl get secret sy-token-5cmnl  -o jsonpath=&#123;.data.token&#125; -n kube-system |base64 -d</span><br><span class="line"># 会生成一串很长的base64后的字符串</span><br></pre></td></tr></table></figure><p>这样就可以控制权限了，需要将登录地址改为namespace=kube-system</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;修改配置文件&quot;&gt;&lt;a href=&quot;#修改配置文件&quot; class=&quot;headerlink&quot; title=&quot;修改配置文件&quot;&gt;&lt;/a&gt;修改配置文件&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gut
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-metrics-server</title>
    <link href="https://shenshengkun.github.io/posts/d3554aa2.html"/>
    <id>https://shenshengkun.github.io/posts/d3554aa2.html</id>
    <published>2019-06-10T03:16:01.000Z</published>
    <updated>2019-06-10T03:22:59.745Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。</p><p>从 Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。</p><p>替代方案如下：</p><ol><li>用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server；</li><li>通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator；</li><li>事件传输：使用第三方工具来传输、归档 kubernetes events；</li></ol><p>Kubernetes Dashboard 还不支持 metrics-server（PR：<a href="https://github.com/kubernetes/dashboard/pull/3504" target="_blank" rel="noopener">#3504</a>），如果使用 metrics-server 替代 Heapster，将无法在 dashboard 中以图形展示 Pod 的内存和 CPU 情况，需要通过 Prometheus、Grafana 等监控方案来弥补。</p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/</span><br><span class="line">git clone https://github.com/kubernetes-incubator/metrics-server.git</span><br><span class="line">cd metrics-server/deploy/1.8+/</span><br><span class="line">ls</span><br><span class="line">aggregated-metrics-reader.yaml  auth-delegator.yaml  auth-reader.yaml  metrics-apiservice.yaml  metrics-server-deployment.yaml  metrics-server-service.yaml  resource-reader.yaml</span><br></pre></td></tr></table></figure><h2 id="修改-metrics-server-deployment-yaml-文件，为-metrics-server-添加三个命令行参数："><a href="#修改-metrics-server-deployment-yaml-文件，为-metrics-server-添加三个命令行参数：" class="headerlink" title="修改 metrics-server-deployment.yaml 文件，为 metrics-server 添加三个命令行参数："></a>修改 <code>metrics-server-deployment.yaml</code> 文件，为 metrics-server 添加三个命令行参数：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: metrics-server</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: metrics-server</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: metrics-server</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: metrics-server</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: metrics-server</span><br><span class="line">      volumes:</span><br><span class="line">      # mount in tmp so we can safely use from-scratch images and/or read-only containers</span><br><span class="line">      - name: tmp-dir</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      containers:</span><br><span class="line">      - name: metrics-server</span><br><span class="line">        image: mirrorgooglecontainers/metrics-server-amd64:v0.3.3</span><br><span class="line">        command:</span><br><span class="line">        - /metrics-server</span><br><span class="line">        - --metric-resolution=30s</span><br><span class="line">        - --requestheader-allowed-names=aggregator</span><br><span class="line">        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: tmp-dir</span><br><span class="line">          mountPath: /tmp</span><br></pre></td></tr></table></figure><ul><li>–metric-resolution=30s：从 kubelet 采集数据的周期；</li><li>–requestheader-allowed-names=aggregator：允许请求 metrics-server API 的用户名，该名称与 kube-apiserver 的 <code>--proxy-client-cert-file</code> 指定的证书 CN 一致；</li><li>–kubelet-preferred-address-types：优先使用 InternalIP 来访问 kubelet，这样可以避免节点名称<strong>没有 DNS 解析</strong>记录时，通过节点名称调用节点 kubelet API 失败的情况（未配置时默认的情况）；</li></ul><h2 id="修改apiserver参数："><a href="#修改apiserver参数：" class="headerlink" title="修改apiserver参数："></a>修改apiserver参数：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/systemd/system</span><br><span class="line"></span><br><span class="line">vim kube-apiserver.service</span><br><span class="line"></span><br><span class="line">--requestheader-allowed-names=&quot;aggregator&quot;</span><br></pre></td></tr></table></figure><p>重启apiserver</p><h2 id="部署-metrics-server："><a href="#部署-metrics-server：" class="headerlink" title="部署 metrics-server："></a>部署 metrics-server：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/metrics-server/deploy/1.8+/</span><br><span class="line">kubectl create -f .</span><br></pre></td></tr></table></figure><h2 id="使用-kubectl-top-命令查看集群节点资源使用情况"><a href="#使用-kubectl-top-命令查看集群节点资源使用情况" class="headerlink" title="使用 kubectl top 命令查看集群节点资源使用情况"></a>使用 kubectl top 命令查看集群节点资源使用情况</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 1.8+]# kubectl top node</span><br><span class="line">NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   </span><br><span class="line">node1   117m         5%     2217Mi          57%       </span><br><span class="line">node2   147m         7%     2680Mi          69%</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-coredns</title>
    <link href="https://shenshengkun.github.io/posts/77sa4nc2.html"/>
    <id>https://shenshengkun.github.io/posts/77sa4nc2.html</id>
    <published>2019-06-06T04:32:01.000Z</published>
    <updated>2019-06-21T12:01:56.403Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>1.11后CoreDNS 已取代 Kube DNS 作为集群服务发现元件,由于 Kubernetes 需要让 Pod 与 Pod 之间能夠互相通信,然而要能够通信需要知道彼此的 IP 才行,而这种做法通常是通过 Kubernetes API 来获取,但是 Pod IP 会因为生命周期变化而改变,因此这种做法无法弹性使用,且还会增加 API Server 负担,基于此问题 Kubernetes 提供了 DNS 服务来作为查询,让 Pod 能夠以 Service 名称作为域名来查询 IP 位址,因此使用者就再不需要关心实际 Pod IP,而 DNS 也会根据 Pod 变化更新资源记录(Record resources)</p><p>CoreDNS 是由 CNCF 维护的开源 DNS 方案,该方案前身是 SkyDNS,其采用了 Caddy 的一部分来开发伺服器框架,使其能够建立一套快速灵活的 DNS,而 CoreDNS 每个功能都可以被当作成一個插件的中介软体,如 Log、Cache、Kubernetes 等功能,甚至能够将源记录存储在 Redis、Etcd 中</p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>coredns 目录是 <code>cluster/addons/dns</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/kubernetes/cluster/addons/dns/coredns</span><br><span class="line">cp coredns.yaml.base coredns.yaml</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">sed -i -e &quot;s/__PILLAR__DNS__DOMAIN__/$&#123;CLUSTER_DNS_DOMAIN&#125;/&quot; -e &quot;s/__PILLAR__DNS__SERVER__/$&#123;CLUSTER_DNS_SVC_IP&#125;/&quot; coredns.yaml</span><br><span class="line"></span><br><span class="line">还需要将镜像修改下，coredns/coredns:1.3.1</span><br></pre></td></tr></table></figure><h2 id="创建-coredns"><a href="#创建-coredns" class="headerlink" title="创建 coredns"></a>创建 coredns</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f coredns.yaml</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cat&lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: busybox</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: busybox</span><br><span class="line">    image: busybox:1.28.3</span><br><span class="line">    command:</span><br><span class="line">      - sleep</span><br><span class="line">      - &quot;3600&quot;</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>创建成功后，我们进行检查</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod</span><br><span class="line">NAME      READY   STATUS    RESTARTS   AGE</span><br><span class="line">busybox   1/1     Running   0          4s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 coredns]# kubectl exec -ti busybox -- nslookup kubernetes</span><br><span class="line">Server:    10.254.0.2</span><br><span class="line">Address 1: 10.254.0.2 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      kubernetes</span><br><span class="line">Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 ~]# kubectl exec -ti busybox ping kubernetes.default.svc.cluster.local</span><br><span class="line">PING kubernetes.default.svc.cluster.local (10.254.0.1): 56 data bytes</span><br><span class="line">64 bytes from 10.254.0.1: seq=0 ttl=64 time=0.099 ms</span><br><span class="line">^C</span><br><span class="line">--- kubernetes.default.svc.cluster.local ping statistics ---</span><br><span class="line">1 packets transmitted, 1 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.099/0.099/0.099 ms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：用my-svc.my-namespace.svc.cluster.local的方式可以访问服务</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;1.11后CoreDNS 已取代 Kube DNS 作为集群服务发现元件,由于 Kubernetes 需要让 Pod 与 Pod 之间能夠互
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s角色</title>
    <link href="https://shenshengkun.github.io/posts/65aa44f.html"/>
    <id>https://shenshengkun.github.io/posts/65aa44f.html</id>
    <published>2019-06-06T02:01:01.000Z</published>
    <updated>2019-07-08T07:42:22.780Z</updated>
    
    <content type="html"><![CDATA[<h1 id="查看node节点"><a href="#查看node节点" class="headerlink" title="查看node节点"></a>查看node节点</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 work]# kubectl get nodes</span><br><span class="line">NAME    STATUS   ROLES    AGE   VERSION</span><br><span class="line">node1   Ready    &lt;none&gt;   41h   v1.14.2</span><br><span class="line">node2   Ready    &lt;none&gt;   41h   v1.14.2</span><br></pre></td></tr></table></figure><h1 id="设置集群角色"><a href="#设置集群角色" class="headerlink" title="设置集群角色"></a>设置集群角色</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 设置 node1 为 master 角色</span><br><span class="line"></span><br><span class="line">kubectl label nodes node1 node-role.kubernetes.io/master=</span><br><span class="line"></span><br><span class="line"># 设置 node2 为 node 角色</span><br><span class="line"></span><br><span class="line">kubectl label nodes node2 node-role.kubernetes.io/node=</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 ~]# kubectl get nodes</span><br><span class="line">NAME    STATUS   ROLES    AGE   VERSION</span><br><span class="line">node1   Ready    master   42h   v1.14.2</span><br><span class="line">node2   Ready    node     42h   v1.14.2</span><br></pre></td></tr></table></figure><h1 id="设置taint"><a href="#设置taint" class="headerlink" title="设置taint"></a>设置taint</h1><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node [node] key=value[effect]   </span><br><span class="line">     其中[effect] 可取值: [ NoSchedule | PreferNoSchedule | NoExecute ]</span><br><span class="line">      NoSchedule: 一定不能被调度</span><br><span class="line">      PreferNoSchedule: 尽量不要调度</span><br><span class="line">      NoExecute: 不仅不会调度, 还会驱逐Node上已有的Pod</span><br></pre></td></tr></table></figure><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl taint nodes node1 node-role.kubernetes.io/master=:NoExecute</span><br><span class="line">node/node1 tainted</span><br><span class="line">[root@node1 ~]# kubectl get pods</span><br><span class="line">NAME             READY   STATUS        RESTARTS   AGE</span><br><span class="line">nginx-ds-kztdz   1/1     Running       0          18h</span><br><span class="line">nginx-ds-vbjh9   0/1     Terminating   0          18h</span><br><span class="line">[root@node1 ~]# kubectl get pods</span><br><span class="line">NAME             READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-ds-kztdz   1/1     Running   0          18h</span><br></pre></td></tr></table></figure><h2 id="查看taint"><a href="#查看taint" class="headerlink" title="查看taint"></a>查看taint</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl describe node node1</span><br><span class="line">Name:               node1</span><br><span class="line">Roles:              master</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/arch=amd64</span><br><span class="line">                    kubernetes.io/hostname=node1</span><br><span class="line">                    kubernetes.io/os=linux</span><br><span class="line">                    node-role.kubernetes.io/master=</span><br><span class="line">Annotations:        node.alpha.kubernetes.io/ttl: 0</span><br><span class="line">                    volumes.kubernetes.io/controller-managed-attach-detach: true</span><br><span class="line">CreationTimestamp:  Tue, 04 Jun 2019 15:28:56 +0800</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoExecute</span><br><span class="line">                    node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">Unschedulable:      false</span><br><span class="line">Conditions:</span><br><span class="line">  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message</span><br><span class="line">  ----             ------  -----------------                 ------------------                ------                       -------</span><br><span class="line">  MemoryPressure   False   Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available</span><br><span class="line">  DiskPressure     False   Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure</span><br><span class="line">  PIDPressure      False   Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available</span><br><span class="line">  Ready            True    Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletReady                 kubelet is posting ready status</span><br><span class="line">Addresses:</span><br><span class="line">  InternalIP:  192.168.6.101</span><br></pre></td></tr></table></figure><h2 id="删除taint"><a href="#删除taint" class="headerlink" title="删除taint"></a>删除taint</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl taint nodes node1 node-role.kubernetes.io/master-</span><br><span class="line">node/node1 untainted</span><br></pre></td></tr></table></figure><h1 id="RBAC"><a href="#RBAC" class="headerlink" title="RBAC"></a>RBAC</h1><p><code>Kubernetes</code>有一个很基本的特性就是它的<a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/" target="_blank" rel="noopener">所有资源对象都是模型化的 API 对象</a>，允许执行 CRUD(Create、Read、Update、Delete)操作(也就是我们常说的增、删、改、查操作)，比如下面的这下资源：</p><ul><li>Pods</li><li>ConfigMaps</li><li>Deployments</li><li>Nodes</li><li>Secrets</li><li>Namespaces</li></ul><p>上面这些资源对象的可能存在的操作有：</p><ul><li>create</li><li>get</li><li>delete</li><li>list</li><li>update</li><li>edit</li><li>watch</li><li>exec</li></ul><p>在更上层，这些资源和 API Group 进行关联，比如<code>Pods</code>属于 Core API Group，而<code>Deployements</code>属于 apps API Group，要在<code>Kubernetes</code>中进行<code>RBAC</code>的管理，除了上面的这些资源和操作以外，我们还需要另外的一些对象：</p><ul><li>Rule：规则，规则是一组属于不同 API Group 资源上的一组操作的集合</li><li>Role 和 ClusterRole：角色和集群角色，这两个对象都包含上面的 Rules 元素，二者的区别在于，在 Role 中，定义的规则只适用于单个命名空间，也就是和 namespace 关联的，而 ClusterRole 是集群范围内的，因此定义的规则不受命名空间的约束。另外 Role 和 ClusterRole 在<code>Kubernetes</code>中都被定义为集群内部的 API 资源，和我们前面学习过的 Pod、ConfigMap 这些类似，都是我们集群的资源对象，所以同样的可以使用我们前面的<code>kubectl</code>相关的命令来进行操作</li><li>Subject：主题，对应在集群中尝试操作的对象，集群中定义了3种类型的主题资源：<ul><li>User Account：用户，这是有外部独立服务进行管理的，管理员进行私钥的分配，用户可以使用 KeyStone或者 Goolge 帐号，甚至一个用户名和密码的文件列表也可以。对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部的 API 来进行管理</li><li>Group：组，这是用来关联多个账户的，集群中有一些默认创建的组，比如cluster-admin</li><li>Service Account：服务帐号，通过<code>Kubernetes</code> API 来管理的一些用户帐号，和 namespace 进行关联的，适用于集群内部运行的应用程序，需要通过 API 来完成权限认证，所以在集群内部进行权限操作，我们都需要使用到 ServiceAccount，这也是我们这节课的重点</li></ul></li><li>RoleBinding 和 ClusterRoleBinding：角色绑定和集群角色绑定，简单来说就是把声明的 Subject 和我们的 Role 进行绑定的过程(给某个用户绑定上操作的权限)，二者的区别也是作用范围的区别：RoleBinding 只会影响到当前 namespace 下面的资源操作权限，而 ClusterRoleBinding 会影响到所有的 namespace。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;查看node节点&quot;&gt;&lt;a href=&quot;#查看node节点&quot; class=&quot;headerlink&quot; title=&quot;查看node节点&quot;&gt;&lt;/a&gt;查看node节点&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td cl
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
</feed>
