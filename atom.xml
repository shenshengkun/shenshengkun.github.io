<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>舒宇的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://shenshengkun.github.io/"/>
  <updated>2021-03-03T06:51:00.657Z</updated>
  <id>https://shenshengkun.github.io/</id>
  
  <author>
    <name>Shu Yu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>k8s的pod名称不宜过长</title>
    <link href="https://shenshengkun.github.io/posts/dv8pan45.html"/>
    <id>https://shenshengkun.github.io/posts/dv8pan45.html</id>
    <published>2021-03-03T06:10:01.000Z</published>
    <updated>2021-03-03T06:51:00.657Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pod名称命名规范"><a href="#pod名称命名规范" class="headerlink" title="pod名称命名规范"></a>pod名称命名规范</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pod名字受DNS Label Names 63位长度限制。</span><br><span class="line">prometheus在生成metric的label时也有64个字符的限制，如果超过了，会导致有些pod label被截断，监控有问题，调metics的一些自动扩容策略也会有问题。</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;pod名称命名规范&quot;&gt;&lt;a href=&quot;#pod名称命名规范&quot; class=&quot;headerlink&quot; title=&quot;pod名称命名规范&quot;&gt;&lt;/a&gt;pod名称命名规范&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;t
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s二次调度</title>
    <link href="https://shenshengkun.github.io/posts/dm0kma1c.html"/>
    <id>https://shenshengkun.github.io/posts/dm0kma1c.html</id>
    <published>2020-06-29T23:20:01.000Z</published>
    <updated>2020-06-30T01:15:59.029Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>在之前文章中，<a href="https://shenshengkun.github.io/posts/dk8k79hg.html">kube-schedule原理</a>，当中我们说到了k8s原始的调度，有一些不合理性，当时也介绍了一些优先级调度以及自定义调度，下面主要说下这个开源的二次调度工具Descheduler。</p><h1 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h1><h2 id="RemoveDuplicates策略"><a href="#RemoveDuplicates策略" class="headerlink" title="RemoveDuplicates策略"></a>RemoveDuplicates策略</h2><p>该策略确保只有一个Pod与同一节点上运行的副本集（RS），Replication Controller（RC），deployment或者job关联。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: &quot;descheduler/v1alpha1&quot;</span><br><span class="line">kind: &quot;DeschedulerPolicy&quot;</span><br><span class="line">strategies:</span><br><span class="line">  &quot;RemoveDuplicates&quot;:</span><br><span class="line">     enabled: true</span><br><span class="line">     params:</span><br><span class="line">       removeDuplicates:</span><br><span class="line">         excludeOwnerKinds:</span><br><span class="line">         - &quot;ReplicaSet&quot;</span><br></pre></td></tr></table></figure><h2 id="LowNodeUtilization策略"><a href="#LowNodeUtilization策略" class="headerlink" title="LowNodeUtilization策略"></a>LowNodeUtilization策略</h2><p>该策略发现未充分利用的节点，并且如果可能的话，从其他节点驱逐pod，希望在这些未充分利用的节点上安排被驱逐的pod的重新创建。此策略的参数配置在 <code>nodeResourceUtilizationThresholds</code>。</p><p>节点的利用率低是由可配置的阈值决定的 <code>thresholds</code>。<code>thresholds</code> 可以按百分比为cpu，内存和pod数量配置阈值 。如果节点的使用率低于所有（cpu，内存和pod数）的阈值，则该节点被视为未充分利用。目前，pods的请求资源需求被考虑用于计算节点资源利用率。</p><p>还有另一个可配置的阈值，<code>targetThresholds</code> 用于计算可以驱逐pod的潜在节点。任何节点，所述阈值之间，<code>thresholds</code> 并且 <code>targetThresholds</code> 被视为适当地利用，并且不考虑驱逐。阈值 <code>targetThresholds</code> 也可以按百分比配置为cpu，内存和pod数量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: &quot;descheduler/v1alpha1&quot;</span><br><span class="line">kind: &quot;DeschedulerPolicy&quot;</span><br><span class="line">strategies:</span><br><span class="line">  &quot;LowNodeUtilization&quot;:</span><br><span class="line">     enabled: true</span><br><span class="line">     params:</span><br><span class="line">       nodeResourceUtilizationThresholds:</span><br><span class="line">         thresholds:</span><br><span class="line">           &quot;cpu&quot; : 20</span><br><span class="line">           &quot;memory&quot;: 20</span><br><span class="line">           &quot;pods&quot;: 20</span><br><span class="line">         targetThresholds:</span><br><span class="line">           &quot;cpu&quot; : 50</span><br><span class="line">           &quot;memory&quot;: 50</span><br><span class="line">           &quot;pods&quot;: 50</span><br></pre></td></tr></table></figure><h2 id="RemovePodsViolatingInterPodAntiAffinity策略"><a href="#RemovePodsViolatingInterPodAntiAffinity策略" class="headerlink" title="RemovePodsViolatingInterPodAntiAffinity策略"></a>RemovePodsViolatingInterPodAntiAffinity策略</h2><p> 该策略可确保从节点中删除违反Interpod反亲和关系的pod。例如，如果某个节点上有<code>podA</code>，并且<code>podB</code>和<code>podC</code>（在同一节点上运行）具有禁止它们在同一节点上运行的反亲和规则，则<code>podA</code>将被从该节点逐出，以便<code>podB</code>和<code>podC</code>正常运行。当 <code>podB</code> 和 <code>podC</code> 已经运行在节点上后，反亲和性规则被创建就会发送这样的问题。目前，没有与该策略关联的参数。要禁用此策略，策略应如下所示： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: &quot;descheduler/v1alpha1&quot;</span><br><span class="line">kind: &quot;DeschedulerPolicy&quot;</span><br><span class="line">strategies:</span><br><span class="line">  &quot;RemovePodsViolatingInterPodAntiAffinity&quot;:</span><br><span class="line">     enabled: false</span><br></pre></td></tr></table></figure><h2 id="RemovePodsViolatingNodeAffinity策略"><a href="#RemovePodsViolatingNodeAffinity策略" class="headerlink" title="RemovePodsViolatingNodeAffinity策略"></a>RemovePodsViolatingNodeAffinity策略</h2><p>启用后，该策略<code>requiredDuringSchedulingRequiredDuringExecution</code>将用作kubelet 的临时实现并逐出该kubelet，不再考虑节点亲和力。</p><p>例如，在nodeA上调度了podA，该podA满足了调度时的节点亲缘性规则<code>requiredDuringSchedulingIgnoredDuringExecution</code>。随着时间的流逝，nodeA停止满足该规则。当执行该策略并且有另一个可用的节点满足该节点相似性规则时，podA被从nodeA中逐出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: &quot;descheduler/v1alpha1&quot;</span><br><span class="line">kind: &quot;DeschedulerPolicy&quot;</span><br><span class="line">strategies:</span><br><span class="line">  &quot;RemovePodsViolatingNodeAffinity&quot;:</span><br><span class="line">    enabled: true</span><br><span class="line">    params:</span><br><span class="line">      nodeAffinityType:</span><br><span class="line">      - &quot;requiredDuringSchedulingIgnoredDuringExecution&quot;</span><br></pre></td></tr></table></figure><h2 id="RemovePodsViolatingNodeTaints-策略"><a href="#RemovePodsViolatingNodeTaints-策略" class="headerlink" title="RemovePodsViolatingNodeTaints 策略"></a>RemovePodsViolatingNodeTaints 策略</h2><p> 该策略可以确保从节点中删除违反 <code>NoSchedule</code> 污点的 <code>Pod</code>。例如，有一个名为 <code>podA</code> 的 <code>Pod</code>，通过配置容忍 <code>key=value:NoSchedule</code> 允许被调度到有该污点配置的节点上，如果节点的污点随后被更新或者删除了，则污点将不再被 <code>Pod</code> 的容忍满足，然后将被驱逐 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: &quot;descheduler/v1alpha1&quot;</span><br><span class="line">kind: &quot;DeschedulerPolicy&quot;</span><br><span class="line">strategies:</span><br><span class="line">  &quot;RemovePodsViolatingNodeTaints&quot;:</span><br><span class="line">    enabled: true</span><br></pre></td></tr></table></figure><h2 id="RemovePodsHavingTooManyRestarts"><a href="#RemovePodsHavingTooManyRestarts" class="headerlink" title="RemovePodsHavingTooManyRestarts"></a>RemovePodsHavingTooManyRestarts</h2><p> 此策略确保从节点中删除重启次数过多的Pod 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: &quot;descheduler/v1alpha1&quot;</span><br><span class="line">kind: &quot;DeschedulerPolicy&quot;</span><br><span class="line">strategies:</span><br><span class="line">  &quot;RemovePodsHavingTooManyRestarts&quot;:</span><br><span class="line">     enabled: true</span><br><span class="line">     params:</span><br><span class="line">       podsHavingTooManyRestarts:</span><br><span class="line">         podRestartThreshold: 100</span><br><span class="line">         includingInitContainers: true</span><br></pre></td></tr></table></figure><h2 id="PodLifeTime"><a href="#PodLifeTime" class="headerlink" title="PodLifeTime"></a>PodLifeTime</h2><p>此策略逐出比<code>.strategies.PodLifeTime.params.maxPodLifeTimeSeconds</code>该策略文件更旧的pod。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: &quot;descheduler/v1alpha1&quot;</span><br><span class="line">kind: &quot;DeschedulerPolicy&quot;</span><br><span class="line">strategies:</span><br><span class="line">  &quot;PodLifeTime&quot;:</span><br><span class="line">     enabled: true</span><br><span class="line">     params:</span><br><span class="line">        maxPodLifeTimeSeconds: 86400</span><br></pre></td></tr></table></figure><h1 id="版本兼容性"><a href="#版本兼容性" class="headerlink" title="版本兼容性"></a>版本兼容性</h1><table><thead><tr><th>Descheduler</th><th>Supported Kubernetes Version</th></tr></thead><tbody><tr><td>v0.18</td><td>v1.18</td></tr><tr><td>v0.10</td><td>v1.17</td></tr><tr><td>v0.4-v0.9</td><td>v1.9+</td></tr><tr><td>v0.1-v0.3</td><td>v1.7-v1.8</td></tr></tbody></table><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><p><strong>注意</strong>：由于生产集群一般都是1.17以前的版本，故本实例是Descheduler0.9版本。</p><p>创建角色与账户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">[root@master01 kubernetes]# cat rbac.yaml </span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler-cluster-role</span><br><span class="line">  namespace: kube-system</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;nodes&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;, &quot;delete&quot;]</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pods/eviction&quot;]</span><br><span class="line">  verbs: [&quot;create&quot;]</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler-sa</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: descehduler-cluster-role-binding</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: descheduler-cluster-role</span><br><span class="line">subjects:</span><br><span class="line">  - name: descheduler-sa</span><br><span class="line">    kind: ServiceAccount</span><br><span class="line">    namespace: kube-system</span><br></pre></td></tr></table></figure><p>创建configmap</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@master01 kubernetes]# cat configmap.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler-policy-configmap</span><br><span class="line">  namespace: kube-system</span><br><span class="line">data:</span><br><span class="line">  policy.yaml: |</span><br><span class="line">    apiVersion: &quot;descheduler/v1alpha1&quot;</span><br><span class="line">    kind: &quot;DeschedulerPolicy&quot;</span><br><span class="line">    strategies:</span><br><span class="line">      &quot;RemoveDuplicates&quot;:</span><br><span class="line">         enabled: true</span><br><span class="line">      &quot;RemovePodsViolatingInterPodAntiAffinity&quot;:</span><br><span class="line">         enabled: true</span><br><span class="line">      &quot;LowNodeUtilization&quot;:</span><br><span class="line">         enabled: true</span><br><span class="line">         params:</span><br><span class="line">           nodeResourceUtilizationThresholds:</span><br><span class="line">             thresholds:</span><br><span class="line">               &quot;pods&quot;: 5</span><br><span class="line">             targetThresholds:</span><br><span class="line">               &quot;pods&quot;: 10</span><br></pre></td></tr></table></figure><p>任务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: descheduler-job</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  parallelism: 1</span><br><span class="line">  completions: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: descheduler-pod</span><br><span class="line">      annotations:</span><br><span class="line">        scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;</span><br><span class="line">    spec:</span><br><span class="line">        containers:</span><br><span class="line">        - name: descheduler</span><br><span class="line">          image: aveshagarwal/descheduler:0.9.0</span><br><span class="line">          volumeMounts:</span><br><span class="line">          - mountPath: /policy-dir</span><br><span class="line">            name: policy-volume</span><br><span class="line">          command:</span><br><span class="line">            - &quot;/bin/descheduler&quot;</span><br><span class="line">          args:</span><br><span class="line">            - &quot;--policy-config-file&quot;</span><br><span class="line">            - &quot;/policy-dir/policy.yaml&quot;</span><br><span class="line">            - &quot;--v&quot;</span><br><span class="line">            - &quot;3&quot;</span><br><span class="line">        restartPolicy: &quot;Never&quot;</span><br><span class="line">        serviceAccountName: descheduler-sa</span><br><span class="line">        volumes:</span><br><span class="line">        - name: policy-volume</span><br><span class="line">          configMap:</span><br><span class="line">            name: descheduler-policy-configmap</span><br></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/kubernetes-sigs/descheduler</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;在之前文章中，&lt;a href=&quot;https://shenshengkun.github.io/posts/dk8k79hg.html&quot;&gt;ku
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>prometheus监控harbor</title>
    <link href="https://shenshengkun.github.io/posts/39b40b31.html"/>
    <id>https://shenshengkun.github.io/posts/39b40b31.html</id>
    <published>2020-06-28T23:00:00.000Z</published>
    <updated>2020-06-28T09:30:43.561Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>随着k8s集群上面服务越来越多，大家更加关心的问题反而是服务能不能更稳定，这时候监控作用就体现出来了，下面主要说一下harbor是怎么用prometheus来监控的。</p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><p> github上的<code>harbor_exporter</code>的轮子都不行，最近馆长同学在写一个 harbor_exporter，也会一直在更新，代码在 <a href="https://github.com/zhangguanzhang/harbor_exporter" target="_blank" rel="noopener">https://github.com/zhangguanzhang/harbor_exporter</a></p><h2 id="打镜像"><a href="#打镜像" class="headerlink" title="打镜像"></a>打镜像</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd harbor_exporter/</span><br><span class="line">docker build -t shenshengkun/harbor-exporter:v0.1 -f Dockerfile .</span><br></pre></td></tr></table></figure><p>如果不想自己打镜像，也可以直接拉我已经打好的，现在已上传到dockerhub上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shenshengkun/harbor-exporter:v0.1</span><br></pre></td></tr></table></figure><h2 id="docker安装harbor-exporter"><a href="#docker安装harbor-exporter" class="headerlink" title="docker安装harbor_exporter"></a>docker安装harbor_exporter</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d -p 9107:9107 -e HARBOR_PASSWORD=Harbor12345 shenshengkun/harbor-exporter:v0.1 --harbor-server=http://x.x.x.x/api --insecure</span><br></pre></td></tr></table></figure><p>github上未提供k8s方式，下面简单写下怎么用k8s部署harbor_exporter，有会helm的也可以自己写个部署下。</p><h2 id="k8s上安装harbor-exporter"><a href="#k8s上安装harbor-exporter" class="headerlink" title="k8s上安装harbor_exporter"></a>k8s上安装harbor_exporter</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: harbor</span><br><span class="line">  namespace: monitor</span><br><span class="line">  labels:</span><br><span class="line">    app: harbor</span><br><span class="line">spec:</span><br><span class="line">  revisionHistoryLimit: 10</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: harbor</span><br><span class="line">    spec:</span><br><span class="line">      nodeSelector:</span><br><span class="line">        apptype: memnode</span><br><span class="line">      containers:</span><br><span class="line">      - name: harbor</span><br><span class="line">        image: shenshengkun/harbor-exporter:v0.1</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        args:</span><br><span class="line">        - &quot;--harbor-server=http://10.48.2.179/api&quot;</span><br><span class="line">        - &quot;--harbor-pass=Harbor12345&quot;</span><br><span class="line">        - &quot;--insecure&quot;</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9107</span><br><span class="line">          name: harbor</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 1000m</span><br><span class="line">            memory: 1024Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 500m</span><br><span class="line">            memory: 512Mi</span><br><span class="line">      securityContext:</span><br><span class="line">        runAsUser: 0</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: harbor</span><br><span class="line">  namespace: monitor</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 9107</span><br><span class="line">  selector:</span><br><span class="line">    app: harbor</span><br></pre></td></tr></table></figure><p>部署完了之后，在prometheus加个job就可以，也可以自己设置自动读取。</p><p>由于github上面还没有grafana的json模板，故我简单的写了一个。模板获取方式，关注微信公众号SY技术小站，在后台回复 “harbor-json”，就可以了。</p><h1 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h1><p><img src="https://shenshengkun.github.io/images/harbor-json.png" alt=""></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/zhangguanzhang/harbor_exporter</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;随着k8s集群上面服务越来越多，大家更加关心的问题反而是服务能不能更稳定，这时候监控作用就体现出来了，下面主要说一下harbor是怎么用pr
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s对接云实现自动扩容pod</title>
    <link href="https://shenshengkun.github.io/posts/dpka1a1v.html"/>
    <id>https://shenshengkun.github.io/posts/dpka1a1v.html</id>
    <published>2020-06-26T23:20:01.000Z</published>
    <updated>2020-06-24T07:26:05.694Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>在之前的文章我介绍了下 Custom Metric 怎么实现自动扩容的。<a href="https://shenshengkun.github.io/posts/0o81am3t.html">k8s基于自定义指标实现自动扩容</a></p><p>实际上Kubernetes定义了三种不同的监控数据接口，分别是<code>Resource Metric</code>，<code>Custom Metric</code>以及<code>External Metric</code>。 </p><p>一般来说<code>Resource Metric</code>是通过metrics-server采集；</p><p><code>Custom Metric</code>是通过prometheus来实现自定义扩容。</p><p><code>External Metric</code>就是针对云场景的了，比方说通过获取slb最大连接数来实现自动扩容。</p><p>下面我来说下具体怎么实现的。</p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><p>安装alibaba-cloud-metrics-adapter，以下是yaml文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: alibaba-cloud-metrics-adapter</span><br><span class="line">  name: alibaba-cloud-metrics-adapter</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: alibaba-cloud-metrics-adapter</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: alibaba-cloud-metrics-adapter</span><br><span class="line">      name: alibaba-cloud-metrics-adapter</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: admin</span><br><span class="line">      containers:</span><br><span class="line">      - name: alibaba-cloud-metrics-adapter</span><br><span class="line">        image: registry.cn-beijing.aliyuncs.com/acs/alibaba-cloud-metrics-adapter-amd64:v0.2.0-alpha-e8f8c17f</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 443</span><br><span class="line">          name: https</span><br><span class="line">        - containerPort: 8080</span><br><span class="line">          name: http</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /tmp</span><br><span class="line">          name: temp-vol</span><br><span class="line">        - name: tz-config</span><br><span class="line">          mountPath: /etc/localtime</span><br><span class="line">          readOnly: true</span><br><span class="line">      volumes:</span><br><span class="line">      - name: temp-vol</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      - name: tz-config</span><br><span class="line">        hostPath:</span><br><span class="line">          path: /etc/localtime</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: alibaba-cloud-metrics-adapter</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: https</span><br><span class="line">    port: 443</span><br><span class="line">    targetPort: 443</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: alibaba-cloud-metrics-adapter</span><br><span class="line">---</span><br><span class="line">apiVersion: apiregistration.k8s.io/v1beta1</span><br><span class="line">kind: APIService</span><br><span class="line">metadata:</span><br><span class="line">  name: v1beta1.external.metrics.k8s.io</span><br><span class="line">spec:</span><br><span class="line">  service:</span><br><span class="line">    name: alibaba-cloud-metrics-adapter</span><br><span class="line">    namespace: kube-system</span><br><span class="line">  group: external.metrics.k8s.io</span><br><span class="line">  version: v1beta1</span><br><span class="line">  insecureSkipTLSVerify: true</span><br><span class="line">  groupPriorityMinimum: 100</span><br><span class="line">  versionPriority: 100</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: external-metrics-server-resources</span><br><span class="line">rules:</span><br><span class="line">- apiGroups:</span><br><span class="line">  - external.metrics.k8s.io</span><br><span class="line">  resources: [&quot;*&quot;]</span><br><span class="line">  verbs: [&quot;*&quot;]</span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: external-metrics-resource-reader</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: external-metrics-server-resources</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: horizontal-pod-autoscaler</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure><p> 可以使用下面的命令来检测是否生效了： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get --raw=&quot;/apis/external.metrics.k8s.io/v1beta1&quot; | jq</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;APIResourceList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;groupVersion&quot;: &quot;external.metrics.k8s.io/v1beta1&quot;,</span><br><span class="line">  &quot;resources&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;sls_ingress_qps&quot;,</span><br><span class="line">      &quot;singularName&quot;: &quot;&quot;,</span><br><span class="line">      &quot;namespaced&quot;: true,</span><br><span class="line">      &quot;kind&quot;: &quot;ExternalMetricValueList&quot;,</span><br><span class="line">      &quot;verbs&quot;: [</span><br><span class="line">        &quot;get&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;slb_l4_connection_utilization&quot;,</span><br><span class="line">      &quot;singularName&quot;: &quot;&quot;,</span><br><span class="line">      &quot;namespaced&quot;: true,</span><br><span class="line">      &quot;kind&quot;: &quot;ExternalMetricValueList&quot;,</span><br><span class="line">      &quot;verbs&quot;: [</span><br><span class="line">        &quot;get&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;slb_l7_qps&quot;,</span><br><span class="line">      &quot;singularName&quot;: &quot;&quot;,</span><br><span class="line">      &quot;namespaced&quot;: true,</span><br><span class="line">      &quot;kind&quot;: &quot;ExternalMetricValueList&quot;,</span><br><span class="line">      &quot;verbs&quot;: [</span><br><span class="line">        &quot;get&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;ahas_sentinel_total_qps&quot;,</span><br><span class="line">      &quot;singularName&quot;: &quot;&quot;,</span><br><span class="line">      &quot;namespaced&quot;: true,</span><br><span class="line">      &quot;kind&quot;: &quot;ExternalMetricValueList&quot;,</span><br><span class="line">      &quot;verbs&quot;: [</span><br><span class="line">        &quot;get&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;ahas_sentinel_avg_rt&quot;,</span><br><span class="line">      &quot;singularName&quot;: &quot;&quot;,</span><br><span class="line">      &quot;namespaced&quot;: true,</span><br><span class="line">      &quot;kind&quot;: &quot;ExternalMetricValueList&quot;,</span><br><span class="line">      &quot;verbs&quot;: [</span><br><span class="line">        &quot;get&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;k8s_workload_cpu_util&quot;,</span><br><span class="line">      &quot;singularName&quot;: &quot;&quot;,</span><br><span class="line">      &quot;namespaced&quot;: true,</span><br><span class="line">      &quot;kind&quot;: &quot;ExternalMetricValueList&quot;,</span><br><span class="line">      &quot;verbs&quot;: [</span><br><span class="line">        &quot;get&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;k8s_workload_memory_request&quot;,</span><br><span class="line">      &quot;singularName&quot;: &quot;&quot;,</span><br><span class="line">      &quot;namespaced&quot;: true,</span><br><span class="line">      &quot;kind&quot;: &quot;ExternalMetricValueList&quot;,</span><br><span class="line">      &quot;verbs&quot;: [</span><br><span class="line">        &quot;get&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;k8s_workload_memory_cache&quot;,</span><br><span class="line">      &quot;singularName&quot;: &quot;&quot;,</span><br><span class="line">      &quot;namespaced&quot;: true,</span><br><span class="line">      &quot;kind&quot;: &quot;ExternalMetricValueList&quot;,</span><br><span class="line">      &quot;verbs&quot;: [</span><br><span class="line">        &quot;get&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    。。。。。。。。。</span><br></pre></td></tr></table></figure><p>简单说下各个指标的含义，方便之后去选择哪个指标去自动扩容。</p><blockquote><p> slb_l4_traffic_rx   每秒流入 </p><p> slb_l4_packet_tx   每秒流入的数据包数 </p><p> slb_l4_active_connection   活动连接 </p><p> slb_l4_max_connection   最大连接数</p><p> slb_l7_qps   QPS </p><p> slb_l7_status_2xx   2xx个请求（每秒） </p><p> slb_l7_upstream_4xx   上游服务4xx请求（每秒） </p><p> sls_ingress_qps   特定入口路由的QPS </p><p> sls_ingress_inflow   入口流入带宽 </p><p> k8s_workload_memory_usage  内存使用情况 </p><p> k8s_workload_memory_rss  rss </p></blockquote><h1 id="扩容例子"><a href="#扩容例子" class="headerlink" title="扩容例子"></a>扩容例子</h1><p> 根据slb_l4_active_connection这个指标，实现自动扩容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1beta2 # for versions before 1.8.0 use apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment-basic</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9 # replace it with your exactly &lt;image_name:tags&gt;</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  externalTrafficPolicy: Local</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">      protocol: TCP</span><br><span class="line">      targetPort: 80</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  sessionAffinity: None</span><br><span class="line">  type: LoadBalancer</span><br><span class="line">---</span><br><span class="line">apiVersion: autoscaling/v2beta2</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: slb-hpa</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1beta2</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: nginx-deployment-basic</span><br><span class="line">  minReplicas: 5</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  metrics:</span><br><span class="line">    - type: External</span><br><span class="line">      external:</span><br><span class="line">        metric:</span><br><span class="line">          name: slb_l4_active_connection</span><br><span class="line">          selector:</span><br><span class="line">            matchLabels:</span><br><span class="line">              # slb.instance.id: &quot;lb-2ze2locy5fk8at1cfx47y&quot;</span><br><span class="line">              slb.instance.id: &quot;&quot;</span><br><span class="line">              # slb.instance.port: &quot;80&quot;</span><br><span class="line">              slb.instance.port: &quot;&quot;</span><br><span class="line">        target:</span><br><span class="line">          type: Value</span><br><span class="line">          value: 100</span><br></pre></td></tr></table></figure><p>这样就实现了通过<code>External Metric</code>自动扩容。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/AliyunContainerService/alibaba-cloud-metrics-adapter</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;在之前的文章我介绍了下 Custom Metric 怎么实现自动扩容的。&lt;a href=&quot;https://shenshengkun.gith
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>比官方K8S Dashboard好用的几个工具</title>
    <link href="https://shenshengkun.github.io/posts/opl9nv3r.html"/>
    <id>https://shenshengkun.github.io/posts/opl9nv3r.html</id>
    <published>2020-06-24T23:10:01.000Z</published>
    <updated>2020-06-24T08:02:16.532Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>推荐三款小工具，具体使用哪个，大家可以自己斟酌，各有各的优点。</p><h1 id="kubeman"><a href="#kubeman" class="headerlink" title="kubeman"></a>kubeman</h1><p>一个很有意思的小工具叫 <code>kubeman</code>，它试图使从Kubernetes集群中查找信息变得更加容易，并且可以调查与Kubernetes和Istio相关的问题。 </p><h2 id="windows和mac安装"><a href="#windows和mac安装" class="headerlink" title="windows和mac安装"></a>windows和mac安装</h2><p>windows和mac安装很简单，下载相应的二进制，然后直接运行就好了 。</p><p>要是windows想执行linux的k8s集群，把.kube目录拷贝到/user/administrator下面就可以。</p><h2 id="linux安装"><a href="#linux安装" class="headerlink" title="linux安装"></a>linux安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">下载kubeman.0.5.0.appimage</span><br><span class="line">chmod a+x *.appimage</span><br><span class="line">wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span><br><span class="line">rpm -ivh epel-release-latest-7.noarch.rpm</span><br><span class="line">yum --enablerepo=epel -y install fuse-sshfs</span><br><span class="line">user=&quot;$(whoami)&quot;</span><br><span class="line">usermod -a -G fuse &quot;$user&quot;</span><br><span class="line">yum provides */libgtk-3.so.0</span><br><span class="line">yum -y install /usr/lib64/libgtk-3.so.0 /usr/lib/libgtk-3.so.0</span><br><span class="line">./kubeman.0.5.0.appimage  --appimage-extract</span><br><span class="line"></span><br><span class="line">启动的话，如果linux没开display，就需要用xmanager执行就可以</span><br><span class="line">命令是：</span><br><span class="line">./kubeman.0.5.0.appimage</span><br></pre></td></tr></table></figure><h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><h3 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h3><p><img src="https://shenshengkun.github.io/images/k8s_kubeman1.png" alt=""></p><h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p><img src="https://shenshengkun.github.io/images/k8s_kubeman2.png" alt=""></p><h3 id="测服务可达"><a href="#测服务可达" class="headerlink" title="测服务可达"></a>测服务可达</h3><p><img src="https://shenshengkun.github.io/images/k8s_kubeman3.png" alt=""></p><h1 id="lens"><a href="#lens" class="headerlink" title="lens"></a>lens</h1><p> <code>Lens</code> 是一个强大的 kubernetes IDE。可以实时查看 kubernetes 集群状态，比如 Pod实时日志查看、集群Events实时查看、集群故障排查等</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p> 从<a href="https://github.com/lensapp/lens/releases" target="_blank" rel="noopener">发布</a>页面下载预构建的软件包 。</p><h2 id="效果-1"><a href="#效果-1" class="headerlink" title="效果"></a>效果</h2><p><img src="https://shenshengkun.github.io/images/lens.png" alt=""></p><h1 id="kuboard"><a href="#kuboard" class="headerlink" title="kuboard"></a>kuboard</h1><p> <code>Kuboard</code> 是一款免费的 Kubernetes 管理工具，提供了丰富的功能，结合代码仓库、镜像仓库、CI/CD工具等，可以便捷的搭建一个生产可用的 Kubernetes 容器云平台，轻松管理和运行云原生应用。 </p><h2 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://kuboard.cn/install-script/kuboard.yaml</span><br><span class="line">kubectl apply -f https://addons.kuboard.cn/metrics-server/0.3.6/metrics-server.yaml</span><br></pre></td></tr></table></figure><p> 查看 Kuboard 运行状态： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -l k8s.kuboard.cn/name=kuboard -n kube-system</span><br></pre></td></tr></table></figure><p>获取token</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk &apos;&#123;print $1&#125;&apos;) -o go-template=&apos;&#123;&#123;.data.token&#125;&#125;&apos; | base64 -d)</span><br></pre></td></tr></table></figure><h2 id="效果-2"><a href="#效果-2" class="headerlink" title="效果"></a>效果</h2><p><img src="https://shenshengkun.github.io/images/kuboard1.png" alt=""></p><p><img src="https://shenshengkun.github.io/images/kuboard2.png" alt=""></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>三款工具各有千秋，大家结合自己公司情况选择吧！</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/walmartlabs/kubeman</span><br><span class="line">https://github.com/lensapp/lens</span><br><span class="line">https://kuboard.cn/</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;推荐三款小工具，具体使用哪个，大家可以自己斟酌，各有各的优点。&lt;/p&gt;
&lt;h1 id=&quot;kubeman&quot;&gt;&lt;a href=&quot;#kubeman
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>K8S多集群切换俩种方法</title>
    <link href="https://shenshengkun.github.io/posts/3349nb3b.html"/>
    <id>https://shenshengkun.github.io/posts/3349nb3b.html</id>
    <published>2020-06-22T23:10:01.000Z</published>
    <updated>2020-06-23T06:48:34.986Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>随着公司的k8s迁移，会发现集群越来越多，尤其是有混合云的公司，开发，测试，预生产，多个生产环境等等，管理k8s集群会越来越麻烦，下面我来介绍下k8s管理多集群的俩种方法。</p><h1 id="通过多种kubeconfig文件管理"><a href="#通过多种kubeconfig文件管理" class="headerlink" title="通过多种kubeconfig文件管理"></a>通过多种kubeconfig文件管理</h1><p>因为我的一台机器，网络打通了多个生产环境，测试，开发环境，故我拿这一台机器举个简单的例子。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#切换到生产集群</span><br><span class="line">kubectl get pod  --kubeconfig=/root/.kube/aliyun_prod-config</span><br><span class="line">#切换到生产idc集群</span><br><span class="line">kubectl get pod  --kubeconfig=/root/.kube/vnet_prod-config</span><br><span class="line">#切换到测试环境</span><br><span class="line">kubectl get pod  --kubeconfig=/root/.kube/bjcs_test-config</span><br></pre></td></tr></table></figure><p>这样虽然能达到管理多集群的目的，但是稍显麻烦且不灵活</p><h1 id="通过kubecm管理k8s多集群"><a href="#通过kubecm管理k8s多集群" class="headerlink" title="通过kubecm管理k8s多集群"></a>通过kubecm管理k8s多集群</h1><p>安装kubecm我就不详细说了，下载二进制包解压可用，地址 <a href="https://github.com/sunny0826/kubecm/releases" target="_blank" rel="noopener">https://github.com/sunny0826/kubecm/releases</a> </p><p>首先将config文件拷贝到sy目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd sy</span><br><span class="line">cp /root/.kube/aliyun_prod-config .</span><br><span class="line">cp /root/.kube/vnet_prod-config .</span><br><span class="line">cp /root/.kube/bjcs_test-config .</span><br></pre></td></tr></table></figure><p>merge一下，合并kubeconfig文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubecm merge -f sy</span><br><span class="line">#直接把新生成的 kubeconfig 文件替换 $HOME/.kube/config 文件</span><br><span class="line">kubecm merge -f sy -c</span><br></pre></td></tr></table></figure><p>切换集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># kubecm </span><br><span class="line">+------------+-----------------------+-----------------------+--------------------+--------------+</span><br><span class="line">|   CURRENT  |          NAME         |        CLUSTER        |        USER        |   Namespace  |</span><br><span class="line">+============+=======================+=======================+====================+======</span><br><span class="line">--------+</span><br><span class="line">|            |    vnet_prod-config   |   cluster-ccccc5  |   user-hbhbh9gmd5  |              |</span><br><span class="line">+------------+-----------------------+-----------------------+--------------------+--------------+</span><br><span class="line">|      *     |   aliyun_prod-config  |   cluster-bbbbb59  |   user-m4fd662d59  |              |</span><br><span class="line">+------------+-----------------------+-----------------------+--------------------+--------------+</span><br><span class="line">|            |    bjcs_test-config   |   cluster-baaaaa  |   user-b9mbtft7b2  |              |</span><br><span class="line">+------------+-----------------------+-----------------------+--------------------+--------------+</span><br><span class="line"></span><br><span class="line">2020/06/22 11:25:41 Cluster check succeeded!</span><br><span class="line">Contains components: [scheduler controller-manager etcd-0 etcd-1 etcd-3 etcd-2 etcd-4]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># kubecm switch</span><br><span class="line">Use the arrow keys to navigate: ↓ ↑ → ←  and / toggles search</span><br><span class="line">Select Kube Context</span><br><span class="line">   aliyun_prod-config(*)</span><br><span class="line">    bjcs_test-config</span><br><span class="line">↓   vnet_prod-config</span><br></pre></td></tr></table></figure><p>切换命名空间</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># kubecm ns</span><br><span class="line">Search: test█</span><br><span class="line">Select Namespace:</span><br><span class="line">    test </span><br><span class="line">   test1</span><br></pre></td></tr></table></figure><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>通过 <code>kubecm</code> 工具能快速的把多个 kubeconfig 文件合并到一个 kubeconfig 文件中，这种方式相比于第一种更好。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/sunny0826/kubecm</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;随着公司的k8s迁移，会发现集群越来越多，尤其是有混合云的公司，开发，测试，预生产，多个生产环境等等，管理k8s集群会越来越麻烦，下面我来介
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>备份与迁移k8s集群神器</title>
    <link href="https://shenshengkun.github.io/posts/olsn73dq.html"/>
    <id>https://shenshengkun.github.io/posts/olsn73dq.html</id>
    <published>2020-06-03T23:10:01.000Z</published>
    <updated>2020-06-03T11:32:25.686Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>一般来说大家都用etcd备份恢复k8s集群，但是有时候我们可能不小心删掉了一个namespace，假设这个ns里面有上百个服务，瞬间没了，怎么办？</p><p>当然了，可以用CI/CD系统发布，但是时间会花费很久，这时候，vmvare的Velero出现了。</p><p>velero可以帮助我们：</p><ul><li>灾备场景，提供备份恢复k8s集群的能力</li><li>迁移场景，提供拷贝集群资源到其他集群的能力（复制同步开发，测试，生产环境的集群配置，简化环境配置）</li></ul><p>下面我就介绍一下如何使用 Velero 完成备份和迁移。</p><blockquote><p>Velero 地址：<a href="https://github.com/vmware-tanzu/velero" target="_blank" rel="noopener">https://github.com/vmware-tanzu/velero</a></p><p>ACK 插件地址：<a href="https://github.com/AliyunContainerService/velero-plugin" target="_blank" rel="noopener">https://github.com/AliyunContainerService/velero-plugin</a></p></blockquote><h1 id="下载-Velero-客户端"><a href="#下载-Velero-客户端" class="headerlink" title="下载 Velero 客户端"></a>下载 Velero 客户端</h1><p>Velero 由客户端和服务端组成，服务器部署在目标 k8s 集群上，而客户端则是运行在本地的命令行工具。</p><ul><li>前往 <a href="https://github.com/vmware-tanzu/velero/releases" target="_blank" rel="noopener">Velero 的 Release 页面</a> 下载客户端，直接在 GitHub 上下载即可</li><li>解压 release 包</li><li>将 release 包中的二进制文件 <code>velero</code> 移动到 <code>$PATH</code> 中的某个目录下</li><li>执行 <code>velero -h</code> 测试</li></ul><h1 id="部署velero-plugin插件"><a href="#部署velero-plugin插件" class="headerlink" title="部署velero-plugin插件"></a>部署velero-plugin插件</h1><p>拉取代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/AliyunContainerService/velero-plugin</span><br></pre></td></tr></table></figure><p>配置修改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#修改 `install/credentials-velero` 文件，将新建用户中获得的 `AccessKeyID` 和 `AccessKeySecret` 填入，这里的 OSS EndPoint 为之前 OSS 的访问域名</span><br><span class="line"></span><br><span class="line">ALIBABA_CLOUD_ACCESS_KEY_ID=&lt;ALIBABA_CLOUD_ACCESS_KEY_ID&gt;</span><br><span class="line">ALIBABA_CLOUD_ACCESS_KEY_SECRET=&lt;ALIBABA_CLOUD_ACCESS_KEY_SECRET&gt;</span><br><span class="line">ALIBABA_CLOUD_OSS_ENDPOINT=&lt;ALIBABA_CLOUD_OSS_ENDPOINT&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line">#修改 `install/01-velero.yaml`，将 OSS 配置填入：</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  namespace: velero</span><br><span class="line">  name: velero</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    component: velero</span><br><span class="line">  name: velero</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: velero</span><br><span class="line">  namespace: velero</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: velero.io/v1</span><br><span class="line">kind: BackupStorageLocation</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    component: velero</span><br><span class="line">  name: default</span><br><span class="line">  namespace: velero</span><br><span class="line">spec:</span><br><span class="line">  config:</span><br><span class="line">    region: cn-beijing</span><br><span class="line">  objectStorage:</span><br><span class="line">    bucket: k8s-backup-test</span><br><span class="line">    prefix: test</span><br><span class="line">  provider: alibabacloud</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: velero.io/v1</span><br><span class="line">kind: VolumeSnapshotLocation</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    component: velero</span><br><span class="line">  name: default</span><br><span class="line">  namespace: velero</span><br><span class="line">spec:</span><br><span class="line">  config:</span><br><span class="line">    region: cn-beijing</span><br><span class="line">  provider: alibabacloud</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: velero</span><br><span class="line">  namespace: velero</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      deploy: velero</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/path: /metrics</span><br><span class="line">        prometheus.io/port: &quot;8085&quot;</span><br><span class="line">        prometheus.io/scrape: &quot;true&quot;</span><br><span class="line">      labels:</span><br><span class="line">        component: velero</span><br><span class="line">        deploy: velero</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: velero</span><br><span class="line">      containers:</span><br><span class="line">      - name: velero</span><br><span class="line">        # sync from velero/velero:v1.2.0</span><br><span class="line">        image: registry.cn-hangzhou.aliyuncs.com/acs/velero:v1.2.0</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        command:</span><br><span class="line">          - /velero</span><br><span class="line">        args:</span><br><span class="line">          - server</span><br><span class="line">          - --default-volume-snapshot-locations=alibabacloud:default</span><br><span class="line">        env:</span><br><span class="line">          - name: VELERO_SCRATCH_DIR</span><br><span class="line">            value: /scratch</span><br><span class="line">          - name: ALIBABA_CLOUD_CREDENTIALS_FILE</span><br><span class="line">            value: /credentials/cloud</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - mountPath: /plugins</span><br><span class="line">            name: plugins</span><br><span class="line">          - mountPath: /scratch</span><br><span class="line">            name: scratch</span><br><span class="line">          - mountPath: /credentials</span><br><span class="line">            name: cloud-credentials</span><br><span class="line">      initContainers:</span><br><span class="line">      - image: registry.cn-hangzhou.aliyuncs.com/acs/velero-plugin-alibabacloud:v1.2-991b590</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        name: velero-plugin-alibabacloud</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - mountPath: /target</span><br><span class="line">          name: plugins</span><br><span class="line">      volumes:</span><br><span class="line">        - emptyDir: &#123;&#125;</span><br><span class="line">          name: plugins</span><br><span class="line">        - emptyDir: &#123;&#125;</span><br><span class="line">          name: scratch</span><br><span class="line">        - name: cloud-credentials</span><br><span class="line">          secret:</span><br><span class="line">            secretName: cloud-credentials</span><br></pre></td></tr></table></figure><p>k8s 部署 Velero 服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 新建 namespace</span><br><span class="line">kubectl create namespace velero</span><br><span class="line"># 部署 credentials-velero 的 secret</span><br><span class="line">kubectl create secret generic cloud-credentials --namespace velero --from-file cloud=install/credentials-velero</span><br><span class="line"># 部署 CRD</span><br><span class="line">kubectl apply -f install/00-crds.yaml</span><br><span class="line"># 部署 Velero</span><br><span class="line">kubectl apply -f install/01-velero.yaml</span><br></pre></td></tr></table></figure><h1 id="备份测试"><a href="#备份测试" class="headerlink" title="备份测试"></a>备份测试</h1><p>这里，我们将使用velero备份一个集群内相关的resource，并在当该集群出现一些故障或误操作的时候，能够快速恢复集群resource， 首先我们用下面的yaml来部署： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-example</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-deployment</span><br><span class="line">  namespace: nginx-example</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx:1.7.9</span><br><span class="line">        name: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">  name: my-nginx</span><br><span class="line">  namespace: nginx-example</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br></pre></td></tr></table></figure><p>我们可以全量备份，也可以只备份需要备份的一个namespace，本处只备份一个namespace：nginx-example</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[rsync@velero-plugin]$ kubectl get pods -n nginx-example</span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-deployment-5c689d88bb-f8vsx   1/1     Running   0          6m31s</span><br><span class="line">nginx-deployment-5c689d88bb-rt2zk   1/1     Running   0          6m32s</span><br><span class="line"></span><br><span class="line">[rsync@velero]$ cd velero-v1.4.0-linux-amd64/</span><br><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ ll</span><br><span class="line">total 56472</span><br><span class="line">drwxrwxr-x 4 rsync rsync     4096 Jun  1 15:02 examples</span><br><span class="line">-rw-r--r-- 1 rsync rsync    10255 Dec 10 01:08 LICENSE</span><br><span class="line">-rwxr-xr-x 1 rsync rsync 57810814 May 27 04:33 velero</span><br><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ ./velero backup create nginx-backup --include-namespaces nginx-example --wait</span><br><span class="line">Backup request &quot;nginx-backup&quot; submitted successfully.</span><br><span class="line">Waiting for backup to complete. You may safely press ctrl-c to stop waiting - your backup will continue in the background.</span><br><span class="line">.</span><br><span class="line">Backup completed with status: Completed. You may check for more information using the commands `velero backup describe nginx-backup` and `velero backup logs nginx-backup`.</span><br></pre></td></tr></table></figure><p><img src="https://shenshengkun.github.io/images/velero_1.png" alt=""></p><p>删除ns</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ kubectl delete namespaces nginx-example</span><br><span class="line">namespace &quot;nginx-example&quot; deleted</span><br><span class="line"></span><br><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ kubectl get pods -n nginx-example  </span><br><span class="line">No resources found.</span><br></pre></td></tr></table></figure><p>恢复</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ ./velero restore create --from-backup nginx-backup --wait</span><br><span class="line">Restore request &quot;nginx-backup-20200603180922&quot; submitted successfully.</span><br><span class="line">Waiting for restore to complete. You may safely press ctrl-c to stop waiting - your restore will continue in the background.</span><br><span class="line"></span><br><span class="line">Restore completed with status: Completed. You may check for more information using the commands `velero restore describe nginx-backup-20200603180922` and `velero restore logs nginx-backup-20200603180922`.</span><br><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ kubectl get pods -n nginx-example</span><br><span class="line">NAME                                READY   STATUS              RESTARTS   AGE</span><br><span class="line">nginx-deployment-5c689d88bb-f8vsx   1/1     Running             0          5s</span><br><span class="line">nginx-deployment-5c689d88bb-rt2zk   0/1     ContainerCreating   0          5s</span><br><span class="line"></span><br><span class="line">可以看到已经恢复了</span><br></pre></td></tr></table></figure><p>另外迁移和备份恢复也是一样的，下面看一个特殊的，再部署一个项目，之后恢复会不会删掉新部署的项目。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">新建了一个tomcat容器</span><br><span class="line">[rsync@tomcat-test]$ kubectl get pods -n nginx-example</span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-deployment-5c689d88bb-f8vsx   1/1     Running   0          65m</span><br><span class="line">nginx-deployment-5c689d88bb-rt2zk   1/1     Running   0          65m</span><br><span class="line">tomcat-test-sy-677ff78f6b-rc5vq     1/1     Running   0          7s</span><br></pre></td></tr></table></figure><p> restore 一下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ ./velero  restore create --from-backup nginx-backup        </span><br><span class="line">Restore request &quot;nginx-backup-20200603191726&quot; submitted successfully.</span><br><span class="line">Run `velero restore describe nginx-backup-20200603191726` or `velero restore logs nginx-backup-20200603191726` for more details.</span><br><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ kubectl get pods -n nginx-example  </span><br><span class="line">NAME                                READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-deployment-5c689d88bb-f8vsx   1/1     Running   0          68m</span><br><span class="line">nginx-deployment-5c689d88bb-rt2zk   1/1     Running   0          68m</span><br><span class="line">tomcat-test-sy-677ff78f6b-rc5vq     1/1     Running   0          2m33s</span><br><span class="line"></span><br><span class="line">可以看到没有覆盖</span><br></pre></td></tr></table></figure><p>删除nginx的deployment，在restore</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ kubectl delete deployment nginx-deployment -n nginx-example</span><br><span class="line">deployment.extensions &quot;nginx-deployment&quot; deleted</span><br><span class="line"></span><br><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ kubectl get pods -n nginx-example</span><br><span class="line">NAME                              READY   STATUS    RESTARTS   AGE</span><br><span class="line">tomcat-test-sy-677ff78f6b-rc5vq   1/1     Running   0          4m18s</span><br><span class="line"></span><br><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ ./velero  restore create --from-backup nginx-backup </span><br><span class="line">Restore request &quot;nginx-backup-20200603191949&quot; submitted successfully.</span><br><span class="line">Run `velero restore describe nginx-backup-20200603191949` or `velero restore logs nginx-backup-20200603191949` for more details.</span><br><span class="line"></span><br><span class="line">[rsync@velero-v1.4.0-linux-amd64]$ kubectl get pods -n nginx-example             NAME                                READY   STATUS              RESTARTS   AGE</span><br><span class="line">nginx-deployment-5c689d88bb-f8vsx   1/1     Running             0          2s</span><br><span class="line">nginx-deployment-5c689d88bb-rt2zk   0/1     ContainerCreating   0          2s</span><br><span class="line">tomcat-test-sy-677ff78f6b-rc5vq     1/1     Running             0          4m49s</span><br><span class="line"></span><br><span class="line">可以看到，对我们的tomcat项目是没影响的。</span><br></pre></td></tr></table></figure><p><strong>结论：</strong>velero恢复不是直接覆盖，而是会恢复当前集群中不存在的resource，已有的resource不会回滚到之前的版本，如需要回滚，需在restore之前提前删除现有的resource。 </p><h1 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h1><p>可以设置一个周期性定时备份</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 每日1点进行备份</span><br><span class="line">velero create schedule &lt;SCHEDULE NAME&gt; --schedule=&quot;0 1 * * *&quot;</span><br><span class="line"># 每日1点进行备份，备份保留48小时</span><br><span class="line">velero create schedule &lt;SCHEDULE NAME&gt; --schedule=&quot;0 1 * * *&quot; --ttl 48h</span><br><span class="line"># 每6小时进行一次备份</span><br><span class="line">velero create schedule &lt;SCHEDULE NAME&gt; --schedule=&quot;@every 6h&quot;</span><br><span class="line"># 每日对 web namespace 进行一次备份</span><br><span class="line">velero create schedule &lt;SCHEDULE NAME&gt; --schedule=&quot;@every 24h&quot; --include-namespaces web</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">定时备份的名称为：`&lt;SCHEDULE NAME&gt;-&lt;TIMESTAMP&gt;`，恢复命令为：`velero restore create --from-backup &lt;SCHEDULE NAME&gt;-&lt;TIMESTAMP&gt;`。</span><br></pre></td></tr></table></figure><p>如需备份恢复持久卷，备份如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">velero backup create nginx-backup-volume --snapshot-volumes --include-namespaces nginx-example</span><br></pre></td></tr></table></figure><p>该备份会在集群所在region给云盘创建快照（当前还不支持NAS和OSS存储），快照恢复云盘只能在同region完成。</p><p>恢复命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">velero  restore create --from-backup nginx-backup-volume --restore-volumes</span><br></pre></td></tr></table></figure><p>删除备份</p><ol><li>方法一，通过命令直接删除</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">velero delete backups default-backup</span><br></pre></td></tr></table></figure><ol><li>方法二，设置备份自动过期，在创建备份时，加上TTL参数</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">velero backup create &lt;BACKUP-NAME&gt; --ttl &lt;DURATION&gt;</span><br></pre></td></tr></table></figure><p>还可为资源添加指定标签，添加标签的资源在备份的时候被排除。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 添加标签</span><br><span class="line">kubectl label -n &lt;ITEM_NAMESPACE&gt; &lt;RESOURCE&gt;/&lt;NAME&gt; velero.io/exclude-from-backup=true</span><br><span class="line"># 为 default namespace 添加标签</span><br><span class="line">kubectl label -n default namespace/default velero.io/exclude-from-backup=true</span><br></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://yq.aliyun.com/articles/705007?spm=a2c4e.11163080.searchblog.140.1a8b2ec1TYJPbF" target="_blank" rel="noopener">https://yq.aliyun.com/articles/705007?spm=a2c4e.11163080.searchblog.140.1a8b2ec1TYJPbF</a> </li></ul><p><strong>—本文结束感谢您的阅读。微信扫描二维码，关注我的公众号—</strong> </p><p><img src="https://shenshengkun.github.io/images/sy_weixin.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;一般来说大家都用etcd备份恢复k8s集群，但是有时候我们可能不小心删掉了一个namespace，假设这个ns里面有上百个服务，瞬间没了，怎
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s基于自定义指标实现自动扩容</title>
    <link href="https://shenshengkun.github.io/posts/0o81am3t.html"/>
    <id>https://shenshengkun.github.io/posts/0o81am3t.html</id>
    <published>2020-06-02T23:10:01.000Z</published>
    <updated>2020-06-02T12:20:33.871Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基于自定义指标"><a href="#基于自定义指标" class="headerlink" title="基于自定义指标"></a>基于自定义指标</h1><p>除了基于 CPU 和内存来进行自动扩缩容之外，我们还可以根据自定义的监控指标来进行。这个我们就需要使用 <code>Prometheus Adapter</code>，Prometheus 用于监控应用的负载和集群本身的各种指标，<code>Prometheus Adapter</code> 可以帮我们使用 Prometheus 收集的指标并使用它们来制定扩展策略，这些指标都是通过 APIServer 暴露的，而且 HPA 资源对象也可以很轻易的直接使用。 </p><p><img src="https://shenshengkun.github.io/images/prometheus_adapter1.png" alt=""></p><p>下面来看具体怎么实现的！</p><h1 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用"></a>部署应用</h1><p> 首先，我们部署一个示例应用，在该应用程序上测试 Prometheus 指标自动缩放，资源清单文件如下所示：（podinfo.yaml） </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: podinfo</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: podinfo</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: podinfo</span><br><span class="line">      annotations:</span><br><span class="line">        prometheus.io/scrape: &apos;true&apos;</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: podinfod</span><br><span class="line">        image: stefanprodan/podinfo:0.0.1</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        command:</span><br><span class="line">          - ./podinfo</span><br><span class="line">          - -port=9898</span><br><span class="line">          - -logtostderr=true</span><br><span class="line">          - -v=2</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - name: metadata</span><br><span class="line">            mountPath: /etc/podinfod/metadata</span><br><span class="line">            readOnly: true</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 9898</span><br><span class="line">          protocol: TCP</span><br><span class="line">        readinessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /readyz</span><br><span class="line">            port: 9898</span><br><span class="line">          initialDelaySeconds: 1</span><br><span class="line">          periodSeconds: 2</span><br><span class="line">          failureThreshold: 1</span><br><span class="line">        livenessProbe:</span><br><span class="line">          httpGet:</span><br><span class="line">            path: /healthz</span><br><span class="line">            port: 9898</span><br><span class="line">          initialDelaySeconds: 1</span><br><span class="line">          periodSeconds: 3</span><br><span class="line">          failureThreshold: 2</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            memory: &quot;32Mi&quot;</span><br><span class="line">            cpu: &quot;1m&quot;</span><br><span class="line">          limits:</span><br><span class="line">            memory: &quot;256Mi&quot;</span><br><span class="line">            cpu: &quot;100m&quot;</span><br><span class="line">      volumes:</span><br><span class="line">        - name: metadata</span><br><span class="line">          downwardAPI:</span><br><span class="line">            items:</span><br><span class="line">              - path: &quot;labels&quot;</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.labels</span><br><span class="line">              - path: &quot;annotations&quot;</span><br><span class="line">                fieldRef:</span><br><span class="line">                  fieldPath: metadata.annotations</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: podinfo</span><br><span class="line">  labels:</span><br><span class="line">    app: podinfo</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - port: 9898</span><br><span class="line">      targetPort: 9898</span><br><span class="line">      nodePort: 31198</span><br><span class="line">      protocol: TCP</span><br><span class="line">  selector:</span><br><span class="line">    app: podinfo</span><br></pre></td></tr></table></figure><p>接下来我们将 Prometheus-Adapter 安装到集群中，这里选用helm安装，当然也可以直接yaml文件安装。</p><h1 id="Prometheus-Adapter规则"><a href="#Prometheus-Adapter规则" class="headerlink" title="Prometheus-Adapter规则"></a>Prometheus-Adapter规则</h1><p>Prometheus-Adapter 规则大致 可以分为以下几个部分： </p><ul><li><p><code>seriesQuery</code>：查询 Prometheus 的语句，通过这个查询语句查询到的所有指标都可以用于 HPA</p></li><li><p><code>seriesFilters</code>：查询到的指标可能会存在不需要的，可以通过它过滤掉。</p></li><li><p><code>resources</code>：通过 <code>seriesQuery</code> 查询到的只是指标，如果需要查询某个 Pod 的指标，肯定要将它的名称和所在的命名空间作为指标的标签进行查询，<code>resources</code> 就是将指标的标签和 k8s 的资源类型关联起来，最常用的就是 pod 和 namespace。有两种添加标签的方式，一种是 <code>overrides</code>，另一种是 <code>template</code>。</p><ul><li><code>overrides</code>：它会将指标中的标签和 k8s 资源关联起来。上面示例中就是将指标中的 pod 和 namespace 标签和 k8s 中的 pod 和 namespace 关联起来，因为 pod 和 namespace 都属于核心 api 组，所以不需要指定 api 组。当我们查询某个 pod 的指标时，它会自动将 pod 的名称和名称空间作为标签加入到查询条件中。比如 <code>pod: {group: &quot;apps&quot;, resource: &quot;deployment&quot;}</code> 这么写表示的就是将指标中 podinfo 这个标签和 apps 这个 api 组中的 <code>deployment</code> 资源关联起来；</li><li>template：通过 go 模板的形式。比如<code>template: &quot;kube_&lt;&lt;.Group&gt;&gt;_&lt;&lt;.Resource&gt;&gt;&quot;</code> 这么写表示，假如 <code>&lt;&lt;.Group&gt;&gt;</code> 为 apps，<code>&lt;&lt;.Resource&gt;&gt;</code> 为 deployment，那么它就是将指标中 <code>kube_apps_deployment</code> 标签和 deployment 资源关联起来。</li></ul></li><li><p><code>name</code>：用来给指标重命名的，之所以要给指标重命名是因为有些指标是只增的，比如以 total 结尾的指标。这些指标拿来做 HPA 是没有意义的，我们一般计算它的速率，以速率作为值，那么此时的名称就不能以 total 结尾了，所以要进行重命名。</p><ul><li><code>matches</code>：通过正则表达式来匹配指标名，可以进行分组</li><li><code>as</code>：默认值为 <code>$1</code>，也就是第一个分组。<code>as</code> 为空就是使用默认值的意思。</li></ul></li><li><p><code>metricsQuery</code>：这就是 Prometheus 的查询语句了，前面的 <code>seriesQuery</code> 查询是获得 HPA 指标。当我们要查某个指标的值时就要通过它指定的查询语句进行了。可以看到查询语句使用了速率和分组，这就是解决上面提到的只增指标的问题。</p><ul><li><code>Series</code>：表示指标名称</li><li><code>LabelMatchers</code>：附加的标签，目前只有 <code>pod</code> 和 <code>namespace</code> 两种，因此我们要在之前使用 <code>resources</code> 进行关联</li><li><code>GroupBy</code>：就是 pod 名称，同样需要使用 <code>resources</code> 进行关联。</li></ul><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>我们新建 <code>hpa-prome-adapter-values.yaml</code> 文件覆盖默认的 Values 值 ，安装Prometheus-Adapter，我用的helm2</p><p>文件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">rules:</span><br><span class="line">  default: false</span><br><span class="line">  custom:</span><br><span class="line">  - seriesQuery: &apos;http_requests_total&apos;</span><br><span class="line">    resources:</span><br><span class="line">      overrides:</span><br><span class="line">        kubernetes_namespace:</span><br><span class="line">          resource: namespace</span><br><span class="line">        kubernetes_pod_name:</span><br><span class="line">          resource: pod</span><br><span class="line">    name:</span><br><span class="line">      matches: &quot;^(.*)_total&quot;</span><br><span class="line">      as: &quot;$&#123;1&#125;_per_second&quot;</span><br><span class="line">    metricsQuery: (sum(rate(&lt;&lt;.Series&gt;&gt;&#123;&lt;&lt;.LabelMatchers&gt;&gt;&#125;[1m])) by (&lt;&lt;.GroupBy&gt;&gt;))</span><br><span class="line">prometheus:</span><br><span class="line">  url: http://prometheus-clusterip.monitor.svc.cluster.local</span><br></pre></td></tr></table></figure><p>安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helm repo add apphub https://apphub.aliyuncs.com/</span><br><span class="line">helm install --name prome-adapter --namespace monitor -f hpa-prome-adapter-values.yaml  apphub/prometheus-adapter</span><br></pre></td></tr></table></figure><p> 等一小会儿，安装完成后，可以使用下面的命令来检测是否生效了： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@prometheus]# kubectl get --raw=&quot;/apis/custom.metrics.k8s.io/v1beta1&quot; | jq</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;APIResourceList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;groupVersion&quot;: &quot;custom.metrics.k8s.io/v1beta1&quot;,</span><br><span class="line">  &quot;resources&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;namespaces/http_requests_per_second&quot;,</span><br><span class="line">      &quot;singularName&quot;: &quot;&quot;,</span><br><span class="line">      &quot;namespaced&quot;: false,</span><br><span class="line">      &quot;kind&quot;: &quot;MetricValueList&quot;,</span><br><span class="line">      &quot;verbs&quot;: [</span><br><span class="line">        &quot;get&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;pods/http_requests_per_second&quot;,</span><br><span class="line">      &quot;singularName&quot;: &quot;&quot;,</span><br><span class="line">      &quot;namespaced&quot;: true,</span><br><span class="line">      &quot;kind&quot;: &quot;MetricValueList&quot;,</span><br><span class="line">      &quot;verbs&quot;: [</span><br><span class="line">        &quot;get&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 我们可以看到 http_requests_per_second 指标可用。 现在，让我们检查该指标的当前值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@prometheus]# kubectl get --raw &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second&quot; | jq .                                    </span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;MetricValueList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;custom.metrics.k8s.io/v1beta1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/http_requests_per_second&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;describedObject&quot;: &#123;</span><br><span class="line">        &quot;kind&quot;: &quot;Pod&quot;,</span><br><span class="line">        &quot;namespace&quot;: &quot;default&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;podinfo-5cdc457c8b-99xtw&quot;,</span><br><span class="line">        &quot;apiVersion&quot;: &quot;/v1&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;metricName&quot;: &quot;http_requests_per_second&quot;,</span><br><span class="line">      &quot;timestamp&quot;: &quot;2020-06-02T12:01:01Z&quot;,</span><br><span class="line">      &quot;value&quot;: &quot;888m&quot;,</span><br><span class="line">      &quot;selector&quot;: null</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;describedObject&quot;: &#123;</span><br><span class="line">        &quot;kind&quot;: &quot;Pod&quot;,</span><br><span class="line">        &quot;namespace&quot;: &quot;default&quot;,</span><br><span class="line">        &quot;name&quot;: &quot;podinfo-5cdc457c8b-b7pfz&quot;,</span><br><span class="line">        &quot;apiVersion&quot;: &quot;/v1&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;metricName&quot;: &quot;http_requests_per_second&quot;,</span><br><span class="line">      &quot;timestamp&quot;: &quot;2020-06-02T12:01:01Z&quot;,</span><br><span class="line">      &quot;value&quot;: &quot;888m&quot;,</span><br><span class="line">      &quot;selector&quot;: null</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面部署hpa对象</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: autoscaling/v2beta1</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: podinfo</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: extensions/v1beta1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: podinfo</span><br><span class="line">  minReplicas: 2</span><br><span class="line">  maxReplicas: 5</span><br><span class="line">  metrics:</span><br><span class="line">  - type: Pods</span><br><span class="line">    pods:</span><br><span class="line">      metricName: http_requests_per_second</span><br><span class="line">      targetAverageValue: 3</span><br></pre></td></tr></table></figure><p>部署之后，可见：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@prometheus-adapter]# kubectl get hpa</span><br><span class="line">NAME      REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">podinfo   Deployment/podinfo   911m/10   2         5         2          70s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@prometheus-adapter]# kubectl describe hpa</span><br><span class="line">Name:                                  podinfo</span><br><span class="line">Namespace:                             default</span><br><span class="line">Labels:                                &lt;none&gt;</span><br><span class="line">Annotations:                           kubectl.kubernetes.io/last-applied-configuration:</span><br><span class="line">                                         &#123;&quot;apiVersion&quot;:&quot;autoscaling/v2beta1&quot;,&quot;kind&quot;:&quot;HorizontalPodAutoscaler&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;podinfo&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,...</span><br><span class="line">CreationTimestamp:                     Tue, 02 Jun 2020 17:53:14 +0800</span><br><span class="line">Reference:                             Deployment/podinfo</span><br><span class="line">Metrics:                               ( current / target )</span><br><span class="line">  &quot;http_requests_per_second&quot; on pods:  911m / 10</span><br><span class="line">Min replicas:                          2</span><br><span class="line">Max replicas:                          5</span><br><span class="line">Deployment pods:                       2 current / 2 desired</span><br><span class="line">Conditions:</span><br><span class="line">  Type            Status  Reason               Message</span><br><span class="line">  ----            ------  ------               -------</span><br><span class="line">  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation</span><br><span class="line">  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from pods metric http_requests_per_second</span><br></pre></td></tr></table></figure><p>做一个ab压测：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ab -n 2000 -c 5 http://sy.test.com:31198/</span><br></pre></td></tr></table></figure><p>观察下hpa变化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">  Type    Reason             Age    From                       Message</span><br><span class="line">  ----    ------             ----   ----                       -------</span><br><span class="line">  Normal  SuccessfulRescale  9m29s  horizontal-pod-autoscaler  New size: 3; reason: pods metric http_requests_per_second above target</span><br><span class="line">  Normal  SuccessfulRescale  9m18s  horizontal-pod-autoscaler  New size: 4; reason: pods metric http_requests_per_second above target</span><br><span class="line">  Normal  SuccessfulRescale  3m34s  horizontal-pod-autoscaler  New size: 3; reason: All metrics below target</span><br><span class="line">  Normal  SuccessfulRescale  3m4s   horizontal-pod-autoscaler  New size: 2; reason: All metrics below target</span><br></pre></td></tr></table></figure><p>发现触发扩容动作了，副本到了4，并且压测结束后，过了5分钟左右，又恢复到最小值2个。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://github.com/directxman12/k8s-prometheus-adapter" target="_blank" rel="noopener">https://github.com/directxman12/k8s-prometheus-adapter</a> </li></ul><p><strong>—本文结束感谢您的阅读。微信扫描二维码，关注我的公众号—</strong> </p><p><img src="https://shenshengkun.github.io/images/sy_weixin.jpg" alt=""></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基于自定义指标&quot;&gt;&lt;a href=&quot;#基于自定义指标&quot; class=&quot;headerlink&quot; title=&quot;基于自定义指标&quot;&gt;&lt;/a&gt;基于自定义指标&lt;/h1&gt;&lt;p&gt;除了基于 CPU 和内存来进行自动扩缩容之外，我们还可以根据自定义的监控指标来进行。这个我们就需要使
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>phpipam地址管理</title>
    <link href="https://shenshengkun.github.io/posts/c5d977yb.html"/>
    <id>https://shenshengkun.github.io/posts/c5d977yb.html</id>
    <published>2020-05-31T23:20:44.000Z</published>
    <updated>2020-05-31T13:56:46.199Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>phpipam是一个开源Web IP地址管理应用程序（IPAM）。其目标是提供轻便，现代且有用的IP地址管理。它是基于PHP的应用程序，具有MySQL数据库后端，使用jQuery库，ajax和HTML5 / CSS3功能。 </p><h1 id="安装前环境配置"><a href="#安装前环境配置" class="headerlink" title="安装前环境配置"></a>安装前环境配置</h1><p>配置主机文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/hosts</span><br><span class="line">10.16.16.13 phpipam.sy.local phpipam</span><br></pre></td></tr></table></figure><p>安装依赖模块 Web server、Database 组件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install httpd mariadb-server php php-cli php-gd php-common php-ldap php-pdo php-pear php-snmp php-xml php-mysql php-mbstring git -y</span><br></pre></td></tr></table></figure><p>设置时区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/php.ini</span><br><span class="line">[Date]</span><br><span class="line">; Defines the default timezone used by the date functions</span><br><span class="line">; http://php.net/date.timezone</span><br><span class="line">date.timezone = Asia/Shanghai</span><br></pre></td></tr></table></figure><p>启动 Apache Web Server 并设置开机启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl start httpd</span><br><span class="line">systemctl enable httpd</span><br></pre></td></tr></table></figure><p>启动 MySQL (MariaDB) database server</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl start mariadb</span><br><span class="line">systemctl enable mariadb</span><br></pre></td></tr></table></figure><p>配置 Apache Web Server</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/etc/httpd/conf/httpd.conf</span><br><span class="line">&lt;Directory &quot;/var/www/html&quot;&gt;</span><br><span class="line">AllowOverride all</span><br><span class="line">Order allow,deny</span><br><span class="line">Allow from all</span><br><span class="line">&lt;/Directory&gt;</span><br></pre></td></tr></table></figure><p>初始化数据库设置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">mysql_secure_installation</span><br><span class="line"></span><br><span class="line">Disallow root login remotely? [Y/n] y</span><br><span class="line"> ... Success!</span><br><span class="line"></span><br><span class="line">By default, MariaDB comes with a database named &apos;test&apos; that anyone can</span><br><span class="line">access.  This is also intended only for testing, and should be removed</span><br><span class="line">before moving into a production environment.</span><br><span class="line"></span><br><span class="line">Remove test database and access to it? [Y/n] y</span><br><span class="line"> - Dropping test database...</span><br><span class="line"> ... Success!</span><br><span class="line"> - Removing privileges on test database...</span><br><span class="line"> ... Success!</span><br><span class="line"></span><br><span class="line">Reloading the privilege tables will ensure that all changes made so far</span><br><span class="line">will take effect immediately.</span><br><span class="line"></span><br><span class="line">Reload privilege tables now? [Y/n] y</span><br><span class="line"> ... Success!</span><br><span class="line"></span><br><span class="line">Cleaning up...</span><br><span class="line"></span><br><span class="line">All done!  If you&apos;ve completed all of the above steps, your MariaDB</span><br><span class="line">installation should now be secure.</span><br><span class="line"></span><br><span class="line">Thanks for using MariaDB!</span><br></pre></td></tr></table></figure><h1 id="安装-phpIPAM"><a href="#安装-phpIPAM" class="headerlink" title="安装 phpIPAM"></a>安装 phpIPAM</h1><p>从 git 下载 phpIPAM 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /var/www/html/</span><br><span class="line">git clone https://github.com/phpipam/phpipam.git phpipam</span><br><span class="line">cd phpipam</span><br><span class="line">git checkout 1.4</span><br></pre></td></tr></table></figure><p>让 apache 用户有权限访问 phpipam</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chown apache:apache -R /var/www/html/</span><br><span class="line">cd /var/www/html/</span><br><span class="line">find . -type f -exec chmod 0644 &#123;&#125; \;</span><br><span class="line">find . -type d -exec chmod 0755 &#123;&#125; \;</span><br></pre></td></tr></table></figure><p>然后配置数据库链接要做到这一点，我们首先需要将示例配置文件复制到phpipam使用的config.php</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp /var/www/html/phpipam/config.dist.php /var/www/html/phpipam/config.dist.php.bak</span><br><span class="line">mv /var/www/html/phpipam/config.dist.php /var/www/html/phpipam/config.php</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vim /var/www//html/phpipam/config.php</span><br><span class="line">/**</span><br><span class="line"> * database connection details</span><br><span class="line"> ******************************/</span><br><span class="line">$db[&apos;host&apos;] = &apos;localhost&apos;;</span><br><span class="line">$db[&apos;user&apos;] = &apos;phpipam_user&apos;;</span><br><span class="line">$db[&apos;pass&apos;] = &apos;phpipam_Pass&apos;;</span><br><span class="line">$db[&apos;name&apos;] = &apos;phpipam_db&apos;;</span><br><span class="line">$db[&apos;port&apos;] = 3306;</span><br><span class="line">/**</span><br><span class="line"></span><br><span class="line">if(!defined(&apos;BASE&apos;))</span><br><span class="line">define(&apos;BASE&apos;, &quot;/phpipam/&quot;);</span><br></pre></td></tr></table></figure><p>重启 http 服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart httpd</span><br></pre></td></tr></table></figure><p>访问</p><p><img src="https://shenshengkun.github.io/images/ipam1.jpg" alt=""></p><p>自动安装并设置密码</p><p><img src="https://shenshengkun.github.io/images/ipam2.png" alt=""></p><h1 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h1><p>登录</p><p><img src="https://shenshengkun.github.io/images/ipam3.png" alt=""></p><p>最终界面</p><p><img src="https://shenshengkun.github.io/images/ipam4.png" alt=""></p><p>简单功能说明</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">PHPIPAM Settings：设置PHPIPAM系统本身的一些配置，站点名称、语言、配色方案、一些功能模块的开关。</span><br><span class="line"></span><br><span class="line">Users：设置本地用户，添加、删除，编辑。Groups：设置本地用户组。</span><br><span class="line"></span><br><span class="line">Authentication methods：设置认证方式，支持本地认证、常用的AD、LDAP、Radius，NetIQ和SAML2笔者也没听过。。</span><br><span class="line"></span><br><span class="line">Mail settings：设置SMTP服务器，即当IPAM需要发邮件的时候使用哪个SMTP server。&lt;br/&gt;API：懂编程的同学可以研究一下了。</span><br><span class="line"></span><br><span class="line">Scan agens：使用什么agent来扫描网中的IP。</span><br><span class="line"></span><br><span class="line">Section：即我们可以根据业务或其他属性将IP地址分section来进行管理，例如数据网，语音网，或者数据中心网，办公网等。</span><br><span class="line"></span><br><span class="line">Subnets：已经可以在这里面创建网段了，然后大网段下再划分明细网段。</span><br><span class="line"></span><br><span class="line">下面有一个Device Management，可以做为一个小的资源管理系统来用，即你的设置在哪个楼哪个机房哪个RACK的哪个U上。</span><br><span class="line">最后支持中文，可以调整中文语言</span><br></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://phpipam.net/news/phpipam-installation-on-centos-7/</span><br></pre></td></tr></table></figure><p> <strong>—本文结束感谢您的阅读。微信扫描二维码，关注我的公众号—</strong> </p><p><img src="https://shenshengkun.github.io/images/sy_weixin.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;phpipam是一个开源Web IP地址管理应用程序（IPAM）。其目标是提供轻便，现代且有用的IP地址管理。它是基于PHP的应用程序，具有
      
    
    </summary>
    
      <category term="版本管理工具" scheme="https://shenshengkun.github.io/categories/%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7/"/>
    
    
  </entry>
  
  <entry>
    <title>prometheus监控pod相关指标</title>
    <link href="https://shenshengkun.github.io/posts/33h3bbdo.html"/>
    <id>https://shenshengkun.github.io/posts/33h3bbdo.html</id>
    <published>2020-05-29T02:10:01.000Z</published>
    <updated>2020-05-29T03:17:11.992Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>临近618了，昨天开发同事来找我，问我为啥看grafana监控，我的服务内存随着压测一直在增长，不释放呢。然后给我看了监控的图。</p><p><img src="https://shenshengkun.github.io/images/prometheus-cadvisor1.png" alt=""></p><p>其实是因为监控pod指标的值用了 container_memory_usage_bytes ，是包含cache的，所以感觉是一直不释放，今天就详细说下这些指标的含义。</p><h1 id="容器监控的内存相关指标"><a href="#容器监控的内存相关指标" class="headerlink" title="容器监控的内存相关指标"></a>容器监控的内存相关指标</h1><table><thead><tr><th>名称</th><th>类型</th><th>单位</th><th>说明</th></tr></thead><tbody><tr><td>container_memory_rss</td><td>gauge</td><td>字节数bytes</td><td>RSS内存，即常驻内存集（Resident Set Size），是分配给进程使用实际物理内存，而不是磁盘上缓存的虚拟内存。RSS内存包括所有分配的栈内存和堆内存，以及加载到物理内存中的共享库占用的内存空间，但不包括进入交换分区的内存。</td></tr><tr><td>container_memory_usage_bytes</td><td>gauge</td><td>字节数bytes</td><td>当前使用的内存量，包括所有使用的内存，不管有没有被访问。</td></tr><tr><td>container_memory_max_usage_bytes</td><td>gauge</td><td>字节数bytes</td><td>最大内存使用量的记录。</td></tr><tr><td>container_memory_cache</td><td>gauge</td><td>字节数bytes</td><td>高速缓存（cache）的使用量。cache是位于CPU与主内存间的一种容量较小但速度很高的存储器，是为了提高cpu和内存之间的数据交换速度而设计的。</td></tr><tr><td>container_memory_swap</td><td>gauge</td><td>字节数bytes</td><td>虚拟内存使用量。虚拟内存（swap）指的是用磁盘来模拟内存使用。当物理内存快要使用完或者达到一定比例，就可以把部分不用的内存数据交换到硬盘保存，需要使用时再调入物理内存</td></tr><tr><td>container_memory_working_set_bytes</td><td>gauge</td><td>字节数bytes</td><td>当前内存工作集（working set）使用量。</td></tr><tr><td>container_memory_failcnt</td><td>counter</td><td>次</td><td>申请内存失败次数计数</td></tr><tr><td>container_memory_failures_total</td><td>counter</td><td>次</td><td>累计的内存申请错误次数</td></tr></tbody></table><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">container_memory_max_usage_bytes &gt; container_memory_usage_bytes &gt;= container_memory_working_set_bytes &gt; container_memory_rss</span><br></pre></td></tr></table></figure><p>如果说开发想看自己应用实际内存占用，那就可以用rss，表达式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sum by(container_name) (container_memory_rss&#123;pod_name=&quot;$pod&quot;, container_name=~&quot;$container&quot;, container_name!=&quot;POD&quot;&#125;)</span><br></pre></td></tr></table></figure><p><img src="https://shenshengkun.github.io/images/prometheus-cadvisor2.png" alt=""></p><p>容器如果做了lxcfs，也可以top去看java进程的内存。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;背景介绍&quot;&gt;&lt;a href=&quot;#背景介绍&quot; class=&quot;headerlink&quot; title=&quot;背景介绍&quot;&gt;&lt;/a&gt;背景介绍&lt;/h1&gt;&lt;p&gt;临近618了，昨天开发同事来找我，问我为啥看grafana监控，我的服务内存随着压测一直在增长，不释放呢。然后给我看了监控的
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>kubernetes分析ExitCode</title>
    <link href="https://shenshengkun.github.io/posts/md13ao65.html"/>
    <id>https://shenshengkun.github.io/posts/md13ao65.html</id>
    <published>2020-05-26T23:20:01.000Z</published>
    <updated>2020-05-26T12:17:46.448Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>最近总有开发小伙伴来找我，为什么我的容器总退出呢，在哪能看到原因。故写篇文章整理下docker退出的状态码。</p><h1 id="如何查看退出码"><a href="#如何查看退出码" class="headerlink" title="如何查看退出码"></a>如何查看退出码</h1><p> 查看pod中的容器退出码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod xxx</span><br><span class="line"></span><br><span class="line">Port:           &lt;none&gt;</span><br><span class="line">    Host Port:      &lt;none&gt;</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Tue, 26 May 2020 20:01:04 +0800</span><br><span class="line">    Last State:     Terminated</span><br><span class="line">      Reason:       Error</span><br><span class="line">      Exit Code:    137</span><br><span class="line">      Started:      Tue, 26 May 2020 19:58:40 +0800</span><br><span class="line">      Finished:     Tue, 26 May 2020 20:01:04 +0800</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  2363</span><br></pre></td></tr></table></figure><p>docker查看 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps --filter &quot;status=exited&quot;</span><br><span class="line">$ docker inspect &lt;container-id&gt; --format=&apos;&#123;&#123;.State.ExitCode&#125;&#125;&apos;</span><br></pre></td></tr></table></figure><h1 id="常见退出码"><a href="#常见退出码" class="headerlink" title="常见退出码"></a>常见退出码</h1><h2 id="Exit-Code-0"><a href="#Exit-Code-0" class="headerlink" title="Exit Code 0"></a>Exit Code 0</h2><ul><li>退出代码0表示特定容器没有附加前台进程。</li><li>该退出代码是所有其他后续退出代码的例外。</li><li>这不一定意味着发生了不好的事情。如果开发人员想要在容器完成其工作后自动停止其容器，则使用此退出代码。</li></ul><h2 id="Exit-Code-1"><a href="#Exit-Code-1" class="headerlink" title="Exit Code 1"></a>Exit Code 1</h2><ul><li>程序错误，或者Dockerfile中引用不存在的文件，如 entrypoint中引用了错误的包</li><li>程序错误可以很简单，例如“除以0”，也可以很复杂，比如空引用或者其他程序 crash</li></ul><h2 id="Exit-Code-137"><a href="#Exit-Code-137" class="headerlink" title="Exit Code 137"></a>Exit Code 137</h2><ul><li>此状态码一般是因为 pod 中容器内存达到了它的资源限制(<code>resources.limits</code>)，一般是内存溢出(OOM)，CPU达到限制只需要不分时间片给程序就可以。因为限制资源是通过 linux 的 cgroup 实现的，所以 cgroup 会将此容器强制杀掉，类似于 <code>kill -9</code> </li><li>还可能是宿主机本身资源不够用了(OOM)，内核会选取一些进程杀掉来释放内存 </li><li>不管是 cgroup 限制杀掉进程还是因为节点机器本身资源不够导致进程死掉，都可以从系统日志中找到记录( <em>journalctl -k</em> )</li></ul><h2 id="Exit-Code-139"><a href="#Exit-Code-139" class="headerlink" title="Exit Code 139"></a>Exit Code 139</h2><ul><li>表明容器收到了SIGSEGV信号，无效的内存引用，对应kill -11</li><li>一般是代码有问题，或者 docker 的基础镜像有问题</li></ul><h2 id="Exit-Code-143"><a href="#Exit-Code-143" class="headerlink" title="Exit Code 143"></a>Exit Code 143</h2><ul><li>表明容器收到了SIGTERM信号，终端关闭，对应kill -15</li><li>一般对应docker stop 命令</li><li>有时docker stop也会导致Exit Code 137。发生在与代码无法处理SIGTERM的情况下，docker进程等待十秒钟然后发出SIGKILL强制退出。</li></ul><h2 id="Exit-Code-1-和-255"><a href="#Exit-Code-1-和-255" class="headerlink" title="Exit Code 1 和 255"></a>Exit Code 1 和 255</h2><ul><li>这种可能是一般错误，具体错误原因只能看容器日志，因为很多程序员写异常退出时习惯用 <code>exit(1)</code> 或 <code>exit(-1)</code>，-1 会根据转换规则转成 255</li></ul><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://imroc.io/posts/kubernetes/analysis-exitcode/" target="_blank" rel="noopener">https://imroc.io/posts/kubernetes/analysis-exitcode/</a> </li><li><a href="http://www.xuyasong.com/?p=1802" target="_blank" rel="noopener">http://www.xuyasong.com/?p=1802</a> </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h1&gt;&lt;p&gt;最近总有开发小伙伴来找我，为什么我的容器总退出呢，在哪能看到原因。故写篇文章整理下docker退出的状态码。&lt;/p&gt;
&lt;h1 id=&quot;如何查
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>grafana修改主题</title>
    <link href="https://shenshengkun.github.io/posts/dmm9nv31.html"/>
    <id>https://shenshengkun.github.io/posts/dmm9nv31.html</id>
    <published>2020-05-25T09:50:01.000Z</published>
    <updated>2020-05-25T12:27:03.299Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Grafana介绍"><a href="#Grafana介绍" class="headerlink" title="Grafana介绍"></a>Grafana介绍</h1><p>Grafana是一个跨平台的开源的度量分析和可视化工具，可以通过将采集的数据查询然后可视化的展示，并及时通知。它主要有以下六大特点：</p><p>1、展示方式：快速灵活的客户端图表，面板插件有许多不同方式的可视化指标和日志，官方库中具有丰富的仪表盘插件，比如热图、折线图、图表等多种展示方式；（主题只有默认的黑与白）</p><p>2、数据源：Graphite，InfluxDB，OpenTSDB，Prometheus，Elasticsearch，CloudWatch和KairosDB等；</p><p>3、通知提醒：以可视方式定义最重要指标的警报规则，Grafana将不断计算并发送通知，在数据达到阈值时通过Slack、PagerDuty等获得通知；</p><p>4、混合展示：在同一图表中混合使用不同的数据源，可以基于每个查询指定数据源，甚至自定义数据源；</p><p>5、注释：使用来自不同数据源的丰富事件注释图表，将鼠标悬停在事件上会显示完整的事件元数据和标记；</p><p>6、过滤器：Ad-hoc过滤器允许动态创建新的键/值过滤器，这些过滤器会自动应用于使用该数据源的所有查询。</p><h1 id="修改主题方式"><a href="#修改主题方式" class="headerlink" title="修改主题方式"></a>修改主题方式</h1><p>注意：我的grafana是k8s安装的，所以需要持久化数据。并且找到的主题修改仅支持grafana6.x.x以上版本。</p><p>安装插件：（ <a href="https://grafana.com/grafana/plugins/yesoreyeram-boomtheme-panel" target="_blank" rel="noopener">https://grafana.com/grafana/plugins/yesoreyeram-boomtheme-panel</a> ）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grafana-cli plugins install yesoreyeram-boomtheme-panel</span><br><span class="line">安装完重启下pod就可以。</span><br></pre></td></tr></table></figure><p>增加panel修改主题</p><p><img src="https://shenshengkun.github.io/images/grafana-theme1.png" alt=""></p><p>修改主题</p><p><img src="https://shenshengkun.github.io/images/grafana-theme2.png" alt=""></p><p>里面的css样式可以从这里寻找（ <a href="https://github.com/gilbN/theme.park/wiki/Themes#grafana-themes" target="_blank" rel="noopener">https://github.com/gilbN/theme.park/wiki/Themes#grafana-themes</a> ）</p><h1 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h1><p><img src="https://shenshengkun.github.io/images/grafana-theme3.png" alt=""></p><p><img src="https://shenshengkun.github.io/images/grafana-theme4.png" alt=""></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/gilbN/theme.park/wiki/Themes#grafana-themes</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Grafana介绍&quot;&gt;&lt;a href=&quot;#Grafana介绍&quot; class=&quot;headerlink&quot; title=&quot;Grafana介绍&quot;&gt;&lt;/a&gt;Grafana介绍&lt;/h1&gt;&lt;p&gt;Grafana是一个跨平台的开源的度量分析和可视化工具，可以通过将采集的数据查询然后
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>kubeadm多master集群升级k8s版本</title>
    <link href="https://shenshengkun.github.io/posts/dw65vbx5.html"/>
    <id>https://shenshengkun.github.io/posts/dw65vbx5.html</id>
    <published>2020-05-22T06:00:05.000Z</published>
    <updated>2020-05-22T06:54:44.538Z</updated>
    
    <content type="html"><![CDATA[<h1 id="版本说明"><a href="#版本说明" class="headerlink" title="版本说明"></a>版本说明</h1><p>本次升级版本为从1.15.3升级至1.16.3。另外更高的k8s版本，要注意内核要为4.4以上，尤其是1.18版本。</p><h1 id="升级"><a href="#升级" class="headerlink" title="升级"></a>升级</h1><h2 id="master节点升级"><a href="#master节点升级" class="headerlink" title="master节点升级"></a>master节点升级</h2><p>查看当前集群组件列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master01 ~]# kubectl get nodes</span><br><span class="line">NAME              STATUS   ROLES    AGE    VERSION</span><br><span class="line">master01.sy.com   Ready    master   3d9h   v1.15.3</span><br><span class="line">master02.sy.com   Ready    master   3d9h   v1.15.3</span><br><span class="line">master03.sy.com   Ready    master   3d9h   v1.15.3</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master01 ~]# kubeadm config images list</span><br><span class="line">I0521 20:18:24.336912   23537 version.go:248] remote version is much newer: v1.18.3; falling back to: stable-1.15</span><br><span class="line">W0521 20:18:34.337440   23537 version.go:98] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.15.txt&quot;: Get https://storage.googleapis.com/kubernetes-release/release/stable-1.15.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">W0521 20:18:34.337495   23537 version.go:99] falling back to the local client version: v1.15.3</span><br><span class="line">k8s.gcr.io/kube-apiserver:v1.15.3</span><br><span class="line">k8s.gcr.io/kube-controller-manager:v1.15.3</span><br><span class="line">k8s.gcr.io/kube-scheduler:v1.15.3</span><br><span class="line">k8s.gcr.io/kube-proxy:v1.15.3</span><br><span class="line">k8s.gcr.io/pause:3.1</span><br><span class="line">k8s.gcr.io/etcd:3.3.10</span><br><span class="line">k8s.gcr.io/coredns:1.3.1</span><br><span class="line">[root@master01 ~]#</span><br></pre></td></tr></table></figure><p>升级 Kubeadm 工具版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@master01 ~]# yum update -y kubeadm-1.16.3-0</span><br></pre></td></tr></table></figure><p>查看待升级的 kubernetes 组件镜像列表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@master01 ~]# kubeadm config images list</span><br><span class="line">W0521 20:33:34.091388   32299 version.go:101] could not fetch a Kubernetes version from the internet: unable to get URL &quot;https://dl.k8s.io/release/stable-1.txt&quot;: Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">W0521 20:33:34.091559   32299 version.go:102] falling back to the local client version: v1.16.3</span><br><span class="line">k8s.gcr.io/kube-apiserver:v1.16.3</span><br><span class="line">k8s.gcr.io/kube-controller-manager:v1.16.3</span><br><span class="line">k8s.gcr.io/kube-scheduler:v1.16.3</span><br><span class="line">k8s.gcr.io/kube-proxy:v1.16.3</span><br><span class="line">k8s.gcr.io/pause:3.1</span><br><span class="line">k8s.gcr.io/etcd:3.3.15-0</span><br><span class="line">k8s.gcr.io/coredns:1.6.2</span><br></pre></td></tr></table></figure><p>创建镜像脚本，并打tag</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@master01 ~]# cat pull-image.sh </span><br><span class="line">## 设置镜像仓库地址</span><br><span class="line">MY_REGISTRY=registry.aliyuncs.com/google_containers</span><br><span class="line"></span><br><span class="line">## 拉取镜像</span><br><span class="line">docker pull $&#123;MY_REGISTRY&#125;/kube-apiserver:v1.16.3</span><br><span class="line">docker pull $&#123;MY_REGISTRY&#125;/kube-controller-manager:v1.16.3</span><br><span class="line">docker pull $&#123;MY_REGISTRY&#125;/kube-scheduler:v1.16.3</span><br><span class="line">docker pull $&#123;MY_REGISTRY&#125;/kube-proxy:v1.16.3</span><br><span class="line">docker pull $&#123;MY_REGISTRY&#125;/etcd:3.3.15-0</span><br><span class="line">docker pull $&#123;MY_REGISTRY&#125;/pause:3.1</span><br><span class="line">docker pull $&#123;MY_REGISTRY&#125;/coredns:1.6.2</span><br><span class="line">## 设置标签</span><br><span class="line">docker tag $&#123;MY_REGISTRY&#125;/kube-apiserver:v1.16.3          k8s.gcr.io/kube-apiserver:v1.16.3</span><br><span class="line">docker tag $&#123;MY_REGISTRY&#125;/kube-scheduler:v1.16.3          k8s.gcr.io/kube-scheduler:v1.16.3</span><br><span class="line">docker tag $&#123;MY_REGISTRY&#125;/kube-controller-manager:v1.16.3 k8s.gcr.io/kube-controller-manager:v1.16.3</span><br><span class="line">docker tag $&#123;MY_REGISTRY&#125;/kube-proxy:v1.16.3              k8s.gcr.io/kube-proxy:v1.16.3</span><br><span class="line">docker tag $&#123;MY_REGISTRY&#125;/etcd:3.3.15-0                    k8s.gcr.io/etcd:3.3.15-0</span><br><span class="line">docker tag $&#123;MY_REGISTRY&#125;/pause:3.1                       k8s.gcr.io/pause:3.1</span><br><span class="line">docker tag $&#123;MY_REGISTRY&#125;/coredns:1.6.2                   k8s.gcr.io/coredns:1.6.2</span><br></pre></td></tr></table></figure><p>升级kubeadm</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@master01 ~]# kubectl drain master01.sy.com --ignore-daemonsets</span><br><span class="line">[root@master01 ~]# kubeadm upgrade apply v1.16.3</span><br><span class="line">[upgrade/config] Making sure the configuration is correct:</span><br><span class="line">[upgrade/config] Reading configuration from the cluster...</span><br><span class="line">[upgrade/config] FYI: You can look at this config file with &apos;kubectl -n kube-system get cm kubeadm-config -oyaml&apos;</span><br><span class="line">[preflight] Running pre-flight checks.</span><br><span class="line">[upgrade] Making sure the cluster is healthy:</span><br><span class="line">[upgrade/version] You have chosen to change the cluster version to &quot;v1.16.3&quot;</span><br><span class="line">[upgrade/versions] Cluster version: v1.15.3</span><br><span class="line">[upgrade/versions] kubeadm version: v1.16.3</span><br><span class="line">[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y</span><br><span class="line">.........</span><br><span class="line">[upgrade/successful] SUCCESS! Your cluster was upgraded to &quot;v1.16.3&quot;. Enjoy!</span><br><span class="line">[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven&apos;t already done so.</span><br></pre></td></tr></table></figure><p>升级 Kubelet</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master01 ~]# yum update -y kubelet-1.16.3-0</span><br><span class="line">[root@master01 ~]# systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure><p>看日志发现kubelet有报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cni.go:237] Unable to update cni config: no valid networks found in /etc/cni/net.d</span><br></pre></td></tr></table></figure><p>解决</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">在v1.16中，kubelet将验证cni配置文件</span><br><span class="line">在cbr0 这一行上面新增一行:</span><br><span class="line">cni-conf.json: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;cniVersion&quot;:&quot;0.2.0&quot;,</span><br><span class="line">      &quot;name&quot;: &quot;cbr0&quot;,</span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">flannal镜像改成quay.io/coreos/flannel:v0.11.0-amd64</span><br><span class="line"></span><br><span class="line">在看node状态</span><br><span class="line">[root@master01 ~]# kubectl uncordon master01.sy.com</span><br><span class="line">node/master01.sy.com uncordoned</span><br><span class="line">[root@master01 ~]# kubectl get nodes</span><br><span class="line">NAME              STATUS   ROLES    AGE    VERSION</span><br><span class="line">master01.sy.com   Ready    master   4d3h   v1.16.3</span><br><span class="line">master02.sy.com   Ready    master   4d3h   v1.15.3</span><br><span class="line">master03.sy.com   Ready    master   4d2h   v1.15.3</span><br></pre></td></tr></table></figure><p>其他master节点升级也一样操作，不过命令替换一下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@master02 ~]# kubeadm upgrade node experimental-control-plane</span><br><span class="line"></span><br><span class="line">[root@master01 ~]# kubectl get nodes</span><br><span class="line">NAME              STATUS   ROLES    AGE    VERSION</span><br><span class="line">master01.sy.com   Ready    master   4d3h   v1.16.3</span><br><span class="line">master02.sy.com   Ready    master   4d3h   v1.16.3</span><br><span class="line">master03.sy.com   Ready    master   4d3h   v1.16.3</span><br></pre></td></tr></table></figure><h2 id="node节点升级"><a href="#node节点升级" class="headerlink" title="node节点升级"></a>node节点升级</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl drain [节点名称] --ignore-daemonsets</span><br><span class="line">yum update -y kubeadm-1.16.3-0</span><br><span class="line">yum update -y kubelet-1.16.3-0</span><br><span class="line">systemctl daemon-reload &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://www.cnblogs.com/winstom/p/11836844.html" target="_blank" rel="noopener">https://www.cnblogs.com/winstom/p/11836844.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;版本说明&quot;&gt;&lt;a href=&quot;#版本说明&quot; class=&quot;headerlink&quot; title=&quot;版本说明&quot;&gt;&lt;/a&gt;版本说明&lt;/h1&gt;&lt;p&gt;本次升级版本为从1.15.3升级至1.16.3。另外更高的k8s版本，要注意内核要为4.4以上，尤其是1.18版本。&lt;/p&gt;
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>nginx根据ip进行灰度发布</title>
    <link href="https://shenshengkun.github.io/posts/8pmz45bs.html"/>
    <id>https://shenshengkun.github.io/posts/8pmz45bs.html</id>
    <published>2020-05-18T23:30:10.000Z</published>
    <updated>2020-05-18T12:43:02.049Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>之前公司做的都是蓝绿发布，但是有的开发想着又能实现根据来源ip做灰度，又能实现蓝绿发布，在这里我写了下简单的实现方式。</p><h1 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h1><p>nginx 负载均衡器判断客户端IP地址，如果是办公室IP，则反向代理到灰度环境；如果不是，则反向代理到生产环境。</p><p>下面是一个域名多项目的事例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  sy.test.com;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    add_header X-Cache $upstream_cache_status;</span><br><span class="line">    charset utf-8;</span><br><span class="line">    access_log /data/nginxlog/sy.test.com.log;</span><br><span class="line">    set $web_backend tomcat-test-scm-v1;</span><br><span class="line">    if ($remote_addr ~ &quot;x.x.x.x&quot;) &#123;</span><br><span class="line">        set $web_backend tomcat-test-scm-v2;</span><br><span class="line">    &#125;</span><br><span class="line">    if ($remote_addr ~ &quot;x.x.x.x&quot;) &#123;</span><br><span class="line">        set $web_backend tomcat-test-scm-v2;</span><br><span class="line">    &#125;</span><br><span class="line">    set $hd_backend live-v1;</span><br><span class="line">    if ($remote_addr ~ &quot;x.x.x.x&quot;) &#123;</span><br><span class="line">        set $hd_backend live-v2;</span><br><span class="line">    &#125;</span><br><span class="line">    if ($remote_addr ~ &quot;x.x.x.x&quot;) &#123;</span><br><span class="line">        set $hd_backend live-v2;</span><br><span class="line">    &#125;</span><br><span class="line">    location ~ ^/live &#123;</span><br><span class="line">        proxy_redirect  off;</span><br><span class="line">        proxy_set_header Host  $host;</span><br><span class="line">        proxy_pass      http://$hd_backend;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    location ~ / &#123;</span><br><span class="line">        proxy_redirect  off;</span><br><span class="line">        proxy_pass      http://$web_backend;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现效果，办公网访问的是灰度的v2版本，当测试通过，只需将v1版本替换成v2。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://www.cnblogs.com/zhangyin6985/p/6064350.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhangyin6985/p/6064350.html</a> </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;之前公司做的都是蓝绿发布，但是有的开发想着又能实现根据来源ip做灰度，又能实现蓝绿发布，在这里我写了下简单的实现方式。&lt;/p&gt;
&lt;h1 id
      
    
    </summary>
    
      <category term="中间件" scheme="https://shenshengkun.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>kubeadm高可用集群安装</title>
    <link href="https://shenshengkun.github.io/posts/omn700fj.html"/>
    <id>https://shenshengkun.github.io/posts/omn700fj.html</id>
    <published>2020-05-18T02:10:01.000Z</published>
    <updated>2020-05-18T03:09:54.427Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h1><p>最近总有小伙伴说kubeadm的高可用集群怎么安装，故写了这篇文章。</p><p>创建高可用首先先有一个 Master 节点，然后再让其他服务器加入组成三个 Master 节点高可用，然后再将工作节点 Node 加入。</p><h2 id="Kuberadm-功能"><a href="#Kuberadm-功能" class="headerlink" title="Kuberadm 功能"></a>Kuberadm 功能</h2><ul><li><strong>kubeadm init：</strong> 启动一个 Kubernetes 主节点</li><li><strong>kubeadm join：</strong> 启动一个 Kubernetes 工作节点并且将其加入到集群</li><li><strong>kubeadm upgrade：</strong> 更新一个 Kubernetes 集群到新版本</li><li><strong>kubeadm config：</strong> 如果使用 v1.7.x 或者更低版本的 kubeadm 初始化集群，您需要对集群做一些配置以便使用 kubeadm upgrade 命令</li><li><strong>kubeadm token：</strong> 管理 kubeadm join 使用的令牌</li><li><strong>kubeadm reset：</strong> 还原 kubeadm init 或者 kubeadm join 对主机所做的任何更改</li><li><strong>kubeadm version：</strong> 打印 kubeadm 版本</li><li><strong>kubeadm alpha：</strong> 预览一组可用的新功能以便从社区搜集反馈</li></ul><h1 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h1><h2 id="主机名称解析"><a href="#主机名称解析" class="headerlink" title="主机名称解析"></a>主机名称解析</h2><p>分别进入不同服务器，进入 /etc/hosts 进行编辑</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">10.16.16.100    master.sy.com      k8s-vip</span><br><span class="line">10.16.16.19    master01.sy.com    sy1</span><br><span class="line">10.16.16.20    master02.sy.com    sy2</span><br><span class="line">10.16.16.28    master03.sy.com    sy3</span><br></pre></td></tr></table></figure><p>我用的openstack虚拟机，vip需要提前开通，具体开通vip地址可以看我另一篇文章</p><p><a href="https://shenshengkun.github.io/posts/dk456akf.html">https://shenshengkun.github.io/posts/dk456akf.html</a></p><h2 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h2><p>以下操作均在所有机器操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y epel-release conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget unzip net-tools</span><br></pre></td></tr></table></figure><h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">iptables -F &amp;&amp; iptables -X &amp;&amp; iptables -F -t nat &amp;&amp; iptables -X -t nat</span><br><span class="line">iptables -P FORWARD ACCEPT</span><br></pre></td></tr></table></figure><h2 id="关闭-swap-分区"><a href="#关闭-swap-分区" class="headerlink" title="关闭 swap 分区"></a>关闭 swap 分区</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a</span><br><span class="line">sed -i &apos;/ swap / s/^\(.*\)$/#\1/g&apos; /etc/fstab</span><br></pre></td></tr></table></figure><h2 id="关闭-SELinux"><a href="#关闭-SELinux" class="headerlink" title="关闭 SELinux"></a>关闭 SELinux</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">setenforce 0</span><br><span class="line">sed -i &apos;s/^SELINUX=.*/SELINUX=disabled/&apos; /etc/selinux/config</span><br></pre></td></tr></table></figure><h2 id="设置系统参数"><a href="#设置系统参数" class="headerlink" title="设置系统参数"></a>设置系统参数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">modprobe ip_vs_rr</span><br><span class="line">modprobe br_netfilter</span><br><span class="line">cat &gt; kubernetes.conf &lt;&lt;EOF</span><br><span class="line">net.bridge.bridge-nf-call-iptables=1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables=1</span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">net.ipv4.tcp_tw_recycle=0</span><br><span class="line">vm.swappiness=0 <span class="comment"># 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它</span></span><br><span class="line">vm.overcommit_memory=1 <span class="comment"># 不检查物理内存是否够用</span></span><br><span class="line">vm.panic_on_oom=0 <span class="comment"># 开启 OOM</span></span><br><span class="line">fs.inotify.max_user_instances=8192</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">net.ipv6.conf.all.disable_ipv6=1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">EOF</span><br><span class="line">cp kubernetes.conf  /etc/sysctl.d/kubernetes.conf</span><br><span class="line">sysctl -p /etc/sysctl.d/kubernetes.conf</span><br></pre></td></tr></table></figure><h1 id="Keepalived安装"><a href="#Keepalived安装" class="headerlink" title="Keepalived安装"></a>Keepalived安装</h1><p>安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y keepalived</span><br></pre></td></tr></table></figure><p>配置Keepalived</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.conf</span><br><span class="line">! Configuration File <span class="keyword">for</span> keepalived</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主要是配置故障发生时的通知对象以及机器标识。</span></span><br><span class="line">global_defs &#123;</span><br><span class="line">   <span class="comment"># 标识本节点的字条串，通常为 hostname，但不一定非得是 hostname。故障发生时，邮件通知会用到。</span></span><br><span class="line">   router_id LVS_k8s</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用来做健康检查的，当时检查失败时会将 vrrp_instance 的 priority 减少相应的值。</span></span><br><span class="line">vrrp_script check_haproxy &#123;</span><br><span class="line">    script <span class="string">"killall -0 haproxy"</span>   <span class="comment">#根据进程名称检测进程是否存活</span></span><br><span class="line">    interval 3</span><br><span class="line">    weight -2</span><br><span class="line">    fall 10</span><br><span class="line">    rise 2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># rp_instance用来定义对外提供服务的 VIP 区域及其相关属性。</span></span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state MASTER   <span class="comment">#当前节点为MASTER，其他两个节点设置为BACKUP</span></span><br><span class="line">    interface eth0 <span class="comment">#改为自己的网卡</span></span><br><span class="line">    virtual_router_id 51</span><br><span class="line">    priority 250</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 3sqP05dQgMSlzrxHj</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        10.16.16.100   <span class="comment">#虚拟ip，即VIP</span></span><br><span class="line">    &#125;</span><br><span class="line">    track_script &#123;</span><br><span class="line">        check_haproxy</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> keepalived &amp;&amp; systemctl start keepalived</span><br></pre></td></tr></table></figure><h1 id="安装haproxy"><a href="#安装haproxy" class="headerlink" title="安装haproxy"></a>安装haproxy</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y haproxy</span><br></pre></td></tr></table></figure><p>此处的haproxy为apiserver提供反向代理，haproxy将所有请求轮询转发到每个master节点上。相对于仅仅使用keepalived主备模式仅单个master节点承载流量，这种方式更加合理、健壮。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/haproxy/haproxy.cfg &lt;&lt; EOF</span><br><span class="line"><span class="comment">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># Global settings</span></span><br><span class="line"><span class="comment">#---------------------------------------------------------------------</span></span><br><span class="line">global</span><br><span class="line">    <span class="comment"># to have these messages end up in /var/log/haproxy.log you will</span></span><br><span class="line">    <span class="comment"># need to:</span></span><br><span class="line">    <span class="comment"># 1) configure syslog to accept network log events.  This is done</span></span><br><span class="line">    <span class="comment">#    by adding the '-r' option to the SYSLOGD_OPTIONS in</span></span><br><span class="line">    <span class="comment">#    /etc/sysconfig/syslog</span></span><br><span class="line">    <span class="comment"># 2) configure local2 events to go to the /var/log/haproxy.log</span></span><br><span class="line">    <span class="comment">#   file. A line like the following can be added to</span></span><br><span class="line">    <span class="comment">#   /etc/sysconfig/syslog</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#    local2.*                       /var/log/haproxy.log</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="built_in">log</span>         127.0.0.1 local2</span><br><span class="line">    </span><br><span class="line">    chroot      /var/lib/haproxy</span><br><span class="line">    pidfile     /var/run/haproxy.pid</span><br><span class="line">    maxconn     4000</span><br><span class="line">    user        haproxy</span><br><span class="line">    group       haproxy</span><br><span class="line">    daemon </span><br><span class="line">       </span><br><span class="line">    <span class="comment"># turn on stats unix socket</span></span><br><span class="line">    stats socket /var/lib/haproxy/stats</span><br><span class="line"><span class="comment">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># common defaults that all the 'listen' and 'backend' sections will</span></span><br><span class="line"><span class="comment"># use if not designated in their block</span></span><br><span class="line"><span class="comment">#---------------------------------------------------------------------  </span></span><br><span class="line">defaults</span><br><span class="line">    mode                    http</span><br><span class="line">    <span class="built_in">log</span>                     global</span><br><span class="line">    option                  httplog</span><br><span class="line">    option                  dontlognull</span><br><span class="line">    option http-server-close</span><br><span class="line">    option forwardfor       except 127.0.0.0/8</span><br><span class="line">    option                  redispatch</span><br><span class="line">    retries                 3</span><br><span class="line">    timeout http-request    10s</span><br><span class="line">    timeout queue           1m</span><br><span class="line">    timeout connect         10s</span><br><span class="line">    timeout client          1m</span><br><span class="line">    timeout server          1m</span><br><span class="line">    timeout http-keep-alive 10s</span><br><span class="line">    timeout check           10s</span><br><span class="line">    maxconn                 3000</span><br><span class="line"><span class="comment">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># kubernetes apiserver frontend which proxys to the backends</span></span><br><span class="line"><span class="comment">#--------------------------------------------------------------------- </span></span><br><span class="line">frontend kubernetes-apiserver</span><br><span class="line">    mode                 tcp</span><br><span class="line">    <span class="built_in">bind</span>                 *:8443</span><br><span class="line">    option               tcplog</span><br><span class="line">    default_backend      kubernetes-apiserver    </span><br><span class="line"><span class="comment">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># round robin balancing between the various backends</span></span><br><span class="line"><span class="comment">#---------------------------------------------------------------------</span></span><br><span class="line">backend kubernetes-apiserver</span><br><span class="line">    mode        tcp</span><br><span class="line">    balance     roundrobin</span><br><span class="line">    server      master01.sy.com   10.16.16.19:6443 check</span><br><span class="line">    server      master02.sy.com   10.16.16.20:6443 check</span><br><span class="line">    server      master03.sy.com   10.16.16.28:6443 check</span><br><span class="line"><span class="comment">#---------------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># collection haproxy statistics message</span></span><br><span class="line"><span class="comment">#---------------------------------------------------------------------</span></span><br><span class="line">listen stats</span><br><span class="line">    <span class="built_in">bind</span>                 *:1080</span><br><span class="line">    stats auth           admin:admin</span><br><span class="line">    stats refresh        5s</span><br><span class="line">    stats realm          HAProxy\ Statistics</span><br><span class="line">    stats uri            /admin?stats</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>启动haproxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> haproxy &amp;&amp; systemctl start haproxy</span><br></pre></td></tr></table></figure><h1 id="安装Docker-所有节点"><a href="#安装Docker-所有节点" class="headerlink" title="安装Docker (所有节点)"></a>安装Docker (所有节点)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils \</span><br><span class="line">  device-mapper-persistent-data \</span><br><span class="line">  lvm2</span><br><span class="line">  </span><br><span class="line">yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">    https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line">    </span><br><span class="line">yum install docker-ce-19.03.1-3.el7 -y</span><br></pre></td></tr></table></figure><h2 id="安加速，并且修改cgroup-driver"><a href="#安加速，并且修改cgroup-driver" class="headerlink" title="安加速，并且修改cgroup driver"></a>安加速，并且修改cgroup driver</h2><p>根据文档CRI installation中的内容，对于使用systemd作为init system的Linux的发行版，使用systemd作为docker的cgroup driver可以确保服务器节点在资源紧张的情况更加稳定，因此这里修改各个节点上docker的cgroup driver为systemd。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@sy1 ~]# systemctl start docker</span><br><span class="line">[root@sy1 ~]# cat /etc/docker/daemon.json </span><br><span class="line">&#123;&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https://dockerhub.mirrors.nwafu.edu.cn/&quot;],</span><br><span class="line">    &quot;bip&quot;: &quot;192.17.10.1/24&quot;</span><br><span class="line">&#125;</span><br><span class="line">[root@sy1 ~]# systemctl restart docker</span><br><span class="line">[root@sy1 ~]# systemctl enable docker </span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span><br></pre></td></tr></table></figure><h1 id="安装kubeadm、kubelet"><a href="#安装kubeadm、kubelet" class="headerlink" title="安装kubeadm、kubelet"></a>安装kubeadm、kubelet</h1><p>配置yum源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">repo_gpgcheck=0</span><br><span class="line">gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg</span><br><span class="line">        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>查看kubelet版本列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum list kubelet --showduplicates | sort -r</span><br></pre></td></tr></table></figure><p>安装 kubeadm、kubelet</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y kubelet-1.15.3-0 kubeadm-1.15.3-0</span><br><span class="line">systemctl enable kubelet.service &amp;&amp; systemctl start kubelet</span><br></pre></td></tr></table></figure><h1 id="初始化第一个kubernetes-master节点"><a href="#初始化第一个kubernetes-master节点" class="headerlink" title="初始化第一个kubernetes master节点"></a>初始化第一个kubernetes master节点</h1><p>创建kubeadm配置的yaml文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; kubeadm-config.yaml &lt;&lt; EOF</span><br><span class="line">apiServer:</span><br><span class="line">  certSANs:</span><br><span class="line">    - master01.sy.com</span><br><span class="line">    - master02.sy.com</span><br><span class="line">    - master03.sy.com</span><br><span class="line">    - master.sy.com</span><br><span class="line">    - 10.16.16.100</span><br><span class="line">    - 10.16.16.19</span><br><span class="line">    - 10.16.16.20</span><br><span class="line">    - 10.16.16.28</span><br><span class="line">    - 127.0.0.1</span><br><span class="line">  extraArgs:</span><br><span class="line">    authorization-mode: Node,RBAC</span><br><span class="line">  timeoutForControlPlane: 4m0s</span><br><span class="line">apiVersion: kubeadm.k8s.io/v1beta2</span><br><span class="line">certificatesDir: /etc/kubernetes/pki</span><br><span class="line">clusterName: kubernetes</span><br><span class="line">controlPlaneEndpoint: <span class="string">"master.sy.com:8443"</span></span><br><span class="line">controllerManager: &#123;&#125;</span><br><span class="line">dns: </span><br><span class="line">  <span class="built_in">type</span>: CoreDNS</span><br><span class="line">etcd:</span><br><span class="line">  <span class="built_in">local</span>:    </span><br><span class="line">    dataDir: /var/lib/etcd</span><br><span class="line">imageRepository: registry.aliyuncs.com/google_containers</span><br><span class="line">kind: ClusterConfiguration</span><br><span class="line">kubernetesVersion: v1.15.3</span><br><span class="line">networking: </span><br><span class="line">  dnsDomain: cluster.local  </span><br><span class="line">  podSubnet: 192.160.0.0/16</span><br><span class="line">  serviceSubnet: 192.168.0.0/17</span><br><span class="line">scheduler: &#123;&#125;</span><br><span class="line">---</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">mode: ipvs</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>初始化第一个master节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --config kubeadm-config.yaml</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">"kubectl apply -f [podnetwork].yaml"</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">You can now join any number of control-plane nodes by copying certificate authorities </span><br><span class="line">and service account keys on each node and <span class="keyword">then</span> running the following as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join master.sy.com:8443 --token ltjuyu.2otdxrrsy4ku6lri \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:f6562358b50a516e7814136c73b57827626a75ae82389fbe0c70b6328700ee4a \</span><br><span class="line">    --control-plane       </span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join master.sy.com:8443 --token ltjuyu.2otdxrrsy4ku6lri \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:f6562358b50a516e7814136c73b57827626a75ae82389fbe0c70b6328700ee4a</span><br></pre></td></tr></table></figure><h3 id="kubeconfig"><a href="#kubeconfig" class="headerlink" title="kubeconfig"></a>kubeconfig</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@sy1 ~]# mkdir -p $HOME/.kube</span><br><span class="line">[root@sy1 ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">[root@sy1 ~]# chown $(id -u):$(id -g) $HOME/.kube/config</span><br></pre></td></tr></table></figure><h1 id="安装网络插件"><a href="#安装网络插件" class="headerlink" title="安装网络插件"></a>安装网络插件</h1><p>配置flannel插件的yaml文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; kube-flannel.yaml &lt;&lt; EOF</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: flannel</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - pods</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - nodes</span><br><span class="line">    verbs:</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line">  - apiGroups:</span><br><span class="line">      - &quot;&quot;</span><br><span class="line">    resources:</span><br><span class="line">      - nodes/status</span><br><span class="line">    verbs:</span><br><span class="line">      - patch</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: flannel</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: flannel</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: flannel</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: flannel</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: kube-flannel-cfg</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    tier: node</span><br><span class="line">    app: flannel</span><br><span class="line">data:</span><br><span class="line">  cni-conf.json: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;: &quot;cbr0&quot;,</span><br><span class="line">      &quot;plugins&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;type&quot;: &quot;flannel&quot;,</span><br><span class="line">          &quot;delegate&quot;: &#123;</span><br><span class="line">            &quot;hairpinMode&quot;: true,</span><br><span class="line">            &quot;isDefaultGateway&quot;: true</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;type&quot;: &quot;portmap&quot;,</span><br><span class="line">          &quot;capabilities&quot;: &#123;</span><br><span class="line">            &quot;portMappings&quot;: true</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  net-conf.json: |</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;Network&quot;: &quot;192.160.0.0/16&quot;,</span><br><span class="line">      &quot;Backend&quot;: &#123;</span><br><span class="line">        &quot;Type&quot;: &quot;vxlan&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: kube-flannel-ds-amd64</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    tier: node</span><br><span class="line">    app: flannel</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        tier: node</span><br><span class="line">        app: flannel</span><br><span class="line">    spec:</span><br><span class="line">      hostNetwork: true</span><br><span class="line">      nodeSelector:</span><br><span class="line">        beta.kubernetes.io/arch: amd64</span><br><span class="line">      tolerations:</span><br><span class="line">      - operator: Exists</span><br><span class="line">        effect: NoSchedule</span><br><span class="line">      serviceAccountName: flannel</span><br><span class="line">      initContainers:</span><br><span class="line">      - name: install-cni</span><br><span class="line">        image: registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64</span><br><span class="line">        command:</span><br><span class="line">        - cp</span><br><span class="line">        args:</span><br><span class="line">        - -f</span><br><span class="line">        - /etc/kube-flannel/cni-conf.json</span><br><span class="line">        - /etc/cni/net.d/10-flannel.conflist</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: cni</span><br><span class="line">          mountPath: /etc/cni/net.d</span><br><span class="line">        - name: flannel-cfg</span><br><span class="line">          mountPath: /etc/kube-flannel/</span><br><span class="line">      containers:</span><br><span class="line">      - name: kube-flannel</span><br><span class="line">        image: registry.cn-shenzhen.aliyuncs.com/cp_m/flannel:v0.10.0-amd64</span><br><span class="line">        command:</span><br><span class="line">        - /opt/bin/flanneld</span><br><span class="line">        args:</span><br><span class="line">        - --ip-masq</span><br><span class="line">        - --kube-subnet-mgr</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: &quot;100m&quot;</span><br><span class="line">            memory: &quot;50Mi&quot;</span><br><span class="line">          limits:</span><br><span class="line">            cpu: &quot;100m&quot;</span><br><span class="line">            memory: &quot;50Mi&quot;</span><br><span class="line">        securityContext:</span><br><span class="line">          privileged: true</span><br><span class="line">        env:</span><br><span class="line">        - name: POD_NAME</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.name</span><br><span class="line">        - name: POD_NAMESPACE</span><br><span class="line">          valueFrom:</span><br><span class="line">            fieldRef:</span><br><span class="line">              fieldPath: metadata.namespace</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: run</span><br><span class="line">          mountPath: /run</span><br><span class="line">        - name: flannel-cfg</span><br><span class="line">          mountPath: /etc/kube-flannel/</span><br><span class="line">      volumes:</span><br><span class="line">        - name: run</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /run</span><br><span class="line">        - name: cni</span><br><span class="line">          hostPath:</span><br><span class="line">            path: /etc/cni/net.d</span><br><span class="line">        - name: flannel-cfg</span><br><span class="line">          configMap:</span><br><span class="line">            name: kube-flannel-cfg</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>部署</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f kube-flannel.yaml</span><br></pre></td></tr></table></figure><h1 id="加入集群"><a href="#加入集群" class="headerlink" title="加入集群"></a>加入集群</h1><h2 id="Master加入集群构成高可用"><a href="#Master加入集群构成高可用" class="headerlink" title="Master加入集群构成高可用"></a>Master加入集群构成高可用</h2><p>复制文件，从master1上复制到2和3</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ssh root@master02.sy.com mkdir -p /etc/kubernetes/pki/etcd</span><br><span class="line">scp /etc/kubernetes/admin.conf root@master02.sy.com:/etc/kubernetes</span><br><span class="line">scp /etc/kubernetes/pki/&#123;ca.*,sa.*,front-proxy-ca.*&#125; root@master02.sy.com:/etc/kubernetes/pki</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.* root@master02.sy.com:/etc/kubernetes/pki/etcd</span><br><span class="line"></span><br><span class="line">ssh root@master03.sy.com mkdir -p /etc/kubernetes/pki/etcd</span><br><span class="line">scp /etc/kubernetes/admin.conf root@master03.sy.com:/etc/kubernetes</span><br><span class="line">scp /etc/kubernetes/pki/&#123;ca.*,sa.*,front-proxy-ca.*&#125; root@master03.sy.com:/etc/kubernetes/pki</span><br><span class="line">scp /etc/kubernetes/pki/etcd/ca.* root@master03.sy.com:/etc/kubernetes/pki/etcd</span><br></pre></td></tr></table></figure><p>master节点加入集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join master.sy.com:8443 --token ltjuyu.2otdxrrsy4ku6lri --discovery-token-ca-cert-hash sha256:f6562358b50a516e7814136c73b57827626a75ae82389fbe0c70b6328700ee4a --experimental-control-plane</span><br></pre></td></tr></table></figure><p>结果如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@sy1 ~]# kubectl get nodes</span><br><span class="line">NAME              STATUS   ROLES    AGE     VERSION</span><br><span class="line">master01.sy.com   Ready    master   28m     v1.15.3</span><br><span class="line">master02.sy.com   Ready    master   3m12s   v1.15.3</span><br><span class="line">master03.sy.com   Ready    master   24s     v1.15.3</span><br></pre></td></tr></table></figure><h2 id="node加入集群"><a href="#node加入集群" class="headerlink" title="node加入集群"></a>node加入集群</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join master.sy.com:8443 --token ltjuyu.2otdxrrsy4ku6lri \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:f6562358b50a516e7814136c73b57827626a75ae82389fbe0c70b6328700ee4a</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前提&quot;&gt;&lt;a href=&quot;#前提&quot; class=&quot;headerlink&quot; title=&quot;前提&quot;&gt;&lt;/a&gt;前提&lt;/h1&gt;&lt;p&gt;最近总有小伙伴说kubeadm的高可用集群怎么安装，故写了这篇文章。&lt;/p&gt;
&lt;p&gt;创建高可用首先先有一个 Master 节点，然后再让其他
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>openstack虚拟机状态修改两种方法</title>
    <link href="https://shenshengkun.github.io/posts/c2okabkf.html"/>
    <id>https://shenshengkun.github.io/posts/c2okabkf.html</id>
    <published>2020-05-13T06:10:01.000Z</published>
    <updated>2020-05-13T06:47:39.062Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>上午线上openstack集群，发现热迁移不了，后经查证是由于新加入的集群操作系统版本太高，导致热迁移少了模块，引发的问题是，热迁移的机器状态一直是迁移中，故做了一些处理。这里列出一些常用的操作命令。</p><h1 id="命令修改"><a href="#命令修改" class="headerlink" title="命令修改"></a>命令修改</h1><p>列出所有租户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nova list --all-tenants</span><br></pre></td></tr></table></figure><p>找到对应虚拟机和其ID，用ID把它设置为active状态:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nova reset-state --active 虚拟机ID</span><br></pre></td></tr></table></figure><p>停止</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nova stop 虚拟机ID</span><br></pre></td></tr></table></figure><p>启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nova start 虚拟机ID</span><br></pre></td></tr></table></figure><p>重启</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nova reboot 虚拟机ID</span><br></pre></td></tr></table></figure><h1 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h1><p>进入数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Use nova； 进入nova数据库</span><br><span class="line"></span><br><span class="line">select * from instances where uuid=’实例的ID &apos;；</span><br></pre></td></tr></table></figure><p>将实例状态更改为active、实例任务状态重置 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE instances SET vm_state = &apos;active&apos; and task_state =NULL where uuid = &apos;实例ID&apos;;</span><br></pre></td></tr></table></figure><p>修改电源状态是running，power_state的状态是从0-4 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE instances SET power_state=&apos;1&apos; where uuid=&apos;实例ID&apos;;</span><br></pre></td></tr></table></figure><p>修改状态是active</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE instances SET vm_state = &apos;active&apos; where uuid=&apos;实例ID&apos;;</span><br></pre></td></tr></table></figure><p>修改任务是none </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UPDATE instances SET task_state =&apos;none&apos; where uuid=&apos;实例ID&apos;;</span><br></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://www.cnblogs.com/wangjq19920210/p/11165138.html" target="_blank" rel="noopener">https://www.cnblogs.com/wangjq19920210/p/11165138.html</a> </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;上午线上openstack集群，发现热迁移不了，后经查证是由于新加入的集群操作系统版本太高，导致热迁移少了模块，引发的问题是，热迁移的机器状
      
    
    </summary>
    
      <category term="openstack" scheme="https://shenshengkun.github.io/categories/openstack/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s二进制1.14版本添加node节点</title>
    <link href="https://shenshengkun.github.io/posts/vkk87cvc.html"/>
    <id>https://shenshengkun.github.io/posts/vkk87cvc.html</id>
    <published>2020-05-11T08:10:01.000Z</published>
    <updated>2020-05-12T02:51:25.669Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>新增俩台node节点加进k8s集群。</p><h1 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h1><h2 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h2><p>以下操作均在所有机器操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y epel-release conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget unzip net-tools fuse-devel fuse fuse-libs</span><br></pre></td></tr></table></figure><h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">iptables -F &amp;&amp; iptables -X &amp;&amp; iptables -F -t nat &amp;&amp; iptables -X -t nat</span><br><span class="line">iptables -P FORWARD ACCEPT</span><br></pre></td></tr></table></figure><h2 id="关闭-swap-分区"><a href="#关闭-swap-分区" class="headerlink" title="关闭 swap 分区"></a>关闭 swap 分区</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a</span><br><span class="line">sed -i &apos;/ swap / s/^\(.*\)$/#\1/g&apos; /etc/fstab</span><br></pre></td></tr></table></figure><h2 id="关闭-SELinux"><a href="#关闭-SELinux" class="headerlink" title="关闭 SELinux"></a>关闭 SELinux</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">setenforce 0</span><br><span class="line">sed -i &apos;s/^SELINUX=.*/SELINUX=disabled/&apos; /etc/selinux/config</span><br></pre></td></tr></table></figure><h2 id="加载内核并优化"><a href="#加载内核并优化" class="headerlink" title="加载内核并优化"></a>加载内核并优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">modprobe ip_vs_rr</span><br><span class="line">modprobe br_netfilter</span><br><span class="line">cat &gt; kubernetes.conf &lt;&lt;EOF</span><br><span class="line">net.bridge.bridge-nf-call-iptables=1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables=1</span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">net.ipv4.tcp_tw_recycle=0</span><br><span class="line">vm.swappiness=0 # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它</span><br><span class="line">vm.overcommit_memory=1 # 不检查物理内存是否够用</span><br><span class="line">vm.panic_on_oom=0 # 开启 OOM</span><br><span class="line">fs.inotify.max_user_instances=8192</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">net.ipv6.conf.all.disable_ipv6=1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">EOF</span><br><span class="line">cp kubernetes.conf  /etc/sysctl.d/kubernetes.conf</span><br><span class="line">sysctl -p /etc/sysctl.d/kubernetes.conf</span><br></pre></td></tr></table></figure><h2 id="ntp"><a href="#ntp" class="headerlink" title="ntp"></a>ntp</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate ntp1.aliyun.com</span><br></pre></td></tr></table></figure><h2 id="创建相关目录"><a href="#创建相关目录" class="headerlink" title="创建相关目录"></a>创建相关目录</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p  /opt/k8s/&#123;bin,work&#125; /etc/&#123;kubernetes,etcd&#125;/cert</span><br></pre></td></tr></table></figure><h1 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h1><h2 id="添加hosts"><a href="#添加hosts" class="headerlink" title="添加hosts"></a>添加hosts</h2><p>#在之前的master节点添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@test-master-1 ~]# echo &quot;10.16.8.86  test-node-16&quot; &gt;&gt;/etc/hosts                  </span><br><span class="line">[root@test-master-1 ~]# echo &quot;10.16.8.80  test-node-17&quot; &gt;&gt;/etc/hosts</span><br></pre></td></tr></table></figure><p>新加的俩台节点加hosts</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cat &gt;&gt;/etc/hosts&lt;&lt;EOF</span><br><span class="line">10.16.8.10  test-master-1</span><br><span class="line">10.16.8.11  test-master-2</span><br><span class="line">10.16.8.12  test-master-3</span><br><span class="line">10.16.8.13  test-node-15</span><br><span class="line">10.16.8.86  test-node-16</span><br><span class="line">10.16.8.80  test-node-17</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h2 id="分发秘钥等信息"><a href="#分发秘钥等信息" class="headerlink" title="分发秘钥等信息"></a>分发秘钥等信息</h2><p>master节点操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub test-node-16</span><br><span class="line">ssh-copy-id -i ~/.ssh/id_rsa.pub test-node-17</span><br></pre></td></tr></table></figure><p>推送CA证书</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/kubernetes/cert</span><br><span class="line">scp ca.pem ca-config.json test-node-16:/etc/kubernetes/cert/</span><br><span class="line">scp ca.pem ca-config.json test-node-17:/etc/kubernetes/cert/</span><br></pre></td></tr></table></figure><h2 id="flannal部署"><a href="#flannal部署" class="headerlink" title="flannal部署"></a>flannal部署</h2><p>拷贝二进制文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/flannel</span><br><span class="line">scp flanneld mk-docker-opts.sh test-node-16:/opt/k8s/bin/</span><br><span class="line">scp flanneld mk-docker-opts.sh test-node-17:/opt/k8s/bin/</span><br></pre></td></tr></table></figure><p>拷贝flanneld密钥 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh test-node-16 &quot;mkdir -p /etc/flanneld/cert&quot;</span><br><span class="line">ssh test-node-17 &quot;mkdir -p /etc/flanneld/cert&quot;</span><br><span class="line">scp /etc/flanneld/cert/flanneld*.pem test-node-16:/etc/flanneld/cert</span><br><span class="line">scp /etc/flanneld/cert/flanneld*.pem test-node-17:/etc/flanneld/cert</span><br></pre></td></tr></table></figure><p>拷贝flannel启动文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/systemd/system/flanneld.service test-node-16:/etc/systemd/system/</span><br><span class="line">scp /etc/systemd/system/flanneld.service test-node-17:/etc/systemd/system/</span><br><span class="line">#启动flannel</span><br><span class="line">ssh test-node-16 &quot;systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld&quot;</span><br><span class="line">ssh test-node-17 &quot;systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld&quot;</span><br></pre></td></tr></table></figure><h2 id="Kubernetes-Node-节点安装Docker"><a href="#Kubernetes-Node-节点安装Docker" class="headerlink" title="Kubernetes Node 节点安装Docker"></a>Kubernetes Node 节点安装Docker</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils \</span><br><span class="line">  device-mapper-persistent-data \</span><br><span class="line">  lvm2</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">    https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line">    </span><br><span class="line">yum install docker-ce-19.03.1-3.el7 -y</span><br></pre></td></tr></table></figure><p>创建配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/docker/</span><br><span class="line">cat &gt; /etc/docker/daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span><br><span class="line">    &quot;registry-mirrors&quot;: [&quot;https://dockerhub.mirrors.nwafu.edu.cn/&quot;,&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;],</span><br><span class="line">    &quot;max-concurrent-downloads&quot;: 20,</span><br><span class="line">    &quot;live-restore&quot;: true,</span><br><span class="line">    &quot;max-concurrent-uploads&quot;: 10,</span><br><span class="line">    &quot;debug&quot;: true,</span><br><span class="line">    &quot;log-opts&quot;: &#123;</span><br><span class="line">      &quot;max-size&quot;: &quot;100m&quot;,</span><br><span class="line">      &quot;max-file&quot;: &quot;5&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>修改docker启动配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/lib/systemd/system/docker.service</span><br><span class="line"></span><br><span class="line">EnvironmentFile=-/run/flannel/docker</span><br><span class="line">ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS -H fd:// --containerd=/run/containerd/containerd.sock</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker</span><br><span class="line"></span><br><span class="line">systemctl status docker|grep Active</span><br></pre></td></tr></table></figure><h2 id="安装kubelet"><a href="#安装kubelet" class="headerlink" title="安装kubelet"></a>安装kubelet</h2><p>创建kubelet bootstrap kubeconfig文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#master节点操作</span><br><span class="line">另一台创建文件等和这个同理，这里只列举一个。</span><br><span class="line"></span><br><span class="line">cd /opt/k8s/work</span><br><span class="line">export BOOTSTRAP_TOKEN=$(kubeadm token create \</span><br><span class="line">  --description kubelet-bootstrap-token \</span><br><span class="line">  --groups system:bootstrappers:test-node-16 \</span><br><span class="line">  --kubeconfig ~/.kube/config)</span><br><span class="line"># 设置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=https://10.16.8.14:6443 \</span><br><span class="line">  --kubeconfig=kubelet-bootstrap-test-node-16.kubeconfig</span><br><span class="line"># 设置客户端认证参数</span><br><span class="line">kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">  --token=$&#123;BOOTSTRAP_TOKEN&#125; \</span><br><span class="line">  --kubeconfig=kubelet-bootstrap-test-node-16.kubeconfig</span><br><span class="line"># 设置上下文参数</span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kubelet-bootstrap \</span><br><span class="line">  --kubeconfig=kubelet-bootstrap-test-node-16.kubeconfig</span><br><span class="line"># 设置默认上下文</span><br><span class="line">kubectl config use-context default --kubeconfig=kubelet-bootstrap-test-node-16.kubeconfig</span><br></pre></td></tr></table></figure><p>分发kubeconfig</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">scp kubelet-bootstrap-test-node-16.kubeconfig test-node-16:/etc/kubernetes/kubelet-bootstrap.kubeconfig</span><br></pre></td></tr></table></figure><p> 查看kubeadm为各个节点创建的token </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm token list --kubeconfig ~/.kube/config</span><br></pre></td></tr></table></figure><p>创建和分发kubelet参数配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">sed -e &quot;s/##NODE_IP##/10.16.8.86/&quot; kubelet-config.yaml.template &gt; kubelet-config-10.16.8.86.yaml.template</span><br><span class="line">sed -e &quot;s/##NODE_IP##/10.16.8.80/&quot; kubelet-config.yaml.template &gt; kubelet-config-10.16.8.80.yaml.template</span><br><span class="line">scp kubelet-config-10.16.8.86.yaml.template root@test-node-16:/etc/kubernetes/kubelet-config.yaml</span><br><span class="line">scp kubelet-config-10.16.8.80.yaml.template root@test-node-17:/etc/kubernetes/kubelet-config.yaml</span><br></pre></td></tr></table></figure><p>拷贝kubelet启动文件和命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">sed -e &quot;s/##NODE_NAME##/test-node-16/&quot; kubelet.service.template &gt; kubelet-test-node-16.service</span><br><span class="line">sed -e &quot;s/##NODE_NAME##/test-node-17/&quot; kubelet.service.template &gt; kubelet-test-node-17.service</span><br><span class="line">scp kubelet-test-node-16.service root@test-node-16:/etc/systemd/system/kubelet.service</span><br><span class="line">scp kubelet-test-node-17.service root@test-node-17:/etc/systemd/system/kubelet.service</span><br><span class="line">scp /opt/k8s/bin/kubelet test-node-16:/opt/k8s/bin/</span><br><span class="line">scp /opt/k8s/bin/kubelet test-node-17:/opt/k8s/bin/</span><br></pre></td></tr></table></figure><p> 启动kubelet </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">ssh root@test-node-16 &quot;mkdir -p $&#123;K8S_DIR&#125;/kubelet/kubelet-plugins/volume/exec/&quot;</span><br><span class="line">ssh root@test-node-16 &quot;systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet&quot;</span><br><span class="line">ssh root@test-node-17 &quot;mkdir -p $&#123;K8S_DIR&#125;/kubelet/kubelet-plugins/volume/exec/&quot;</span><br><span class="line">ssh root@test-node-17 &quot;systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet&quot;</span><br></pre></td></tr></table></figure><p>签注证书</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get csr | grep Pending | awk &apos;&#123;print $1&#125;&apos; | xargs kubectl certificate approve</span><br></pre></td></tr></table></figure><h2 id="安装kube-proxy"><a href="#安装kube-proxy" class="headerlink" title="安装kube-proxy"></a>安装kube-proxy</h2><p> 推送kube-proxy二进制启动文件和kubeconfig文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/</span><br><span class="line">scp kubernetes/server/bin/kube-proxy test-node-16:/opt/k8s/bin/</span><br><span class="line">scp kubernetes/server/bin/kube-proxy test-node-17:/opt/k8s/bin/</span><br><span class="line">scp kube-proxy.kubeconfig root@test-node-16:/etc/kubernetes/</span><br><span class="line">scp kube-proxy.kubeconfig root@test-node-17:/etc/kubernetes/</span><br></pre></td></tr></table></figure><p> 创建和分发kube-proxy配置文件 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/</span><br><span class="line">sed -e &quot;s/##NODE_NAME##/test-node-16/&quot; -e &quot;s/##NODE_IP##/10.16.8.86/&quot; kube-proxy-config.yaml.template &gt; kube-proxy-config-test-node-16.yaml.template</span><br><span class="line">sed -e &quot;s/##NODE_NAME##/test-node-17/&quot; -e &quot;s/##NODE_IP##/10.16.8.80/&quot; kube-proxy-config.yaml.template &gt; kube-proxy-config-test-node-17.yaml.template</span><br><span class="line">scp kube-proxy-config-test-node-16.yaml.template root@test-node-16:/etc/kubernetes/kube-proxy-config.yaml</span><br><span class="line">scp kube-proxy-config-test-node-17.yaml.template root@test-node-17:/etc/kubernetes/kube-proxy-config.yaml</span><br><span class="line">scp kube-proxy.service root@test-node-16:/etc/systemd/system/</span><br><span class="line">scp kube-proxy.service root@test-node-17:/etc/systemd/system/</span><br></pre></td></tr></table></figure><p> 启动kube-proxy服务 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">ssh root@test-node-16 &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-proxy&quot;</span><br><span class="line">ssh root@test-node-16 &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy&quot;</span><br><span class="line">ssh root@test-node-17 &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-proxy&quot;</span><br><span class="line">ssh root@test-node-17 &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;新增俩台node节点加进k8s集群。&lt;/p&gt;
&lt;h1 id=&quot;初始化&quot;&gt;&lt;a href=&quot;#初始化&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>envoy介绍</title>
    <link href="https://shenshengkun.github.io/posts/d3oolab1.html"/>
    <id>https://shenshengkun.github.io/posts/d3oolab1.html</id>
    <published>2020-05-09T01:30:01.000Z</published>
    <updated>2020-05-09T01:56:53.422Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Envoy-介绍"><a href="#Envoy-介绍" class="headerlink" title="Envoy 介绍"></a>Envoy 介绍</h1><p><code>Envoy</code> 是专为大型现代 SOA（面向服务架构）架构设计的 L7 代理和通信总线，体积小，性能高。它的诞生源于以下理念：</p><blockquote><p>对应用程序而言，网络应该是透明的。当网络和应用程序出现故障时，应该能够很容易确定问题的根源。</p></blockquote><h1 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h1><p><strong>独立于进程的架构</strong>：Envoy是一个独立的进程，旨在与每个应用程序服务器一起运行。所有Envoy组成了一个透明的通信网格，其中每个应用程序发送和接收来自本地主机的消息，并且不用知道网络拓扑。与传统的服务通信服务的库方法相比，进程外架构有两个实质性好处：</p><ul><li>Envoy支持任何编程语言写的服务。只用部署一个Envoy就可以在Java、C++、Go、PHP、Python等服务间形成网格。面向服务的体系结构使用多个应用程序框架和语言的情况越来越普遍。Envoy以透明的方式弥合了这些差距。</li><li>任何使用过大型面向服务的体系结构的人都知道，部署库升级可能会非常痛苦。Envoy可以在整个基础设施中迅速部署和升级。</li></ul><p><strong>基于最新的C++11开发</strong>：Envoy是基于C++11编写的。选择本机代码是因为我们认为像Envoy这样的体系结构组件应该尽可能给应用程序让路。现代应用程序开发人员习惯于在共享云环境中的部署，以及使用非常高效但性能不是特别好的语言 (如 PHP、Python、Ruby、Scala 等)， 在这种环境下，找到尾延迟的原因变得非常的困难。本机代码通常提供出色的延迟属性，不会给已经令人困惑的情况增加额外的混乱。与用 C 编写的其他本机代码代理解决方案不同，C++11 提供了出色的开发人员工作效率和性能。</p><p><strong>基于 L3/L4 网络Filter的架构</strong>：Envoy的核心使用的是基于 L3/L4 的网络代理。可插拔的Filter链机制允许编写Filter以执行不同的 tcp 代理任务并插入主服务器。当然Envoy也提供现成的Filter以支持各种任务，如原始 TCP代理、HTTP 代理、TLS客户端证书身份验证等。</p><p><strong>基于 L7 网络的HTTP Filter架构</strong>：HTTP是现代应用程序体系结构的重要组成部分，Envoy支持额外的 HTTP L7 Filter层。HTTP Filter可以插入到 HTTP 连接管理子系统中，该子系统支持执行不同的任务，如缓冲、速率限制、路由、嗅探亚马逊的 Dynamodb 等。</p><p><strong>对HTTP/2 的极佳支持</strong>：在 HTTP 模式下运行时，Envoy同时支持 HTTP/1.1 和 HTTP/2。Envoy可以做到让 HTTP/1.1 和 HTTP/2 之间的通讯保持透明。这意味着对于任意的 HTTP/1.1 和 HTTP/2 的客户端和目标服务器的组合，Envoy都可以将他们桥接起来。当然建议在配置Envoy服务时使用 HTTP/2 在所有组件之间创建一个长链的网格，这样请求和响应可以多路复用。Envoy 不支持 SPDY，因为这个协议正在逐渐被淘汰。</p><blockquote><p>SPDY（读作“SPeeDY”）是 Google 开发的基于 TCP 的应用层协议，用以最小化网络延迟，提升网络速度，优化用户的网络使用体验。互联网工程任务组（IETF）对谷歌提出的 SPDY 协议进行了标准化，于2015年5推出了类似于 SPDY 协议的 HTTP 2.0 协议标准（简称HTTP/2）。谷歌因此宣布放弃对SPDY协议的支持，转而支持HTTP/2。</p></blockquote><p><strong>HTTP L7 路由</strong>：在 HTTP 模式下运行时，Envoy 的路由子系统能够根据路径、权限、内容类型、运行时值等来路由和重定向请求。在使用 Envoy 作为前端代理时，此功能非常有用。同时在构建服务网格时也会利用此功能。</p><p><strong>对 gRPC 的支持</strong>：gRPC 是一个来自谷歌的 RPC框架, 使用 HTTP/2 作为底层的多路复用传输。Envoy支持所有需要用作 gPRC请求和响应的路由和负载均衡基础的 HTTP/2 功能。这两个系统是非常互补的。</p><p><strong>对MongoDB的 L7 网络协议的支持</strong>：对于当今的 Web 应用，MongoDB数据库非常流行。因为Envoy支持基于 L7 的网络协议，所以Envoy 支持 MongoDB 连接的嗅探、数据统计和日志记录。</p><p><strong>对DynamoDB的 L7 网络协议的支持</strong>：DynamoDB是由Amazion提供的基于键值对的NoSQL数据库。因为Envoy支持基于 L7 的网络协议，所以Envoy 支持 DynamoDB 连接的嗅探和数据统计。</p><p><strong>服务的动态注册和发现</strong>：Envoy 可以选择使用一组分层的动态配置 API 来进行集中管理。这些层为Envoy提供了以下方面的动态更新: 后端群集的主机、后端群集本身、HTTP 路由、侦听套接字和通信加密。为了实现更简单的部署, 后端主机发现可以通过 DNS 解析 (甚至完全跳过) 完成, 层也可以替换为静态配置文件。</p><p><strong>健康检查</strong>：构建 Envoy 网格的建议方法是将服务发现视为最终一致的过程。 Envoy 包括一个运行状况检查子系统，该子系统可以选择对上游服务集群执行主动运行状况检查。然后，Envoy 使用服务发现和运行状况检查信息的联合来确定健康的负载均衡服务器。Envoy 还支持通过异常检测子系统进行被动运行状况检查。</p><p><strong>高级负载均衡</strong>：分布式系统中不同组件之间的负载平衡是一个复杂的问题。由于 Envoy 是一个独立的代理而不是库，因此它能够在一个位置实现高级负载平衡技术，并使任何应用程序都可以访问。目前Envoy 包括支持自动重试、断路、通过外部速率限制服务限制全局速率、请求隐藏和异常值检测。未来计划为Request Racing提供支持。</p><p><strong>前端/边缘系统代理支持</strong>： 虽然 Envoy 主要是为服务通信系统而设计的，但对前端/边缘系统也是很有用的（可观测性、管理、相同的服务发现和负载平衡算法等）。Envoy包含足够的功能，使其可用作大多数 Web 应用服务用例的边缘代理。这包括作为 TLS 的终点、HTTP/1.1 和 HTTP/2 支持, 以及 HTTP L7 路由。</p><p><strong>最好的观察统计能力</strong>：如上所述，Envoy 的首要目标是使网络透明。但是在网络级别和应用程序级都无法避免的容易出现问题。Envoy 包含了对所有子系统的强有力的统计支持。 <a href="https://github.com/etsy/statsd" target="_blank" rel="noopener">statsd</a>(和其他兼容的数据提供程序) 是当前支持的统计接收器，插入不同的统计接收器也并不困难。Envoy 可以通过管理端口查看统计信息，还支持通过第三方供应商进行分布式追踪。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><h2 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h2><p>自己的镜像，加了一些安装工具</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull shenshengkun/envoy:v1.14.1</span><br></pre></td></tr></table></figure><h2 id="centos"><a href="#centos" class="headerlink" title="centos"></a>centos</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 安装 yum-config-manager </span><br><span class="line">yum install -y yum-utils</span><br><span class="line"># 添加 Envoy 仓库</span><br><span class="line">yum-config-manager --add-repo https://getenvoy.io/linux/centos/tetrate-getenvoy.repo</span><br><span class="line"># 安装 Envoy</span><br><span class="line">yum install -y getenvoy-envoy</span><br></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://www.envoyproxy.io/docs/envoy/latest/" target="_blank" rel="noopener">https://www.envoyproxy.io/docs/envoy/latest/</a></li><li><a href="https://www.jianshu.com/p/a6f7f46683e1" target="_blank" rel="noopener">https://www.jianshu.com/p/a6f7f46683e1</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Envoy-介绍&quot;&gt;&lt;a href=&quot;#Envoy-介绍&quot; class=&quot;headerlink&quot; title=&quot;Envoy 介绍&quot;&gt;&lt;/a&gt;Envoy 介绍&lt;/h1&gt;&lt;p&gt;&lt;code&gt;Envoy&lt;/code&gt; 是专为大型现代 SOA（面向服务架构）架构设计的 L7
      
    
    </summary>
    
      <category term="envoy" scheme="https://shenshengkun.github.io/categories/envoy/"/>
    
    
  </entry>
  
  <entry>
    <title>nginx合并前端资源nginx-http-concat模块</title>
    <link href="https://shenshengkun.github.io/posts/8plm645b.html"/>
    <id>https://shenshengkun.github.io/posts/8plm645b.html</id>
    <published>2020-05-08T01:40:10.000Z</published>
    <updated>2020-05-08T01:51:02.826Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p> nginx-http-concat,可以减少http请求,从而减轻了服务器的请求压力,更快的响应。 每个http请求的应答都是非常昂贵的, 并且我们知道浏览器本身也有并发请求限制,当一个网站并发请求非常多时,也会非常影响性能的。</p><p>通常来说合并css、js也是为了将很多小的css文件全部合并成一个http返回,也是非常重要的优化手段,对于前端工程实现来说,也比较清晰.</p><p>像淘宝的链接很多都是taobao.com/a.js,b.js,c.js </p><p><img src="https://shenshengkun.github.io/images/nginx-contat.png" alt=""></p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/alibaba/nginx-http-concat.git</span><br><span class="line">./configure --add-module=./nginx-http-concat/</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure><h1 id="模块详解"><a href="#模块详解" class="headerlink" title="模块详解"></a>模块详解</h1><p> 在配置的地方使模块有效（失效） </p><blockquote><p> <strong>concat</strong> <code>on</code> | <code>off</code><br><strong>默认:</strong> <code>concat off</code><br><strong>上下文:</strong> <code>http, server, location</code> </p></blockquote><p> 定义哪些mime types是可以被接受 </p><blockquote><p> <strong>concat_types</strong> <code>MIME types</code><br><strong>默认:</strong> <code>concat_types: text/css application/x-javascript</code><br><strong>上下文:</strong> <code>http, server, location</code> </p></blockquote><p> 定义是否只接受在[MIME types]中的相同类型的文件 </p><blockquote><p> <strong>concat_unique</strong> <code>on</code> | <code>off</code><br><strong>默认:</strong> <code>concat_unique on</code><br><strong>上下文:</strong> <code>http, server, location</code> </p></blockquote><p> 定义最大能接受的文件数量。 </p><blockquote><p> <strong>concat_max_files</strong> <code>number</code><br><strong>默认:</strong> <code>concat_max_files 10</code><br><strong>上下文:</strong> <code>http, server, location</code> </p></blockquote><p> 定义在文件之间添加分隔符 </p><blockquote><p> <strong>concat_delimiter</strong> string<br><strong>默认:</strong> 无<br><strong>上下文</strong> ‘http, server, location’ </p></blockquote><p> 定义模块是否忽略文件不存在（404）或者没有权限（403）错误 </p><blockquote><p> <strong>concat_ignore_file_error</strong> ‘on | off’<br><strong>默认</strong> ‘concat_ignore_file_error off’<br><strong>上下文</strong> ‘http, server, location’ </p></blockquote><p><strong>注意：</strong>使用中发现对JS使用标准MIME-Type（application/X-javascript）导致的400 Bad Request ，我是通过修改contat的源码，加上javascript，就可以了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">location / &#123;</span><br><span class="line">                        concat on;</span><br><span class="line">                        concat_max_files 100;</span><br><span class="line">                        concat_delimiter &apos;&apos;;</span><br><span class="line">                        concat_unique off;</span><br><span class="line">                        concat_ignore_file_error on;</span><br></pre></td></tr></table></figure><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://cloud.tencent.com/developer/article/1463929" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1463929</a> </li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt; nginx-http-concat,可以减少http请求,从而减轻了服务器的请求压力,更快的响应。 每个http请求的应答都是非常昂贵的,
      
    
    </summary>
    
      <category term="中间件" scheme="https://shenshengkun.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>prometheus查询</title>
    <link href="https://shenshengkun.github.io/posts/pccv7vb4.html"/>
    <id>https://shenshengkun.github.io/posts/pccv7vb4.html</id>
    <published>2020-05-07T01:30:01.000Z</published>
    <updated>2020-05-07T01:33:59.533Z</updated>
    
    <content type="html"><![CDATA[<h1 id="表达式语言数据类型"><a href="#表达式语言数据类型" class="headerlink" title="表达式语言数据类型"></a>表达式语言数据类型</h1><p>在Prometheus的表达式语言中，任何表达式或者子表达式都可以归为四种类型：</p><ul><li>即时向量(instant vector) 包含每个时间序列的单个样本的一组时间序列，共享相同的时间戳。</li><li>范围向量(Range vector) 包含每个时间序列随时间变化的数据点的一组时间序列。</li><li>标量(Scalar) 一个简单的数字浮点值</li><li>字符串(String) 一个简单的字符串值(目前未被使用)</li></ul><p>根据使用情况（例如绘图或者显示表达式的输出），这些类型中只有一些是由用户指定的表达式产生的结果而有效的，例如，即时向量表达式是可以绘图的唯一类型。</p><h1 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h1><p>字符串可以用单引号、双引号或者反引号表示</p><p>PromQL遵循与Go相同的转义规则。在单引号，双引号中，反斜杠成为了转义字符，后面可以跟着a, b, f, n, r, t, v或者\。 可以使用八进制(\nnn)或者十六进制(\xnn, \unnnn和\Unnnnnnnn)提供特定字符。</p><p>在反引号内不处理转义字符。与Go不同，Prom不会丢弃反引号中的换行符。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;this is a string&quot;</span><br><span class="line">&apos;these are unescaped: \n \\ \t&apos;</span><br><span class="line">`these are not unescaped: \n &apos; &quot; \t`</span><br></pre></td></tr></table></figure><h1 id="浮点数"><a href="#浮点数" class="headerlink" title="浮点数"></a>浮点数</h1><p>标量浮点值可以直接写成形式<a href="https://www.bookstack.cn/read/prometheus-manual/$prometheus-querying-digits" target="_blank" rel="noopener">-</a>[.(digits)]。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-2.43</span><br></pre></td></tr></table></figure><h1 id="时间序列选择器"><a href="#时间序列选择器" class="headerlink" title="时间序列选择器"></a>时间序列选择器</h1><h2 id="即时向量选择器"><a href="#即时向量选择器" class="headerlink" title="即时向量选择器"></a>即时向量选择器</h2><p>瞬时向量选择器可以对一组时间序列数据进行筛选，并给出结果中的每个结果键值对（时间戳-样本值）: 最简单的形式是，只有一个度量名称被指定。在一个瞬时向量中这个结果包含有这个度量指标名称的所有样本数据键值对。</p><p>下面这个例子选择所有时间序列度量名称为<code>http_requests_total</code>的样本数据：</p><blockquote><p>http_requests_total</p></blockquote><p>通过在度量指标后面增加{}一组标签可以进一步地过滤这些时间序列数据。</p><p>下面这个例子选择了度量指标名称为<code>http_requests_total</code>，且一组标签为<code>job=prometheus</code>, <code>group=canary</code>:</p><blockquote><p>http_requests_total{job=”prometheus”,group=”canary”}</p></blockquote><p>可以采用不匹配的标签值也是可以的，或者用正则表达式不匹配标签。标签匹配操作如下所示：</p><ul><li><code>=</code>: 精确地匹配标签给定的值</li><li><code>!=</code>: 不等于给定的标签值</li><li><code>=~</code>: 正则表达匹配给定的标签值</li><li><code>!=</code>: 给定的标签值不符合正则表达式</li></ul><p>例如：度量指标名称为<code>http_requests_total</code>，正则表达式匹配标签<code>environment</code>为<code>staging, testing, development</code>的值，且http请求方法不等于<code>GET</code>。</p><blockquote><p>http_requests_total{environment=~”staging|testing|development”, method!=”GET”}</p></blockquote><p>匹配空标签值的标签匹配器也可以选择没有设置任何标签的所有时间序列数据。正则表达式完全匹配。</p><p>向量选择器必须指定一个度量指标名称或者至少不能为空字符串的标签值。以下表达式是非法的:</p><blockquote><p>{job=~”.*”} #Bad!</p></blockquote><p>上面这个例子既没有度量指标名称，标签选择器也可以正则匹配空标签值，所以不符合向量选择器的条件</p><p>相反地，下面这些表达式是有效的，第一个一定有一个字符。第二个有一个有用的标签method</p><blockquote><p>{job=~”.+”} # Good!{job=~”.*”, method=”get”} # Good!</p></blockquote><p>标签匹配器能够被应用到度量指标名称，使用<code>__name__</code>标签筛选度量指标名称。例如：表达式<code>http_requests_total</code>等价于<code>{__name__=&quot;http_requests_total&quot;}</code>。 其他的匹配器，如：<code>= ( !=, =~, !~)</code>都可以使用。下面的表达式选择了度量指标名称以<code>job:</code>开头的时间序列数据：</p><blockquote><p>{<strong>name</strong>=~”^job:.*”} #</p></blockquote><h2 id="范围向量选择器"><a href="#范围向量选择器" class="headerlink" title="范围向量选择器"></a>范围向量选择器</h2><p>范围向量类似瞬时向量, 不同在于，它们从当前实例选择样本范围区间。在语法上，时间长度被追加在向量选择器尾部的方括号[]中，用以指定对于每个样本范围区间中的每个元素应该抓取的时间范围样本区间。</p><p>时间长度有一个数值决定，后面可以跟下面的单位：</p><ul><li><code>s</code> - seconds</li><li><code>m</code> - minutes</li><li><code>h</code> - hours</li><li><code>d</code> - days</li><li><code>w</code> - weeks</li><li><code>y</code> - years</li></ul><p>在下面这个例子中, 选择过去5分钟内，度量指标名称为<code>http_requests_total</code>， 标签为<code>job=&quot;prometheus&quot;</code>的时间序列数据:</p><blockquote><p>http_requests_total{job=”prometheus”}[5m]</p></blockquote><h2 id="偏移修饰符"><a href="#偏移修饰符" class="headerlink" title="偏移修饰符"></a>偏移修饰符</h2><p>这个<code>offset</code>偏移修饰符允许在查询中改变单个瞬时向量和范围向量中的时间偏移</p><p>例如，下面的表达式返回相对于当前时间的前5分钟时的时刻, 度量指标名称为<code>http_requests_total</code>的时间序列数据：</p><blockquote><p>http_requests_total offset 5m</p></blockquote><p>注意：<code>offset</code>偏移修饰符必须直接跟在选择器后面，例如：</p><blockquote><p>sum(http_requests_total{method=”GET”} offset 5m) // GOOD.</p></blockquote><p>然而，下面这种情况是不正确的</p><blockquote><p>sum(http_requests_total{method=”GET”}) offset 5m // INVALID.</p></blockquote><p>offset偏移修饰符在范围向量上和瞬时向量用法一样的。下面这个返回了相对于当前时间的前一周时，过去5分钟的度量指标名称为<code>http_requests_total</code>的速率：</p><blockquote><p>rate(http_requests_total[5m] offset 1w)</p></blockquote><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://prometheus.io/docs/prometheus/latest/querying/functions/" target="_blank" rel="noopener">https://prometheus.io/docs/prometheus/latest/querying/functions/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;表达式语言数据类型&quot;&gt;&lt;a href=&quot;#表达式语言数据类型&quot; class=&quot;headerlink&quot; title=&quot;表达式语言数据类型&quot;&gt;&lt;/a&gt;表达式语言数据类型&lt;/h1&gt;&lt;p&gt;在Prometheus的表达式语言中，任何表达式或者子表达式都可以归为四种类型：&lt;/
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
</feed>
