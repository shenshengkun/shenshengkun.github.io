<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>舒宇的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://shenshengkun.github.io/"/>
  <updated>2019-07-05T06:59:46.817Z</updated>
  <id>https://shenshengkun.github.io/</id>
  
  <author>
    <name>Shu Yu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes Pod无法删除</title>
    <link href="https://shenshengkun.github.io/posts/dk7569vg.html"/>
    <id>https://shenshengkun.github.io/posts/dk7569vg.html</id>
    <published>2019-07-04T05:53:01.000Z</published>
    <updated>2019-07-05T06:59:46.817Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题发现"><a href="#问题发现" class="headerlink" title="问题发现"></a>问题发现</h1><p>在node节点断电 重启后，发现有的pod节点状态不正常，之前的回收策略也都做了，就调研下是什么原因导致的</p><p>pod一直处于Terminated: ExitCode 状态</p><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><h2 id="直接删除，无法飘移"><a href="#直接删除，无法飘移" class="headerlink" title="直接删除，无法飘移"></a>直接删除，无法飘移</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete pod &lt;podname&gt; --namespace=&lt;namspacer&gt; --grace-period=0 --force</span><br><span class="line">发现pod无法漂移</span><br><span class="line">docker ps -a查看对应docker容器的状态，发现这两个Pod的docker容器处于Dead状态。使用docker rm &lt;container id&gt;，提示Device is Busy，无法删除。</span><br></pre></td></tr></table></figure><h2 id="现象是由于systemd服务PrivateTmp-true引起"><a href="#现象是由于systemd服务PrivateTmp-true引起" class="headerlink" title="现象是由于systemd服务PrivateTmp=true引起"></a>现象是由于systemd服务<code>PrivateTmp=true</code>引起</h2><p>最根本的方法是，当机器加入时，在 <code>docker.service</code> 中加上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">MountFlags=slave</span><br></pre></td></tr></table></figure><h1 id="关于Systemd的MountFlags"><a href="#关于Systemd的MountFlags" class="headerlink" title="关于Systemd的MountFlags"></a>关于Systemd的MountFlags</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MountFlags: 配置Systemd服务的Mount Namespace配置。会影响服务进程上下文中挂载点的信息，即服务是否会继承主机上已有的挂载点，以及如果服务运行时执行了挂载或卸载设备的操作，是否会真实地在主机上产生效果。可选值为shared、slave和private</span><br><span class="line"></span><br><span class="line">shared：服务与主机共用一个Mount Namespace，会继承主机挂载点，服务挂载或卸载设备时会真实地反映到主机上</span><br><span class="line"></span><br><span class="line">slave：服务使用独立的Mount Namespace，会继承主机挂载点，但服务对挂载点的操作只在自己的Namespace内生效，不会反映到主机上</span><br><span class="line"></span><br><span class="line">private: 服务使用独立的Mount Namespace，在启动时没有任何挂载点，服务对挂载点的操作也不会反映到主机上</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;问题发现&quot;&gt;&lt;a href=&quot;#问题发现&quot; class=&quot;headerlink&quot; title=&quot;问题发现&quot;&gt;&lt;/a&gt;问题发现&lt;/h1&gt;&lt;p&gt;在node节点断电 重启后，发现有的pod节点状态不正常，之前的回收策略也都做了，就调研下是什么原因导致的&lt;/p&gt;
&lt;p&gt;p
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>tomcat8导致文件权限访问不到</title>
    <link href="https://shenshengkun.github.io/posts/kfkd454f.html"/>
    <id>https://shenshengkun.github.io/posts/kfkd454f.html</id>
    <published>2019-07-01T03:09:10.000Z</published>
    <updated>2019-07-01T03:19:37.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>之前在tomcat 7下文件上传后访问一直没问题，现在tomcat版本升到8.5，在测试文件http上传时，发现所传文件无法通过nginx访问了：报错 403 forbidden </p><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><p>看下系统的umask</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/profile后发现</span><br><span class="line"></span><br><span class="line">if [ $UID -gt 199 ] &amp;&amp; [ &quot;`/usr/bin/id -gn`&quot; = &quot;`/usr/bin/id -un`&quot; ]; then</span><br><span class="line">    umask 002</span><br><span class="line">else</span><br><span class="line">    umask 022</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">022是没问题的</span><br></pre></td></tr></table></figure><p>看下tomcat的catlina.sh</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">if [ -z &quot;$UMASK&quot; ]; then</span><br><span class="line">UMASK=&quot;0027&quot;</span><br><span class="line">fi</span><br><span class="line">umask $UMASK</span><br><span class="line"></span><br><span class="line">tomcat8改成0027了，把这个改成0022就好了</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h1&gt;&lt;p&gt;之前在tomcat 7下文件上传后访问一直没问题，现在tomcat版本升到8.5，在测试文件http上传时，发现所传文件无法通过nginx访
      
    
    </summary>
    
      <category term="中间件" scheme="https://shenshengkun.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>Kubelet状态更新机制</title>
    <link href="https://shenshengkun.github.io/posts/dfkdaa65.html"/>
    <id>https://shenshengkun.github.io/posts/dfkdaa65.html</id>
    <published>2019-06-18T06:14:01.000Z</published>
    <updated>2019-06-20T06:18:22.061Z</updated>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>当 Kubernetes 中 Node 节点出现状态异常的情况下，节点上的 Pod 会被重新调度到其他节点上去，但是有的时候我们会发现节点 Down 掉以后，Pod 并不会立即触发重新调度，这实际上就是和 Kubelet 的状态更新机制密切相关的，Kubernetes 提供了一些参数配置来触发重新调度到嗯时间，下面我们来分析下 Kubelet 状态更新的基本流程。</p><ol><li>kubelet 自身会定期更新状态到 apiserver，通过参数<code>--node-status-update-frequency</code>指定上报频率，默认是 10s 上报一次。</li><li>kube-controller-manager 会每隔<code>--node-monitor-period</code>时间去检查 kubelet 的状态，默认是 5s。</li><li>当 node 失联一段时间后，kubernetes 判定 node 为 <code>notready</code> 状态，这段时长通过<code>--node-monitor-grace-period</code>参数配置，默认 40s。</li><li>当 node 失联一段时间后，kubernetes 判定 node 为 <code>unhealthy</code> 状态，这段时长通过<code>--node-startup-grace-period</code>参数配置，默认 1m0s。</li><li>当 node 失联一段时间后，kubernetes 开始删除原 node 上的 pod，这段时长是通过<code>--pod-eviction-timeout</code>参数配置，默认 5m0s。</li></ol><blockquote><p>kube-controller-manager 和 kubelet 是异步工作的，这意味着延迟可能包括任何的网络延迟、apiserver 的延迟、etcd 延迟，一个节点上的负载引起的延迟等等。因此，如果<code>--node-status-update-frequency</code>设置为5s，那么实际上 etcd 中的数据变化会需要 6-7s，甚至更长时间。</p></blockquote><p>Kubelet在更新状态失败时，会进行<code>nodeStatusUpdateRetry</code>次重试，默认为 5 次。</p><p>Kubelet 会在函数<code>tryUpdateNodeStatus</code>中尝试进行状态更新。Kubelet 使用了 Golang 中的<code>http.Client()</code>方法，但是没有指定超时时间，因此，如果 API Server 过载时，当建立 TCP 连接时可能会出现一些故障。</p><p>因此，在<code>nodeStatusUpdateRetry</code> * <code>--node-status-update-frequency</code>时间后才会更新一次节点状态。</p><p>同时，Kubernetes 的 controller manager 将尝试每<code>--node-monitor-period</code>时间周期内检查<code>nodeStatusUpdateRetry</code>次。在<code>--node-monitor-grace-period</code>之后，会认为节点 unhealthy，然后会在<code>--pod-eviction-timeout</code>后删除 Pod。</p><p>kube proxy 有一个 watcher API，一旦 Pod 被驱逐了，kube proxy 将会通知更新节点的 iptables 规则，将 Pod 从 Service 的 Endpoints 中移除，这样就不会访问到来自故障节点的 Pod 了。</p><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>对于这些参数的配置，需要根据不通的集群规模场景来进行配置。</p><h3 id="社区默认的配置"><a href="#社区默认的配置" class="headerlink" title="社区默认的配置"></a>社区默认的配置</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>10s</td></tr><tr><td>–node-monitor-period</td><td>5s</td></tr><tr><td>–node-monitor-grace-period</td><td>40s</td></tr><tr><td>–pod-eviction-timeout</td><td>5m</td></tr></tbody></table><h3 id="快速更新和快速响应"><a href="#快速更新和快速响应" class="headerlink" title="快速更新和快速响应"></a>快速更新和快速响应</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>4s</td></tr><tr><td>–node-monitor-period</td><td>2s</td></tr><tr><td>–node-monitor-grace-period</td><td>20s</td></tr><tr><td>–pod-eviction-timeout</td><td>30s</td></tr></tbody></table><p>在这种情况下，Pod 将在 50s 被驱逐，因为该节点在 20s 后被视为Down掉了，<code>--pod-eviction-timeout</code>在 30s 之后发生，但是，这种情况会给 etcd 产生很大的开销，因为每个节点都会尝试每 2s 更新一次状态。</p><p>如果环境有1000个节点，那么每分钟将有15000次节点更新操作，这可能需要大型 etcd 容器甚至是 etcd 的专用节点。</p><blockquote><p>如果我们计算尝试次数，则除法将给出5，但实际上每次尝试的 nodeStatusUpdateRetry 尝试将从3到5。 由于所有组件的延迟，尝试总次数将在15到25之间变化。</p></blockquote><h3 id="中等更新和平均响应"><a href="#中等更新和平均响应" class="headerlink" title="中等更新和平均响应"></a>中等更新和平均响应</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>20s</td></tr><tr><td>–node-monitor-period</td><td>5s</td></tr><tr><td>–node-monitor-grace-period</td><td>2m</td></tr><tr><td>–pod-eviction-timeout</td><td>1m</td></tr></tbody></table><p>这种场景下会 20s 更新一次 node 状态，controller manager 认为 node 状态不正常之前，会有 2m<em>60⁄20</em>5=30 次的 node 状态更新，Node 状态为 down 之后 1m，就会触发驱逐操作。</p><p>如果有 1000 个节点，1分钟之内就会有 60s/20s*1000=3000 次的节点状态更新操作。</p><h3 id="低更新和慢响应"><a href="#低更新和慢响应" class="headerlink" title="低更新和慢响应"></a>低更新和慢响应</h3><table><thead><tr><th>参数</th><th>值</th></tr></thead><tbody><tr><td>–node-status-update-frequency</td><td>1m</td></tr><tr><td>–node-monitor-period</td><td>5s</td></tr><tr><td>–node-monitor-grace-period</td><td>5m</td></tr><tr><td>–pod-eviction-timeout</td><td>1m</td></tr></tbody></table><p>Kubelet 将会 1m 更新一次节点的状态，在认为不健康之后会有 5m/1m*5=25 次重试更新的机会。Node为不健康的时候，1m 之后 pod开始被驱逐。</p><p>可以有不同的组合，例如快速更新和慢反应以满足特定情况。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;当 Kubernetes 中 Node 节点出现状态异常的情况下，节点上的 Pod 会被重新调度到其他节点上去，但是有的时候我们会发现节点 
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>istio架构与技术</title>
    <link href="https://shenshengkun.github.io/posts/5a4fga4h.html"/>
    <id>https://shenshengkun.github.io/posts/5a4fga4h.html</id>
    <published>2019-06-18T02:10:01.000Z</published>
    <updated>2019-06-21T01:27:54.328Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p>使用云平台可以为组织提供丰富的好处。然而，不可否认的是，采用云可能会给 DevOps 团队带来压力。开发人员必须使用微服务以满足应用的可移植性，同时运营商管理了极其庞大的混合和多云部署。Istio 允许您连接、保护、控制和观测服务。</p><p>在较高的层次上，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。它是一个完全开源的服务网格，可以透明地分层到现有的分布式应用程序上。它也是一个平台，包括允许它集成到任何日志记录平台、遥测或策略系统的 API。Istio 的多样化功能集使您能够成功高效地运行分布式微服务架构，并提供保护、连接和监控微服务的统一方法。</p><h1 id="Service-Mesh"><a href="#Service-Mesh" class="headerlink" title="Service Mesh"></a>Service Mesh</h1><p>• 治理能力独立（Sidecar）</p><p>• 应用程序无感知 </p><p>• 服务通信的基础设施层</p><p><img src="https://shenshengkun.github.io/images/istio1.png" alt=""></p><h1 id="为什么要使用-Istio？"><a href="#为什么要使用-Istio？" class="headerlink" title="为什么要使用 Istio？"></a>为什么要使用 Istio？</h1><p>Istio 提供一种简单的方式来为已部署的服务建立网络，该网络具有负载均衡、服务间认证、监控等功能，只需要对服务的代码进行一点或不需要做任何改动。想要让服务支持 Istio，只需要在您的环境中部署一个特殊的 sidecar 代理，使用 Istio 控制平面功能配置和管理代理，拦截微服务之间的所有网络通信：</p><ul><li>HTTP、gRPC、WebSocket 和 TCP 流量的自动负载均衡。</li><li>通过丰富的路由规则、重试、故障转移和故障注入，可以对流量行为进行细粒度控制。</li><li>可插入的策略层和配置 API，支持访问控制、速率限制和配额。</li><li>对出入集群入口和出口中所有流量的自动度量指标、日志记录和追踪。</li><li>通过强大的基于身份的验证和授权，在集群中实现安全的服务间通信。</li></ul><p>Istio 旨在实现可扩展性，满足各种部署需求。</p><p><img src="https://shenshengkun.github.io/images/istio2.png" alt=""></p><h1 id="核心功能"><a href="#核心功能" class="headerlink" title="核心功能"></a>核心功能</h1><p>Istio 在服务网络中统一提供了许多关键功能：</p><h2 id="流量管理"><a href="#流量管理" class="headerlink" title="流量管理"></a>流量管理</h2><p>通过简单的规则配置和流量路由，您可以控制服务之间的流量和 API 调用。Istio 简化了断路器、超时和重试等服务级别属性的配置，并且可以轻松设置 A/B测试、金丝雀部署和基于百分比的流量分割的分阶段部署等重要任务。</p><p>通过更好地了解您的流量和开箱即用的故障恢复功能，您可以在问题出现之前先发现问题，使调用更可靠，并且使您的网络更加强大——无论您面临什么条件。</p><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><p>Istio 的安全功能使开发人员可以专注于应用程序级别的安全性。Istio 提供底层安全通信信道，并大规模管理服务通信的认证、授权和加密。使用Istio，服务通信在默认情况下是安全的，它允许您跨多种协议和运行时一致地实施策略——所有这些都很少或根本不需要应用程序更改。</p><p>虽然 Istio 与平台无关，但将其与 Kubernetes（或基础架构）网络策略结合使用，其优势会更大，包括在网络和应用层保护 pod 间或服务间通信的能力。</p><h2 id="可观察性"><a href="#可观察性" class="headerlink" title="可观察性"></a>可观察性</h2><p>Istio 强大的追踪、监控和日志记录可让您深入了解服务网格部署。通过 Istio 的监控功能，可以真正了解服务性能如何影响上游和下游的功能，而其自定义仪表板可以提供对所有服务性能的可视性，并让您了解该性能如何影响您的其他进程。</p><p>Istio 的 Mixer 组件负责策略控制和遥测收集。它提供后端抽象和中介，将 Istio 的其余部分与各个基础架构后端的实现细节隔离开来，并为运维提供对网格和基础架构后端之间所有交互的细粒度控制。</p><p>所有这些功能可以让您可以更有效地设置、监控和实施服务上的 SLO。当然，最重要的是，您可以快速有效地检测和修复问题。</p><h2 id="平台支持"><a href="#平台支持" class="headerlink" title="平台支持"></a>平台支持</h2><p>Istio 是独立于平台的，旨在运行在各种环境中，包括跨云、内部部署、Kubernetes、Mesos 等。您可以在 Kubernetes 上部署 Istio 或具有 Consul 的 Nomad 上部署。Istio 目前支持：</p><ul><li>在 Kubernetes 上部署的服务</li><li>使用 Consul 注册的服务</li><li>在虚拟机上部署的服务</li></ul><h2 id="集成和定制"><a href="#集成和定制" class="headerlink" title="集成和定制"></a>集成和定制</h2><p>策略执行组件可以扩展和定制，以便与现有的 ACL、日志、监控、配额、审计等方案集成。</p><p><img src="https://shenshengkun.github.io/images/istio3.png" alt=""></p><h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p>Istio 服务网格逻辑上分为<strong>数据平面</strong>和<strong>控制平面</strong>。</p><ul><li><strong>数据平面</strong>由一组以 sidecar 方式部署的智能代理（<a href="https://www.envoyproxy.io/" target="_blank" rel="noopener">Envoy</a>）组成。这些代理可以调节和控制微服务及 <a href="https://istio.io/zh/docs/concepts/policies-and-telemetry/" target="_blank" rel="noopener">Mixer</a> 之间所有的网络通信。</li><li><strong>控制平面</strong>负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。</li></ul><p>下图显示了构成每个面板的不同组件：</p><p><img src="https://shenshengkun.github.io/images/istio4.png" alt=""></p><h2 id="Envoy"><a href="#Envoy" class="headerlink" title="Envoy"></a>Envoy</h2><p>Istio 使用 Envoy代理的扩展版本，Envoy 是以 C++ 开发的高性能代理，用于调解服务网格中所有服务的所有入站和出站流量。Envoy 的许多内置功能被 Istio 发扬光大，例如：</p><ul><li>动态服务发现</li><li>负载均衡</li><li>TLS 终止</li><li>HTTP/2 &amp; gRPC 代理</li><li>熔断器</li><li>健康检查、基于百分比流量拆分的灰度发布</li><li>故障注入</li><li>丰富的度量指标</li></ul><p>Envoy 被部署为 <strong>sidecar</strong>，和对应服务在同一个 Kubernetes pod 中。这允许 Istio 将大量关于流量行为的信号作为属性提取出来，而这些属性又可以在 Mixer 中用于执行策略决策，并发送给监控系统，以提供整个网格行为的信息。</p><p>Sidecar 代理模型还可以将 Istio 的功能添加到现有部署中，而无需重新构建或重写代码。可以阅读更多来了解为什么我们在设计目标中选择这种方式。</p><h2 id="Mixer"><a href="#Mixer" class="headerlink" title="Mixer"></a>Mixer</h2><p>Mixer是一个独立于平台的组件，负责在服务网格上执行访问控制和使用策略，并从 Envoy 代理和其他服务收集遥测数据。代理提取请求级属性，发送到 Mixer 进行评估</p><p>Mixer 中包括一个灵活的插件模型，使其能够接入到各种主机环境和基础设施后端，从这些细节中抽象出 Envoy 代理和 Istio 管理的服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">可以配一些监控</span><br></pre></td></tr></table></figure><p><img src="https://shenshengkun.github.io/images/istio5.png" alt=""></p><h2 id="Pilot"><a href="#Pilot" class="headerlink" title="Pilot"></a>Pilot</h2><p>Pilot为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。它将控制流量行为的高级路由规则转换为特定于 Envoy 的配置，并在运行时将它们传播到 sidecar。</p><p>Pilot 将平台特定的服务发现机制抽象化并将其合成为符合 Envoy 数据平面 API的任何 sidecar 都可以使用的标准格式。这种松散耦合使得 Istio 能够在多种环境下运行（例如，Kubernetes、Consul、Nomad），同时保持用于流量管理的相同操作界面。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">k8s来说的话，比如说pod这样的元数据，传给Envoy</span><br></pre></td></tr></table></figure><h2 id="Citadel"><a href="#Citadel" class="headerlink" title="Citadel"></a>Citadel</h2><p>Citadel通过内置身份和凭证管理赋能强大的服务间和最终用户身份验证。可用于升级服务网格中未加密的流量，并为运维人员提供基于服务标识而不是网络控制的强制执行策略的能力。从 0.5 版本开始，Istio 支持基于角色的访问控制，以控制谁可以访问您的服务，而不是基于不稳定的三层或四层网络标识。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">证书安全管理中心，证书生成以及下发</span><br></pre></td></tr></table></figure><h2 id="Galley"><a href="#Galley" class="headerlink" title="Galley"></a>Galley</h2><p>Galley 将担任 Istio 的配置验证，获取配置，处理和分配组件的任务。它负责将其余的 Istio 组件与从底层平台（例如 Kubernetes）获取用户配置的细节中隔离开来。</p><h1 id="和k8s结合"><a href="#和k8s结合" class="headerlink" title="和k8s结合"></a>和k8s结合</h1><p><img src="https://shenshengkun.github.io/images/istio6.png" alt=""></p><h1 id="在-Helm-和-Tiller-的环境中使用-helm-install-命令进行安装"><a href="#在-Helm-和-Tiller-的环境中使用-helm-install-命令进行安装" class="headerlink" title="在 Helm 和 Tiller 的环境中使用 helm install 命令进行安装"></a>在 Helm 和 Tiller 的环境中使用 <code>helm install</code> 命令进行安装</h1><h2 id="下载istio"><a href="#下载istio" class="headerlink" title="下载istio"></a>下载istio</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.1.9 sh -</span><br><span class="line">mv istio-1.1.9 /opt/k8s/work/</span><br><span class="line">cd /opt/k8s/work/</span><br><span class="line">cd istio-1.1.9/</span><br><span class="line">export PATH=$PWD/bin:$PATH</span><br><span class="line">helm repo add istio.io https://storage.googleapis.com/istio-release/releases/1.1.9/charts/</span><br></pre></td></tr></table></figure><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">helm install install/kubernetes/helm/istio-init --name istio-init --namespace istio-system</span><br><span class="line"></span><br><span class="line">kubectl get crds | grep &apos;istio.io\|certmanager.k8s.io&apos; | wc -l</span><br><span class="line">53</span><br><span class="line"></span><br><span class="line">helm install install/kubernetes/helm/istio --name istio --namespace istio-system</span><br><span class="line"></span><br><span class="line">[root@node1 istio-1.1.9]# kubectl get pods -n istio-system</span><br><span class="line">NAME                                      READY   STATUS      RESTARTS   AGE</span><br><span class="line">istio-citadel-b6d6889c4-96fwx             1/1     Running     0          44h</span><br><span class="line">istio-galley-654c696595-2rbr9             1/1     Running     0          44h</span><br><span class="line">istio-ingressgateway-6b47b76cc6-2rxbq     1/1     Running     0          44h</span><br><span class="line">istio-init-crd-10-smj28                   0/1     Completed   0          44h</span><br><span class="line">istio-init-crd-11-wktdb                   0/1     Completed   0          44h</span><br><span class="line">istio-pilot-5c99cfc94-g7t84               2/2     Running     0          44h</span><br><span class="line">istio-policy-6c5795449-tkzmp              2/2     Running     3          44h</span><br><span class="line">istio-sidecar-injector-79c88d56cf-lmv9j   1/1     Running     0          44h</span><br><span class="line">istio-telemetry-64f99d84c7-ksjmh          2/2     Running     2          44h</span><br><span class="line">prometheus-d8d46c5b5-sdljw                1/1     Running     0          44h</span><br></pre></td></tr></table></figure><h2 id="卸载"><a href="#卸载" class="headerlink" title="卸载"></a>卸载</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm delete --purge istio</span><br><span class="line">helm delete --purge istio-init</span><br><span class="line">kubectl delete -f install/kubernetes/helm/istio-init/files</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h1&gt;&lt;p&gt;使用云平台可以为组织提供丰富的好处。然而，不可否认的是，采用云可能会给 DevOps 团队带来压力。开发人员必须使用微服务以满足应用的可移植
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-helm</title>
    <link href="https://shenshengkun.github.io/posts/dkgg3h4f.html"/>
    <id>https://shenshengkun.github.io/posts/dkgg3h4f.html</id>
    <published>2019-06-17T10:20:01.000Z</published>
    <updated>2019-06-17T03:07:27.417Z</updated>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p><code>Helm</code>这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套测试环境，对于单体服务，部署一套测试环境我相信还是非常快的，但是对于微服务架构的应用，要部署一套新的环境，就有点折磨人了，微服务越多、你就会越绝望的。虽然我们线上和测试环境已经都迁移到了<code>kubernetes</code>环境，但是每个微服务也得维护一套<code>yaml</code>文件，而且每个环境下的配置文件也不太一样，部署一套新的环境成本是真的很高。如果我们能使用类似于<code>yum</code>的工具来安装我们的应用的话是不是就很爽歪歪了啊？<code>Helm</code>就相当于<code>kubernetes</code>环境下的<code>yum</code>包管理工具。 </p><h1 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h1><p>做为 Kubernetes 的一个包管理工具，<code>Helm</code>具有如下功能：</p><ul><li>创建新的 chart</li><li>chart 打包成 tgz 格式</li><li>上传 chart 到 chart 仓库或从仓库中下载 chart</li><li>在<code>Kubernetes</code>集群中安装或卸载 chart</li><li>管理用<code>Helm</code>安装的 chart 的发布周期</li></ul><h1 id="重要概念"><a href="#重要概念" class="headerlink" title="重要概念"></a>重要概念</h1><p>Helm 有三个重要概念：</p><ul><li>chart：包含了创建<code>Kubernetes</code>的一个应用实例的必要信息</li><li>config：包含了应用发布配置信息</li><li>release：是一个 chart 及其配置的一个运行实例</li></ul><h1 id="Helm组件"><a href="#Helm组件" class="headerlink" title="Helm组件"></a>Helm组件</h1><p>Helm 有以下两个组成部分：</p><p><code>Helm Client</code> 是用户命令行工具，其主要负责如下：</p><ul><li>本地 chart 开发</li><li>仓库管理</li><li>与 Tiller sever 交互</li><li>发送预安装的 chart</li><li>查询 release 信息</li><li>要求升级或卸载已存在的 release</li></ul><p><code>Tiller Server</code>是一个部署在<code>Kubernetes</code>集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下：</p><ul><li>监听来自 Helm client 的请求</li><li>通过 chart 及其配置构建一次发布</li><li>安装 chart 到<code>Kubernetes</code>集群，并跟踪随后的发布</li><li>通过与<code>Kubernetes</code>交互升级或卸载 chart</li><li>简单的说，client 管理 charts，而 server 管理发布 release</li></ul><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>我们可以在<a href="https://github.com/kubernetes/helm/releases" target="_blank" rel="noopener">Helm Realese</a>页面下载二进制文件，这里下载的2.14.1版本，解压后将可执行文件<code>helm</code>拷贝到<code>/usr/local/bin</code>目录下即可，这样<code>Helm</code>客户端就在这台机器上安装完成了。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br><span class="line"></span><br><span class="line">另外还需要在每个node节点安装yum install socat -y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]# helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.14.1&quot;, GitCommit:&quot;5270352a09c7e8b6e8c9593002a73535276507c0&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.14.1&quot;, GitCommit:&quot;5270352a09c7e8b6e8c9593002a73535276507c0&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">自Kubernetes 1.6版本开始，API Server启用了RBAC授权。而目前的Tiller部署没有定义授权的ServiceAccount，这会导致访问API Server时被拒绝。我们可以采用如下方法，明确为Tiller部署添加授权。</span><br><span class="line"></span><br><span class="line">kubectl create serviceaccount --namespace kube-system tiller</span><br><span class="line">kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span><br><span class="line">kubectl patch deploy --namespace kube-system tiller-deploy -p &apos;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&apos;</span><br><span class="line">kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default</span><br></pre></td></tr></table></figure><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>我们现在了尝试创建一个 Chart： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 helm]# helm create hello-helm</span><br><span class="line">Creating hello-helm</span><br><span class="line">[root@node1 helm]# ls</span><br><span class="line">hello-helm  helm-v2.14.1-linux-amd64.tar.gz  linux-amd64</span><br><span class="line">[root@node1 helm]# helm install ./hello-helm</span><br><span class="line">NAME:   virulent-wolverine</span><br><span class="line">LAST DEPLOYED: Mon Jun 17 10:56:39 2019</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Deployment</span><br><span class="line">NAME                           READY  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">virulent-wolverine-hello-helm  0/1    0           0          1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                            READY  STATUS             RESTARTS  AGE</span><br><span class="line">virulent-wolverine-hello-helm-6f54d6f866-d5t7v  0/1    ContainerCreating  0         1s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                           TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">virulent-wolverine-hello-helm  ClusterIP  10.254.123.130  &lt;none&gt;       80/TCP   1s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export POD_NAME=$(kubectl get pods --namespace default -l &quot;app.kubernetes.io/name=hello-helm,app.kubernetes.io/instance=virulent-wolverine&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">  echo &quot;Visit http://127.0.0.1:8080 to use your application&quot;</span><br><span class="line">  kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><p>然后我们根据提示执行下面的命令： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export POD_NAME=$(kubectl get pods --namespace default -l &quot;app.kubernetes.io/name=hello-helm,app.kubernetes.io/instance=virulent-wolverine&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><p>访问：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# curl 127.0.0.1:8080</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>查看<code>release</code>： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# helm list</span><br><span class="line">NAME                    REVISION        UPDATED                         STATUS          CHART                   APP VERSION     NAMESPACE</span><br><span class="line">virulent-wolverine      1               Mon Jun 17 10:56:39 2019        DEPLOYED        hello-helm-0.1.0        1.0             default</span><br></pre></td></tr></table></figure><p>打包<code>chart</code>： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm package hello-helm</span><br></pre></td></tr></table></figure><p>删除：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# helm delete virulent-wolverine</span><br><span class="line">release &quot;virulent-wolverine&quot; deleted</span><br><span class="line">[root@node1 ~]# helm list</span><br><span class="line">[root@node1 ~]# helm list --all</span><br><span class="line">NAME                    REVISION        UPDATED                         STATUS  CHART                   APP VERSION     NAMESPACE</span><br><span class="line">virulent-wolverine      1               Mon Jun 17 10:56:39 2019        DELETED hello-helm-0.1.0        1.0             default</span><br></pre></td></tr></table></figure><p>彻底删除：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# helm delete virulent-wolverine --purge</span><br><span class="line">release &quot;virulent-wolverine&quot; deleted</span><br><span class="line">[root@node1 ~]# helm list --all</span><br><span class="line">[root@node1 ~]#</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h1&gt;&lt;p&gt;&lt;code&gt;Helm&lt;/code&gt;这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-dashboard</title>
    <link href="https://shenshengkun.github.io/posts/c2556dcf.html"/>
    <id>https://shenshengkun.github.io/posts/c2556dcf.html</id>
    <published>2019-06-10T08:16:01.000Z</published>
    <updated>2019-06-12T08:39:46.837Z</updated>
    
    <content type="html"><![CDATA[<h1 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/kubernetes/cluster/addons/dashboard</span><br></pre></td></tr></table></figure><p>修改 service 定义，指定端口类型为 NodePort，这样外界可以通过地址 NodeIP:NodePort 访问 dashboard；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cat dashboard-service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">    kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class="line">    addonmanager.kubernetes.io/mode: Reconcile</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort # 增加这一行</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">  ports:</span><br><span class="line">  - port: 443</span><br><span class="line">    targetPort: 8443</span><br></pre></td></tr></table></figure><p>修改镜像地址mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1在dashboard-controller.yaml中</p><h1 id="执行所有定义文件"><a href="#执行所有定义文件" class="headerlink" title="执行所有定义文件"></a>执行所有定义文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f  .</span><br></pre></td></tr></table></figure><h2 id="查看分配的-NodePort"><a href="#查看分配的-NodePort" class="headerlink" title="查看分配的 NodePort"></a>查看分配的 NodePort</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl get deployment kubernetes-dashboard  -n kube-system</span><br><span class="line"></span><br><span class="line">NAME                   READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line"></span><br><span class="line">kubernetes-dashboard   1/1     1            1           23h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 dashboard]# kubectl --namespace kube-system get pods -o wide</span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE     IP             NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-8854569d4-g2hth                 1/1     Running   4          5d22h   172.30.40.2    node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard-7848d45466-6pm2q   1/1     Running   0          23h     172.30.200.3   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">metrics-server-5f7cf7659-59swk          1/1     Running   0          2d5h    172.30.40.3    node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 dashboard]# kubectl get services kubernetes-dashboard -n kube-system</span><br><span class="line">NAME                   TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE</span><br><span class="line">kubernetes-dashboard   NodePort   10.254.234.85   &lt;none&gt;        443:32681/TCP   23h</span><br></pre></td></tr></table></figure><h1 id="访问-dashboard"><a href="#访问-dashboard" class="headerlink" title="访问 dashboard"></a>访问 dashboard</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://192.168.6.101:32681</span><br></pre></td></tr></table></figure><h2 id="创建登录-Dashboard-的-token-和-kubeconfig-配置文件"><a href="#创建登录-Dashboard-的-token-和-kubeconfig-配置文件" class="headerlink" title="创建登录 Dashboard 的 token 和 kubeconfig 配置文件"></a>创建登录 Dashboard 的 token 和 kubeconfig 配置文件</h2><p>dashboard 默认只支持 token 认证（不支持 client 证书认证），所以如果使用 Kubeconfig 文件，需要将 token 写入到该文件。</p><h3 id="创建登录-token"><a href="#创建登录-token" class="headerlink" title="创建登录 token"></a>创建登录 token</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl create sa dashboard-admin -n kube-system</span><br><span class="line">kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin</span><br><span class="line">ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk &apos;&#123;print $1&#125;&apos;)</span><br><span class="line">DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system $&#123;ADMIN_SECRET&#125; | grep -E &apos;^token&apos; | awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">echo $&#123;DASHBOARD_LOGIN_TOKEN&#125;</span><br></pre></td></tr></table></figure><p>使用输出的 token 登录 Dashboard。</p><h3 id="创建使用-token-的-KubeConfig-文件"><a href="#创建使用-token-的-KubeConfig-文件" class="headerlink" title="创建使用 token 的 KubeConfig 文件"></a>创建使用 token 的 KubeConfig 文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line"># 设置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置客户端认证参数，使用上面创建的 Token</span><br><span class="line">kubectl config set-credentials dashboard_user \</span><br><span class="line">  --token=$&#123;DASHBOARD_LOGIN_TOKEN&#125; \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置上下文参数</span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=dashboard_user \</span><br><span class="line">  --kubeconfig=dashboard.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置默认上下文</span><br><span class="line">kubectl config use-context default --kubeconfig=dashboard.kubeconfig</span><br></pre></td></tr></table></figure><p>用生成的 dashboard.kubeconfig 登录 Dashboard。</p><h1 id="为kubernetes-dashboard访问用户添加权限控制"><a href="#为kubernetes-dashboard访问用户添加权限控制" class="headerlink" title="为kubernetes dashboard访问用户添加权限控制"></a>为kubernetes dashboard访问用户添加权限控制</h1><h2 id="Role"><a href="#Role" class="headerlink" title="Role"></a>Role</h2><p><code>Role</code>表示是一组规则权限，只能累加，<code>Role</code>可以定义在一个<code>namespace</code>中，只能用于授予对单个命名空间中的资源访问的权限。比如我们新建一个对默认命名空间中<code>Pods</code>具有访问权限的角色：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: default</span><br><span class="line">  name: pod-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br></pre></td></tr></table></figure><h2 id="ClusterRole"><a href="#ClusterRole" class="headerlink" title="ClusterRole"></a>ClusterRole</h2><p><code>ClusterRole</code>具有与<code>Role</code>相同的权限角色控制能力，不同的是<code>ClusterRole</code>是集群级别的，可以用于:</p><ul><li>集群级别的资源控制(例如 node 访问权限)</li><li>非资源型 endpoints(例如 /healthz 访问)</li><li>所有命名空间资源控制(例如 pods)</li></ul><p>比如我们要创建一个授权某个特定命名空间或全部命名空间(取决于绑定方式)访问<strong>secrets</strong>的集群角色：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  # &quot;namespace&quot; omitted since ClusterRoles are not namespaced</span><br><span class="line">  name: secret-reader</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;secrets&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br></pre></td></tr></table></figure><h2 id="RoleBinding和ClusterRoleBinding"><a href="#RoleBinding和ClusterRoleBinding" class="headerlink" title="RoleBinding和ClusterRoleBinding"></a>RoleBinding和ClusterRoleBinding</h2><p><code>RoloBinding</code>可以将角色中定义的权限授予用户或用户组，<code>RoleBinding</code>包含一组权限列表(<code>subjects</code>)，权限列表中包含有不同形式的待授予权限资源类型(users、groups、service accounts)，<code>RoleBinding</code>适用于某个命名空间内授权，而 <code>ClusterRoleBinding</code>适用于集群范围内的授权。</p><p>比如我们将默认命名空间的<code>pod-reader</code>角色授予用户jane，这样以后该用户在默认命名空间中将具有<code>pod-reader</code>的权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># This role binding allows &quot;jane&quot; to read pods in the &quot;default&quot; namespace.</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-pods</span><br><span class="line">  namespace: default</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: jane</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: pod-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p><code>RoleBinding</code>同样可以引用<code>ClusterRole</code>来对当前 namespace 内用户、用户组或 ServiceAccount 进行授权，这种操作允许集群管理员在整个集群内定义一些通用的 ClusterRole，然后在不同的 namespace 中使用 RoleBinding 来引用</p><p>例如，以下 RoleBinding 引用了一个 ClusterRole，这个 ClusterRole 具有整个集群内对 secrets 的访问权限；但是其授权用户 dave 只能访问 development 空间中的 secrets(因为 RoleBinding 定义在 development 命名空间)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># This role binding allows &quot;dave&quot; to read secrets in the &quot;development&quot; namespace.</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-secrets</span><br><span class="line">  namespace: development # This only grants permissions within the &quot;development&quot; namespace.</span><br><span class="line">subjects:</span><br><span class="line">- kind: User</span><br><span class="line">  name: dave</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: secret-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>最后，使用 ClusterRoleBinding 可以对整个集群中的所有命名空间资源权限进行授权；以下 ClusterRoleBinding 样例展示了授权 manager 组内所有用户在全部命名空间中对 secrets 进行访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># This cluster role binding allows anyone in the &quot;manager&quot; group to read secrets in any namespace.</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: read-secrets-global</span><br><span class="line">subjects:</span><br><span class="line">- kind: Group</span><br><span class="line">  name: manager</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: secret-reader</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><h1 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h1><ul><li>新增一个新的用户sy</li><li>该用户只能对命名空间<code>kube-system</code>下面的<code>pods</code>和<code>deployments</code>进行管理</li></ul><p>第一步新建一个<code>ServiceAccount</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl create sa sy -n kube-system</span><br><span class="line">serviceaccount/sy created</span><br></pre></td></tr></table></figure><p>然后我们新建一个角色<strong>role-sy</strong>：(role.yaml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  name: role-sy</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;&quot;]</span><br><span class="line">  resources: [&quot;pods&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]</span><br><span class="line">- apiGroups: [&quot;extensions&quot;, &quot;apps&quot;]</span><br><span class="line">  resources: [&quot;deployments&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;]</span><br></pre></td></tr></table></figure><p>注意上面的<code>rules</code>规则：管理<code>pods</code>和<code>deployments</code>的权限。</p><p>然后我们创建一个角色绑定，将上面的角色<code>role-sy绑定到**sy**的</code>ServiceAccount`上：(role-bind.yaml)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: role-bind-sy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: sy</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: role-sy</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>分别执行上面两个<code>yaml</code>文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl create -f role.yaml</span><br><span class="line">role.rbac.authorization.k8s.io/role-sy created</span><br><span class="line">[root@node1 dashboard]# kubectl create -f role-bind.yaml </span><br><span class="line">rolebinding.rbac.authorization.k8s.io/role-bind-sy created</span><br></pre></td></tr></table></figure><p>接下来该怎么做？和前面一样的，我们只需要拿到sy这个<code>ServiceAccount</code>的<code>token</code>就可以登录<code>Dashboard</code>了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 dashboard]# kubectl get secret -n kube-system |grep sy</span><br><span class="line">sy-token-5cmnl                                   kubernetes.io/service-account-token   3      3m2s</span><br><span class="line"></span><br><span class="line">[root@node1 dashboard]# kubectl get secret sy-token-5cmnl  -o jsonpath=&#123;.data.token&#125; -n kube-system |base64 -d</span><br><span class="line"># 会生成一串很长的base64后的字符串</span><br></pre></td></tr></table></figure><p>这样就可以控制权限了，需要将登录地址改为namespace=kube-system</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;修改配置文件&quot;&gt;&lt;a href=&quot;#修改配置文件&quot; class=&quot;headerlink&quot; title=&quot;修改配置文件&quot;&gt;&lt;/a&gt;修改配置文件&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gut
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-metrics-server</title>
    <link href="https://shenshengkun.github.io/posts/d3554aa2.html"/>
    <id>https://shenshengkun.github.io/posts/d3554aa2.html</id>
    <published>2019-06-10T03:16:01.000Z</published>
    <updated>2019-06-10T03:22:59.745Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。</p><p>从 Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。</p><p>替代方案如下：</p><ol><li>用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server；</li><li>通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator；</li><li>事件传输：使用第三方工具来传输、归档 kubernetes events；</li></ol><p>Kubernetes Dashboard 还不支持 metrics-server（PR：<a href="https://github.com/kubernetes/dashboard/pull/3504" target="_blank" rel="noopener">#3504</a>），如果使用 metrics-server 替代 Heapster，将无法在 dashboard 中以图形展示 Pod 的内存和 CPU 情况，需要通过 Prometheus、Grafana 等监控方案来弥补。</p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/</span><br><span class="line">git clone https://github.com/kubernetes-incubator/metrics-server.git</span><br><span class="line">cd metrics-server/deploy/1.8+/</span><br><span class="line">ls</span><br><span class="line">aggregated-metrics-reader.yaml  auth-delegator.yaml  auth-reader.yaml  metrics-apiservice.yaml  metrics-server-deployment.yaml  metrics-server-service.yaml  resource-reader.yaml</span><br></pre></td></tr></table></figure><h2 id="修改-metrics-server-deployment-yaml-文件，为-metrics-server-添加三个命令行参数："><a href="#修改-metrics-server-deployment-yaml-文件，为-metrics-server-添加三个命令行参数：" class="headerlink" title="修改 metrics-server-deployment.yaml 文件，为 metrics-server 添加三个命令行参数："></a>修改 <code>metrics-server-deployment.yaml</code> 文件，为 metrics-server 添加三个命令行参数：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: metrics-server</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: metrics-server</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: metrics-server</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: metrics-server</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: metrics-server</span><br><span class="line">      volumes:</span><br><span class="line">      # mount in tmp so we can safely use from-scratch images and/or read-only containers</span><br><span class="line">      - name: tmp-dir</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      containers:</span><br><span class="line">      - name: metrics-server</span><br><span class="line">        image: mirrorgooglecontainers/metrics-server-amd64:v0.3.3</span><br><span class="line">        command:</span><br><span class="line">        - /metrics-server</span><br><span class="line">        - --metric-resolution=30s</span><br><span class="line">        - --requestheader-allowed-names=aggregator</span><br><span class="line">        - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP</span><br><span class="line">        imagePullPolicy: Always</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: tmp-dir</span><br><span class="line">          mountPath: /tmp</span><br></pre></td></tr></table></figure><ul><li>–metric-resolution=30s：从 kubelet 采集数据的周期；</li><li>–requestheader-allowed-names=aggregator：允许请求 metrics-server API 的用户名，该名称与 kube-apiserver 的 <code>--proxy-client-cert-file</code> 指定的证书 CN 一致；</li><li>–kubelet-preferred-address-types：优先使用 InternalIP 来访问 kubelet，这样可以避免节点名称<strong>没有 DNS 解析</strong>记录时，通过节点名称调用节点 kubelet API 失败的情况（未配置时默认的情况）；</li></ul><h2 id="修改apiserver参数："><a href="#修改apiserver参数：" class="headerlink" title="修改apiserver参数："></a>修改apiserver参数：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/systemd/system</span><br><span class="line"></span><br><span class="line">vim kube-apiserver.service</span><br><span class="line"></span><br><span class="line">--requestheader-allowed-names=&quot;aggregator&quot;</span><br></pre></td></tr></table></figure><p>重启apiserver</p><h2 id="部署-metrics-server："><a href="#部署-metrics-server：" class="headerlink" title="部署 metrics-server："></a>部署 metrics-server：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/metrics-server/deploy/1.8+/</span><br><span class="line">kubectl create -f .</span><br></pre></td></tr></table></figure><h2 id="使用-kubectl-top-命令查看集群节点资源使用情况"><a href="#使用-kubectl-top-命令查看集群节点资源使用情况" class="headerlink" title="使用 kubectl top 命令查看集群节点资源使用情况"></a>使用 kubectl top 命令查看集群节点资源使用情况</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 1.8+]# kubectl top node</span><br><span class="line">NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   </span><br><span class="line">node1   117m         5%     2217Mi          57%       </span><br><span class="line">node2   147m         7%     2680Mi          69%</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-coredns</title>
    <link href="https://shenshengkun.github.io/posts/77sa4nc2.html"/>
    <id>https://shenshengkun.github.io/posts/77sa4nc2.html</id>
    <published>2019-06-06T04:32:01.000Z</published>
    <updated>2019-06-21T12:01:56.403Z</updated>
    
    <content type="html"><![CDATA[<h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>1.11后CoreDNS 已取代 Kube DNS 作为集群服务发现元件,由于 Kubernetes 需要让 Pod 与 Pod 之间能夠互相通信,然而要能够通信需要知道彼此的 IP 才行,而这种做法通常是通过 Kubernetes API 来获取,但是 Pod IP 会因为生命周期变化而改变,因此这种做法无法弹性使用,且还会增加 API Server 负担,基于此问题 Kubernetes 提供了 DNS 服务来作为查询,让 Pod 能夠以 Service 名称作为域名来查询 IP 位址,因此使用者就再不需要关心实际 Pod IP,而 DNS 也会根据 Pod 变化更新资源记录(Record resources)</p><p>CoreDNS 是由 CNCF 维护的开源 DNS 方案,该方案前身是 SkyDNS,其采用了 Caddy 的一部分来开发伺服器框架,使其能够建立一套快速灵活的 DNS,而 CoreDNS 每个功能都可以被当作成一個插件的中介软体,如 Log、Cache、Kubernetes 等功能,甚至能够将源记录存储在 Redis、Etcd 中</p><h1 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h1><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>coredns 目录是 <code>cluster/addons/dns</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/kubernetes/cluster/addons/dns/coredns</span><br><span class="line">cp coredns.yaml.base coredns.yaml</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">sed -i -e &quot;s/__PILLAR__DNS__DOMAIN__/$&#123;CLUSTER_DNS_DOMAIN&#125;/&quot; -e &quot;s/__PILLAR__DNS__SERVER__/$&#123;CLUSTER_DNS_SVC_IP&#125;/&quot; coredns.yaml</span><br><span class="line"></span><br><span class="line">还需要将镜像修改下，coredns/coredns:1.3.1</span><br></pre></td></tr></table></figure><h2 id="创建-coredns"><a href="#创建-coredns" class="headerlink" title="创建 coredns"></a>创建 coredns</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f coredns.yaml</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cat&lt;&lt;EOF | kubectl apply -f -</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: busybox</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: busybox</span><br><span class="line">    image: busybox:1.28.3</span><br><span class="line">    command:</span><br><span class="line">      - sleep</span><br><span class="line">      - &quot;3600&quot;</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">  restartPolicy: Always</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>创建成功后，我们进行检查</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod</span><br><span class="line">NAME      READY   STATUS    RESTARTS   AGE</span><br><span class="line">busybox   1/1     Running   0          4s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 coredns]# kubectl exec -ti busybox -- nslookup kubernetes</span><br><span class="line">Server:    10.254.0.2</span><br><span class="line">Address 1: 10.254.0.2 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      kubernetes</span><br><span class="line">Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 ~]# kubectl exec -ti busybox ping kubernetes.default.svc.cluster.local</span><br><span class="line">PING kubernetes.default.svc.cluster.local (10.254.0.1): 56 data bytes</span><br><span class="line">64 bytes from 10.254.0.1: seq=0 ttl=64 time=0.099 ms</span><br><span class="line">^C</span><br><span class="line">--- kubernetes.default.svc.cluster.local ping statistics ---</span><br><span class="line">1 packets transmitted, 1 packets received, 0% packet loss</span><br><span class="line">round-trip min/avg/max = 0.099/0.099/0.099 ms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">注意：用my-svc.my-namespace.svc.cluster.local的方式可以访问服务</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h1&gt;&lt;p&gt;1.11后CoreDNS 已取代 Kube DNS 作为集群服务发现元件,由于 Kubernetes 需要让 Pod 与 Pod 之间能夠互
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s角色</title>
    <link href="https://shenshengkun.github.io/posts/65aa44f.html"/>
    <id>https://shenshengkun.github.io/posts/65aa44f.html</id>
    <published>2019-06-06T02:01:01.000Z</published>
    <updated>2019-06-06T02:13:54.474Z</updated>
    
    <content type="html"><![CDATA[<h1 id="查看node节点"><a href="#查看node节点" class="headerlink" title="查看node节点"></a>查看node节点</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 work]# kubectl get nodes</span><br><span class="line">NAME    STATUS   ROLES    AGE   VERSION</span><br><span class="line">node1   Ready    &lt;none&gt;   41h   v1.14.2</span><br><span class="line">node2   Ready    &lt;none&gt;   41h   v1.14.2</span><br></pre></td></tr></table></figure><h1 id="设置集群角色"><a href="#设置集群角色" class="headerlink" title="设置集群角色"></a>设置集群角色</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 设置 node1 为 master 角色</span><br><span class="line"></span><br><span class="line">kubectl label nodes node1 node-role.kubernetes.io/master=</span><br><span class="line"></span><br><span class="line"># 设置 node2 为 node 角色</span><br><span class="line"></span><br><span class="line">kubectl label nodes node2 node-role.kubernetes.io/node=</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[root@node1 ~]# kubectl get nodes</span><br><span class="line">NAME    STATUS   ROLES    AGE   VERSION</span><br><span class="line">node1   Ready    master   42h   v1.14.2</span><br><span class="line">node2   Ready    node     42h   v1.14.2</span><br></pre></td></tr></table></figure><h1 id="设置taint"><a href="#设置taint" class="headerlink" title="设置taint"></a>设置taint</h1><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node [node] key=value[effect]   </span><br><span class="line">     其中[effect] 可取值: [ NoSchedule | PreferNoSchedule | NoExecute ]</span><br><span class="line">      NoSchedule: 一定不能被调度</span><br><span class="line">      PreferNoSchedule: 尽量不要调度</span><br><span class="line">      NoExecute: 不仅不会调度, 还会驱逐Node上已有的Pod</span><br></pre></td></tr></table></figure><h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl taint nodes node1 node-role.kubernetes.io/master=:NoExecute</span><br><span class="line">node/node1 tainted</span><br><span class="line">[root@node1 ~]# kubectl get pods</span><br><span class="line">NAME             READY   STATUS        RESTARTS   AGE</span><br><span class="line">nginx-ds-kztdz   1/1     Running       0          18h</span><br><span class="line">nginx-ds-vbjh9   0/1     Terminating   0          18h</span><br><span class="line">[root@node1 ~]# kubectl get pods</span><br><span class="line">NAME             READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-ds-kztdz   1/1     Running   0          18h</span><br></pre></td></tr></table></figure><h2 id="查看taint"><a href="#查看taint" class="headerlink" title="查看taint"></a>查看taint</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl describe node node1</span><br><span class="line">Name:               node1</span><br><span class="line">Roles:              master</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/arch=amd64</span><br><span class="line">                    kubernetes.io/hostname=node1</span><br><span class="line">                    kubernetes.io/os=linux</span><br><span class="line">                    node-role.kubernetes.io/master=</span><br><span class="line">Annotations:        node.alpha.kubernetes.io/ttl: 0</span><br><span class="line">                    volumes.kubernetes.io/controller-managed-attach-detach: true</span><br><span class="line">CreationTimestamp:  Tue, 04 Jun 2019 15:28:56 +0800</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoExecute</span><br><span class="line">                    node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">Unschedulable:      false</span><br><span class="line">Conditions:</span><br><span class="line">  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message</span><br><span class="line">  ----             ------  -----------------                 ------------------                ------                       -------</span><br><span class="line">  MemoryPressure   False   Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletHasSufficientMemory   kubelet has sufficient memory available</span><br><span class="line">  DiskPressure     False   Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletHasNoDiskPressure     kubelet has no disk pressure</span><br><span class="line">  PIDPressure      False   Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletHasSufficientPID      kubelet has sufficient PID available</span><br><span class="line">  Ready            True    Thu, 06 Jun 2019 10:08:16 +0800   Tue, 04 Jun 2019 15:28:57 +0800   KubeletReady                 kubelet is posting ready status</span><br><span class="line">Addresses:</span><br><span class="line">  InternalIP:  192.168.6.101</span><br></pre></td></tr></table></figure><h2 id="删除taint"><a href="#删除taint" class="headerlink" title="删除taint"></a>删除taint</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl taint nodes node1 node-role.kubernetes.io/master-</span><br><span class="line">node/node1 untainted</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;查看node节点&quot;&gt;&lt;a href=&quot;#查看node节点&quot; class=&quot;headerlink&quot; title=&quot;查看node节点&quot;&gt;&lt;/a&gt;查看node节点&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td cl
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-node节点</title>
    <link href="https://shenshengkun.github.io/posts/44qq5gb2.html"/>
    <id>https://shenshengkun.github.io/posts/44qq5gb2.html</id>
    <published>2019-06-05T07:33:01.000Z</published>
    <updated>2019-06-25T07:37:30.394Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;yum install -y epel-release&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;yum install -y conntrack ipvsadm ntp ntpdate ipset jq iptables curl sysstat libseccomp &amp;&amp; modprobe ip_vs &quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h1 id="docker"><a href="#docker" class="headerlink" title="docker"></a>docker</h1><h2 id="下载和分发-docker-二进制文件"><a href="#下载和分发-docker-二进制文件" class="headerlink" title="下载和分发 docker 二进制文件"></a>下载和分发 docker 二进制文件</h2><p>到 <a href="https://download.docker.com/linux/static/stable/x86_64/" target="_blank" rel="noopener">docker 下载页面</a> 下载最新发布包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">wget https://download.docker.com/linux/static/stable/x86_64/docker-18.09.6.tgz</span><br><span class="line">tar -xvf docker-18.09.6.tgz</span><br></pre></td></tr></table></figure><p>分发二进制文件到所有 worker 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp docker/*  root@$&#123;node_ip&#125;:/opt/k8s/bin/</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-systemd-unit-文件"><a href="#创建和分发-systemd-unit-文件" class="headerlink" title="创建和分发 systemd unit 文件"></a>创建和分发 systemd unit 文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; docker.service &lt;&lt;&quot;EOF&quot;</span><br><span class="line">[Unit]</span><br><span class="line">Description=Docker Application Container Engine</span><br><span class="line">Documentation=http://docs.docker.io</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=##DOCKER_DIR##</span><br><span class="line">Environment=&quot;PATH=/opt/k8s/bin:/bin:/sbin:/usr/bin:/usr/sbin&quot;</span><br><span class="line">EnvironmentFile=-/run/flannel/docker</span><br><span class="line">ExecStart=/opt/k8s/bin/dockerd $DOCKER_NETWORK_OPTIONS</span><br><span class="line">ExecReload=/bin/kill -s HUP $MAINPID</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=infinity</span><br><span class="line">LimitNPROC=infinity</span><br><span class="line">LimitCORE=infinity</span><br><span class="line">Delegate=yes</span><br><span class="line">KillMode=process</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发 systemd unit 文件到所有 worker 机器:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">sed -i -e &quot;s|##DOCKER_DIR##|$&#123;DOCKER_DIR&#125;|&quot; docker.service</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp docker.service root@$&#123;node_ip&#125;:/etc/systemd/system/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="配置和分发-docker-配置文件"><a href="#配置和分发-docker-配置文件" class="headerlink" title="配置和分发 docker 配置文件"></a>配置和分发 docker 配置文件</h2><p>使用国内的仓库镜像服务器以加快 pull image 的速度，同时增加下载的并发数 (需要重启 dockerd 生效)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; docker-daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;],</span><br><span class="line">    &quot;insecure-registries&quot;: [&quot;docker02:35000&quot;],</span><br><span class="line">    &quot;max-concurrent-downloads&quot;: 20,</span><br><span class="line">    &quot;live-restore&quot;: true,</span><br><span class="line">    &quot;max-concurrent-uploads&quot;: 10,</span><br><span class="line">    &quot;debug&quot;: true,</span><br><span class="line">    &quot;data-root&quot;: &quot;$&#123;DOCKER_DIR&#125;/data&quot;,</span><br><span class="line">    &quot;exec-root&quot;: &quot;$&#123;DOCKER_DIR&#125;/exec&quot;,</span><br><span class="line">    &quot;log-opts&quot;: &#123;</span><br><span class="line">      &quot;max-size&quot;: &quot;100m&quot;,</span><br><span class="line">      &quot;max-file&quot;: &quot;5&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发 docker 配置文件到所有 worker 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p  /etc/docker/ $&#123;DOCKER_DIR&#125;/&#123;data,exec&#125;&quot;</span><br><span class="line">    scp docker-daemon.json root@$&#123;node_ip&#125;:/etc/docker/daemon.json</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="启动-docker-服务"><a href="#启动-docker-服务" class="headerlink" title="启动 docker 服务"></a>启动 docker 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="检查服务运行状态"><a href="#检查服务运行状态" class="headerlink" title="检查服务运行状态"></a>检查服务运行状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status docker|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code></p><h1 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h1><h2 id="创建-kubelet-bootstrap-kubeconfig-文件"><a href="#创建-kubelet-bootstrap-kubeconfig-文件" class="headerlink" title="创建 kubelet bootstrap kubeconfig 文件"></a>创建 kubelet bootstrap kubeconfig 文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_name in $&#123;NODE_NAMES[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;</span><br><span class="line"></span><br><span class="line">    # 创建 token</span><br><span class="line">    export BOOTSTRAP_TOKEN=$(kubeadm token create \</span><br><span class="line">      --description kubelet-bootstrap-token \</span><br><span class="line">      --groups system:bootstrappers:$&#123;node_name&#125; \</span><br><span class="line">      --kubeconfig ~/.kube/config)</span><br><span class="line"></span><br><span class="line">    # 设置集群参数</span><br><span class="line">    kubectl config set-cluster kubernetes \</span><br><span class="line">      --certificate-authority=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">      --embed-certs=true \</span><br><span class="line">      --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig</span><br><span class="line"></span><br><span class="line">    # 设置客户端认证参数</span><br><span class="line">    kubectl config set-credentials kubelet-bootstrap \</span><br><span class="line">      --token=$&#123;BOOTSTRAP_TOKEN&#125; \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig</span><br><span class="line"></span><br><span class="line">    # 设置上下文参数</span><br><span class="line">    kubectl config set-context default \</span><br><span class="line">      --cluster=kubernetes \</span><br><span class="line">      --user=kubelet-bootstrap \</span><br><span class="line">      --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig</span><br><span class="line"></span><br><span class="line">    # 设置默认上下文</span><br><span class="line">    kubectl config use-context default --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>向 kubeconfig 写入的是 token，bootstrap 结束后 kube-controller-manager 为 kubelet 创建 client 和 server 证书；</li></ul><p>查看 kubeadm 为各节点创建的 token： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubeadm token list --kubeconfig ~/.kube/config</span><br><span class="line">TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION               EXTRA GROUPS</span><br><span class="line">kp5seh.klhbcowm40rkaoh1   &lt;invalid&gt;   2019-06-05T15:24:51+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node1</span><br><span class="line">u2zt2n.3tqw704a4ndqdj1k   &lt;invalid&gt;   2019-06-05T15:24:51+08:00   authentication,signing   kubelet-bootstrap-token   system:bootstrappers:node2</span><br><span class="line">[root@node1 ~]#</span><br></pre></td></tr></table></figure><h2 id="分发-bootstrap-kubeconfig-文件到所有-worker-节点"><a href="#分发-bootstrap-kubeconfig-文件到所有-worker-节点" class="headerlink" title="分发 bootstrap kubeconfig 文件到所有 worker 节点"></a>分发 bootstrap kubeconfig 文件到所有 worker 节点</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_name in $&#123;NODE_NAMES[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;</span><br><span class="line">    scp kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig root@$&#123;node_name&#125;:/etc/kubernetes/kubelet-bootstrap.kubeconfig</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kubelet-参数配置文件"><a href="#创建和分发-kubelet-参数配置文件" class="headerlink" title="创建和分发 kubelet 参数配置文件"></a>创建和分发 kubelet 参数配置文件</h2><p>创建 kubelet 参数配置文件模板（可配置项参考<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/config/types.go" target="_blank" rel="noopener">代码中注释</a> ）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kubelet-config.yaml.template &lt;&lt;EOF</span><br><span class="line">kind: KubeletConfiguration</span><br><span class="line">apiVersion: kubelet.config.k8s.io/v1beta1</span><br><span class="line">address: &quot;##NODE_IP##&quot;</span><br><span class="line">staticPodPath: &quot;&quot;</span><br><span class="line">syncFrequency: 1m</span><br><span class="line">fileCheckFrequency: 20s</span><br><span class="line">httpCheckFrequency: 20s</span><br><span class="line">staticPodURL: &quot;&quot;</span><br><span class="line">port: 10250</span><br><span class="line">readOnlyPort: 0</span><br><span class="line">rotateCertificates: true</span><br><span class="line">serverTLSBootstrap: true</span><br><span class="line">authentication:</span><br><span class="line">  anonymous:</span><br><span class="line">    enabled: false</span><br><span class="line">  webhook:</span><br><span class="line">    enabled: true</span><br><span class="line">  x509:</span><br><span class="line">    clientCAFile: &quot;/etc/kubernetes/cert/ca.pem&quot;</span><br><span class="line">authorization:</span><br><span class="line">  mode: Webhook</span><br><span class="line">registryPullQPS: 0</span><br><span class="line">registryBurst: 20</span><br><span class="line">eventRecordQPS: 0</span><br><span class="line">eventBurst: 20</span><br><span class="line">enableDebuggingHandlers: true</span><br><span class="line">enableContentionProfiling: true</span><br><span class="line">healthzPort: 10248</span><br><span class="line">healthzBindAddress: &quot;##NODE_IP##&quot;</span><br><span class="line">clusterDomain: &quot;$&#123;CLUSTER_DNS_DOMAIN&#125;&quot;</span><br><span class="line">clusterDNS:</span><br><span class="line">  - &quot;$&#123;CLUSTER_DNS_SVC_IP&#125;&quot;</span><br><span class="line">nodeStatusUpdateFrequency: 10s</span><br><span class="line">nodeStatusReportFrequency: 1m</span><br><span class="line">imageMinimumGCAge: 2m</span><br><span class="line">imageGCHighThresholdPercent: 85</span><br><span class="line">imageGCLowThresholdPercent: 80</span><br><span class="line">volumeStatsAggPeriod: 1m</span><br><span class="line">kubeletCgroups: &quot;&quot;</span><br><span class="line">systemCgroups: &quot;&quot;</span><br><span class="line">cgroupRoot: &quot;&quot;</span><br><span class="line">cgroupsPerQOS: true</span><br><span class="line">cgroupDriver: cgroupfs</span><br><span class="line">runtimeRequestTimeout: 10m</span><br><span class="line">hairpinMode: promiscuous-bridge</span><br><span class="line">maxPods: 220</span><br><span class="line">podCIDR: &quot;$&#123;CLUSTER_CIDR&#125;&quot;</span><br><span class="line">podPidsLimit: -1</span><br><span class="line">resolvConf: /etc/resolv.conf</span><br><span class="line">maxOpenFiles: 1000000</span><br><span class="line">kubeAPIQPS: 1000</span><br><span class="line">kubeAPIBurst: 2000</span><br><span class="line">serializeImagePulls: false</span><br><span class="line">evictionHard:</span><br><span class="line">  memory.available:  &quot;100Mi&quot;</span><br><span class="line">nodefs.available:  &quot;10%&quot;</span><br><span class="line">nodefs.inodesFree: &quot;5%&quot;</span><br><span class="line">imagefs.available: &quot;15%&quot;</span><br><span class="line">evictionSoft: &#123;&#125;</span><br><span class="line">enableControllerAttachDetach: true</span><br><span class="line">failSwapOn: true</span><br><span class="line">containerLogMaxSize: 20Mi</span><br><span class="line">containerLogMaxFiles: 10</span><br><span class="line">systemReserved: &#123;&#125;</span><br><span class="line">kubeReserved: &#123;&#125;</span><br><span class="line">systemReservedCgroup: &quot;&quot;</span><br><span class="line">kubeReservedCgroup: &quot;&quot;</span><br><span class="line">enforceNodeAllocatable: [&quot;pods&quot;]</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>address：kubelet 安全端口（https，10250）监听的地址，不能为 127.0.0.1，否则 kube-apiserver、heapster 等不能调用 kubelet 的 API；</li><li>readOnlyPort=0：关闭只读端口(默认 10255)，等效为未指定；</li><li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li><li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTP 证书认证；</li><li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li><li>对于未通过 x509 证书和 webhook 认证的请求(kube-apiserver 或其他客户端)，将被拒绝，提示 Unauthorized；</li><li>authroization.mode=Webhook：kubelet 使用 SubjectAccessReview API 查询 kube-apiserver 某 user、group 是否具有操作资源的权限(RBAC)；</li><li>featureGates.RotateKubeletClientCertificate、featureGates.RotateKubeletServerCertificate：自动 rotate 证书，证书的有效期取决于 kube-controller-manager 的 –experimental-cluster-signing-duration 参数；</li><li>需要 root 账户运行；</li></ul><p>为各节点创建和分发 kubelet 配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do </span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    sed -e &quot;s/##NODE_IP##/$&#123;node_ip&#125;/&quot; kubelet-config.yaml.template &gt; kubelet-config-$&#123;node_ip&#125;.yaml.template</span><br><span class="line">    scp kubelet-config-$&#123;node_ip&#125;.yaml.template root@$&#123;node_ip&#125;:/etc/kubernetes/kubelet-config.yaml</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kubelet-systemd-unit-文件"><a href="#创建和分发-kubelet-systemd-unit-文件" class="headerlink" title="创建和分发 kubelet systemd unit 文件"></a>创建和分发 kubelet systemd unit 文件</h2><p>创建 kubelet systemd unit 文件模板：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kubelet.service.template &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kubelet</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=docker.service</span><br><span class="line">Requires=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=$&#123;K8S_DIR&#125;/kubelet</span><br><span class="line">ExecStart=/opt/k8s/bin/kubelet \\</span><br><span class="line">  --allow-privileged=true \\</span><br><span class="line">  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \\</span><br><span class="line">  --cert-dir=/etc/kubernetes/cert \\</span><br><span class="line">  --cni-conf-dir=/etc/cni/net.d \\</span><br><span class="line">  --container-runtime=docker \\</span><br><span class="line">  --container-runtime-endpoint=unix:///var/run/dockershim.sock \\</span><br><span class="line">  --root-dir=$&#123;K8S_DIR&#125;/kubelet \\</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\</span><br><span class="line">  --config=/etc/kubernetes/kubelet-config.yaml \\</span><br><span class="line">  --hostname-override=##NODE_NAME## \\</span><br><span class="line">  --pod-infra-container-image=registry.cn-beijing.aliyuncs.com/k8s_images/pause-amd64:3.1 \\</span><br><span class="line">  --image-pull-progress-deadline=15m \\</span><br><span class="line">  --volume-plugin-dir=$&#123;K8S_DIR&#125;/kubelet/kubelet-plugins/volume/exec/ \\</span><br><span class="line">  --logtostderr=true \\</span><br><span class="line">  --v=2</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>如果设置了 <code>--hostname-override</code> 选项，则 <code>kube-proxy</code> 也需要设置该选项，否则会出现找不到 Node 的情况；</li><li><code>--bootstrap-kubeconfig</code>：指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求；</li><li>K8S approve kubelet 的 csr 请求后，在 <code>--cert-dir</code> 目录创建证书和私钥文件，然后写入 <code>--kubeconfig</code> 文件；</li><li><code>--pod-infra-container-image</code> 不使用 redhat 的 <code>pod-infrastructure:latest</code> 镜像，它不能回收容器的僵尸；</li></ul><p>为各节点创建和分发 kubelet systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_name in $&#123;NODE_NAMES[@]&#125;</span><br><span class="line">  do </span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;node_name&#125;/&quot; kubelet.service.template &gt; kubelet-$&#123;node_name&#125;.service</span><br><span class="line">    scp kubelet-$&#123;node_name&#125;.service root@$&#123;node_name&#125;:/etc/systemd/system/kubelet.service</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="Bootstrap-Token-Auth-和授予权限"><a href="#Bootstrap-Token-Auth-和授予权限" class="headerlink" title="Bootstrap Token Auth 和授予权限"></a>Bootstrap Token Auth 和授予权限</h2><p>kubelet 启动时查找 <code>--kubeletconfig</code> 参数对应的文件是否存在，如果不存在则使用 <code>--bootstrap-kubeconfig</code> 指定的 kubeconfig 文件向 kube-apiserver 发送证书签名请求 (CSR)。</p><p>kube-apiserver 收到 CSR 请求后，对其中的 Token 进行认证，认证通过后将请求的 user 设置为 <code>system:bootstrap:&lt;Token ID&gt;</code>，group 设置为 <code>system:bootstrappers</code>，这一过程称为 Bootstrap Token Auth。</p><p>如果说kubelet启动失败的话：</p><p>创建一个 clusterrolebinding，将 group system:bootstrappers 和 clusterrole system:node-bootstrapper 绑定：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers</span><br></pre></td></tr></table></figure><h2 id="启动-kubelet-服务"><a href="#启动-kubelet-服务" class="headerlink" title="启动 kubelet 服务"></a>启动 kubelet 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kubelet/kubelet-plugins/volume/exec/&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;/usr/sbin/swapoff -a&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>kubelet 启动后使用 –bootstrap-kubeconfig 向 kube-apiserver 发送 CSR 请求，当这个 CSR 被 approve 后，kube-controller-manager 为 kubelet 创建 TLS 客户端证书、私钥和 –kubeletconfig 文件。</p><p>注意：kube-controller-manager 需要配置 <code>--cluster-signing-cert-file</code> 和 <code>--cluster-signing-key-file</code>参数，才会为 TLS Bootstrap 创建证书和私钥。</p><h2 id="自动-approve-CSR-请求"><a href="#自动-approve-CSR-请求" class="headerlink" title="自动 approve CSR 请求"></a>自动 approve CSR 请求</h2><p>创建三个 ClusterRoleBinding，分别用于自动 approve client、renew client、renew server 证书：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; csr-crb.yaml &lt;&lt;EOF</span><br><span class="line"> # Approve all CSRs for the group &quot;system:bootstrappers&quot;</span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: auto-approve-csrs-for-group</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:bootstrappers</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: system:certificates.k8s.io:certificatesigningrequests:nodeclient</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line"> # To let a node of the group &quot;system:nodes&quot; renew its own credentials</span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: node-client-cert-renewal</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:nodes</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line"># A ClusterRole which instructs the CSR approver to approve a node requesting a</span><br><span class="line"># serving cert matching its client cert.</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: approve-node-server-renewal-csr</span><br><span class="line">rules:</span><br><span class="line">- apiGroups: [&quot;certificates.k8s.io&quot;]</span><br><span class="line">  resources: [&quot;certificatesigningrequests/selfnodeserver&quot;]</span><br><span class="line">  verbs: [&quot;create&quot;]</span><br><span class="line">---</span><br><span class="line"> # To let a node of the group &quot;system:nodes&quot; renew its own server credentials</span><br><span class="line"> kind: ClusterRoleBinding</span><br><span class="line"> apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line"> metadata:</span><br><span class="line">   name: node-server-cert-renewal</span><br><span class="line"> subjects:</span><br><span class="line"> - kind: Group</span><br><span class="line">   name: system:nodes</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line"> roleRef:</span><br><span class="line">   kind: ClusterRole</span><br><span class="line">   name: approve-node-server-renewal-csr</span><br><span class="line">   apiGroup: rbac.authorization.k8s.io</span><br><span class="line">EOF</span><br><span class="line">kubectl apply -f csr-crb.yaml</span><br></pre></td></tr></table></figure><ul><li>auto-approve-csrs-for-group：自动 approve node 的第一次 CSR； 注意第一次 CSR 时，请求的 Group 为 system:bootstrappers；</li><li>node-client-cert-renewal：自动 approve node 后续过期的 client 证书，自动生成的证书 Group 为 system:nodes;</li><li>node-server-cert-renewal：自动 approve node 后续过期的 server 证书，自动生成的证书 Group 为 system:nodes;</li></ul><h2 id="手动-approve-server-cert-csr"><a href="#手动-approve-server-cert-csr" class="headerlink" title="手动 approve server cert csr"></a>手动 approve server cert csr</h2><p>基于<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#kubelet-configuration" target="_blank" rel="noopener">安全性考虑</a>，CSR approving controllers 不会自动 approve kubelet server 证书签名请求，需要手动 approve：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get csr</span><br><span class="line">NAME        AGE     REQUESTOR                    CONDITION</span><br><span class="line">csr-5f4vh   9m25s   system:bootstrap:82jfrm      Approved,Issued</span><br><span class="line">csr-5r7j7   6m11s   system:node:zhangjun-k8s03   Pending</span><br><span class="line">csr-5rw7s   9m23s   system:bootstrap:b1f7np      Approved,Issued</span><br><span class="line">csr-9snww   8m3s    system:bootstrap:82jfrm      Approved,Issued</span><br><span class="line">csr-c7z56   6m12s   system:node:zhangjun-k8s02   Pending</span><br><span class="line">csr-j55lh   6m12s   system:node:zhangjun-k8s01   Pending</span><br><span class="line">csr-m29fm   9m25s   system:bootstrap:3gzd53      Approved,Issued</span><br><span class="line">csr-rc8w7   8m3s    system:bootstrap:3gzd53      Approved,Issued</span><br><span class="line">csr-vd52r   8m2s    system:bootstrap:b1f7np      Approved,Issued</span><br><span class="line"></span><br><span class="line">$ kubectl certificate approve csr-5r7j7</span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-5r7j7 approved</span><br><span class="line"></span><br><span class="line">$ kubectl certificate approve csr-c7z56</span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-c7z56 approved</span><br><span class="line"></span><br><span class="line">$ kubectl certificate approve csr-j55lh</span><br><span class="line">certificatesigningrequest.certificates.k8s.io/csr-j55lh approved</span><br></pre></td></tr></table></figure><h2 id="kubelet-提供的-API-接口"><a href="#kubelet-提供的-API-接口" class="headerlink" title="kubelet 提供的 API 接口"></a>kubelet 提供的 API 接口</h2><ul><li>10248: healthz http 服务；</li><li>10250: https 服务，访问该端口时需要认证和授权（即使访问 /healthz 也需要）；</li><li>未开启只读端口 10255；</li><li>从 K8S v1.10 开始，去除了 <code>--cadvisor-port</code> 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp; API。</li></ul><p>例如执行 <code>kubectl exec -it nginx-ds-5rmws -- sh</code> 命令时，kube-apiserver 会向 kubelet 发送如下请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">POST /exec/default/nginx-ds-5rmws/my-nginx?command=sh&amp;input=1&amp;output=1&amp;tty=1</span><br></pre></td></tr></table></figure><p>kubelet 接收 10250 端口的 https 请求，可以访问如下资源：</p><ul><li>/pods、/runningpods</li><li>/metrics、/metrics/cadvisor、/metrics/probes</li><li>/spec</li><li>/stats、/stats/container</li><li>/logs</li><li>/run/、/exec/, /attach/, /portForward/, /containerLogs/</li></ul><p>详情参考：<a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3</a></p><p>由于关闭了匿名认证，同时开启了 webhook 授权，所有访问 10250 端口 https API 的请求都需要被认证和授权。</p><p>预定义的 ClusterRole system:kubelet-api-admin 授予访问 kubelet 所有 API 的权限(kube-apiserver 使用的 kubernetes 证书 User 授予了该权限)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe clusterrole system:kubelet-api-admin</span><br><span class="line">Name:         system:kubelet-api-admin</span><br><span class="line">Labels:       kubernetes.io/bootstrapping=rbac-defaults</span><br><span class="line">Annotations:  rbac.authorization.kubernetes.io/autoupdate=true</span><br><span class="line">PolicyRule:</span><br><span class="line">  Resources      Non-Resource URLs  Resource Names  Verbs</span><br><span class="line">  ---------      -----------------  --------------  -----</span><br><span class="line">  nodes          []                 []              [get list watch proxy]</span><br><span class="line">  nodes/log      []                 []              [*]</span><br><span class="line">  nodes/metrics  []                 []              [*]</span><br><span class="line">  nodes/proxy    []                 []              [*]</span><br><span class="line">  nodes/spec     []                 []              [*]</span><br><span class="line">  nodes/stats    []                 []              [*]</span><br></pre></td></tr></table></figure><h2 id="kubelet-api-认证和授权"><a href="#kubelet-api-认证和授权" class="headerlink" title="kubelet api 认证和授权"></a>kubelet api 认证和授权</h2><p>kubelet 配置了如下认证参数：</p><ul><li>authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口；</li><li>authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTPs 证书认证；</li><li>authentication.webhook.enabled=true：开启 HTTPs bearer token 认证；</li></ul><p>同时配置了如下授权参数：</p><ul><li>authroization.mode=Webhook：开启 RBAC 授权；</li></ul><p>kubelet 收到请求后，使用 clientCAFile 对证书签名进行认证，或者查询 bearer token 是否有效。如果两者都没通过，则拒绝请求，提示 Unauthorized：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ curl -s --cacert /etc/kubernetes/cert/ca.pem https://192.168.6.101:10250/metrics</span><br><span class="line">Unauthorized</span><br><span class="line"></span><br><span class="line">$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer 123456&quot; https://192.168.6.101:10250/metrics</span><br><span class="line">Unauthorized</span><br></pre></td></tr></table></figure><p>通过认证后，kubelet 使用 SubjectAccessReview API 向 kube-apiserver 发送请求，查询证书或 token 对应的 user、group 是否有操作资源的权限(RBAC)；</p><h3 id="证书认证和授权"><a href="#证书认证和授权" class="headerlink" title="证书认证和授权"></a>证书认证和授权</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ # 权限不足的证书；</span><br><span class="line">$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /etc/kubernetes/cert/kube-controller-manager.pem --key /etc/kubernetes/cert/kube-controller-manager-key.pem https://192.168.6.101:10250/metrics</span><br><span class="line">Forbidden (user=system:kube-controller-manager, verb=get, resource=nodes, subresource=metrics)</span><br><span class="line"></span><br><span class="line">$ # 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；</span><br><span class="line">$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://192.168.6.101:10250/metrics|head</span><br><span class="line"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span><br><span class="line"># TYPE apiserver_audit_event_total counter</span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span><br><span class="line"># TYPE apiserver_audit_requests_rejected_total counter</span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span><br><span class="line"># TYPE apiserver_client_certificate_expiration_seconds histogram</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;0&quot;&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;1800&quot;&#125; 0</span><br></pre></td></tr></table></figure><ul><li><code>--cacert</code>、<code>--cert</code>、<code>--key</code> 的参数值必须是文件路径，如上面的 <code>./admin.pem</code> 不能省略 <code>./</code>，否则返回 <code>401 Unauthorized</code>；</li></ul><h3 id="bear-token-认证和授权"><a href="#bear-token-认证和授权" class="headerlink" title="bear token 认证和授权"></a>bear token 认证和授权</h3><p>创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">kubectl create sa kubelet-api-test</span><br><span class="line">kubectl create clusterrolebinding kubelet-api-test --clusterrole=system:kubelet-api-admin --serviceaccount=default:kubelet-api-test</span><br><span class="line">SECRET=$(kubectl get secrets | grep kubelet-api-test | awk &apos;&#123;print $1&#125;&apos;)</span><br><span class="line">TOKEN=$(kubectl describe secret $&#123;SECRET&#125; | grep -E &apos;^token&apos; | awk &apos;&#123;print $2&#125;&apos;)</span><br><span class="line">echo $&#123;TOKEN&#125;</span><br><span class="line">$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer $&#123;TOKEN&#125;&quot; https://192.168.6.101:10250/metrics|head</span><br><span class="line"># HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.</span><br><span class="line"># TYPE apiserver_audit_event_total counter</span><br><span class="line">apiserver_audit_event_total 0</span><br><span class="line"># HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.</span><br><span class="line"># TYPE apiserver_audit_requests_rejected_total counter</span><br><span class="line">apiserver_audit_requests_rejected_total 0</span><br><span class="line"># HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.</span><br><span class="line"># TYPE apiserver_client_certificate_expiration_seconds histogram</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;0&quot;&#125; 0</span><br><span class="line">apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;1800&quot;&#125; 0</span><br></pre></td></tr></table></figure><h1 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h1><h2 id="创建-kube-proxy-证书"><a href="#创建-kube-proxy-证书" class="headerlink" title="创建 kube-proxy 证书"></a>创建 kube-proxy 证书</h2><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-proxy-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;system:kube-proxy&quot;,</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>CN：指定该证书的 User 为 <code>system:kube-proxy</code>；</li><li>预定义的 RoleBinding <code>system:node-proxier</code> 将User <code>system:kube-proxy</code> 与 Role <code>system:node-proxier</code> 绑定，该 Role 授予了调用 <code>kube-apiserver</code> Proxy 相关 API 的权限；</li><li>该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空；</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy</span><br><span class="line">ls kube-proxy*</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kubeconfig-文件"><a href="#创建和分发-kubeconfig-文件" class="headerlink" title="创建和分发 kubeconfig 文件"></a>创建和分发 kubeconfig 文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/opt/k8s/work/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials kube-proxy \</span><br><span class="line">  --client-certificate=kube-proxy.pem \</span><br><span class="line">  --client-key=kube-proxy-key.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-context default \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=kube-proxy \</span><br><span class="line">  --kubeconfig=kube-proxy.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig</span><br></pre></td></tr></table></figure><ul><li><code>--embed-certs=true</code>：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl-proxy.kubeconfig 文件中(不加时，写入的是证书文件路径)；</li></ul><p>分发 kubeconfig 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_name in $&#123;NODE_NAMES[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;</span><br><span class="line">    scp kube-proxy.kubeconfig root@$&#123;node_name&#125;:/etc/kubernetes/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-kube-proxy-配置文件"><a href="#创建-kube-proxy-配置文件" class="headerlink" title="创建 kube-proxy 配置文件"></a>创建 kube-proxy 配置文件</h2><p>从 v1.10 开始，kube-proxy <strong>部分参数</strong>可以配置文件中配置。可以使用 <code>--write-config-to</code> 选项生成该配置文件，或者参考 <a href="https://github.com/kubernetes/kubernetes/blob/release-1.14/pkg/proxy/apis/config/types.go" target="_blank" rel="noopener">源代码的注释</a>。</p><p>创建 kube-proxy config 文件模板：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-proxy-config.yaml.template &lt;&lt;EOF</span><br><span class="line">kind: KubeProxyConfiguration</span><br><span class="line">apiVersion: kubeproxy.config.k8s.io/v1alpha1</span><br><span class="line">clientConnection:</span><br><span class="line">  burst: 200</span><br><span class="line">  kubeconfig: &quot;/etc/kubernetes/kube-proxy.kubeconfig&quot;</span><br><span class="line">  qps: 100</span><br><span class="line">bindAddress: ##NODE_IP##</span><br><span class="line">healthzBindAddress: ##NODE_IP##:10256</span><br><span class="line">metricsBindAddress: ##NODE_IP##:10249</span><br><span class="line">enableProfiling: true</span><br><span class="line">clusterCIDR: $&#123;CLUSTER_CIDR&#125;</span><br><span class="line">hostnameOverride: ##NODE_NAME##</span><br><span class="line">mode: &quot;ipvs&quot;</span><br><span class="line">portRange: &quot;&quot;</span><br><span class="line">kubeProxyIPTablesConfiguration:</span><br><span class="line">  masqueradeAll: false</span><br><span class="line">kubeProxyIPVSConfiguration:</span><br><span class="line">  scheduler: rr</span><br><span class="line">  excludeCIDRs: []</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>bindAddress</code>: 监听地址；</li><li><code>clientConnection.kubeconfig</code>: 连接 apiserver 的 kubeconfig 文件；</li><li><code>clusterCIDR</code>: kube-proxy 根据 <code>--cluster-cidr</code> 判断集群内部和外部流量，指定 <code>--cluster-cidr</code> 或 <code>--masquerade-all</code> 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT；</li><li><code>hostnameOverride</code>: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则；</li><li><code>mode</code>: 使用 ipvs 模式；</li></ul><p>为各节点创建和分发 kube-proxy 配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 3; i++ ))</span><br><span class="line">  do </span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;NODE_NAMES[i]&#125;&quot;</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-proxy-config.yaml.template &gt; kube-proxy-config-$&#123;NODE_NAMES[i]&#125;.yaml.template</span><br><span class="line">    scp kube-proxy-config-$&#123;NODE_NAMES[i]&#125;.yaml.template root@$&#123;NODE_NAMES[i]&#125;:/etc/kubernetes/kube-proxy-config.yaml</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kube-proxy-systemd-unit-文件"><a href="#创建和分发-kube-proxy-systemd-unit-文件" class="headerlink" title="创建和分发 kube-proxy systemd unit 文件"></a>创建和分发 kube-proxy systemd unit 文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kube-proxy.service &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Kube-Proxy Server</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=$&#123;K8S_DIR&#125;/kube-proxy</span><br><span class="line">ExecStart=/opt/k8s/bin/kube-proxy \\</span><br><span class="line">  --config=/etc/kubernetes/kube-proxy-config.yaml \\</span><br><span class="line">  --logtostderr=true \\</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发 kube-proxy systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_name in $&#123;NODE_NAMES[@]&#125;</span><br><span class="line">  do </span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot;</span><br><span class="line">    scp kube-proxy.service root@$&#123;node_name&#125;:/etc/systemd/system/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="启动-kube-proxy-服务"><a href="#启动-kube-proxy-服务" class="headerlink" title="启动 kube-proxy 服务"></a>启动 kube-proxy 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-proxy&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;modprobe ip_vs_rr&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>启动服务前必须先创建工作目录；</li></ul><h2 id="检查启动结果"><a href="#检查启动结果" class="headerlink" title="检查启动结果"></a>检查启动结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-proxy|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code></p><h2 id="查看-ipvs-路由规则"><a href="#查看-ipvs-路由规则" class="headerlink" title="查看 ipvs 路由规则"></a>查看 ipvs 路由规则</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;/usr/sbin/ipvsadm -ln&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;安装依赖包&quot;&gt;&lt;a href=&quot;#安装依赖包&quot; class=&quot;headerlink&quot; title=&quot;安装依赖包&quot;&gt;&lt;/a&gt;安装依赖包&lt;/h1&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-controller、schedule</title>
    <link href="https://shenshengkun.github.io/posts/544ccaa2.html"/>
    <id>https://shenshengkun.github.io/posts/544ccaa2.html</id>
    <published>2019-06-05T07:16:01.000Z</published>
    <updated>2019-06-05T08:52:13.176Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kube-controller-manager-集群"><a href="#kube-controller-manager-集群" class="headerlink" title="kube-controller-manager 集群"></a>kube-controller-manager 集群</h1><h2 id="创建-kube-controller-manager-证书和私钥"><a href="#创建-kube-controller-manager-证书和私钥" class="headerlink" title="创建 kube-controller-manager 证书和私钥"></a>创建 kube-controller-manager 证书和私钥</h2><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-controller-manager-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">      &quot;127.0.0.1&quot;,</span><br><span class="line">      &quot;192.168.6.101&quot;,</span><br><span class="line">      &quot;192.168.6.102&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">        &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">        &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">        &quot;O&quot;: &quot;system:kube-controller-manager&quot;,</span><br><span class="line">        &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>hosts 列表包含<strong>所有</strong> kube-controller-manager 节点 IP；</li><li>CN 和 O 均为 <code>system:kube-controller-manager</code>，kubernetes 内置的 ClusterRoleBindings <code>system:kube-controller-manager</code> 赋予 kube-controller-manager 工作所需的权限。</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager</span><br></pre></td></tr></table></figure><p>将生成的证书和私钥分发到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-controller-manager*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kubeconfig-文件"><a href="#创建和分发-kubeconfig-文件" class="headerlink" title="创建和分发 kubeconfig 文件"></a>创建和分发 kubeconfig 文件</h2><p>kube-controller-manager 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-controller-manager 证书：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/opt/k8s/work/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials system:kube-controller-manager \</span><br><span class="line">  --client-certificate=kube-controller-manager.pem \</span><br><span class="line">  --client-key=kube-controller-manager-key.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-context system:kube-controller-manager \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=system:kube-controller-manager \</span><br><span class="line">  --kubeconfig=kube-controller-manager.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig</span><br></pre></td></tr></table></figure><p>分发 kubeconfig 到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-controller-manager.kubeconfig root@$&#123;node_ip&#125;:/etc/kubernetes/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-kube-controller-manager-systemd-unit-模板文件"><a href="#创建-kube-controller-manager-systemd-unit-模板文件" class="headerlink" title="创建 kube-controller-manager systemd unit 模板文件"></a>创建 kube-controller-manager systemd unit 模板文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kube-controller-manager.service.template &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Controller Manager</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=$&#123;K8S_DIR&#125;/kube-controller-manager</span><br><span class="line">ExecStart=/opt/k8s/bin/kube-controller-manager \\</span><br><span class="line">  --profiling \\</span><br><span class="line">  --cluster-name=kubernetes \\</span><br><span class="line">  --controllers=*,bootstrapsigner,tokencleaner \\</span><br><span class="line">  --kube-api-qps=1000 \\</span><br><span class="line">  --kube-api-burst=2000 \\</span><br><span class="line">  --leader-elect \\</span><br><span class="line">  --use-service-account-credentials\\</span><br><span class="line">  --concurrent-service-syncs=2 \\</span><br><span class="line">  --bind-address=##NODE_IP## \\</span><br><span class="line">  --secure-port=10252 \\</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/cert/kube-controller-manager.pem \\</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/cert/kube-controller-manager-key.pem \\</span><br><span class="line">  --port=0 \\</span><br><span class="line">  --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\</span><br><span class="line">  --client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --requestheader-allowed-names=&quot;&quot; \\</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \\</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \\</span><br><span class="line">  --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\</span><br><span class="line">  --cluster-signing-cert-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --cluster-signing-key-file=/etc/kubernetes/cert/ca-key.pem \\</span><br><span class="line">  --experimental-cluster-signing-duration=8760h \\</span><br><span class="line">  --horizontal-pod-autoscaler-sync-period=10s \\</span><br><span class="line">  --concurrent-deployment-syncs=10 \\</span><br><span class="line">  --concurrent-gc-syncs=30 \\</span><br><span class="line">  --node-cidr-mask-size=24 \\</span><br><span class="line">  --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\</span><br><span class="line">  --pod-eviction-timeout=6m \\</span><br><span class="line">  --terminated-pod-gc-threshold=10000 \\</span><br><span class="line">  --root-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --service-account-private-key-file=/etc/kubernetes/cert/ca-key.pem \\</span><br><span class="line">  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\</span><br><span class="line">  --logtostderr=true \\</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>--port=0</code>：关闭监听非安全端口（http），同时 <code>--address</code> 参数无效，<code>--bind-address</code> 参数有效；</li><li><code>--secure-port=10252</code>、<code>--bind-address=0.0.0.0</code>: 在所有网络接口监听 10252 端口的 https /metrics 请求；</li><li><code>--kubeconfig</code>：指定 kubeconfig 文件路径，kube-controller-manager 使用它连接和验证 kube-apiserver；</li><li><code>--authentication-kubeconfig</code> 和 <code>--authorization-kubeconfig</code>：kube-controller-manager 使用它连接 apiserver，对 client 的请求进行认证和授权。<code>kube-controller-manager</code> 不再使用 <code>--tls-ca-file</code>对请求 https metrics 的 Client 证书进行校验。如果没有配置这两个 kubeconfig 参数，则 client 连接 kube-controller-manager https 端口的请求会被拒绝(提示权限不足)。</li><li><code>--cluster-signing-*-file</code>：签名 TLS Bootstrap 创建的证书；</li><li><code>--experimental-cluster-signing-duration</code>：指定 TLS Bootstrap 证书的有效期；</li><li><code>--root-ca-file</code>：放置到容器 ServiceAccount 中的 CA 证书，用来对 kube-apiserver 的证书进行校验；</li><li><code>--service-account-private-key-file</code>：签名 ServiceAccount 中 Token 的私钥文件，必须和 kube-apiserver 的 <code>--service-account-key-file</code> 指定的公钥文件配对使用；</li><li><code>--service-cluster-ip-range</code> ：指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致；</li><li><code>--leader-elect=true</code>：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li><li><code>--controllers=*,bootstrapsigner,tokencleaner</code>：启用的控制器列表，tokencleaner 用于自动清理过期的 Bootstrap token；</li><li><code>--horizontal-pod-autoscaler-*</code>：custom metrics 相关参数，支持 autoscaling/v2alpha1；</li><li><code>--tls-cert-file</code>、<code>--tls-private-key-file</code>：使用 https 输出 metrics 时使用的 Server 证书和秘钥；</li><li><code>--use-service-account-credentials=true</code>: kube-controller-manager 中各 controller 使用 serviceaccount 访问 kube-apiserver；</li></ul><h2 id="创建和分发-kube-controller-mananger-systemd-unit-文件"><a href="#创建和分发-kube-controller-mananger-systemd-unit-文件" class="headerlink" title="创建和分发 kube-controller-mananger systemd unit 文件"></a>创建和分发 kube-controller-mananger systemd unit 文件</h2><p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 2; i++ ))</span><br><span class="line">  do</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-controller-manager.service.template &gt; kube-controller-manager-$&#123;NODE_IPS[i]&#125;.service </span><br><span class="line">  done</span><br><span class="line">ls kube-controller-manager*.service</span><br></pre></td></tr></table></figure><ul><li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li></ul><p>分发到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-controller-manager-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-controller-manager.service</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>文件重命名为 kube-controller-manager.service;</li></ul><h2 id="启动-kube-controller-manager-服务"><a href="#启动-kube-controller-manager-服务" class="headerlink" title="启动 kube-controller-manager 服务"></a>启动 kube-controller-manager 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-controller-manager&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl restart kube-controller-manager&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>启动服务前必须先创建工作目录；</li></ul><h2 id="检查服务运行状态"><a href="#检查服务运行状态" class="headerlink" title="检查服务运行状态"></a>检查服务运行状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-controller-manager|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -u kube-controller-manager</span><br></pre></td></tr></table></figure><h2 id="查看当前的-leader"><a href="#查看当前的-leader" class="headerlink" title="查看当前的 leader"></a>查看当前的 leader</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get endpoints kube-controller-manager --namespace=kube-system  -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    control-plane.alpha.kubernetes.io/leader: &apos;&#123;&quot;holderIdentity&quot;:&quot;node1_3e3a8815-8698-11e9-87d5-005056b16e40&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2019-06-04T07:14:15Z&quot;,&quot;renewTime&quot;:&quot;2019-06-05T07:22:40Z&quot;,&quot;leaderTransitions&quot;:2&#125;&apos;</span><br><span class="line">  creationTimestamp: &quot;2019-06-04T07:00:39Z&quot;</span><br><span class="line">  name: kube-controller-manager</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: &quot;124731&quot;</span><br><span class="line">  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager</span><br><span class="line">  uid: 75e30eec-8696-11e9-b371-005056b1d2de</span><br></pre></td></tr></table></figure><h1 id="kube-scheduler-集群"><a href="#kube-scheduler-集群" class="headerlink" title="kube-scheduler 集群"></a>kube-scheduler 集群</h1><h2 id="创建-kube-scheduler-证书和私钥"><a href="#创建-kube-scheduler-证书和私钥" class="headerlink" title="创建 kube-scheduler 证书和私钥"></a>创建 kube-scheduler 证书和私钥</h2><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-scheduler-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;CN&quot;: &quot;system:kube-scheduler&quot;,</span><br><span class="line">    &quot;hosts&quot;: [</span><br><span class="line">      &quot;127.0.0.1&quot;,</span><br><span class="line">      &quot;192.168.6.101&quot;,</span><br><span class="line">      &quot;192.168.6.102&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;key&quot;: &#123;</span><br><span class="line">        &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">        &quot;size&quot;: 2048</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;names&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">        &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">        &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">        &quot;O&quot;: &quot;system:kube-scheduler&quot;,</span><br><span class="line">        &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>hosts 列表包含<strong>所有</strong> kube-scheduler 节点 IP；</li><li>CN 和 O 均为 <code>system:kube-scheduler</code>，kubernetes 内置的 ClusterRoleBindings <code>system:kube-scheduler</code> 将赋予 kube-scheduler 工作所需的权限；</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler</span><br><span class="line">ls kube-scheduler*pem</span><br></pre></td></tr></table></figure><p>将生成的证书和私钥分发到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-scheduler*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建和分发-kubeconfig-文件-1"><a href="#创建和分发-kubeconfig-文件-1" class="headerlink" title="创建和分发 kubeconfig 文件"></a>创建和分发 kubeconfig 文件</h2><p>kube-scheduler 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-scheduler 证书：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/opt/k8s/work/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-credentials system:kube-scheduler \</span><br><span class="line">  --client-certificate=kube-scheduler.pem \</span><br><span class="line">  --client-key=kube-scheduler-key.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config set-context system:kube-scheduler \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=system:kube-scheduler \</span><br><span class="line">  --kubeconfig=kube-scheduler.kubeconfig</span><br><span class="line"></span><br><span class="line">kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig</span><br></pre></td></tr></table></figure><p>分发 kubeconfig 到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-scheduler.kubeconfig root@$&#123;node_ip&#125;:/etc/kubernetes/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-kube-scheduler-配置文件"><a href="#创建-kube-scheduler-配置文件" class="headerlink" title="创建 kube-scheduler 配置文件"></a>创建 kube-scheduler 配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt;kube-scheduler.yaml.template &lt;&lt;EOF</span><br><span class="line">apiVersion: kubescheduler.config.k8s.io/v1alpha1</span><br><span class="line">kind: KubeSchedulerConfiguration</span><br><span class="line">bindTimeoutSeconds: 600</span><br><span class="line">clientConnection:</span><br><span class="line">  burst: 200</span><br><span class="line">  kubeconfig: &quot;/etc/kubernetes/kube-scheduler.kubeconfig&quot;</span><br><span class="line">  qps: 100</span><br><span class="line">enableContentionProfiling: false</span><br><span class="line">enableProfiling: true</span><br><span class="line">hardPodAffinitySymmetricWeight: 1</span><br><span class="line">healthzBindAddress: ##NODE_IP##:10251</span><br><span class="line">leaderElection:</span><br><span class="line">  leaderElect: true</span><br><span class="line">metricsBindAddress: ##NODE_IP##:10251</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>--kubeconfig</code>：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver；</li><li><code>--leader-elect=true</code>：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</li></ul><p>替换模板文件中的变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 3; i++ ))</span><br><span class="line">  do</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-scheduler.yaml.template &gt; kube-scheduler-$&#123;NODE_IPS[i]&#125;.yaml</span><br><span class="line">  done</span><br><span class="line">ls kube-scheduler*.yaml</span><br></pre></td></tr></table></figure><ul><li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li></ul><p>分发 kube-scheduler 配置文件到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-scheduler-$&#123;node_ip&#125;.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/kube-scheduler.yaml</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>重命名为 kube-scheduler.yaml;</li></ul><h2 id="创建-kube-scheduler-systemd-unit-模板文件"><a href="#创建-kube-scheduler-systemd-unit-模板文件" class="headerlink" title="创建 kube-scheduler systemd unit 模板文件"></a>创建 kube-scheduler systemd unit 模板文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-scheduler.service.template &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes Scheduler</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=$&#123;K8S_DIR&#125;/kube-scheduler</span><br><span class="line">ExecStart=/opt/k8s/bin/kube-scheduler \\</span><br><span class="line">  --config=/etc/kubernetes/kube-scheduler.yaml \\</span><br><span class="line">  --bind-address=##NODE_IP## \\</span><br><span class="line">  --secure-port=10259 \\</span><br><span class="line">  --port=0 \\</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/cert/kube-scheduler.pem \\</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/cert/kube-scheduler-key.pem \\</span><br><span class="line">  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\</span><br><span class="line">  --client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --requestheader-allowed-names=&quot;&quot; \\</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \\</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \\</span><br><span class="line">  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\</span><br><span class="line">  --logtostderr=true \\</span><br><span class="line">  --v=2</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h2 id="为各节点创建和分发-kube-scheduler-systemd-unit-文件"><a href="#为各节点创建和分发-kube-scheduler-systemd-unit-文件" class="headerlink" title="为各节点创建和分发 kube-scheduler systemd unit 文件"></a>为各节点创建和分发 kube-scheduler systemd unit 文件</h2><p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 2; i++ ))</span><br><span class="line">  do</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-scheduler.service.template &gt; kube-scheduler-$&#123;NODE_IPS[i]&#125;.service </span><br><span class="line">  done</span><br><span class="line">ls kube-scheduler*.service</span><br></pre></td></tr></table></figure><p>分发 systemd unit 文件到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-scheduler-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-scheduler.service</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>重命名为 kube-scheduler.service；</li></ul><h2 id="启动-kube-scheduler-服务"><a href="#启动-kube-scheduler-服务" class="headerlink" title="启动 kube-scheduler 服务"></a>启动 kube-scheduler 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-scheduler&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl restart kube-scheduler&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>启动服务前必须先创建工作目录；</li></ul><h2 id="检查服务运行状态-1"><a href="#检查服务运行状态-1" class="headerlink" title="检查服务运行状态"></a>检查服务运行状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-scheduler|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code></p><h2 id="查看当前的-leader-1"><a href="#查看当前的-leader-1" class="headerlink" title="查看当前的 leader"></a>查看当前的 leader</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get endpoints kube-scheduler --namespace=kube-system  -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    control-plane.alpha.kubernetes.io/leader: &apos;&#123;&quot;holderIdentity&quot;:&quot;node1_b23eda23-8698-11e9-b281-005056b16e40&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2019-06-04T07:17:00Z&quot;,&quot;renewTime&quot;:&quot;2019-06-05T07:31:12Z&quot;,&quot;leaderTransitions&quot;:1&#125;&apos;</span><br><span class="line">  creationTimestamp: &quot;2019-06-04T07:07:02Z&quot;</span><br><span class="line">  name: kube-scheduler</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  resourceVersion: &quot;125460&quot;</span><br><span class="line">  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler</span><br><span class="line">  uid: 5a3888a1-8697-11e9-b371-005056b1d2de</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;kube-controller-manager-集群&quot;&gt;&lt;a href=&quot;#kube-controller-manager-集群&quot; class=&quot;headerlink&quot; title=&quot;kube-controller-manager 集群&quot;&gt;&lt;/a&gt;kube-con
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-apiserver集群</title>
    <link href="https://shenshengkun.github.io/posts/863q77b5.html"/>
    <id>https://shenshengkun.github.io/posts/863q77b5.html</id>
    <published>2019-06-05T06:54:01.000Z</published>
    <updated>2019-06-05T08:52:01.693Z</updated>
    
    <content type="html"><![CDATA[<h1 id="nginx代理"><a href="#nginx代理" class="headerlink" title="nginx代理"></a>nginx代理</h1><h2 id="基于-nginx-代理的-kube-apiserver-高可用方案"><a href="#基于-nginx-代理的-kube-apiserver-高可用方案" class="headerlink" title="基于 nginx 代理的 kube-apiserver 高可用方案"></a>基于 nginx 代理的 kube-apiserver 高可用方案</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- 控制节点的 kube-controller-manager、kube-scheduler 是多实例部署，所以只要有一个实例正常，就可以保证高可用；</span><br><span class="line">- 集群内的 Pod 使用 K8S 服务域名 kubernetes 访问 kube-apiserver， kube-dns 会自动解析出多个 kube-apiserver 节点的 IP，所以也是高可用的；</span><br><span class="line">- 在每个节点起一个 nginx 进程，后端对接多个 apiserver 实例，nginx 对它们做健康检查和负载均衡；</span><br><span class="line">- kubelet、kube-proxy、controller-manager、scheduler 通过本地的 nginx（监听 127.0.0.1）访问 kube-apiserver，从而实现 kube-apiserver 的高可用；</span><br></pre></td></tr></table></figure><h2 id="下载和编译-nginx"><a href="#下载和编译-nginx" class="headerlink" title="下载和编译 nginx"></a>下载和编译 nginx</h2><p>下载源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">wget http://nginx.org/download/nginx-1.15.3.tar.gz</span><br><span class="line">tar -xzvf nginx-1.15.3.tar.gz</span><br></pre></td></tr></table></figure><p>配置编译参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/nginx-1.15.3</span><br><span class="line">mkdir nginx-prefix</span><br><span class="line">./configure --with-stream --without-http --prefix=$(pwd)/nginx-prefix --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module</span><br></pre></td></tr></table></figure><ul><li><code>--with-stream</code>：开启 4 层透明转发(TCP Proxy)功能；</li><li><code>--without-xxx</code>：关闭所有其他功能，这样生成的动态链接二进制程序依赖最小；</li></ul><p>编译和安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work/nginx-1.15.3</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure><h2 id="安装和部署-nginx"><a href="#安装和部署-nginx" class="headerlink" title="安装和部署 nginx"></a>安装和部署 nginx</h2><p>创建目录结构：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    mkdir -p /opt/k8s/kube-nginx/&#123;conf,logs,sbin&#125;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>拷贝二进制程序：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp /opt/k8s/work/nginx-1.15.3/nginx-prefix/sbin/nginx  root@$&#123;node_ip&#125;:/opt/k8s/kube-nginx/sbin/kube-nginx</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod a+x /opt/k8s/kube-nginx/sbin/*&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /opt/k8s/kube-nginx/&#123;conf,logs,sbin&#125;&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>重命名二进制文件为 kube-nginx；</li></ul><p>配置 nginx，开启 4 层透明转发功能：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-nginx.conf &lt;&lt;EOF</span><br><span class="line">worker_processes 1;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stream &#123;</span><br><span class="line">    upstream backend &#123;</span><br><span class="line">        hash $remote_addr consistent;</span><br><span class="line">        server 192.168.6.101:6443        max_fails=3 fail_timeout=30s;</span><br><span class="line">        server 192.168.6.102:6443        max_fails=3 fail_timeout=30s;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen 127.0.0.1:8443;</span><br><span class="line">        proxy_connect_timeout 1s;</span><br><span class="line">        proxy_pass backend;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-nginx.conf  root@$&#123;node_ip&#125;:/opt/k8s/kube-nginx/conf/kube-nginx.conf</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="配置-systemd-unit-文件，启动服务"><a href="#配置-systemd-unit-文件，启动服务" class="headerlink" title="配置 systemd unit 文件，启动服务"></a>配置 systemd unit 文件，启动服务</h2><p>配置 kube-nginx systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; kube-nginx.service &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=kube-apiserver nginx proxy</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=forking</span><br><span class="line">ExecStartPre=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -t</span><br><span class="line">ExecStart=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx</span><br><span class="line">ExecReload=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -s reload</span><br><span class="line">PrivateTmp=true</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-nginx.service  root@$&#123;node_ip&#125;:/etc/systemd/system/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>启动 kube-nginx 服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-nginx &amp;&amp; systemctl restart kube-nginx&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h1 id="apiserver集群"><a href="#apiserver集群" class="headerlink" title="apiserver集群"></a>apiserver集群</h1><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="下载最新版本二进制文件"><a href="#下载最新版本二进制文件" class="headerlink" title="下载最新版本二进制文件"></a>下载最新版本二进制文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">wget https://dl.k8s.io/v1.14.2/kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">tar -xzvf kubernetes-server-linux-amd64.tar.gz</span><br><span class="line">cd kubernetes</span><br><span class="line">tar -xzvf  kubernetes-src.tar.gz</span><br></pre></td></tr></table></figure><p>将二进制文件拷贝到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kubernetes/server/bin/&#123;apiextensions-apiserver,cloud-controller-manager,kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,kubeadm,kubectl,kubelet,mounter&#125; root@$&#123;node_ip&#125;:/opt/k8s/bin/</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-kubernetes-证书和私钥"><a href="#创建-kubernetes-证书和私钥" class="headerlink" title="创建 kubernetes 证书和私钥"></a>创建 kubernetes 证书和私钥</h2><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kubernetes-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">  &quot;hosts&quot;: [</span><br><span class="line">    &quot;127.0.0.1&quot;,</span><br><span class="line">    &quot;192.168.6.101&quot;,</span><br><span class="line">    &quot;192.168.6.102&quot;,</span><br><span class="line">    &quot;$&#123;CLUSTER_KUBERNETES_SVC_IP&#125;&quot;,</span><br><span class="line">    &quot;kubernetes&quot;,</span><br><span class="line">    &quot;kubernetes.default&quot;,</span><br><span class="line">    &quot;kubernetes.default.svc&quot;,</span><br><span class="line">    &quot;kubernetes.default.svc.cluster&quot;,</span><br><span class="line">    &quot;kubernetes.default.svc.cluster.local.&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>hosts 字段指定授权使用该证书的 <strong>IP 和域名列表</strong>，这里列出了 master 节点 IP、kubernetes 服务的 IP 和域名；</li><li>kubernetes 服务 IP 是 apiserver 自动创建的，一般是 <code>--service-cluster-ip-range</code> 参数指定的网段的<strong>第一个IP</strong>，后续可以通过下面命令获取：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# kubectl get svc kubernetes</span><br><span class="line">NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.254.0.1   &lt;none&gt;        443/TCP   24h</span><br></pre></td></tr></table></figure><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes</span><br><span class="line">ls kubernetes*pem</span><br></pre></td></tr></table></figure><p>将生成的证书和私钥文件拷贝到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/kubernetes/cert&quot;</span><br><span class="line">    scp kubernetes*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建加密配置文件"><a href="#创建加密配置文件" class="headerlink" title="创建加密配置文件"></a>创建加密配置文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; encryption-config.yaml &lt;&lt;EOF</span><br><span class="line">kind: EncryptionConfig</span><br><span class="line">apiVersion: v1</span><br><span class="line">resources:</span><br><span class="line">  - resources:</span><br><span class="line">      - secrets</span><br><span class="line">    providers:</span><br><span class="line">      - aescbc:</span><br><span class="line">          keys:</span><br><span class="line">            - name: key1</span><br><span class="line">              secret: $&#123;ENCRYPTION_KEY&#125;</span><br><span class="line">      - identity: &#123;&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>将加密配置文件拷贝到 master 节点的 <code>/etc/kubernetes</code> 目录下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp encryption-config.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建审计策略文件"><a href="#创建审计策略文件" class="headerlink" title="创建审计策略文件"></a>创建审计策略文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; audit-policy.yaml &lt;&lt;EOF</span><br><span class="line">apiVersion: audit.k8s.io/v1beta1</span><br><span class="line">kind: Policy</span><br><span class="line">rules:</span><br><span class="line">  # The following requests were manually identified as high-volume and low-risk, so drop them.</span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - endpoints</span><br><span class="line">          - services</span><br><span class="line">          - services/status</span><br><span class="line">    users:</span><br><span class="line">      - &apos;system:kube-proxy&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - watch</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - nodes</span><br><span class="line">          - nodes/status</span><br><span class="line">    userGroups:</span><br><span class="line">      - &apos;system:nodes&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    namespaces:</span><br><span class="line">      - kube-system</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - endpoints</span><br><span class="line">    users:</span><br><span class="line">      - &apos;system:kube-controller-manager&apos;</span><br><span class="line">      - &apos;system:kube-scheduler&apos;</span><br><span class="line">      - &apos;system:serviceaccount:kube-system:endpoint-controller&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - update</span><br><span class="line"></span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - namespaces</span><br><span class="line">          - namespaces/status</span><br><span class="line">          - namespaces/finalize</span><br><span class="line">    users:</span><br><span class="line">      - &apos;system:apiserver&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line"></span><br><span class="line">  # Don&apos;t log HPA fetching metrics.</span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">    users:</span><br><span class="line">      - &apos;system:kube-controller-manager&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line"></span><br><span class="line">  # Don&apos;t log these read-only URLs.</span><br><span class="line">  - level: None</span><br><span class="line">    nonResourceURLs:</span><br><span class="line">      - &apos;/healthz*&apos;</span><br><span class="line">      - /version</span><br><span class="line">      - &apos;/swagger*&apos;</span><br><span class="line"></span><br><span class="line">  # Don&apos;t log events requests.</span><br><span class="line">  - level: None</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - events</span><br><span class="line"></span><br><span class="line">  # node and pod status calls from nodes are high-volume and can be large, don&apos;t log responses for expected updates from nodes</span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - nodes/status</span><br><span class="line">          - pods/status</span><br><span class="line">    users:</span><br><span class="line">      - kubelet</span><br><span class="line">      - &apos;system:node-problem-detector&apos;</span><br><span class="line">      - &apos;system:serviceaccount:kube-system:node-problem-detector&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - nodes/status</span><br><span class="line">          - pods/status</span><br><span class="line">    userGroups:</span><br><span class="line">      - &apos;system:nodes&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - update</span><br><span class="line">      - patch</span><br><span class="line"></span><br><span class="line">  # deletecollection calls can be large, don&apos;t log responses for expected namespace deletions</span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    users:</span><br><span class="line">      - &apos;system:serviceaccount:kube-system:namespace-controller&apos;</span><br><span class="line">    verbs:</span><br><span class="line">      - deletecollection</span><br><span class="line"></span><br><span class="line">  # Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data,</span><br><span class="line">  # so only log at the Metadata level.</span><br><span class="line">  - level: Metadata</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">        resources:</span><br><span class="line">          - secrets</span><br><span class="line">          - configmaps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">        resources:</span><br><span class="line">          - tokenreviews</span><br><span class="line">  # Get repsonses can be large; skip them.</span><br><span class="line">  - level: Request</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">      - group: admissionregistration.k8s.io</span><br><span class="line">      - group: apiextensions.k8s.io</span><br><span class="line">      - group: apiregistration.k8s.io</span><br><span class="line">      - group: apps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">      - group: authorization.k8s.io</span><br><span class="line">      - group: autoscaling</span><br><span class="line">      - group: batch</span><br><span class="line">      - group: certificates.k8s.io</span><br><span class="line">      - group: extensions</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">      - group: networking.k8s.io</span><br><span class="line">      - group: policy</span><br><span class="line">      - group: rbac.authorization.k8s.io</span><br><span class="line">      - group: scheduling.k8s.io</span><br><span class="line">      - group: settings.k8s.io</span><br><span class="line">      - group: storage.k8s.io</span><br><span class="line">    verbs:</span><br><span class="line">      - get</span><br><span class="line">      - list</span><br><span class="line">      - watch</span><br><span class="line"></span><br><span class="line">  # Default level for known APIs</span><br><span class="line">  - level: RequestResponse</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">    resources:</span><br><span class="line">      - group: &quot;&quot;</span><br><span class="line">      - group: admissionregistration.k8s.io</span><br><span class="line">      - group: apiextensions.k8s.io</span><br><span class="line">      - group: apiregistration.k8s.io</span><br><span class="line">      - group: apps</span><br><span class="line">      - group: authentication.k8s.io</span><br><span class="line">      - group: authorization.k8s.io</span><br><span class="line">      - group: autoscaling</span><br><span class="line">      - group: batch</span><br><span class="line">      - group: certificates.k8s.io</span><br><span class="line">      - group: extensions</span><br><span class="line">      - group: metrics.k8s.io</span><br><span class="line">      - group: networking.k8s.io</span><br><span class="line">      - group: policy</span><br><span class="line">      - group: rbac.authorization.k8s.io</span><br><span class="line">      - group: scheduling.k8s.io</span><br><span class="line">      - group: settings.k8s.io</span><br><span class="line">      - group: storage.k8s.io</span><br><span class="line"></span><br><span class="line">  # Default level for all other requests.</span><br><span class="line">  - level: Metadata</span><br><span class="line">    omitStages:</span><br><span class="line">      - RequestReceived</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>分发审计策略文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp audit-policy.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/audit-policy.yaml</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建后续访问-metrics-server-使用的证书"><a href="#创建后续访问-metrics-server-使用的证书" class="headerlink" title="创建后续访问 metrics-server 使用的证书"></a>创建后续访问 metrics-server 使用的证书</h2><p>创建证书签名请求:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; proxy-client-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;aggregator&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>CN 名称为 aggregator，需要与 metrics-server 的 <code>--requestheader-allowed-names</code> 参数配置一致，否则访问会被 metrics-server 拒绝；</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  -ca-key=/etc/kubernetes/cert/ca-key.pem  \</span><br><span class="line">  -config=/etc/kubernetes/cert/ca-config.json  \</span><br><span class="line">  -profile=kubernetes proxy-client-csr.json | cfssljson -bare proxy-client</span><br><span class="line">ls proxy-client*.pem</span><br></pre></td></tr></table></figure><p>将生成的证书和私钥文件拷贝到所有 master 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp proxy-client*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-kube-apiserver-systemd-unit-模板文件"><a href="#创建-kube-apiserver-systemd-unit-模板文件" class="headerlink" title="创建 kube-apiserver systemd unit 模板文件"></a>创建 kube-apiserver systemd unit 模板文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; kube-apiserver.service.template &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Kubernetes API Server</span><br><span class="line">Documentation=https://github.com/GoogleCloudPlatform/kubernetes</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">WorkingDirectory=$&#123;K8S_DIR&#125;/kube-apiserver</span><br><span class="line">ExecStart=/opt/k8s/bin/kube-apiserver \\</span><br><span class="line">  --advertise-address=##NODE_IP## \\</span><br><span class="line">  --default-not-ready-toleration-seconds=360 \\</span><br><span class="line">  --default-unreachable-toleration-seconds=360 \\</span><br><span class="line">  --feature-gates=DynamicAuditing=true \\</span><br><span class="line">  --max-mutating-requests-inflight=2000 \\</span><br><span class="line">  --max-requests-inflight=4000 \\</span><br><span class="line">  --default-watch-cache-size=200 \\</span><br><span class="line">  --delete-collection-workers=2 \\</span><br><span class="line">  --encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\</span><br><span class="line">  --etcd-cafile=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --etcd-certfile=/etc/kubernetes/cert/kubernetes.pem \\</span><br><span class="line">  --etcd-keyfile=/etc/kubernetes/cert/kubernetes-key.pem \\</span><br><span class="line">  --etcd-servers=$&#123;ETCD_ENDPOINTS&#125; \\</span><br><span class="line">  --bind-address=##NODE_IP## \\</span><br><span class="line">  --secure-port=6443 \\</span><br><span class="line">  --tls-cert-file=/etc/kubernetes/cert/kubernetes.pem \\</span><br><span class="line">  --tls-private-key-file=/etc/kubernetes/cert/kubernetes-key.pem \\</span><br><span class="line">  --insecure-port=0 \\</span><br><span class="line">  --audit-dynamic-configuration \\</span><br><span class="line">  --audit-log-maxage=15 \\</span><br><span class="line">  --audit-log-maxbackup=3 \\</span><br><span class="line">  --audit-log-maxsize=100 \\</span><br><span class="line">  --audit-log-mode=batch \\</span><br><span class="line">  --audit-log-truncate-enabled \\</span><br><span class="line">  --audit-log-batch-buffer-size=20000 \\</span><br><span class="line">  --audit-log-batch-max-size=2 \\</span><br><span class="line">  --audit-log-path=$&#123;K8S_DIR&#125;/kube-apiserver/audit.log \\</span><br><span class="line">  --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\</span><br><span class="line">  --profiling \\</span><br><span class="line">  --anonymous-auth=false \\</span><br><span class="line">  --client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --enable-bootstrap-token-auth \\</span><br><span class="line">  --requestheader-allowed-names=&quot;&quot; \\</span><br><span class="line">  --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\</span><br><span class="line">  --requestheader-group-headers=X-Remote-Group \\</span><br><span class="line">  --requestheader-username-headers=X-Remote-User \\</span><br><span class="line">  --service-account-key-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --authorization-mode=Node,RBAC \\</span><br><span class="line">  --runtime-config=api/all=true \\</span><br><span class="line">  --enable-admission-plugins=NodeRestriction \\</span><br><span class="line">  --allow-privileged=true \\</span><br><span class="line">  --apiserver-count=3 \\</span><br><span class="line">  --event-ttl=168h \\</span><br><span class="line">  --kubelet-certificate-authority=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --kubelet-client-certificate=/etc/kubernetes/cert/kubernetes.pem \\</span><br><span class="line">  --kubelet-client-key=/etc/kubernetes/cert/kubernetes-key.pem \\</span><br><span class="line">  --kubelet-https=true \\</span><br><span class="line">  --kubelet-timeout=10s \\</span><br><span class="line">  --proxy-client-cert-file=/etc/kubernetes/cert/proxy-client.pem \\</span><br><span class="line">  --proxy-client-key-file=/etc/kubernetes/cert/proxy-client-key.pem \\</span><br><span class="line">  --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\</span><br><span class="line">  --service-node-port-range=$&#123;NODE_PORT_RANGE&#125; \\</span><br><span class="line">  --logtostderr=true \\</span><br><span class="line">  --v=2</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=10</span><br><span class="line">Type=notify</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>--advertise-address</code>：apiserver 对外通告的 IP（kubernetes 服务后端节点 IP）；</li><li><code>--default-*-toleration-seconds</code>：设置节点异常相关的阈值；</li><li><code>--max-*-requests-inflight</code>：请求相关的最大阈值；</li><li><code>--etcd-*</code>：访问 etcd 的证书和 etcd 服务器地址；</li><li><code>--experimental-encryption-provider-config</code>：指定用于加密 etcd 中 secret 的配置；</li><li><code>--bind-address</code>： https 监听的 IP，不能为 <code>127.0.0.1</code>，否则外界不能访问它的安全端口 6443；</li><li><code>--secret-port</code>：https 监听端口；</li><li><code>--insecure-port=0</code>：关闭监听 http 非安全端口(8080)；</li><li><code>--tls-*-file</code>：指定 apiserver 使用的证书、私钥和 CA 文件；</li><li><code>--audit-*</code>：配置审计策略和审计日志文件相关的参数；</li><li><code>--client-ca-file</code>：验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy 等)请求所带的证书；</li><li><code>--enable-bootstrap-token-auth</code>：启用 kubelet bootstrap 的 token 认证；</li><li><code>--requestheader-*</code>：kube-apiserver 的 aggregator layer 相关的配置参数，proxy-client &amp; HPA 需要使用；</li><li><code>--requestheader-client-ca-file</code>：用于签名 <code>--proxy-client-cert-file</code> 和 <code>--proxy-client-key-file</code> 指定的证书；在启用了 metric aggregator 时使用；</li><li>如果 <code>--requestheader-allowed-names</code> 不为空，则<code>--proxy-client-cert-file</code> 证书的 CN 必须位于 allowed-names 中，默认为 aggregator;</li><li><code>--service-account-key-file</code>：签名 ServiceAccount Token 的公钥文件，kube-controller-manager 的 <code>--service-account-private-key-file</code> 指定私钥文件，两者配对使用；</li><li><code>--runtime-config=api/all=true</code>： 启用所有版本的 APIs，如 autoscaling/v2alpha1；</li><li><code>--authorization-mode=Node,RBAC</code>、<code>--anonymous-auth=false</code>： 开启 Node 和 RBAC 授权模式，拒绝未授权的请求；</li><li><code>--enable-admission-plugins</code>：启用一些默认关闭的 plugins；</li><li><code>--allow-privileged</code>：运行执行 privileged 权限的容器；</li><li><code>--apiserver-count=3</code>：指定 apiserver 实例的数量；</li><li><code>--event-ttl</code>：指定 events 的保存时间；</li><li><code>--kubelet-*</code>：如果指定，则使用 https 访问 kubelet APIs；需要为证书对应的用户(上面 kubernetes*.pem 证书的用户为 kubernetes) 用户定义 RBAC 规则，否则访问 kubelet API 时提示未授权；</li><li><code>--proxy-client-*</code>：apiserver 访问 metrics-server 使用的证书；</li><li><code>--service-cluster-ip-range</code>： 指定 Service Cluster IP 地址段；</li><li><code>--service-node-port-range</code>： 指定 NodePort 的端口范围；</li></ul><p>如果 kube-apiserver 机器<strong>没有</strong>运行 kube-proxy，则还需要添加 <code>--enable-aggregator-routing=true</code> 参数；</p><p>关于 <code>--requestheader-XXX</code> 相关参数，参考：</p><ul><li><a href="https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md" target="_blank" rel="noopener">https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md</a></li><li><a href="https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/" target="_blank" rel="noopener">https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/</a></li></ul><p>注意：requestheader-client-ca-file 指定的 CA 证书，必须具有 client auth and server auth；</p><h2 id="为各节点创建和分发-kube-apiserver-systemd-unit-文件"><a href="#为各节点创建和分发-kube-apiserver-systemd-unit-文件" class="headerlink" title="为各节点创建和分发 kube-apiserver systemd unit 文件"></a>为各节点创建和分发 kube-apiserver systemd unit 文件</h2><p>替换模板文件中的变量，为各节点生成 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 2; i++ ))</span><br><span class="line">  do</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-apiserver.service.template &gt; kube-apiserver-$&#123;NODE_IPS[i]&#125;.service </span><br><span class="line">  done</span><br><span class="line">ls kube-apiserver*.service</span><br></pre></td></tr></table></figure><ul><li>NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP；</li></ul><p>分发生成的 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kube-apiserver-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-apiserver.service</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>文件重命名为 kube-apiserver.service;</li></ul><h2 id="启动-kube-apiserver-服务"><a href="#启动-kube-apiserver-服务" class="headerlink" title="启动 kube-apiserver 服务"></a>启动 kube-apiserver 服务</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-apiserver&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl restart kube-apiserver&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>启动服务前必须先创建工作目录；</li></ul><h2 id="检查-kube-apiserver-运行状态"><a href="#检查-kube-apiserver-运行状态" class="headerlink" title="检查 kube-apiserver 运行状态"></a>检查 kube-apiserver 运行状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-apiserver |grep &apos;Active:&apos;&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code></p><h2 id="打印-kube-apiserver-写入-etcd-的数据"><a href="#打印-kube-apiserver-写入-etcd-的数据" class="headerlink" title="打印 kube-apiserver 写入 etcd 的数据"></a>打印 kube-apiserver 写入 etcd 的数据</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">ETCDCTL_API=3 etcdctl \</span><br><span class="line">    --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">    --cacert=/opt/k8s/work/ca.pem \</span><br><span class="line">    --cert=/opt/k8s/work/etcd.pem \</span><br><span class="line">    --key=/opt/k8s/work/etcd-key.pem \</span><br><span class="line">    get /registry/ --prefix --keys-only</span><br></pre></td></tr></table></figure><h2 id="授予-kube-apiserver-访问-kubelet-API-的权限"><a href="#授予-kube-apiserver-访问-kubelet-API-的权限" class="headerlink" title="授予 kube-apiserver 访问 kubelet API 的权限"></a>授予 kube-apiserver 访问 kubelet API 的权限</h2><p>在执行 kubectl exec、run、logs 等命令时，apiserver 会将请求转发到 kubelet 的 https 端口。这里定义 RBAC 规则，授权 apiserver 使用的证书（kubernetes.pem）用户名（CN：kuberntes）访问 kubelet API 的权限：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;nginx代理&quot;&gt;&lt;a href=&quot;#nginx代理&quot; class=&quot;headerlink&quot; title=&quot;nginx代理&quot;&gt;&lt;/a&gt;nginx代理&lt;/h1&gt;&lt;h2 id=&quot;基于-nginx-代理的-kube-apiserver-高可用方案&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-flannel网络、kubectl工具</title>
    <link href="https://shenshengkun.github.io/posts/66ae7fg23.html"/>
    <id>https://shenshengkun.github.io/posts/66ae7fg23.html</id>
    <published>2019-06-05T06:30:01.000Z</published>
    <updated>2019-06-05T08:51:51.368Z</updated>
    
    <content type="html"><![CDATA[<h1 id="flannel网络"><a href="#flannel网络" class="headerlink" title="flannel网络"></a>flannel网络</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Flannel是CoreOS团队针对Kubernetes设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。 </p><h3 id="flannel在k8s工作"><a href="#flannel在k8s工作" class="headerlink" title="flannel在k8s工作"></a>flannel在k8s工作</h3><p>kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472（<strong>需要开放该端口</strong>，如公有云 AWS 等）。</p><p>flanneld 第一次启动时，从 etcd 获取配置的 Pod 网段信息，为本节点分配一个未使用的地址段，然后创建 <code>flannedl.1</code> 网络接口（也可能是其它名称，如 flannel1 等）。</p><p>flannel 将分配给自己的 Pod 网段信息写入 <code>/run/flannel/docker</code> 文件，docker 后续使用这个文件中的环境变量设置 <code>docker0</code> 网桥，从而从这个地址段为本节点的所有 Pod 容器分配 IP。</p><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h3 id="下载和分发-flanneld-二进制文件"><a href="#下载和分发-flanneld-二进制文件" class="headerlink" title="下载和分发 flanneld 二进制文件"></a>下载和分发 flanneld 二进制文件</h3><p>从 flannel 的 <a href="https://github.com/coreos/flannel/releases" target="_blank" rel="noopener">release 页面</a> 下载最新版本的安装包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">mkdir flannel</span><br><span class="line">wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz</span><br><span class="line">tar -xzvf flannel-v0.11.0-linux-amd64.tar.gz -C flannel</span><br></pre></td></tr></table></figure><p>分发二进制文件到集群所有节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp flannel/&#123;flanneld,mk-docker-opts.sh&#125; root@$&#123;node_ip&#125;:/opt/k8s/bin/</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h3 id="创建-flannel-证书和私钥"><a href="#创建-flannel-证书和私钥" class="headerlink" title="创建 flannel 证书和私钥"></a>创建 flannel 证书和私钥</h3><p>flanneld 从 etcd 集群存取网段分配信息，而 etcd 集群启用了双向 x509 证书认证，所以需要为 flanneld 生成证书和私钥。</p><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; flanneld-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;flanneld&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld</span><br><span class="line">ls flanneld*pem</span><br></pre></td></tr></table></figure><p>将生成的证书和私钥分发到<strong>所有节点</strong>（master 和 worker）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/flanneld/cert&quot;</span><br><span class="line">    scp flanneld*.pem root@$&#123;node_ip&#125;:/etc/flanneld/cert</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h3 id="在etcd-写入集群-Pod-网段信息"><a href="#在etcd-写入集群-Pod-网段信息" class="headerlink" title="在etcd 写入集群 Pod 网段信息"></a>在etcd 写入集群 Pod 网段信息</h3><p>注意：本步骤<strong>只需执行一次</strong>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">etcdctl \</span><br><span class="line">  --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">  --ca-file=/opt/k8s/work/ca.pem \</span><br><span class="line">  --cert-file=/opt/k8s/work/flanneld.pem \</span><br><span class="line">  --key-file=/opt/k8s/work/flanneld-key.pem \</span><br><span class="line">  mk $&#123;FLANNEL_ETCD_PREFIX&#125;/config &apos;&#123;&quot;Network&quot;:&quot;&apos;$&#123;CLUSTER_CIDR&#125;&apos;&quot;, &quot;SubnetLen&quot;: 21, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;&apos;</span><br></pre></td></tr></table></figure><ul><li>flanneld <strong>当前版本 (v0.11.0) 不支持 etcd v3</strong>，故使用 etcd v2 API 写入配置 key 和网段数据；</li><li>写入的 Pod 网段 <code>${CLUSTER_CIDR}</code> 地址段（如 /16）必须小于 <code>SubnetLen</code>，必须与 <code>kube-controller-manager</code> 的 <code>--cluster-cidr</code> 参数值一致；</li></ul><h3 id="创建-flanneld-的-systemd"><a href="#创建-flanneld-的-systemd" class="headerlink" title="创建 flanneld 的 systemd"></a>创建 flanneld 的 systemd</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; flanneld.service &lt;&lt; EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Flanneld overlay address etcd agent</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line">After=etcd.service</span><br><span class="line">Before=docker.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">ExecStart=/opt/k8s/bin/flanneld \\</span><br><span class="line">  -etcd-cafile=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  -etcd-certfile=/etc/flanneld/cert/flanneld.pem \\</span><br><span class="line">  -etcd-keyfile=/etc/flanneld/cert/flanneld-key.pem \\</span><br><span class="line">  -etcd-endpoints=$&#123;ETCD_ENDPOINTS&#125; \\</span><br><span class="line">  -etcd-prefix=$&#123;FLANNEL_ETCD_PREFIX&#125; \\</span><br><span class="line">  -iface=$&#123;IFACE&#125; \\</span><br><span class="line">  -ip-masq</span><br><span class="line">ExecStartPost=/opt/k8s/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker</span><br><span class="line">Restart=always</span><br><span class="line">RestartSec=5</span><br><span class="line">StartLimitInterval=0</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">RequiredBy=docker.service</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>mk-docker-opts.sh</code> 脚本将分配给 flanneld 的 Pod 子网段信息写入 <code>/run/flannel/docker</code> 文件，后续 docker 启动时使用这个文件中的环境变量配置 docker0 网桥；</li><li>flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 <code>-iface</code> 参数指定通信接口;</li><li>flanneld 运行时需要 root 权限；</li><li><code>-ip-masq</code>: flanneld 为访问 Pod 网络外的流量设置 SNAT 规则，同时将传递给 Docker 的变量 <code>--ip-masq</code>（<code>/run/flannel/docker</code> 文件中）设置为 false，这样 Docker 将不再创建 SNAT 规则； Docker 的 <code>--ip-masq</code> 为 true 时，创建的 SNAT 规则比较“暴力”：将所有本节点 Pod 发起的、访问非 docker0 接口的请求做 SNAT，这样访问其他节点 Pod 的请求来源 IP 会被设置为 flannel.1 接口的 IP，导致目的 Pod 看不到真实的来源 Pod IP。 flanneld 创建的 SNAT 规则比较温和，只对访问非 Pod 网段的请求做 SNAT。</li></ul><h3 id="分发-flanneld-systemd-unit"><a href="#分发-flanneld-systemd-unit" class="headerlink" title="分发 flanneld systemd unit"></a>分发 flanneld systemd unit</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp flanneld.service root@$&#123;node_ip&#125;:/etc/systemd/system/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h3 id="启动-flanneld-服务"><a href="#启动-flanneld-服务" class="headerlink" title="启动 flanneld 服务"></a>启动 flanneld 服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h3 id="检查启动结果"><a href="#检查启动结果" class="headerlink" title="检查启动结果"></a>检查启动结果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status flanneld|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code></p><h3 id="检查分配给各-flanneld-的-Pod-网段信息"><a href="#检查分配给各-flanneld-的-Pod-网段信息" class="headerlink" title="检查分配给各 flanneld 的 Pod 网段信息"></a>检查分配给各 flanneld 的 Pod 网段信息</h3><p>查看集群 Pod 网段(/16)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">etcdctl \</span><br><span class="line">  --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">  --ca-file=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/cert/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/cert/flanneld-key.pem \</span><br><span class="line">  get $&#123;FLANNEL_ETCD_PREFIX&#125;/config</span><br></pre></td></tr></table></figure><p>输出：</p><p><code>{&quot;Network&quot;:&quot;172.30.0.0/16&quot;, &quot;SubnetLen&quot;: 21, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;}}</code></p><p>查看已分配的 Pod 子网段列表(/24):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">etcdctl \</span><br><span class="line">  --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">  --ca-file=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/cert/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/cert/flanneld-key.pem \</span><br><span class="line">  ls $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets</span><br></pre></td></tr></table></figure><p>输出（结果视部署情况而定）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# etcdctl \</span><br><span class="line">&gt;   --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">&gt;   --ca-file=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">&gt;   --cert-file=/etc/flanneld/cert/flanneld.pem \</span><br><span class="line">&gt;   --key-file=/etc/flanneld/cert/flanneld-key.pem \</span><br><span class="line">&gt;   ls $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets</span><br><span class="line">/kubernetes/network/subnets/172.30.168.0-21</span><br><span class="line">/kubernetes/network/subnets/172.30.48.0-21</span><br></pre></td></tr></table></figure><p>查看某一 Pod 网段对应的节点 IP 和 flannel 接口地址:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">etcdctl \</span><br><span class="line">  --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">  --ca-file=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --cert-file=/etc/flanneld/cert/flanneld.pem \</span><br><span class="line">  --key-file=/etc/flanneld/cert/flanneld-key.pem \</span><br><span class="line">  get $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets/172.30.168.0-21</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# etcdctl \</span><br><span class="line">&gt;   --endpoints=$&#123;ETCD_ENDPOINTS&#125; \</span><br><span class="line">&gt;   --ca-file=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">&gt;   --cert-file=/etc/flanneld/cert/flanneld.pem \</span><br><span class="line">&gt;   --key-file=/etc/flanneld/cert/flanneld-key.pem \</span><br><span class="line">&gt;   get $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets/172.30.168.0-21</span><br><span class="line">&#123;&quot;PublicIP&quot;:&quot;192.168.6.101&quot;,&quot;BackendType&quot;:&quot;vxlan&quot;,&quot;BackendData&quot;:&#123;&quot;VtepMAC&quot;:&quot;62:58:f9:a2:16:73&quot;&#125;&#125;</span><br><span class="line">[root@node1 ~]#</span><br></pre></td></tr></table></figure><h3 id="验证各节点能通过-Pod-网段互通"><a href="#验证各节点能通过-Pod-网段互通" class="headerlink" title="验证各节点能通过 Pod 网段互通"></a>验证各节点能通过 Pod 网段互通</h3><p>在<strong>各节点上部署</strong> flannel 后，检查是否创建了 flannel 接口(名称可能为 flannel0、flannel.0、flannel.1 等)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh $&#123;node_ip&#125; &quot;/usr/sbin/ip addr show flannel.1|grep -w inet&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">&gt;   do</span><br><span class="line">&gt;     echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">&gt;     ssh $&#123;node_ip&#125; &quot;/usr/sbin/ip addr show flannel.1|grep -w inet&quot;</span><br><span class="line">&gt;   done</span><br><span class="line">&gt;&gt;&gt; 192.168.6.101</span><br><span class="line">    inet 172.30.168.0/32 scope global flannel.1</span><br><span class="line">&gt;&gt;&gt; 192.168.6.102</span><br><span class="line">    inet 172.30.48.0/32 scope global flannel.1</span><br></pre></td></tr></table></figure><p>在各节点上 ping 所有 flannel 接口 IP，确保能通 </p><h1 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h1><h2 id="下载和分发-kubectl-二进制文件"><a href="#下载和分发-kubectl-二进制文件" class="headerlink" title="下载和分发 kubectl 二进制文件"></a>下载和分发 kubectl 二进制文件</h2><p>下载和解压：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">wget https://dl.k8s.io/v1.14.2/kubernetes-client-linux-amd64.tar.gz</span><br><span class="line">tar -xzvf kubernetes-client-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>分发到所有使用 kubectl 的节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp kubernetes/client/bin/kubectl root@$&#123;node_ip&#125;:/opt/k8s/bin/</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="创建-admin-证书和私钥"><a href="#创建-admin-证书和私钥" class="headerlink" title="创建 admin 证书和私钥"></a>创建 admin 证书和私钥</h2><p>kubectl 与 apiserver https 安全端口通信，apiserver 对提供的证书进行认证和授权。</p><p>kubectl 作为集群的管理工具，需要被授予最高权限，这里创建具有<strong>最高权限</strong>的 admin 证书。</p><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; admin-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;admin&quot;,</span><br><span class="line">  &quot;hosts&quot;: [],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;system:masters&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>O 为 <code>system:masters</code>，kube-apiserver 收到该证书后将请求的 Group 设置为 system:masters；</li><li>预定义的 ClusterRoleBinding <code>cluster-admin</code> 将 Group <code>system:masters</code> 与 Role <code>cluster-admin</code> 绑定，该 Role 授予<strong>所有 API</strong>的权限；</li><li>该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空；</li></ul><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">  -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">  -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">  -profile=kubernetes admin-csr.json | cfssljson -bare admin</span><br></pre></td></tr></table></figure><h2 id="创建-kubeconfig-文件"><a href="#创建-kubeconfig-文件" class="headerlink" title="创建 kubeconfig 文件"></a>创建 kubeconfig 文件</h2><p>kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书；</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line"></span><br><span class="line"># 设置集群参数</span><br><span class="line">kubectl config set-cluster kubernetes \</span><br><span class="line">  --certificate-authority=/opt/k8s/work/ca.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --server=$&#123;KUBE_APISERVER&#125; \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置客户端认证参数</span><br><span class="line">kubectl config set-credentials admin \</span><br><span class="line">  --client-certificate=/opt/k8s/work/admin.pem \</span><br><span class="line">  --client-key=/opt/k8s/work/admin-key.pem \</span><br><span class="line">  --embed-certs=true \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置上下文参数</span><br><span class="line">kubectl config set-context kubernetes \</span><br><span class="line">  --cluster=kubernetes \</span><br><span class="line">  --user=admin \</span><br><span class="line">  --kubeconfig=kubectl.kubeconfig</span><br><span class="line"></span><br><span class="line"># 设置默认上下文</span><br><span class="line">kubectl config use-context kubernetes --kubeconfig=kubectl.kubeconfig</span><br></pre></td></tr></table></figure><ul><li><code>--certificate-authority</code>：验证 kube-apiserver 证书的根证书；</li><li><code>--client-certificate</code>、<code>--client-key</code>：刚生成的 <code>admin</code> 证书和私钥，连接 kube-apiserver 时使用；</li><li><code>--embed-certs=true</code>：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(不加时，写入的是证书文件路径，后续拷贝 kubeconfig 到其它机器时，还需要单独拷贝证书文件，不方便。)；</li></ul><h2 id="分发-kubeconfig-文件"><a href="#分发-kubeconfig-文件" class="headerlink" title="分发 kubeconfig 文件"></a>分发 kubeconfig 文件</h2><p>分发到所有使用 <code>kubectl</code> 命令的节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p ~/.kube&quot;</span><br><span class="line">    scp kubectl.kubeconfig root@$&#123;node_ip&#125;:~/.kube/config</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><ul><li>保存的文件名为 <code>~/.kube/config</code>；</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;flannel网络&quot;&gt;&lt;a href=&quot;#flannel网络&quot; class=&quot;headerlink&quot; title=&quot;flannel网络&quot;&gt;&lt;/a&gt;flannel网络&lt;/h1&gt;&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-etcd集群</title>
    <link href="https://shenshengkun.github.io/posts/7dqa4nb2.html"/>
    <id>https://shenshengkun.github.io/posts/7dqa4nb2.html</id>
    <published>2019-06-04T08:30:01.000Z</published>
    <updated>2019-06-05T08:51:39.964Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>ETCD 是一个高可用的分布式键值数据库，可用于服务发现。ETCD 采用 raft 一致性算法，基于 Go 语言实现。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">特点</span><br><span class="line"></span><br><span class="line">简单：安装配置使用简单，提供 HTTP API </span><br><span class="line"></span><br><span class="line">安全：支持 SSL 证书 </span><br><span class="line"></span><br><span class="line">可靠：采用 raft 算法，实现分布式系统数据的可用性和一致性</span><br></pre></td></tr></table></figure><p>kubernetes 使用 etcd 存储所有运行数据 </p><h1 id="下载和分发-etcd-二进制文件"><a href="#下载和分发-etcd-二进制文件" class="headerlink" title="下载和分发 etcd 二进制文件"></a>下载和分发 etcd 二进制文件</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">wget https://github.com/coreos/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gz</span><br><span class="line">tar -xvf etcd-v3.3.13-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>分发二进制文件到集群所有节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp etcd-v3.3.13-linux-amd64/etcd* root@$&#123;node_ip&#125;:/opt/k8s/bin</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h1 id="创建-etcd-证书和私钥"><a href="#创建-etcd-证书和私钥" class="headerlink" title="创建 etcd 证书和私钥"></a>创建 etcd 证书和私钥</h1><p>创建证书签名请求：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; etcd-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;etcd&quot;,</span><br><span class="line">  &quot;hosts&quot;: [</span><br><span class="line">    &quot;127.0.0.1&quot;,</span><br><span class="line">    &quot;192.168.6.101&quot;,</span><br><span class="line">    &quot;192.168.6.102&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>生成证书和私钥：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cfssl gencert -ca=/opt/k8s/work/ca.pem \</span><br><span class="line">    -ca-key=/opt/k8s/work/ca-key.pem \</span><br><span class="line">    -config=/opt/k8s/work/ca-config.json \</span><br><span class="line">    -profile=kubernetes etcd-csr.json | cfssljson -bare etcd</span><br><span class="line">ls etcd*pem</span><br></pre></td></tr></table></figure><p>分发生成的证书和私钥到各 etcd 节点：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/etcd/cert&quot;</span><br><span class="line">    scp etcd*.pem root@$&#123;node_ip&#125;:/etc/etcd/cert/</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h1 id="创建-etcd-的-systemd"><a href="#创建-etcd-的-systemd" class="headerlink" title="创建 etcd 的 systemd"></a>创建 etcd 的 systemd</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">cat &gt; etcd.service.template &lt;&lt;EOF</span><br><span class="line">[Unit]</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line">Documentation=https://github.com/coreos</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=$&#123;ETCD_DATA_DIR&#125;</span><br><span class="line">ExecStart=/opt/k8s/bin/etcd \\</span><br><span class="line">  --data-dir=$&#123;ETCD_DATA_DIR&#125; \\</span><br><span class="line">  --wal-dir=$&#123;ETCD_WAL_DIR&#125; \\</span><br><span class="line">  --name=##NODE_NAME## \\</span><br><span class="line">  --cert-file=/etc/etcd/cert/etcd.pem \\</span><br><span class="line">  --key-file=/etc/etcd/cert/etcd-key.pem \\</span><br><span class="line">  --trusted-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --peer-cert-file=/etc/etcd/cert/etcd.pem \\</span><br><span class="line">  --peer-key-file=/etc/etcd/cert/etcd-key.pem \\</span><br><span class="line">  --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem \\</span><br><span class="line">  --peer-client-cert-auth \\</span><br><span class="line">  --client-cert-auth \\</span><br><span class="line">  --listen-peer-urls=https://##NODE_IP##:2380 \\</span><br><span class="line">  --initial-advertise-peer-urls=https://##NODE_IP##:2380 \\</span><br><span class="line">  --listen-client-urls=https://##NODE_IP##:2379,http://127.0.0.1:2379 \\</span><br><span class="line">  --advertise-client-urls=https://##NODE_IP##:2379 \\</span><br><span class="line">  --initial-cluster-token=etcd-cluster-0 \\</span><br><span class="line">  --initial-cluster=$&#123;ETCD_NODES&#125; \\</span><br><span class="line">  --initial-cluster-state=new \\</span><br><span class="line">  --auto-compaction-mode=periodic \\</span><br><span class="line">  --auto-compaction-retention=1 \\</span><br><span class="line">  --max-request-bytes=33554432 \\</span><br><span class="line">  --quota-backend-bytes=6442450944 \\</span><br><span class="line">  --heartbeat-interval=250 \\</span><br><span class="line">  --election-timeout=2000</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>WorkingDirectory</code>、<code>--data-dir</code>：指定工作目录和数据目录为 <code>${ETCD_DATA_DIR}</code>，需在启动服务前创建这个目录；</li><li><code>--wal-dir</code>：指定 wal 目录，为了提高性能，一般使用 SSD 或者和 <code>--data-dir</code> 不同的磁盘；</li><li><code>--name</code>：指定节点名称，当 <code>--initial-cluster-state</code> 值为 <code>new</code> 时，<code>--name</code> 的参数值必须位于 <code>--initial-cluster</code> 列表中；</li><li><code>--cert-file</code>、<code>--key-file</code>：etcd server 与 client 通信时使用的证书和私钥；</li><li><code>--trusted-ca-file</code>：签名 client 证书的 CA 证书，用于验证 client 证书；</li><li><code>--peer-cert-file</code>、<code>--peer-key-file</code>：etcd 与 peer 通信使用的证书和私钥；</li><li><code>--peer-trusted-ca-file</code>：签名 peer 证书的 CA 证书，用于验证 peer 证书；</li></ul><p>替换模板文件中的变量，为各节点创建 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for (( i=0; i &lt; 2; i++ ))</span><br><span class="line">  do</span><br><span class="line">    sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; etcd.service.template &gt; etcd-$&#123;NODE_IPS[i]&#125;.service </span><br><span class="line">  done</span><br><span class="line">ls *.service</span><br></pre></td></tr></table></figure><p>分发生成的 systemd unit 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp etcd-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/etcd.service</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h1 id="启动-etcd-服务"><a href="#启动-etcd-服务" class="headerlink" title="启动 etcd 服务"></a>启动 etcd 服务</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;ETCD_DATA_DIR&#125; $&#123;ETCD_WAL_DIR&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl restart etcd &quot; &amp;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="检查结果"><a href="#检查结果" class="headerlink" title="检查结果"></a>检查结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;systemctl status etcd|grep Active&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><p>确保状态为 <code>active (running)</code>，否则查看日志，确认原因：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">journalctl -u etcd</span><br></pre></td></tr></table></figure><h2 id="验证服务状态"><a href="#验证服务状态" class="headerlink" title="验证服务状态"></a>验证服务状态</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ETCDCTL_API=3 /opt/k8s/bin/etcdctl \</span><br><span class="line">    --endpoints=https://$&#123;node_ip&#125;:2379 \</span><br><span class="line">    --cacert=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">    --cert=/etc/etcd/cert/etcd.pem \</span><br><span class="line">    --key=/etc/etcd/cert/etcd-key.pem endpoint health</span><br><span class="line">  done</span><br></pre></td></tr></table></figure><h2 id="结果显示"><a href="#结果显示" class="headerlink" title="结果显示"></a>结果显示</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">&gt;   do</span><br><span class="line">&gt;     echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">&gt;     ETCDCTL_API=3 /opt/k8s/bin/etcdctl \</span><br><span class="line">&gt;     --endpoints=https://$&#123;node_ip&#125;:2379 \</span><br><span class="line">&gt;     --cacert=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">&gt;     --cert=/etc/etcd/cert/etcd.pem \</span><br><span class="line">&gt;     --key=/etc/etcd/cert/etcd-key.pem endpoint health</span><br><span class="line">&gt;   done</span><br><span class="line">&gt;&gt;&gt; 192.168.6.101</span><br><span class="line">https://192.168.6.101:2379 is healthy: successfully committed proposal: took = 2.45561ms</span><br><span class="line">&gt;&gt;&gt; 192.168.6.102</span><br><span class="line">https://192.168.6.102:2379 is healthy: successfully committed proposal: took = 3.898134ms</span><br></pre></td></tr></table></figure><h2 id="查看当前的-leader"><a href="#查看当前的-leader" class="headerlink" title="查看当前的 leader"></a>查看当前的 leader</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">ETCDCTL_API=3 /opt/k8s/bin/etcdctl \</span><br><span class="line">  -w table --cacert=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">  --cert=/etc/etcd/cert/etcd.pem \</span><br><span class="line">  --key=/etc/etcd/cert/etcd-key.pem \</span><br><span class="line">  --endpoints=$&#123;ETCD_ENDPOINTS&#125; endpoint status</span><br></pre></td></tr></table></figure><p>结果为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# ETCDCTL_API=3 /opt/k8s/bin/etcdctl \</span><br><span class="line">&gt;   -w table --cacert=/etc/kubernetes/cert/ca.pem \</span><br><span class="line">&gt;   --cert=/etc/etcd/cert/etcd.pem \</span><br><span class="line">&gt;   --key=/etc/etcd/cert/etcd-key.pem \</span><br><span class="line">&gt;   --endpoints=$&#123;ETCD_ENDPOINTS&#125; endpoint status</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br><span class="line">| https://192.168.6.101:2379 | 77fff77a6e7d24c5 |  3.3.13 |  864 kB |      true |         8 |      41721 |</span><br><span class="line">| https://192.168.6.102:2379 |  e82e7402173c61e |  3.3.13 |  872 kB |     false |         8 |      41721 |</span><br><span class="line">+----------------------------+------------------+---------+---------+-----------+-----------+------------+</span><br></pre></td></tr></table></figure><p>可以看到6.101为leader</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;ETCD 是一个高可用的分布式键值数据库，可用于服务发现。ETCD 采用 raft 一致性算法，基于 Go 语言实现。&lt;/p&gt;
&lt;figur
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-cfssl证书</title>
    <link href="https://shenshengkun.github.io/posts/8d664qf5.html"/>
    <id>https://shenshengkun.github.io/posts/8d664qf5.html</id>
    <published>2019-06-04T08:10:01.000Z</published>
    <updated>2019-06-05T08:51:30.034Z</updated>
    
    <content type="html"><![CDATA[<h1 id="k8s证书的三种方式"><a href="#k8s证书的三种方式" class="headerlink" title="k8s证书的三种方式"></a>k8s证书的三种方式</h1><ul><li>cfssl</li><li>easyrsa</li><li>openssl</li></ul><p>本文使用cfssl签发证书</p><h1 id="安装-cfssl-工具集"><a href="#安装-cfssl-工具集" class="headerlink" title="安装 cfssl 工具集"></a>安装 cfssl 工具集</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /opt/k8s/cert &amp;&amp; cd /opt/k8s</span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64</span><br><span class="line">mv cfssl_linux-amd64 /opt/k8s/bin/cfssl</span><br><span class="line"></span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64</span><br><span class="line">mv cfssljson_linux-amd64 /opt/k8s/bin/cfssljson</span><br><span class="line"></span><br><span class="line">wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64</span><br><span class="line">mv cfssl-certinfo_linux-amd64 /opt/k8s/bin/cfssl-certinfo</span><br><span class="line"></span><br><span class="line">chmod +x /opt/k8s/bin/*</span><br><span class="line">export PATH=/opt/k8s/bin:$PATH</span><br></pre></td></tr></table></figure><h1 id="创建根证书-CA"><a href="#创建根证书-CA" class="headerlink" title="创建根证书 (CA)"></a>创建根证书 (CA)</h1><p>CA 证书是集群所有节点共享的，<strong>只需要创建一个 CA 证书</strong>，后续创建的所有证书都由它签名。 </p><h2 id="创建配置文件"><a href="#创建配置文件" class="headerlink" title="创建配置文件"></a>创建配置文件</h2><p>CA 配置文件用于配置根证书的使用场景 (profile) 和具体参数 (usage，过期时间、服务端认证、客户端认证、加密等)，后续在签名其它证书时需要指定特定场景。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; ca-config.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;signing&quot;: &#123;</span><br><span class="line">    &quot;default&quot;: &#123;</span><br><span class="line">      &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;profiles&quot;: &#123;</span><br><span class="line">      &quot;kubernetes&quot;: &#123;</span><br><span class="line">        &quot;usages&quot;: [</span><br><span class="line">            &quot;signing&quot;,</span><br><span class="line">            &quot;key encipherment&quot;,</span><br><span class="line">            &quot;server auth&quot;,</span><br><span class="line">            &quot;client auth&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;expiry&quot;: &quot;87600h&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><code>signing</code>：表示该证书可用于签名其它证书，生成的 <code>ca.pem</code> 证书中 <code>CA=TRUE</code>；</li><li><code>server auth</code>：表示 client 可以用该该证书对 server 提供的证书进行验证；</li><li><code>client auth</code>：表示 server 可以用该该证书对 client 提供的证书进行验证；</li></ul><h2 id="创建证书签名请求文件"><a href="#创建证书签名请求文件" class="headerlink" title="创建证书签名请求文件"></a>创建证书签名请求文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cat &gt; ca-csr.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;CN&quot;: &quot;kubernetes&quot;,</span><br><span class="line">  &quot;key&quot;: &#123;</span><br><span class="line">    &quot;algo&quot;: &quot;rsa&quot;,</span><br><span class="line">    &quot;size&quot;: 2048</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;names&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;C&quot;: &quot;CN&quot;,</span><br><span class="line">      &quot;ST&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;L&quot;: &quot;BeiJing&quot;,</span><br><span class="line">      &quot;O&quot;: &quot;k8s&quot;,</span><br><span class="line">      &quot;OU&quot;: &quot;4Paradigm&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h2 id="生成-CA-证书和私钥"><a href="#生成-CA-证书和私钥" class="headerlink" title="生成 CA 证书和私钥"></a>生成 CA 证书和私钥</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">cfssl gencert -initca ca-csr.json | cfssljson -bare ca</span><br></pre></td></tr></table></figure><h1 id="分发证书文件"><a href="#分发证书文件" class="headerlink" title="分发证书文件"></a>分发证书文件</h1><p>将生成的 CA 证书、秘钥文件、配置文件拷贝到<strong>所有节点</strong>的 <code>/etc/kubernetes/cert</code> 目录下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/k8s/work</span><br><span class="line">source /opt/k8s/bin/environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/kubernetes/cert&quot;</span><br><span class="line">    scp ca*.pem ca-config.json root@$&#123;node_ip&#125;:/etc/kubernetes/cert</span><br><span class="line">  done</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;k8s证书的三种方式&quot;&gt;&lt;a href=&quot;#k8s证书的三种方式&quot; class=&quot;headerlink&quot; title=&quot;k8s证书的三种方式&quot;&gt;&lt;/a&gt;k8s证书的三种方式&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;cfssl&lt;/li&gt;
&lt;li&gt;easyrsa&lt;/li&gt;
&lt;li&gt;
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s1.14集群部署-系统初始化</title>
    <link href="https://shenshengkun.github.io/posts/9c775ab5.html"/>
    <id>https://shenshengkun.github.io/posts/9c775ab5.html</id>
    <published>2019-06-04T08:01:01.000Z</published>
    <updated>2019-06-05T08:51:18.826Z</updated>
    
    <content type="html"><![CDATA[<h1 id="K8s环境准备"><a href="#K8s环境准备" class="headerlink" title="K8s环境准备"></a>K8s环境准备</h1><h2 id="本次安装版本"><a href="#本次安装版本" class="headerlink" title="本次安装版本"></a>本次安装版本</h2><ul><li>Kubernetes 1.14.2</li><li>Docker 18.09.6-ce</li><li>Etcd 3.3.13</li><li>Flanneld 0.11.0</li></ul><h2 id="机器"><a href="#机器" class="headerlink" title="机器"></a>机器</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.6.101 node1</span><br><span class="line"></span><br><span class="line">192.168.6.102 node2</span><br></pre></td></tr></table></figure><p>其中node1，node2做master集群，也都是node节点</p><h2 id="主机名"><a href="#主机名" class="headerlink" title="主机名"></a>主机名</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hostnamectl set-hostname node1</span><br><span class="line">hostnamectl set-hostname node2</span><br><span class="line">cat &gt;&gt; /etc/hosts &lt;&lt;EOF</span><br><span class="line">192.168.6.101 node1</span><br><span class="line">192.168.6.102 node2</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h2 id="免秘钥"><a href="#免秘钥" class="headerlink" title="免秘钥"></a>免秘钥</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意：在node1上操作即可</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line">ssh-copy-id root@node1</span><br><span class="line">ssh-copy-id root@node2</span><br></pre></td></tr></table></figure><h1 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h1><h2 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h2><p>以下操作均在所有机器操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y epel-release conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget unzip net-tools</span><br></pre></td></tr></table></figure><h2 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">iptables -F &amp;&amp; iptables -X &amp;&amp; iptables -F -t nat &amp;&amp; iptables -X -t nat</span><br><span class="line">iptables -P FORWARD ACCEPT</span><br></pre></td></tr></table></figure><h2 id="关闭-swap-分区"><a href="#关闭-swap-分区" class="headerlink" title="关闭 swap 分区"></a>关闭 swap 分区</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">swapoff -a</span><br><span class="line">sed -i &apos;/ swap / s/^\(.*\)$/#\1/g&apos; /etc/fstab</span><br></pre></td></tr></table></figure><h2 id="关闭-SELinux"><a href="#关闭-SELinux" class="headerlink" title="关闭 SELinux"></a>关闭 SELinux</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">setenforce 0</span><br><span class="line">sed -i &apos;s/^SELINUX=.*/SELINUX=disabled/&apos; /etc/selinux/config</span><br></pre></td></tr></table></figure><h2 id="加载内核并优化"><a href="#加载内核并优化" class="headerlink" title="加载内核并优化"></a>加载内核并优化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">modprobe ip_vs_rr</span><br><span class="line">modprobe br_netfilter</span><br><span class="line">cat &gt; kubernetes.conf &lt;&lt;EOF</span><br><span class="line">net.bridge.bridge-nf-call-iptables=1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables=1</span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">net.ipv4.tcp_tw_recycle=0</span><br><span class="line">vm.swappiness=0 # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它</span><br><span class="line">vm.overcommit_memory=1 # 不检查物理内存是否够用</span><br><span class="line">vm.panic_on_oom=0 # 开启 OOM</span><br><span class="line">fs.inotify.max_user_instances=8192</span><br><span class="line">fs.inotify.max_user_watches=1048576</span><br><span class="line">fs.file-max=52706963</span><br><span class="line">fs.nr_open=52706963</span><br><span class="line">net.ipv6.conf.all.disable_ipv6=1</span><br><span class="line">net.netfilter.nf_conntrack_max=2310720</span><br><span class="line">EOF</span><br><span class="line">cp kubernetes.conf  /etc/sysctl.d/kubernetes.conf</span><br><span class="line">sysctl -p /etc/sysctl.d/kubernetes.conf</span><br></pre></td></tr></table></figure><h2 id="ntp"><a href="#ntp" class="headerlink" title="ntp"></a>ntp</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate ntp1.aliyun.com</span><br></pre></td></tr></table></figure><h2 id="创建相关目录"><a href="#创建相关目录" class="headerlink" title="创建相关目录"></a>创建相关目录</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p  /opt/k8s/&#123;bin,work&#125; /etc/&#123;kubernetes,etcd&#125;/cert</span><br></pre></td></tr></table></figure><h1 id="升级内核"><a href="#升级内核" class="headerlink" title="升级内核"></a>升级内核</h1><p>CentOS 7.x 系统自带的 3.10.x 内核存在一些 Bugs，导致运行的 Docker、Kubernetes 不稳定，例如：</p><ol><li>高版本的 docker(1.13 以后) 启用了 3.10 kernel 实验支持的 kernel memory account 功能(无法关闭)，当节点压力大如频繁启动和停止容器时会导致 cgroup memory leak；</li><li>网络设备引用计数泄漏，会导致类似于报错：”kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1”;</li></ol><p>解决方案如下：</p><ol><li>升级内核到 4.4.X 以上；</li><li>或者，手动编译内核，disable CONFIG_MEMCG_KMEM 特性；</li><li>或者，安装修复了该问题的 Docker 18.09.1 及以上的版本。但由于 kubelet 也会设置 kmem（它 vendor 了 runc），所以需要重新编译 kubelet 并指定 GOFLAGS=”-tags=nokmem”；</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone --branch v1.14.1 --single-branch --depth 1 https://github.com/kubernetes/kubernetes</span><br><span class="line">cd kubernetes</span><br><span class="line">KUBE_GIT_VERSION=v1.14.1 ./build/run.sh make kubelet GOFLAGS=&quot;-tags=nokmem&quot;</span><br></pre></td></tr></table></figure><p>这里采用升级内核的解决办法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm</span><br><span class="line"># 安装完成后检查 /boot/grub2/grub.cfg 中对应内核 menuentry 中是否包含 initrd16 配置，如果没有，再安装一次！</span><br><span class="line">yum --enablerepo=elrepo-kernel install -y kernel-lt</span><br><span class="line"># 设置开机从新内核启动</span><br><span class="line">grub2-set-default 0</span><br></pre></td></tr></table></figure><p>安装内核源文件（可选，在升级完内核并重启机器后执行）:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># yum erase kernel-headers</span><br><span class="line">yum --enablerepo=elrepo-kernel install kernel-lt-devel-$(uname -r) kernel-lt-headers-$(uname -r)</span><br></pre></td></tr></table></figure><h1 id="设置配置参数脚本"><a href="#设置配置参数脚本" class="headerlink" title="设置配置参数脚本"></a>设置配置参数脚本</h1><h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# cat environment.sh</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#!/usr/bin/bash</span><br><span class="line"></span><br><span class="line"># 生成 EncryptionConfig 所需的加密 key</span><br><span class="line">export ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)</span><br><span class="line"></span><br><span class="line"># 集群各机器 IP 数组</span><br><span class="line">export NODE_IPS=(192.168.6.101 192.168.6.102)</span><br><span class="line"></span><br><span class="line"># 集群各 IP 对应的主机名数组</span><br><span class="line">export NODE_NAMES=(node1 node2)</span><br><span class="line"></span><br><span class="line"># etcd 集群服务地址列表</span><br><span class="line">export ETCD_ENDPOINTS=&quot;https://192.168.6.101:2379,https://192.168.6.102:2379&quot;</span><br><span class="line"></span><br><span class="line"># etcd 集群间通信的 IP 和端口</span><br><span class="line">export ETCD_NODES=&quot;node1=https://192.168.6.101:2380,node2=https://192.168.6.102:2380&quot;</span><br><span class="line"></span><br><span class="line"># kube-apiserver 的反向代理(kube-nginx)地址端口</span><br><span class="line">export KUBE_APISERVER=&quot;https://127.0.0.1:8443&quot;</span><br><span class="line"></span><br><span class="line"># 节点间互联网络接口名称</span><br><span class="line">export IFACE=&quot;ens160&quot;</span><br><span class="line"></span><br><span class="line"># etcd 数据目录</span><br><span class="line">export ETCD_DATA_DIR=&quot;/data/k8s/etcd/data&quot;</span><br><span class="line"></span><br><span class="line"># etcd WAL 目录，建议是 SSD 磁盘分区，或者和 ETCD_DATA_DIR 不同的磁盘分区</span><br><span class="line">export ETCD_WAL_DIR=&quot;/data/k8s/etcd/wal&quot;</span><br><span class="line"></span><br><span class="line"># k8s 各组件数据目录</span><br><span class="line">export K8S_DIR=&quot;/data/k8s/k8s&quot;</span><br><span class="line"></span><br><span class="line"># docker 数据目录</span><br><span class="line">export DOCKER_DIR=&quot;/data/k8s/docker&quot;</span><br><span class="line"></span><br><span class="line">## 以下参数一般不需要修改</span><br><span class="line"></span><br><span class="line"># TLS Bootstrapping 使用的 Token，可以使用命令 head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos; 生成</span><br><span class="line">BOOTSTRAP_TOKEN=&quot;4d8a35f48da304e4433ba0bda5b8ffd1&quot;</span><br><span class="line"></span><br><span class="line"># 最好使用 当前未用的网段 来定义服务网段和 Pod 网段</span><br><span class="line"></span><br><span class="line"># 服务网段，部署前路由不可达，部署后集群内路由可达(kube-proxy 保证)</span><br><span class="line">SERVICE_CIDR=&quot;10.254.0.0/16&quot;</span><br><span class="line"></span><br><span class="line"># Pod 网段，建议 /16 段地址，部署前路由不可达，部署后集群内路由可达(flanneld 保证)</span><br><span class="line">CLUSTER_CIDR=&quot;172.30.0.0/16&quot;</span><br><span class="line"></span><br><span class="line"># 服务端口范围 (NodePort Range)</span><br><span class="line">export NODE_PORT_RANGE=&quot;30000-32767&quot;</span><br><span class="line"></span><br><span class="line"># flanneld 网络配置前缀</span><br><span class="line">export FLANNEL_ETCD_PREFIX=&quot;/kubernetes/network&quot;</span><br><span class="line"></span><br><span class="line"># kubernetes 服务 IP (一般是 SERVICE_CIDR 中第一个IP)</span><br><span class="line">export CLUSTER_KUBERNETES_SVC_IP=&quot;10.254.0.1&quot;</span><br><span class="line"></span><br><span class="line"># 集群 DNS 服务 IP (从 SERVICE_CIDR 中预分配)</span><br><span class="line">export CLUSTER_DNS_SVC_IP=&quot;10.254.0.2&quot;</span><br><span class="line"></span><br><span class="line"># 集群 DNS 域名（末尾不带点号）</span><br><span class="line">export CLUSTER_DNS_DOMAIN=&quot;cluster.local&quot;</span><br><span class="line"></span><br><span class="line"># 将二进制目录 /opt/k8s/bin 加到 PATH 中</span><br><span class="line">export PATH=/opt/k8s/bin:$PATH</span><br></pre></td></tr></table></figure><h2 id="分发到所有节点"><a href="#分发到所有节点" class="headerlink" title="分发到所有节点"></a>分发到所有节点</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">source environment.sh</span><br><span class="line">for node_ip in $&#123;NODE_IPS[@]&#125;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;</span><br><span class="line">    scp environment.sh root@$&#123;node_ip&#125;:/opt/k8s/bin/</span><br><span class="line">    ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot;</span><br><span class="line">  done</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;K8s环境准备&quot;&gt;&lt;a href=&quot;#K8s环境准备&quot; class=&quot;headerlink&quot; title=&quot;K8s环境准备&quot;&gt;&lt;/a&gt;K8s环境准备&lt;/h1&gt;&lt;h2 id=&quot;本次安装版本&quot;&gt;&lt;a href=&quot;#本次安装版本&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>tomcat报SEVERE Error listenerStart</title>
    <link href="https://shenshengkun.github.io/posts/1b8b1cc1.html"/>
    <id>https://shenshengkun.github.io/posts/1b8b1cc1.html</id>
    <published>2019-05-28T09:49:10.000Z</published>
    <updated>2019-05-30T02:29:19.208Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>启动tomcat时报错，错误信息如下： </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">org.apache.catalina.core.StandardContext startInternal</span><br><span class="line">SEVERE: Error listenerStart</span><br><span class="line">org.apache.catalina.core.StandardContext startInternal</span><br><span class="line">SEVERE: Context [/projectname] startup failed due to previous errors</span><br></pre></td></tr></table></figure><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>在<code>WEB-INF/classes</code>目录下新建一个文件叫<code>logging.properties</code>，内容如下 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">handlers = org.apache.juli.FileHandler, java.util.logging.ConsoleHandler  </span><br><span class="line">org.apache.juli.FileHandler.level = FINE  </span><br><span class="line">org.apache.juli.FileHandler.directory = $&#123;catalina.base&#125;/logs  </span><br><span class="line">org.apache.juli.FileHandler.prefix = error-debug.   </span><br><span class="line">java.util.logging.ConsoleHandler.level = FINE  </span><br><span class="line">java.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter</span><br></pre></td></tr></table></figure><p>之后，重启tomcat查看日志，就可以看到是由于数据库连接或者jdk版本不兼容等原因导致的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;问题&quot;&gt;&lt;a href=&quot;#问题&quot; class=&quot;headerlink&quot; title=&quot;问题&quot;&gt;&lt;/a&gt;问题&lt;/h1&gt;&lt;p&gt;启动tomcat时报错，错误信息如下： &lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;t
      
    
    </summary>
    
      <category term="中间件" scheme="https://shenshengkun.github.io/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>k8s强制删除</title>
    <link href="https://shenshengkun.github.io/posts/267edcc5.html"/>
    <id>https://shenshengkun.github.io/posts/267edcc5.html</id>
    <published>2019-05-27T09:43:01.000Z</published>
    <updated>2019-05-30T02:29:19.201Z</updated>
    
    <content type="html"><![CDATA[<h1 id="可使用kubectl中的强制删除命令"><a href="#可使用kubectl中的强制删除命令" class="headerlink" title="可使用kubectl中的强制删除命令"></a>可使用kubectl中的强制删除命令</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 删除POD</span><br><span class="line">kubectl delete pod PODNAME --force --grace-period=0</span><br><span class="line"></span><br><span class="line"># 删除NAMESPACE</span><br><span class="line">kubectl delete namespace NAMESPACENAME --force --grace-period=0</span><br></pre></td></tr></table></figure><p>有时候这种方法也删除不掉，可能是之前删除顺序有问题，没有删干净pod，就删除命名空间，导致删除不掉</p><h1 id="直接从ETCD中删除源数据"><a href="#直接从ETCD中删除源数据" class="headerlink" title="直接从ETCD中删除源数据"></a>直接从ETCD中删除源数据</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 删除default namespace下的pod名为pod-to-be-deleted-0</span><br><span class="line">ETCDCTL_API=3 etcdctl del /registry/pods/default/pod-to-be-deleted-0</span><br><span class="line"></span><br><span class="line"># 删除需要删除的NAMESPACE</span><br><span class="line">etcdctl del /registry/namespaces/NAMESPACENAME</span><br></pre></td></tr></table></figure><h2 id="添加别名"><a href="#添加别名" class="headerlink" title="添加别名"></a>添加别名</h2><p>上面直接etcd删除，是证书直接能找到时候，如果证书配置方式不一样，就需要手动配一下！</p><p>配置别名etcdctl3，添加证书等参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">alias etcdctl3=&apos;docker run --rm -it \</span><br><span class="line">--net host -e ETCDCTL_API=3 \</span><br><span class="line">-v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:3.3.10 etcdctl \</span><br><span class="line">--cert /etc/kubernetes/pki/etcd/peer.crt \</span><br><span class="line">--key /etc/kubernetes/pki/etcd/peer.key \</span><br><span class="line">--cacert /etc/kubernetes/pki/etcd/ca.crt \</span><br><span class="line">--endpoints https://192.168.3.101:2379,https://192.168.3.102:2379,https://192.168.3.103:2379&apos;</span><br></pre></td></tr></table></figure><p>查询都有哪些daemonsets</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tcdctl3 get /registry/daemonsets/ --prefix --keys-only</span><br><span class="line">/registry/daemonsets/default/testpod</span><br><span class="line">/registry/daemonsets/kube-system/calico-node</span><br><span class="line">/registry/daemonsets/kube-system/kube-proxy</span><br></pre></td></tr></table></figure><p>与kubectl查看的结果一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get daemonsets --all-namespaces </span><br><span class="line">NAMESPACE     NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE</span><br><span class="line">default       testpod       3         3         3       3            3           &lt;none&gt;                        91m</span><br><span class="line">kube-system   calico-node   3         3         3       3            3           beta.kubernetes.io/os=linux   116m</span><br><span class="line">kube-system   kube-proxy    3         3         3       3            3           &lt;none&gt;                        122m</span><br></pre></td></tr></table></figure><p>在etcd中查询default namespace中的pod</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">etcdctl3 get /registry/pods/default --prefix --keys-only </span><br><span class="line">/registry/pods/default/testpod-5wtb7</span><br><span class="line">/registry/pods/default/testpod-646d8</span><br><span class="line">/registry/pods/default/testpod-t7ps7</span><br></pre></td></tr></table></figure><p>kubectl命令看到结果与etcd中一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -l app=fortest</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE</span><br><span class="line">testpod-5wtb7   1/1     Running   0          93m</span><br><span class="line">testpod-646d8   1/1     Running   0          93m</span><br><span class="line">testpod-t7ps7   1/1     Running   0          93m</span><br></pre></td></tr></table></figure><p>在etcd中删除pod testpod-t7ps7</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">etcdctl3 del /registry/pods/default/testpod-t7ps7    </span><br><span class="line">1</span><br></pre></td></tr></table></figure><p>再次查看pod，发现testpod-t7ps7已经没有了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE</span><br><span class="line">testpod-5wtb7   1/1     Running   0          96m</span><br><span class="line">testpod-646d8   1/1     Running   0          96m</span><br><span class="line">testpod-qczvt   1/1     Running   0          17s</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;可使用kubectl中的强制删除命令&quot;&gt;&lt;a href=&quot;#可使用kubectl中的强制删除命令&quot; class=&quot;headerlink&quot; title=&quot;可使用kubectl中的强制删除命令&quot;&gt;&lt;/a&gt;可使用kubectl中的强制删除命令&lt;/h1&gt;&lt;figure c
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
  <entry>
    <title>fabric问题汇总</title>
    <link href="https://shenshengkun.github.io/posts/9c5623c7.html"/>
    <id>https://shenshengkun.github.io/posts/9c5623c7.html</id>
    <published>2019-05-22T05:04:10.000Z</published>
    <updated>2019-05-30T02:29:19.211Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h1><h2 id="fabric测试项目安装问题"><a href="#fabric测试项目安装问题" class="headerlink" title="fabric测试项目安装问题"></a>fabric测试项目安装问题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install probuf的时候报的错</span><br><span class="line">fabric依赖的sdk需要依赖c++的编译库，windows也许windows-tools，linux也需要支持的gc++,gc</span><br></pre></td></tr></table></figure><h2 id="fabric-ca-server-存储私钥么"><a href="#fabric-ca-server-存储私钥么" class="headerlink" title="fabric-ca-server 存储私钥么"></a>fabric-ca-server 存储私钥么</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">从mysql中不存储私钥</span><br></pre></td></tr></table></figure><h2 id="启动order遇到问题"><a href="#启动order遇到问题" class="headerlink" title="启动order遇到问题"></a>启动order遇到问题</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Failed to initialize local MSP: the supplied identity is not valid: x509: certificate signed by unknown authority</span><br><span class="line"></span><br><span class="line">原因：实体的证书不是组织的证书签发的</span><br></pre></td></tr></table></figure><h2 id="docker-compose创建报错"><a href="#docker-compose创建报错" class="headerlink" title="docker-compose创建报错"></a>docker-compose创建报错</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[blocksProvider] DeliverBlocks -&gt; ERRO 039 [vaccinechannel] Got error &amp;&#123;FORBIDDEN&#125;</span><br><span class="line"></span><br><span class="line">解决办法： 需要在组织的msp中增加config.yaml</span><br></pre></td></tr></table></figure><h1 id="应用过程中问题"><a href="#应用过程中问题" class="headerlink" title="应用过程中问题"></a>应用过程中问题</h1><h2 id="Peer或者Orderer不通"><a href="#Peer或者Orderer不通" class="headerlink" title="Peer或者Orderer不通"></a>Peer或者Orderer不通</h2><p>当不通的时候，先确认域名对应的IP是否正确，然后用telnet检查服务端口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ping peer0.org1.example.com</span><br><span class="line">telnet peer0.org1.example.com 7051</span><br></pre></td></tr></table></figure><p>如果不通，检查一下/etc/hosts中是否设置了域名和IP的对应关系是否正确。</p><p>如果还是不通，看一下系统有没有防火墙，7051端口有没有被防火墙禁止。</p><h2 id="目标Peer上的Docker没有启动，导致合约实例化失败"><a href="#目标Peer上的Docker没有启动，导致合约实例化失败" class="headerlink" title="目标Peer上的Docker没有启动，导致合约实例化失败"></a>目标Peer上的Docker没有启动，导致合约实例化失败</h2><p>实例化合约时出错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./peer.sh chaincode instantiate -o orderer.example.com:7050 --tls true --cafile ./tlsca.example.com-cert.pem -C mychannel -n demo -v 0.0.1 -c &apos;&#123;&quot;Args&quot;:[&quot;init&quot;]&#125;&apos; -P &quot;OR(&apos;Org1MSP.member&apos;,&apos;Org2MSP.member&apos;)&quot;</span><br></pre></td></tr></table></figure><p>错误如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: Error endorsing chaincode: rpc error: code = Unknown desc = error starting container: Post http://unix.sock/containers/create?name=dev-peer1.org1.example.com-demo-0.0.1: dial unix /var/run/docker.sock: connect: no such file or directory</span><br></pre></td></tr></table></figure><p>这是目标peer上的docker没有启动造成的。</p><h2 id="genesisblock中admin证书错误导致orderer-panic-x509-ECDSA-verification-failure"><a href="#genesisblock中admin证书错误导致orderer-panic-x509-ECDSA-verification-failure" class="headerlink" title="genesisblock中admin证书错误导致orderer panic: x509: ECDSA verification failure"></a>genesisblock中admin证书错误导致orderer panic: x509: ECDSA verification failure</h2><p>orderer在启动的时候报错，直接panic：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-----END CERTIFICATE-----</span><br><span class="line">2018-06-22 14:27:30.462 UTC [orderer/commmon/multichannel] newLedgerResources -&gt; CRIT 04d Error creating channelconfig bundle: initializing channelconfig failed: could not create channel Consortiums sub-group config: setting up the MSP manager failed: the supplied identity is not valid: x509: certificate signed by unknown authority (possibly because of &quot;x509: ECDSA verification failure&quot; while trying to verify candidate authority certificate &quot;ca.org1.example.com&quot;)</span><br><span class="line">panic: Error creating channelconfig bundle: initializing channelconfig failed: could not create channel Consortiums sub-group config: setting up the MSP manager failed: the supplied identity is not valid: x509: certificate signed by unknown authority (possibly because of &quot;x509: ECDSA verification failure&quot; while trying to verify candidate authority certificate &quot;ca.org1.example.com&quot;)</span><br><span class="line"></span><br><span class="line">goroutine 1 [running]:</span><br><span class="line">github.com/hyperledger/fabric/vendor/github.com/op/go-logging.(*Logger).Panicf(0xc4201ee120, 0x108668e, 0x27, 0xc42026af50, 0x1, 0x1)</span><br><span class="line">/w/workspace/fabric-binaries-x86_64/gopath/src/github.com/hyperledger/fabric/vendor/github.com/op/go-logging/logger.go:194 +0x134</span><br><span class="line">github.com/hyperledger/fabric/orderer/common/multichannel.(*Registrar).newLedgerResources(0xc42010a380, 0xc420138840, 0xc420138840)</span><br><span class="line">/w/workspace/fabric-binaries-x86_64/gopath/src/github.com/hyperledger/fabric/orderer/common/multichannel/registrar.go:253 +0x391</span><br></pre></td></tr></table></figure><p>怀疑是创世块的原因，用下面的命令将创始块解开：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/configtxgen -profile TwoOrgsOrdererGenesis -outputBlock ./genesisblock</span><br></pre></td></tr></table></figure><p>发现比较奇怪的地方，Org1的Admin证书有两个：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&quot;groups&quot;: &#123;</span><br><span class="line">  &quot;Org1MSP&quot;: &#123;</span><br><span class="line">  &quot;mod_policy&quot;: &quot;Admins&quot;,</span><br><span class="line">  ...</span><br><span class="line">  &quot;mod_policy&quot;: &quot;Admins&quot;,</span><br><span class="line">  &quot;value&quot;: &#123;</span><br><span class="line">  &quot;config&quot;: &#123;</span><br><span class="line">  &quot;admins&quot;: [</span><br><span class="line">  &quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNHVENDQWIrZ0F3SUJBZ0lRVXRxQWxlZENzWkErWStWdlZMUTZQakFLQmdncWhrak9QUVFEQWpCek1Rc3cKQ1FZRFZRUUdFd0pWVXpFVE1CRUdBMVVFQ0JNS1EyRnNhV1p2Y201cFlURVdNQlFHQTFVRUJ4TU5VMkZ1SUVaeQpZVzVqYVhOamJ6RVpNQmNHQTFVRUNoTVFiM0puTVM1bGVHRnRjR3hsTG1OdmJURWNNQm9HQTFVRUF4TVRZMkV1CmIzSm5NUzVsZUdGdGNHeGxMbU52YlRBZUZ3MHhPREEyTWpFd05qVTNNekJhRncweU9EQTJNVGd3TmpVM016QmEKTUZzeEN6QUpCZ05WQkFZVEFsVlRNUk13RVFZRFZRUUlFd3BEWVd4cFptOXlibWxoTVJZd0ZBWURWUVFIRXcxVApZVzRnUm5KaGJtTnBjMk52TVI4d0hRWURWUVFEREJaQlpHMXBia0J2Y21jeExtVjRZVzF3YkdVdVkyOXRNRmt3CkV3WUhLb1pJemowQ0FRWUlLb1pJemowREFRY0RRZ0FFRVp3cUhTVmxxRGNKNC9aVSt0YnB5RVBSTkl5ellMdTMKRGlRVUZOMklBZm5vVGhjTjRmY3Y4c2dsdXUxcnpJYUVHSFRFLzd0TC9EdEg2U3Fjd2tOQkthTk5NRXN3RGdZRApWUjBQQVFIL0JBUURBZ2VBTUF3R0ExVWRFd0VCL3dRQ01BQXdLd1lEVlIwakJDUXdJb0FnbkpjYVVLVFlseVJxCjcyckk4QXNINHNVZHB0ZytWY3IvbHkxZlp3QndrOEF3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUloQUsvRXh6NlYKRVYwUFl4M1BQbitPMysvODQrdXFEVkZ2Q1ZRUEVNcU1yV3dkQWlBNVVqTDcyb2drTHB3UUtGZ1ptdTJqRmtPWApSVnhpY0htLzZCR3htelFRc1E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==&quot;,</span><br><span class="line">  &quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNGVENDQWJ5Z0F3SUJBZ0lRU3E0VzJ1SEVqbHdXZHdGY21WNUlpekFLQmdncWhrak9QUVFEQWpCek1Rc3cKQ1FZRFZRUUdFd0pWVXpFVE1CRUdBMVVFQ0JNS1EyRnNhV1p2Y201cFlURVdNQlFHQTFVRUJ4TU5VMkZ1SUVaeQpZVzVqYVhOamJ6RVpNQmNHQTFVRUNoTVFiM0puTVM1bGVHRnRjR3hsTG1OdmJURWNNQm9HQTFVRUF4TVRZMkV1CmIzSm5NUzVsZUdGdGNHeGxMbU52YlRBZUZ3MHhPREEyTWpFd056VXdNVEZhRncweU9EQTJNVGd3TnpVd01URmEKTUZneEN6QUpCZ05WQkFZVEFsVlRNUk13RVFZRFZRUUlFd3BEWVd4cFptOXlibWxoTVJZd0ZBWURWUVFIRXcxVApZVzRnUm5KaGJtTnBjMk52TVJ3d0dnWURWUVFERXhOallTNXZjbWN4TG1WNFlXMXdiR1V1WTI5dE1Ga3dFd1lICktvWkl6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVxNHl6K0tqSTR2ZmtObzQ0bWp0Q25HQ2cwLzA3L2Y5VW1sZlEKMlpSZWtHN2lyVm1QY0N6YnRVVEcvTFJjbndVemgyaFMvZkg5cGxvZEM4a1pwSlpXQzZOTk1Fc3dEZ1lEVlIwUApBUUgvQkFRREFnZUFNQXdHQTFVZEV3RUIvd1FDTUFBd0t3WURWUjBqQkNRd0lvQWdPc1NNQ2VqcnBOMnBhNEZSCnBOMVE2eXJkVHJleXNGY0Q1Ym9TcVNzSnFLNHdDZ1lJS29aSXpqMEVBd0lEUndBd1JBSWdCQWo1Q3l2cEFhU0kKaTh4anpVVHZxbUt5dmxSOFFPeExBUTAvVi9jRGpTNENJRVg3V1lnZzYwTFUwTy9LNEpmVVpiQmoyNHRBbTkxcgpkQmczN21IZHZVcSsKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=&quot;</span><br><span class="line">  ],</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>将上面的两大行字符串分别用base64解码得到证书，然后用openssl命令查看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;LS0tLS1CRUdJTiBDRVJUSU....tLS0tCg==&quot; |base64 -D &gt;a.cert</span><br><span class="line">openssl x509 -in a.cert -text</span><br></pre></td></tr></table></figure><p>第一个证书正确：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"> Subject: C=US, ST=California, L=San Francisco, CN=Admin@org1.example.com</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>查看第二行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;LS0tLS1CRUdJTi....tLS0tLQo=&quot; |base64 -D &gt;b.cert</span><br><span class="line">openssl x509 -in b.cert -text</span><br></pre></td></tr></table></figure><p>发现第二个证书是CA证书，不是用户证书！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Subject: C=US, ST=California, L=San Francisco, CN=ca.org1.example.com</span><br></pre></td></tr></table></figure><p>检查生成genesisblock时使用的configtx.yaml文件，发现configtx.yaml中配置的msp目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MSPDir: ./certs/peerOrganizations/org1.example.com/msp</span><br></pre></td></tr></table></figure><p>msp的admincerts子目录中，多出了一个ca证书：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls ./certs/peerOrganizations/org1.example.com/msp/admincerts/</span><br><span class="line">Admin@org1.example.com-cert.pem ca.org1.example.com-cert.pem</span><br></pre></td></tr></table></figure><p>把多出的ca证书删除。</p><h2 id="残留数据导致orderer启动失败"><a href="#残留数据导致orderer启动失败" class="headerlink" title="残留数据导致orderer启动失败"></a>残留数据导致orderer启动失败</h2><p>启动orderer的时候报错，orderer直接panic：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2018-06-21 11:01:47.892 CST [orderer/commmon/multichannel] newLedgerResources -&gt; CRIT 052 Error creating channelconfig bundle: initializing channelconfig failed: could not create channel Orderer sub-group config: setting up the MSP manager failed: the supplied identity is not valid: x509: certificate signed by unknown authority (possibly because of &quot;x509: ECDSA verification failure&quot; while trying to verify candidate authority certificate &quot;ca.example.com&quot;)</span><br><span class="line">panic: Error creating channelconfig bundle: initializing channelconfig failed: could not create channel Orderer sub-group config: setting up the MSP manager failed: the supplied identity is not valid: x509: certificate signed by unknown authority (possibly because of &quot;x509: ECDSA verification failure&quot; while trying to verify candidate authority certificate &quot;ca.example.com&quot;)</span><br></pre></td></tr></table></figure><p>排查发现，部署orderer的机器上以前部署过orderer，并且orderer.yaml中配置的数据路径<code>/opt/app/fabric/orderer/data</code>中残留了以前的数据。</p><p>将/opt/app/fabric/orderer/data中的文件都删除后，问题解决。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;安装部署&quot;&gt;&lt;a href=&quot;#安装部署&quot; class=&quot;headerlink&quot; title=&quot;安装部署&quot;&gt;&lt;/a&gt;安装部署&lt;/h1&gt;&lt;h2 id=&quot;fabric测试项目安装问题&quot;&gt;&lt;a href=&quot;#fabric测试项目安装问题&quot; class=&quot;headerli
      
    
    </summary>
    
      <category term="区块链" scheme="https://shenshengkun.github.io/categories/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
    
  </entry>
  
  <entry>
    <title>Prometheus添加验证登录</title>
    <link href="https://shenshengkun.github.io/posts/239bdf86.html"/>
    <id>https://shenshengkun.github.io/posts/239bdf86.html</id>
    <published>2019-05-22T01:38:01.000Z</published>
    <updated>2019-05-30T02:29:19.206Z</updated>
    
    <content type="html"><![CDATA[<h1 id="prometheus添加nginx验证"><a href="#prometheus添加nginx验证" class="headerlink" title="prometheus添加nginx验证"></a>prometheus添加nginx验证</h1><p>Prometheus默认开箱即食，并没有设置认证方式，如果你使用Grafana那就另当别论。</p><p>如果你想直接访问Prometheus并且需要设置个认证，那么通过Nginx反向代理是一个不错的选择。</p><p>本文通过Nginx反向代理增加401认证方式来实现。</p><h2 id="安装apache-htpasswd工具"><a href="#安装apache-htpasswd工具" class="headerlink" title="安装apache-htpasswd工具"></a>安装apache-htpasswd工具</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum -y install httpd-tools</span><br></pre></td></tr></table></figure><h2 id="加密认证密码"><a href="#加密认证密码" class="headerlink" title="加密认证密码"></a>加密认证密码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">htpasswd -cs /usr/local/nginx/conf/401htpasswd sy</span><br></pre></td></tr></table></figure><h2 id="设置Nginx反向代理及401认证"><a href="#设置Nginx反向代理及401认证" class="headerlink" title="设置Nginx反向代理及401认证"></a>设置Nginx反向代理及401认证</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/nginx/conf/vhost</span><br><span class="line">vi demo.conf</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name 192.168.10.177;</span><br><span class="line"> </span><br><span class="line">    location / &#123;</span><br><span class="line">        auth_basic &quot;Prometheus&quot;;</span><br><span class="line">        auth_basic_user_file /usr/local/nginx/conf/401htpasswd;</span><br><span class="line">        proxy_pass http://localhost:9090/;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动nginx；访问192.168.10.177</p><p>正常输入密码后，会看到Prometheus页面，如果提示403则表示账号密码不正确，或者路径配错。 </p><h1 id="加密node-exporter"><a href="#加密node-exporter" class="headerlink" title="加密node_exporter"></a>加密node_exporter</h1><p>node_exporter是Prometheus的一个扩展程序，也是通过go语言编写，同样是开箱即食，主要用来采集服务器上的数据（CPU、内存等等）。 所以为了安全考虑，也需要加密一下。</p><h2 id="Nginx配置如下"><a href="#Nginx配置如下" class="headerlink" title="Nginx配置如下"></a>Nginx配置如下</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 19100;</span><br><span class="line">    server_name 你的远程主机IP;</span><br><span class="line"> </span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass http://localhost:9100/;</span><br><span class="line">        auth_basic &quot;Prometheus&quot;;</span><br><span class="line">        auth_basic_user_file /usr/local/nginx/conf/401htpasswd;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="prometheus配置"><a href="#prometheus配置" class="headerlink" title="prometheus配置"></a>prometheus配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在Prometheus配置文件下面添加以下内容，username是远程服务器认证账号，password为加密密码，此处IP为远程服务器的IP地址，不需要加http。</span><br><span class="line"></span><br><span class="line">- job_name: server</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&apos;IP:19100&apos;]</span><br><span class="line">        labels:</span><br><span class="line">          instance: name</span><br><span class="line">    basic_auth:</span><br><span class="line">      username: sy</span><br><span class="line">      password: 123456</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;prometheus添加nginx验证&quot;&gt;&lt;a href=&quot;#prometheus添加nginx验证&quot; class=&quot;headerlink&quot; title=&quot;prometheus添加nginx验证&quot;&gt;&lt;/a&gt;prometheus添加nginx验证&lt;/h1&gt;&lt;p&gt;Pr
      
    
    </summary>
    
      <category term="k8s" scheme="https://shenshengkun.github.io/categories/k8s/"/>
    
    
  </entry>
  
</feed>
