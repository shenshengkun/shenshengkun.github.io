<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[k8s上用ceph-rbd存储]]></title>
    <url>%2Fposts%2F0ijfj445.html</url>
    <content type="text"><![CDATA[k8s默认使用的本地存储，集群容灾性差，ceph作为开源的分布式存储系统，与openstack环境搭配使用，已经很多云计算公司运用于生产环境，可靠性得到验证。这里介绍一下在k8s环境下ceph如何使用. Kubernetes支持后两种存储接口,支持的接入模式如下图: ceph端新建pool新建一个pool pool_1包含90个pg 1ceph osd pool create pool_1 90 RBD块设备在ceph集群中新建1个rbd块设备，lun1 1rbd create pool_1/lun1 --size 10G ceph权限控制123456789101112131415161718192021使用ceph-deploy --overwrite-conf admin部署的keyring权限太大，可以自己创建一个keyring client.rdb给块设备客户端node用# ceph auth get-or-create client.rbd mon &apos;allow r&apos; osd &apos;allow class-read object_prefix rbd_children, allow rwx pool=pool_1&apos; &gt; ceph.client.rbd.keyringk8s节点需要安装cephyum install ceph-commonecho &apos;rbd&apos; &gt; /etc/modules-load.d/rbd.confmodprobe rbdlsmod | grep rbdrbd 83640 0 libceph 306625 1 rbd配置文件秘钥传到k8s上[root@ceph ceph]# scp ceph.client.rbd.keyring 192.168.6.102:/etc/ceph/root@192.168.6.102&apos;s password: ceph.client.rdb.keyring 100% 63 8.5KB/s 00:00 [root@ceph ceph]# scp ceph.conf 192.168.6.102:/etc/ceph/ root@192.168.6.102&apos;s password: ceph.conf 100% 310 25.1KB/s 00:00 [root@ceph ceph]# k8s的node上操作1234567891011121314151617181920[root@node1 ceph]# ceph -s --name client.rdb cluster: id: cbc04385-1cdf-4512-a3f5-a5b3e8686a05 health: HEALTH_WARN application not enabled on 1 pool(s) services: mon: 1 daemons, quorum ceph mgr: ceph(active) osd: 1 osds: 1 up, 1 in data: pools: 1 pools, 90 pgs objects: 5 objects, 709B usage: 1.00GiB used, 19.0GiB / 20.0GiB avail pgs: 90 active+clean 警告解决办法：ceph health detailceph osd pool application enable pool_1 rbd map设备123456789# rbd map pool_1/lun1 --name client.rbdrbd: sysfs write failedRBD image feature set mismatch. Try disabling features unsupported by the kernel with &quot;rbd feature disable&quot;.In some cases useful info is found in syslog - try &quot;dmesg | tail&quot;.rbd: map failed: (6) No such device or address解决办法：在ceph节点上rbd feature disable pool_1/lun1 exclusive-lock, object-map, fast-diff, deep-flatten 将块设备挂载在操作系统中进行格式化12rbd map pool_1/lun1 --name client.rbdmkfs.ext4 /dev/rbd0 创建pv、pvc123对ceph.client.admin.keyring 的内容进行base64编码[root@node1 ceph]# ceph auth get-key client.rbd | base64QVFCTktERmRzeXpKQUJBQVVvVGVvWVYyamxhRi8zNU1hZ2R2dFE9PQ== 123456789101112根据上面的输出，创建secret ceph-client-rbd[root@node1 ceph]# cat ceph-secret.yml apiVersion: v1kind: Secretmetadata: name: ceph-client-rbdtype: &quot;kubernetes.io/rbd&quot; data: key: QVFCTktERmRzeXpKQUJBQVVvVGVvWVYyamxhRi8zNU1hZ2R2dFE9PQ== kubectl apply -f ceph-secret.yml 123456789101112131415161718192021222324252627282930创建pv，注意： 这里是user：rbd 而不是user: client.rbd[root@node1 ceph]# cat pv.yml kind: PersistentVolumeapiVersion: v1metadata: name: ceph-pool1-lun1spec: storageClassName: manual capacity: storage: 10Gi accessModes: - ReadWriteOnce rbd: fsType: ext4 image: lun1 monitors: - &apos;192.168.6.101:6789&apos; pool: pool_1 readOnly: false secretRef: name: ceph-client-rbd namespace: default user: rbd [root@node1 ceph]# kubectl apply -f pv.yml persistentvolume/ceph-pool1-lun1 created[root@node1 ceph]# kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEceph-pool1-lun1 10Gi RWO Retain Available manual 4s 12345678910111213141516171819202122创建pvc[root@node1 ceph]# cat pvc.yml kind: PersistentVolumeClaimapiVersion: v1metadata: name: pvc1spec: storageClassName: manual accessModes: - ReadWriteOnce resources: requests: storage: 10Gi [root@node1 ceph]# kubectl apply -f pvc.yml persistentvolumeclaim/pvc1 created[root@node1 ceph]# kubectl get pvcNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpvc1 Bound ceph-pool1-lun1 10Gi RWO manual 7s]]></content>
      <categories>
        <category>ceph</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ceph对象存储集群部署]]></title>
    <url>%2Fposts%2Fouy564tra.html</url>
    <content type="text"><![CDATA[集群架构12345192.168.10.186 ceph1 admin、mon、mgr、osd、rgw192.168.10.187 ceph2 mon、mgr、osd、rgw 192.168.10.188 ceph3 mon、mgr、osd、rgw 部署123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263[root@10dot186 ~]# vim /etc/hosts192.168.10.186 ceph1192.168.10.187 ceph2192.168.10.188 ceph3hostnamectl set-hostname ceph1hostnamectl set-hostname ceph2hostnamectl set-hostname ceph3ntpdate ntp1.aliyun.comssh-keygenssh-copy-id ceph1ssh-copy-id ceph2ssh-copy-id ceph3[root@ceph1 ~]# vim /etc/yum.repos.d/ceph.repo[ceph]name=Ceph packages for $basearchbaseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/$basearchenabled=1gpgcheck=1priority=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc[ceph-noarch]name=Ceph noarch packagesbaseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarchenabled=1gpgcheck=1priority=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMSenabled=0gpgcheck=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.ascpriority=1yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmyum makecacheyum update -yyum install -y ceph-deploymkdir /etc/ceph &amp;&amp; cd /etc/cephceph-deploy new ceph1 ceph2 ceph3yum install -y python-setuptools 123456在配置文件中增加：osd_pool_default_size = 3[mgr]mgr modules = dashboard[mon]mon allow pool delete = true mon123456789101112131415161718192021ceph-deploy install ceph1 ceph2 ceph3ceph-deploy mon create-initial[root@ceph1 ceph]# ceph -s cluster: id: fcb2fa5e-481a-4494-9a27-374048f37113 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0B usage: 0B used, 0B / 0B avail pgs: mgr123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148ceph-deploy mgr create ceph1 ceph2 ceph3[root@ceph1 ceph]# ceph -s cluster: id: fcb2fa5e-481a-4494-9a27-374048f37113 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 mgr: ceph1(active), standbys: ceph2, ceph3 osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0B usage: 0B used, 0B / 0B avail pgs: [root@ceph1 ceph]# ceph mgr dump&#123; &quot;epoch&quot;: 4, &quot;active_gid&quot;: 4122, &quot;active_name&quot;: &quot;ceph1&quot;, &quot;active_addr&quot;: &quot;192.168.10.186:6800/22316&quot;, &quot;available&quot;: true, &quot;standbys&quot;: [ &#123; &quot;gid&quot;: 4129, &quot;name&quot;: &quot;ceph2&quot;, &quot;available_modules&quot;: [ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; ] &#125;, &#123; &quot;gid&quot;: 4132, &quot;name&quot;: &quot;ceph3&quot;, &quot;available_modules&quot;: [ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; ] &#125; ], &quot;modules&quot;: [ &quot;balancer&quot;, &quot;restful&quot;, &quot;status&quot; ], &quot;available_modules&quot;: [ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; ], &quot;services&quot;: &#123;&#125;&#125;[root@ceph1 ceph]# ceph mgr module enable dashboard[root@ceph1 ceph]# ceph mgr dump&#123; &quot;epoch&quot;: 7, &quot;active_gid&quot;: 4139, &quot;active_name&quot;: &quot;ceph1&quot;, &quot;active_addr&quot;: &quot;192.168.10.186:6800/22316&quot;, &quot;available&quot;: true, &quot;standbys&quot;: [ &#123; &quot;gid&quot;: 4136, &quot;name&quot;: &quot;ceph3&quot;, &quot;available_modules&quot;: [ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; ] &#125;, &#123; &quot;gid&quot;: 4141, &quot;name&quot;: &quot;ceph2&quot;, &quot;available_modules&quot;: [ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; ] &#125; ], &quot;modules&quot;: [ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;restful&quot;, &quot;status&quot; ], &quot;available_modules&quot;: [ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; ], &quot;services&quot;: &#123;&#125;&#125;[root@ceph1 ceph]# ceph config-key put mgr/dashboard/server_addr 192.168.6.101set mgr/dashboard/server_addr[root@ceph1 ceph]# ceph config-key put mgr/dashboard/server_port 7000set mgr/dashboard/server_port[root@ceph1 ~]# netstat -tulnp |grep 7000tcp 0 0 192.168.6.101:7000 0.0.0.0:* LISTEN 19836/ceph-mgr 这时看下danshboard图： osd1234567891011121314151617181920212223242526272829303132每台机器做逻辑卷[root@ceph1 ceph]# pvcreate /dev/sdb Physical volume &quot;/dev/sdb&quot; successfully created.[root@ceph1 ceph]# vgcreate data_vg1 /dev/sdb Volume group &quot;data_vg1&quot; successfully created [root@ceph1 ceph]# lvcreate -n data_lv1 -L 99g data_vg1 Logical volume &quot;data_lv1&quot; created. ceph-deploy osd create ceph1 --data data_vg1/data_lv1ceph-deploy osd create ceph2 --data data_vg1/data_lv1ceph-deploy osd create ceph3 --data data_vg1/data_lv1[root@ceph1 ceph]# ceph -s cluster: id: fcb2fa5e-481a-4494-9a27-374048f37113 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 mgr: ceph1(active), standbys: ceph3, ceph2 osd: 3 osds: 3 up, 3 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0B usage: 3.01GiB used, 294GiB / 297GiB avail pgs: 这时看下danshboard图： rgw集群12345678910111213141516171819202122ceph-deploy install --rgw ceph1 ceph2 ceph3ceph-deploy admin ceph1 ceph2 ceph3ceph-deploy rgw create ceph1 ceph2 ceph3[root@ceph1 ceph]# ceph -s cluster: id: fcb2fa5e-481a-4494-9a27-374048f37113 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 mgr: ceph1(active), standbys: ceph3, ceph2 osd: 3 osds: 3 up, 3 in rgw: 3 daemons active data: pools: 4 pools, 32 pgs objects: 191 objects, 3.08KiB usage: 3.01GiB used, 294GiB / 297GiB avail pgs: 32 active+clean 这时看下danshboard图： NGINX代理12345678910111213141516171819202122安装这里就不介绍了[root@ceph1 conf.d]# cat cephcloud.dev.goago.cn.conf upstream cephcloud.dev.goago.cn &#123; server 192.168.10.186:7480; server 192.168.10.187:7480; server 192.168.10.188:7480; &#125; server &#123; listen 80; server_name cephcloud.dev.goago.cn; location / &#123; proxy_intercept_errors on; access_log /var/log/nginx/cephcloud_log; proxy_pass http://cephcloud.dev.goago.cn; proxy_set_header X-Real-IP $remote_addr; proxy_set_header Host $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Request_Uri $request_uri; &#125; &#125; s3和swift1234567891011121314具体安装这里不叙述了，可以看我上篇文章New settings: Access Key: M954JYYAOBES65B7UNEZ Secret Key: 11MZu3N9vB4S4C4N8U2Ywgkhxro3Xi6K9HPyRQ9v Default Region: US S3 Endpoint: cephcloud.dev.goago.cn DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.cephcloud.dev.goago.cn bucket Encryption password: 123456 Path to GPG program: /usr/bin/gpg Use HTTPS protocol: False HTTP Proxy server name: HTTP Proxy server port: 0]]></content>
      <categories>
        <category>ceph</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ceph-luminous-bluestore]]></title>
    <url>%2Fposts%2F78fhjj54.html</url>
    <content type="text"><![CDATA[对比1234567891011ceph后端支持多种存储引擎，以插件化的形式来进行管理使用，目前支持filestore，kvstore，memstore以及bluestore1）Firestore存在的问题是：在写数据前需要先写journal，会有一倍的写放大；若是另外配备SSD盘给journal使用又增加额外的成本；filestore一开始只是对于SATA/SAS这一类机械盘进行设计的，没有专门针对SSD这一类的Flash介质盘做考虑。2）而Bluestore的优势在于：减少写放大；针对FLASH介质盘做优化；直接管理裸盘，进一步减少文件系统部分的开销。 Bluestore原理说明123对象可以直接存放在裸盘上，不需要任何文件系统接口。BlueStore 直接使用一个原始分区，ceph对象将直接写在块设备上，不再需要任何的文件系统；和osd一起进来的元数据将存储在 一个 名为 RocksDB 的键值对 数据库； 各层意义1234RocksDB ：存储 WAL 日志和元数据（omap）BlueRocksEnv: 与RocksDB 交互的接口BlueFS： 一个类似文件系统的 mini C++，使 rocksdb 生效，ENv 接口（存储 RocksDB 日志和 sst 文件）；因为rocksdb 一般跑在一个文件系统的上层，所以创建了 BlueFS。 RocksDB 存放的数据类型1234对象的元数据write-ahead 日志ceph omap 数据allocator metadata(元数据分配器)：决定数据存放位置；此功能可插拔 默认BlueStore模型123第一个小分区（XFS或者ext4）,包括ceph files （init system descriptor,status,id,fsid,keyring 等）和RocksDB 文件第二个分区是一个原始分区 优点1每一部分都可以存放在不同的磁盘中，RocksDB WAL 和 DB 可以存放在不同的磁盘或者小分区中 添加osd案例1由于Luminous里默认使用Bluestore，可以直接操作裸盘,data和block-db会使用lv。综合成本及性能，我们把block.db使用ssd的分区，osd仍然使用sas，block.wal不指定. 这里vdb作为osd盘，vdc作为block-db盘 123456789101112131415161718192021222324252627首先ssh到各个存储节点，block.db使用的ssd分区,这里node1举例：# ssh node1# pvcreate /dev/vdb # 创建pv, 这里使用的整块磁盘(与后面的分区对比), pvs 查看pv列表Physical volume &quot;/dev/vdb&quot; successfully created.# vgcreate data_vg1 /dev/vdb # 创建vg, vgs查看vg列表Volume group &quot;data_vg1&quot; successfully created# lvcreate -n data_lv1 -L 1020.00m data_vg1 #创建lv,lvs查看lv列表, -n指定lv名称, -L指定lv的大小,需要小于或者等于vg的VSizeLogical volume &quot;data_lv1&quot; created.---------------------------------------------生产环境一块ssd磁盘会对应多块osd，所以这里也需要把ssd多个分区# parted /dev/vdc (parted) mklabel gpt (parted) mkpart primary 0% 25% #因为测试，这里只做了一个占据磁盘25%容量的分区，实际情况根据osd数目划分相应的分区数(parted) quit# pvcreate /dev/vdc1 # 创建pv, 这里使用的是磁盘分区, pvs 查看pv列表Physical volume &quot;/dev/vdc1&quot; successfully created.# vgcreate block_db_vg1 /dev/vdc1 # 创建vg, vgs查看vg列表Volume group &quot;block_db_vg1&quot; successfully created# lvcreate -n block_db_lv1 -L 1020.00m block_db_vg1 # 创建lv, lvs查看lv列表, -L指定lv的大小，需要小于或者等于 vg的VSizeLogical volume &quot;block_db_lv1&quot; created.---------------------------------------------# 不需要加--bluestore 参数，默认就是使用bluestore方式，data_vg1/data_lv1 是数据盘，block_db_vg1/block_db_lv1是block-db管理节点执行：ceph-deploy --overwrite-conf osd create node1 --data data_vg1/data_lv1 --block-db block_db_vg1/block_db_lv1ceph-deploy --overwrite-conf osd create node2 --data data_vg1/data_lv1 --block-db block_db_vg1/block_db_lv1 案例2创建具有3个逻辑卷的OSD（模拟不同类型的存储介质） 12345678910#pvcreate /dev/sdb Physical volume &quot;/dev/sdb&quot; successfully created.#vgcreate ceph-pool /dev/sdb Volume group &quot;ceph-pool&quot; successfully created#lvcreate -n osd0.wal -L 1G ceph-pool Logical volume &quot;osd0.wal&quot; created.# lvcreate -n osd0.db -L 1G ceph-pool Logical volume &quot;osd0.db&quot; created.# lvcreate -n osd0 -l 100%FREE ceph-pool Logical volume &quot;osd0&quot; created. 完成逻辑卷的创建后我们就可以创建 OSD 了。 12345ceph-deploy osd create \ --data ceph-pool/osd0 \ --block-db ceph-pool/osd0.db \ --block-wal ceph-pool/osd0.wal \ --bluestore node1 wal&amp; db 的大小问题123在 ceph bluestore 的情况下，wal 是 RocksDB 的write-ahead log, 相当于之前的 journal 数据，db 是 RocksDB 的metadata 信息。在磁盘选择原则是 block.wal &gt; block.db &gt; block。当然所有的数据也可以放到同一块盘上。默认情况下， wal 和 db 的大小分别是 512 MB 和 1GB，现在没有一个好的理论值，好像和 ceph 本身承载的数据类型有关系。值得注意的是，如果所有的数据都在单块盘上，那是没有必要指定 wal &amp;db 的大小的。如果 wal &amp; db 是在不同的盘上，由于 wal/db 一般都会分的比较小，是有满的可能性的。如果满了，这些数据会迁移到下一个快的盘上(wal - db - main)。所以最少不会因为数据满了，而造成无法写入。]]></content>
      <categories>
        <category>ceph</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ceph的pg算法]]></title>
    <url>%2Fposts%2Ffd22jkk6.html</url>
    <content type="text"><![CDATA[PG介绍PG, Placement Groups。CRUSH先将数据分解成一组对象，然后根据对象名称、复制级别和系统中的PG数等信息执行散列操作，再将结果生成PG ID。可以将PG看做一个逻辑容器，这个容器包含多个对象，同时这个逻辑对象映射之多个OSD上。如果没有PG，在成千上万个OSD上管理和跟踪数百万计的对象的复制和传播是相当困难的。没有PG这一层，管理海量的对象所消耗的计算资源也是不可想象的。建议每个OSD上配置50~100个PG。 计算PG数官方推荐如下： Ceph集群中的PG总数： 1PG总数 = (OSD总数 * 100) / 最大副本数 结果必须舍入到最接近的2的N次方幂的值。 Ceph集群中每个pool中的PG总数： 1存储池PG总数 = (OSD总数 * 100 / 最大副本数) / 池数 平衡每个存储池中的PG数和每个OSD中的PG数对于降低OSD的方差、避免速度缓慢的恢复再平衡进程是相当重要的。 修改PG和PGPPGP是为了实现定位而设置的PG，它的值应该和PG的总数(即pg_num)保持一致。对于Ceph的一个pool而言，如果增加pg_num，还应该调整pgp_num为同样的值，这样集群才可以开始再平衡。参数pg_num定义了PG的数量，PG映射至OSD。当任意pool的PG数增加时，PG依然保持和源OSD的映射。直至目前，Ceph还未开始再平衡。此时，增加pgp_num的值，PG才开始从源OSD迁移至其他的OSD，正式开始再平衡。PGP，Placement Groups of Placement。 获取现有的PG数和PGP数值： 123ceph osd pool get data pg_numceph osd pool get data pgp_num 检查存储池的副本数 1ceph osd dump|grep -i size 计算pg_num和pgp_num 123456789# pg_num calculationpg_num = (num_osds * 100) / num_copiesnum_up = pow(2, int(log(pg_num,2) + 0.5))num_down = pow(2, int(log(pg_num,2)))if abs(pg_num - num_up) &lt;= abs(pg_num - num_down): pg_num = num_upelse: pg_num = num_downpgp_num = pg_num 修改存储池的PG和PGP 123ceph osd pool set data pg_num ceph osd pool set data pgp_num 例子： 123ceph osd pool lsceph osd pool set .rgw.root pg_num 16ceph osd pool set .rgw.root pgp_num 16]]></content>
      <categories>
        <category>ceph</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ceph对象存储]]></title>
    <url>%2Fposts%2Fdld987c6.html</url>
    <content type="text"><![CDATA[Ceph RGW简介Ceph RGW(即RADOS Gateway)是Ceph对象存储网关服务，是基于LIBRADOS接口封装实现的FastCGI服务，对外提供存储和管理对象数据的Restful API。 对象存储适用于图片、视频等各类文件的上传下载，可以设置相应的访问权限。目前Ceph RGW兼容常见的对象存储API，例如兼容绝大部分Amazon S3 API，兼容OpenStack Swift API。 部署 RGW 服务1234567891011121314151617181920[root@ceph1 ceph]# ceph-deploy install --rgw ceph1[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy install --rgw ceph1[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] testing : None[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fa3faca5e60&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] dev_commit : None[ceph_deploy.cli][INFO ] install_mds : False[ceph_deploy.cli][INFO ] stable : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] adjust_repos : True[ceph_deploy.cli][INFO ] func : &lt;function install at 0x7fa3fbb955f0&gt;[ceph_deploy.cli][INFO ] install_mgr : False[ceph_deploy.cli][INFO ] install_all : False[ceph_deploy.cli][INFO ] repo : False[ceph_deploy.cli][INFO ] host : [&apos;ceph1&apos;][ceph_deploy.cli][INFO ] install_rgw : True 将配置文件、密钥文件同步到 ceph1： 12345678910111213141516171819[root@ceph1 ceph]# ceph-deploy admin ceph1[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy admin ceph1[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fe0e152d3b0&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] client : [&apos;ceph1&apos;][ceph_deploy.cli][INFO ] func : &lt;function admin at 0x7fe0e1dc0230&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph1[ceph1][DEBUG ] connected to host: ceph1 [ceph1][DEBUG ] detect platform information from remote host[ceph1][DEBUG ] detect machine type[ceph1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf 启动一个RGW服务 ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657先将ceph.conf加一个参数配置[root@ceph1 ceph]# vim ceph.conf [global]fsid = cde3244e-89e0-4630-84d5-bf08c0e33b24mon_initial_members = ceph1mon_host = 192.168.6.101auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxosd_pool_default_size = 2[mgr]mgr modules = dashboard[mon]mon allow pool delete = true ####有这个配置，生成的pool才可以被删除[root@ceph1 ceph]# ceph-deploy rgw create ceph1[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (2.0.1): /usr/bin/ceph-deploy rgw create ceph1[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] rgw : [(&apos;ceph1&apos;, &apos;rgw.ceph1&apos;)][ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] subcommand : create[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x7fda85404ab8&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] func : &lt;function rgw at 0x7fda85a53050&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.rgw][DEBUG ] Deploying rgw, cluster ceph hosts ceph1:rgw.ceph1[ceph1][DEBUG ] connected to host: ceph1 [ceph1][DEBUG ] detect platform information from remote host[ceph1][DEBUG ] detect machine type[ceph_deploy.rgw][INFO ] Distro info: CentOS Linux 7.6.1810 Core[ceph_deploy.rgw][DEBUG ] remote host will use systemd[ceph_deploy.rgw][DEBUG ] deploying rgw bootstrap to ceph1[ceph1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph1][DEBUG ] create path recursively if it doesn&apos;t exist[ceph1][INFO ] Running command: ceph --cluster ceph --name client.bootstrap-rgw --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create client.rgw.ceph1 osd allow rwx mon allow rw -o /var/lib/ceph/radosgw/ceph-rgw.ceph1/keyring[ceph1][INFO ] Running command: systemctl enable ceph-radosgw@rgw.ceph1[ceph1][INFO ] Running command: systemctl start ceph-radosgw@rgw.ceph1[ceph1][INFO ] Running command: systemctl enable ceph.target[ceph_deploy.rgw][INFO ] The Ceph Object Gateway (RGW) is now running on host ceph1 and default port 7480验证：[root@ceph1 ceph]# systemctl status ceph-radosgw@rgw.ceph1● ceph-radosgw@rgw.ceph1.service - Ceph rados gateway Loaded: loaded (/usr/lib/systemd/system/ceph-radosgw@.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2019-07-11 15:03:24 CST; 9s ago Main PID: 21057 (radosgw) CGroup: /system.slice/system-ceph\x2dradosgw.slice/ceph-radosgw@rgw.ceph1.service └─21057 /usr/bin/radosgw -f --cluster ceph --name client.rgw.ceph1 --setuser ceph --setgroup cephJul 11 15:03:24 ceph1 systemd[1]: Started Ceph rados gateway. 12345678910111213141516[root@ceph1 ceph]# ceph -s cluster: id: cde3244e-89e0-4630-84d5-bf08c0e33b24 health: HEALTH_OK services: mon: 1 daemons, quorum ceph1 mgr: ceph1(active) osd: 2 osds: 2 up, 2 in rgw: 1 daemon active data: pools: 4 pools, 32 pgs objects: 187 objects, 1.09KiB usage: 2.01GiB used, 30.0GiB / 32.0GiB avail pgs: 32 active+clean 这时看下danshboard图： 使用亚马逊 s3 客户端进行访问用户创建用户 ： 1234567891011121314151617181920212223242526272829303132333435363738[root@ceph1 ceph]# radosgw-admin user create --uid=&quot;radosgw&quot; --display-name=&quot;First User&quot;&#123; &quot;user_id&quot;: &quot;radosgw&quot;, &quot;display_name&quot;: &quot;First User&quot;, &quot;email&quot;: &quot;&quot;, &quot;suspended&quot;: 0, &quot;max_buckets&quot;: 1000, &quot;auid&quot;: 0, &quot;subusers&quot;: [], &quot;keys&quot;: [ &#123; &quot;user&quot;: &quot;radosgw&quot;, &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;, &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot; &#125; ], &quot;swift_keys&quot;: [], &quot;caps&quot;: [], &quot;op_mask&quot;: &quot;read, write, delete&quot;, &quot;default_placement&quot;: &quot;&quot;, &quot;placement_tags&quot;: [], &quot;bucket_quota&quot;: &#123; &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 &#125;, &quot;user_quota&quot;: &#123; &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 &#125;, &quot;temp_url_keys&quot;: [], &quot;type&quot;: &quot;rgw&quot;&#125; 这个是后续需要的账户信息 123&quot;user&quot;: &quot;radosgw&quot;, &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;, &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot; 授权用户，允许 radosgw 读写 users 信息： 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@ceph1 ceph]# radosgw-admin caps add --uid=radosgw --caps=&quot;users=*&quot;&#123; &quot;user_id&quot;: &quot;radosgw&quot;, &quot;display_name&quot;: &quot;First User&quot;, &quot;email&quot;: &quot;&quot;, &quot;suspended&quot;: 0, &quot;max_buckets&quot;: 1000, &quot;auid&quot;: 0, &quot;subusers&quot;: [], &quot;keys&quot;: [ &#123; &quot;user&quot;: &quot;radosgw&quot;, &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;, &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot; &#125; ], &quot;swift_keys&quot;: [], &quot;caps&quot;: [ &#123; &quot;type&quot;: &quot;users&quot;, &quot;perm&quot;: &quot;*&quot; &#125; ], &quot;op_mask&quot;: &quot;read, write, delete&quot;, &quot;default_placement&quot;: &quot;&quot;, &quot;placement_tags&quot;: [], &quot;bucket_quota&quot;: &#123; &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 &#125;, &quot;user_quota&quot;: &#123; &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 &#125;, &quot;temp_url_keys&quot;: [], &quot;type&quot;: &quot;rgw&quot;&#125; 允许 radosgw 读写所有的usage信息： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647[root@ceph1 ceph]# radosgw-admin caps add --uid=radosgw --caps=&quot;usage=read,write&quot;&#123; &quot;user_id&quot;: &quot;radosgw&quot;, &quot;display_name&quot;: &quot;First User&quot;, &quot;email&quot;: &quot;&quot;, &quot;suspended&quot;: 0, &quot;max_buckets&quot;: 1000, &quot;auid&quot;: 0, &quot;subusers&quot;: [], &quot;keys&quot;: [ &#123; &quot;user&quot;: &quot;radosgw&quot;, &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;, &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot; &#125; ], &quot;swift_keys&quot;: [], &quot;caps&quot;: [ &#123; &quot;type&quot;: &quot;usage&quot;, &quot;perm&quot;: &quot;*&quot; &#125;, &#123; &quot;type&quot;: &quot;users&quot;, &quot;perm&quot;: &quot;*&quot; &#125; ], &quot;op_mask&quot;: &quot;read, write, delete&quot;, &quot;default_placement&quot;: &quot;&quot;, &quot;placement_tags&quot;: [], &quot;bucket_quota&quot;: &#123; &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 &#125;, &quot;user_quota&quot;: &#123; &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 &#125;, &quot;temp_url_keys&quot;: [], &quot;type&quot;: &quot;rgw&quot;&#125; 创建子用户，做为后面 swift 客户端访问时使用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[root@ceph1 ceph]# radosgw-admin subuser create --uid=radosgw --subuser=radosgw:swift --access=full&#123; &quot;user_id&quot;: &quot;radosgw&quot;, &quot;display_name&quot;: &quot;First User&quot;, &quot;email&quot;: &quot;&quot;, &quot;suspended&quot;: 0, &quot;max_buckets&quot;: 1000, &quot;auid&quot;: 0, &quot;subusers&quot;: [ &#123; &quot;id&quot;: &quot;radosgw:swift&quot;, &quot;permissions&quot;: &quot;full-control&quot; &#125; ], &quot;keys&quot;: [ &#123; &quot;user&quot;: &quot;radosgw&quot;, &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;, &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot; &#125; ], &quot;swift_keys&quot;: [ &#123; &quot;user&quot;: &quot;radosgw:swift&quot;, &quot;secret_key&quot;: &quot;A3GDj2yjkGJahkCM6YJS4QKQlGz2zd65GXvCkiwV&quot; &#125; ], &quot;caps&quot;: [ &#123; &quot;type&quot;: &quot;usage&quot;, &quot;perm&quot;: &quot;*&quot; &#125;, &#123; &quot;type&quot;: &quot;users&quot;, &quot;perm&quot;: &quot;*&quot; &#125; ], &quot;op_mask&quot;: &quot;read, write, delete&quot;, &quot;default_placement&quot;: &quot;&quot;, &quot;placement_tags&quot;: [], &quot;bucket_quota&quot;: &#123; &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 &#125;, &quot;user_quota&quot;: &#123; &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 &#125;, &quot;temp_url_keys&quot;: [], &quot;type&quot;: &quot;rgw&quot;&#125; 创建密钥 ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[root@ceph1 ceph]# radosgw-admin key create --subuser=radosgw:swift --key-type=swift --gen-secret&#123; &quot;user_id&quot;: &quot;radosgw&quot;, &quot;display_name&quot;: &quot;First User&quot;, &quot;email&quot;: &quot;&quot;, &quot;suspended&quot;: 0, &quot;max_buckets&quot;: 1000, &quot;auid&quot;: 0, &quot;subusers&quot;: [ &#123; &quot;id&quot;: &quot;radosgw:swift&quot;, &quot;permissions&quot;: &quot;full-control&quot; &#125; ], &quot;keys&quot;: [ &#123; &quot;user&quot;: &quot;radosgw&quot;, &quot;access_key&quot;: &quot;CQE7E6ZDVA74KVJ0077A&quot;, &quot;secret_key&quot;: &quot;wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8&quot; &#125; ], &quot;swift_keys&quot;: [ &#123; &quot;user&quot;: &quot;radosgw:swift&quot;, &quot;secret_key&quot;: &quot;CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh&quot; &#125; ], &quot;caps&quot;: [ &#123; &quot;type&quot;: &quot;usage&quot;, &quot;perm&quot;: &quot;*&quot; &#125;, &#123; &quot;type&quot;: &quot;users&quot;, &quot;perm&quot;: &quot;*&quot; &#125; ], &quot;op_mask&quot;: &quot;read, write, delete&quot;, &quot;default_placement&quot;: &quot;&quot;, &quot;placement_tags&quot;: [], &quot;bucket_quota&quot;: &#123; &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 &#125;, &quot;user_quota&quot;: &#123; &quot;enabled&quot;: false, &quot;check_on_raw&quot;: false, &quot;max_size&quot;: -1, &quot;max_size_kb&quot;: 0, &quot;max_objects&quot;: -1 &#125;, &quot;temp_url_keys&quot;: [], &quot;type&quot;: &quot;rgw&quot;&#125; 安装 s3 客户端软件1[root@ceph1 ceph]# yum -y install s3cmd.noarch 对 s3 进行配置1[root@ceph1 ceph]# s3cmd --configure 123456789101112131415161718192021New settings: Access Key: CQE7E6ZDVA74KVJ0077A Secret Key: wnRy76RM2s85ozWvKwTBezrBU0RwcCTQJf1HFAM8 Default Region: US S3 Endpoint: 192.168.6.101:7480 DNS-style bucket+hostname:port template for accessing a bucket: %(bucket)s.192.168.6.101:7480 bucket Encryption password: 123456 Path to GPG program: /usr/bin/gpg Use HTTPS protocol: False HTTP Proxy server name: HTTP Proxy server port: 0Test access with supplied credentials? [Y/n] yPlease wait, attempting to list all buckets...Success. Your access key and secret key worked fine :-)Now verifying that encryption works...Success. Encryption and decryption worked fine :-)Save settings? [y/N] yConfiguration saved to &apos;/root/.s3cfg&apos; 格式是这样： 12345678Default Region [US]: #这里一定不要修改，否则后面会报错S3 Endpoint [s3.amazonaws.com]: 192.168.6.101:7480DNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: %(bucket)s.192.168.20.148:7480 bucket #相当于百度网盘的创建文件夹，这里是固定格式Path to GPG program [/usr/bin/gpg]: #保持默认Use HTTPS protocol [Yes]: no #这里写 no ，因为没有提供 https 端口HTTP Proxy server name: #这里不用写，因为没有代理Test access with supplied credentials? [Y/n] ySave settings? [y/N] y 由于我没把端口改成80，所以需要带端口访问的，后续可以nginx代理 创建存储数据的 bucket1234567[root@ceph1 ~]# s3cmd mb s3://cephdir[root@ceph1 ~]# s3cmd put /etc/hosts s3://ceph_dirupload: &apos;/etc/hosts&apos; -&gt; &apos;s3://ceph_dir/hosts&apos; [1 of 1] 200 of 200 100% in 1s 133.14 B/s done [root@ceph1 ~]# s3cmd ls s3://ceph_dir2019-07-11 08:41 200 s3://ceph_dir/hosts s3 的测试脚本： 1234567891011121314151617[root@ceph-f ~]# yum -y install python-boto[root@ceph-f ~]# vim s3test.pyimport boto.s3.connectionaccess_key = &apos;N6ALEK0KS0ISYCIM5JBG&apos;secret_key = &apos;qK9hrpX2uwna4elPP1VsuErmAHBw3So40fE2K4yM&apos;conn = boto.connect_s3( aws_access_key_id=access_key, aws_secret_access_key=secret_key, host=&apos;ceph1&apos;, port=7480, is_secure=False, calling_format=boto.s3.connection.OrdinaryCallingFormat(), )bucket = conn.create_bucket(&apos;xxx_yyy&apos;)for bucket in conn.get_all_buckets(): print &quot;&#123;name&#125; &#123;created&#125;&quot;.format( name=bucket.name, created=bucket.creation_date, ) 在使用时，请替换自己的 access_key、secret_key、主机名和端口 swift 接口测试1234yum install python-setuptoolseasy_install pippip install --upgrade setuptoolspip install --upgrade python-swiftclient 命令行访问 ： 1234567891011121314[root@ceph1 ~]# swift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh listceph_dirswift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh post sy-ytswift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh post sy_yt[root@ceph1 ~]# swift -A http://192.168.6.101:7480/auth/v1.0 -U radosgw:swift -K CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh listceph_dirsy-ytsy_yt 这里提供 swift 的测试脚本： 123456789101112[root@ceph-f ~]# vim swift.pyimport swiftclientuser = &apos;radosgw:swift&apos;key = &apos;CZ2F5lqOfdmTCLSPyzqcw5pPNeetruAL4HjHkiAh&apos;conn = swiftclient.Connection( user=user, key=key, authurl=&apos;http://192.168.6.101:7480/auth/v1.0&apos;, )for container in conn.get_account()[1]: print container[&apos;name&apos;] 在使用时，请替换自己的 access_key、secret_key、authurl]]></content>
      <categories>
        <category>ceph</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ceph简单搭建]]></title>
    <url>%2Fposts%2Fldfl554c.html</url>
    <content type="text"><![CDATA[ceph介绍Ceph基础介绍​ Ceph是一个可靠地、自动重均衡、自动恢复的分布式存储系统，根据场景划分可以将Ceph分为三大块，分别是对象存储、块设备存储和文件系统服务。在虚拟化领域里，比较常用到的是Ceph的块设备存储，比如在OpenStack项目里，Ceph的块设备存储可以对接OpenStack的cinder后端存储、Glance的镜像存储和虚拟机的数据存储，比较直观的是Ceph集群可以提供一个raw格式的块存储来作为虚拟机实例的硬盘。 ​ Ceph相比其它存储的优势点在于它不单单是存储，同时还充分利用了存储节点上的计算能力，在存储每一个数据时，都会通过计算得出该数据存储的位置，尽量将数据分布均衡，同时由于Ceph的良好设计，采用了CRUSH算法、HASH环等方法，使得它不存在传统的单点故障的问题，且随着规模的扩大性能并不会受到影响。 Ceph的核心组件​ Ceph的核心组件包括Ceph OSD、Ceph Monitor和Ceph MDS。 Ceph OSD：OSD的英文全称是Object Storage Device，它的主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其它OSD间进行心跳检查等，并将一些变化情况上报给Ceph Monitor。一般情况下一块硬盘对应一个OSD，由OSD来对硬盘存储进行管理，当然一个分区也可以成为一个OSD。 Ceph OSD的架构实现由物理磁盘驱动器、Linux文件系统和Ceph OSD服务组成，对于Ceph OSD Deamon而言，Linux文件系统显性的支持了其拓展性，一般Linux文件系统有好几种，比如有BTRFS、XFS、Ext4等，BTRFS虽然有很多优点特性，但现在还没达到生产环境所需的稳定性，一般比较推荐使用XFS。 伴随OSD的还有一个概念叫做Journal盘，一般写数据到Ceph集群时，都是先将数据写入到Journal盘中，然后每隔一段时间比如5秒再将Journal盘中的数据刷新到文件系统中。一般为了使读写时延更小，Journal盘都是采用SSD，一般分配10G以上，当然分配多点那是更好，Ceph中引入Journal盘的概念是因为Journal允许Ceph OSD功能很快做小的写操作；一个随机写入首先写入在上一个连续类型的journal，然后刷新到文件系统，这给了文件系统足够的时间来合并写入磁盘，一般情况下使用SSD作为OSD的journal可以有效缓冲突发负载。 Ceph Monitor：由该英文名字我们可以知道它是一个监视器，负责监视Ceph集群，维护Ceph集群的健康状态，同时维护着Ceph集群中的各种Map图，比如OSD Map、Monitor Map、PG Map和CRUSH Map，这些Map统称为Cluster Map，Cluster Map是RADOS的关键数据结构，管理集群中的所有成员、关系、属性等信息以及数据的分发，比如当用户需要存储数据到Ceph集群时，OSD需要先通过Monitor获取最新的Map图，然后根据Map图和object id等计算出数据最终存储的位置。 Ceph MDS：全称是Ceph MetaData Server，主要保存的文件系统服务的元数据，但对象存储和块存储设备是不需要使用该服务的。 查看各种Map的信息可以通过如下命令：ceph osd(mon、pg) dump ceph-deploy安装ceph基本环境 192.168.6.101 ceph1 192.168.6.102 ceph2 俩台机器都挂俩块盘，一块系统盘，一块osd 配hosts： 123[root@ceph1 ~]# vim /etc/hosts192.168.6.101 ceph1192.168.6.102 ceph2 时间同步： 1ntpdate ntp1.aliyun.com 允许无密码SSH登录： 1234在ceph1上执行ssh-keygenssh-copy-id ceph1ssh-copy-id ceph2 配置主机名： 12hostnamectl set-hostname ceph1hostnamectl set-hostname ceph2 配置ceph.repo123456789101112131415161718192021222324252627[root@ceph1 ~]# vim /etc/yum.repos.d/ceph.repo[ceph]name=Ceph packages for $basearchbaseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/$basearchenabled=1gpgcheck=1priority=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc[ceph-noarch]name=Ceph noarch packagesbaseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarchenabled=1gpgcheck=1priority=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMSenabled=0gpgcheck=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.ascpriority=1 安装ceph-deploy： 1234yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpmyum makecacheyum update -yyum install -y ceph-deploy 创建一个 Ceph 存储集群，它有一个 Monitor 和两个 OSD 守护进程12345678910111213mkdir /etc/ceph &amp;&amp; cd /etc/cephceph-deploy new ceph1 ###配置一般会遇到个报错：Traceback (most recent call last): File &quot;/usr/bin/ceph-deploy&quot;, line 18, in &lt;module&gt; from ceph_deploy.cli import main File &quot;/usr/lib/python2.7/site-packages/ceph_deploy/cli.py&quot;, line 1, in &lt;module&gt; import pkg_resourcesImportError: No module named pkg_resources解决：yum install -y python-setuptools 在ceph.conf中追加以下内容 123# 存储集群副本个数osd_pool_default_size = 2 管理节点和osd节点都需要安装ceph 集群： 1ceph-deploy install ceph1 ceph2 配置MON初始化: 1ceph-deploy mon create-initial 查看ceph集群状态： 123456789101112131415[root@ceph1 ceph]# ceph -s cluster: id: cde3244e-89e0-4630-84d5-bf08c0e33b24 health: HEALTH_OK services: mon: 1 daemons, quorum ceph1 mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0B usage: 0B used, 0B / 0B avail pgs: 开启监控模块1ceph-deploy mgr create ceph1 在/etc/ceph/ceph.conf中添加 12[mgr]mgr modules = dashboard 查看集群支持的模块 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[root@ceph1 ceph]# ceph mgr dump [root@ceph1 ceph]# ceph mgr module enable dashboard #启用dashboard模块[root@ceph1 ceph]# ceph mgr dump&#123; &quot;epoch&quot;: 3, &quot;active_gid&quot;: 4110, &quot;active_name&quot;: &quot;ceph1&quot;, &quot;active_addr&quot;: &quot;192.168.6.101:6800/6619&quot;, &quot;available&quot;: true, &quot;standbys&quot;: [], &quot;modules&quot;: [ &quot;balancer&quot;, &quot;restful&quot;, &quot;status&quot; ], &quot;available_modules&quot;: [ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; ], &quot;services&quot;: &#123;&#125;&#125;[root@ceph1 ceph]# ceph mgr module enable dashboard[root@ceph1 ceph]# ceph mgr dump&#123; &quot;epoch&quot;: 6, &quot;active_gid&quot;: 4114, &quot;active_name&quot;: &quot;ceph1&quot;, &quot;active_addr&quot;: &quot;192.168.6.101:6800/6619&quot;, &quot;available&quot;: true, &quot;standbys&quot;: [], &quot;modules&quot;: [ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;restful&quot;, &quot;status&quot; ], &quot;available_modules&quot;: [ &quot;balancer&quot;, &quot;dashboard&quot;, &quot;influx&quot;, &quot;localpool&quot;, &quot;prometheus&quot;, &quot;restful&quot;, &quot;selftest&quot;, &quot;status&quot;, &quot;zabbix&quot; ], &quot;services&quot;: &#123;&#125;&#125;[root@ceph1 ceph]# ceph -s cluster: id: cde3244e-89e0-4630-84d5-bf08c0e33b24 health: HEALTH_OK services: mon: 1 daemons, quorum ceph1 mgr: ceph1(active) osd: 2 osds: 2 up, 2 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0B usage: 2.00GiB used, 30.0GiB / 32.0GiB avail pgs: 设置dashboard的ip和端口 1234[root@ceph-node1 ceph]# ceph config-key put mgr/dashboard/server_addr 192.168.6.101set mgr/dashboard/server_addr[root@ceph-node1 ceph]# ceph config-key put mgr/dashboard/server_port 7000set mgr/dashboard/server_port 12[root@ceph1 ~]# netstat -tulnp |grep 7000tcp 0 0 192.168.6.101:7000 0.0.0.0:* LISTEN 19836/ceph-mgr 创建osd删除磁盘数据 12[root@ceph1 ceph]# ceph-deploy disk zap ceph1 /dev/sdb[root@ceph1 ceph]# ceph-deploy disk zap ceph2 /dev/sdb 创建osd(一共俩个) 12[root@ceph1 ceph]# ceph-deploy osd create ceph1 --data /dev/sdb[root@ceph1 ceph]# ceph-deploy osd create ceph2 --data /dev/sdb ceph秘钥拷贝（主节点执行）及修改密钥权限 用 ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 monitor 地址和 ceph.client.admin.keyring 了 1[root@ceph1 ceph]# ceph-deploy admin ceph1 ceph2 修改密钥权限（所有节点上执行） 12[root@ceph1 ceph] # chmod +r /etc/ceph/ceph.client.admin.keyring[root@ceph2] # chmod +r /etc/ceph/ceph.client.admin.keyring 这时看下danshboard图：]]></content>
      <categories>
        <category>ceph</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes Pod无法删除]]></title>
    <url>%2Fposts%2Fdk7569vg.html</url>
    <content type="text"><![CDATA[问题发现在node节点断电 重启后，发现有的pod节点状态不正常，之前的回收策略也都做了，就调研下是什么原因导致的 pod一直处于Terminated: ExitCode 状态 解决直接删除，无法飘移123kubectl delete pod &lt;podname&gt; --namespace=&lt;namspacer&gt; --grace-period=0 --force发现pod无法漂移docker ps -a查看对应docker容器的状态，发现这两个Pod的docker容器处于Dead状态。使用docker rm &lt;container id&gt;，提示Device is Busy，无法删除。 现象是由于systemd服务PrivateTmp=true引起最根本的方法是，当机器加入时，在 docker.service 中加上： 12[Service]MountFlags=slave 关于Systemd的MountFlags1234567MountFlags: 配置Systemd服务的Mount Namespace配置。会影响服务进程上下文中挂载点的信息，即服务是否会继承主机上已有的挂载点，以及如果服务运行时执行了挂载或卸载设备的操作，是否会真实地在主机上产生效果。可选值为shared、slave和privateshared：服务与主机共用一个Mount Namespace，会继承主机挂载点，服务挂载或卸载设备时会真实地反映到主机上slave：服务使用独立的Mount Namespace，会继承主机挂载点，但服务对挂载点的操作只在自己的Namespace内生效，不会反映到主机上private: 服务使用独立的Mount Namespace，在启动时没有任何挂载点，服务对挂载点的操作也不会反映到主机上]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tomcat8导致文件权限访问不到]]></title>
    <url>%2Fposts%2Fkfkd454f.html</url>
    <content type="text"><![CDATA[问题之前在tomcat 7下文件上传后访问一直没问题，现在tomcat版本升到8.5，在测试文件http上传时，发现所传文件无法通过nginx访问了：报错 403 forbidden 解决看下系统的umask 12345678910cat /etc/profile后发现if [ $UID -gt 199 ] &amp;&amp; [ &quot;`/usr/bin/id -gn`&quot; = &quot;`/usr/bin/id -un`&quot; ]; then umask 002else umask 022fi022是没问题的 看下tomcat的catlina.sh 123456if [ -z &quot;$UMASK&quot; ]; thenUMASK=&quot;0027&quot;fiumask $UMASKtomcat8改成0027了，把这个改成0022就好了]]></content>
      <categories>
        <category>中间件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kubelet状态更新机制]]></title>
    <url>%2Fposts%2Fdfkdaa65.html</url>
    <content type="text"><![CDATA[摘要当 Kubernetes 中 Node 节点出现状态异常的情况下，节点上的 Pod 会被重新调度到其他节点上去，但是有的时候我们会发现节点 Down 掉以后，Pod 并不会立即触发重新调度，这实际上就是和 Kubelet 的状态更新机制密切相关的，Kubernetes 提供了一些参数配置来触发重新调度到嗯时间，下面我们来分析下 Kubelet 状态更新的基本流程。 kubelet 自身会定期更新状态到 apiserver，通过参数--node-status-update-frequency指定上报频率，默认是 10s 上报一次。 kube-controller-manager 会每隔--node-monitor-period时间去检查 kubelet 的状态，默认是 5s。 当 node 失联一段时间后，kubernetes 判定 node 为 notready 状态，这段时长通过--node-monitor-grace-period参数配置，默认 40s。 当 node 失联一段时间后，kubernetes 判定 node 为 unhealthy 状态，这段时长通过--node-startup-grace-period参数配置，默认 1m0s。 当 node 失联一段时间后，kubernetes 开始删除原 node 上的 pod，这段时长是通过--pod-eviction-timeout参数配置，默认 5m0s。 kube-controller-manager 和 kubelet 是异步工作的，这意味着延迟可能包括任何的网络延迟、apiserver 的延迟、etcd 延迟，一个节点上的负载引起的延迟等等。因此，如果--node-status-update-frequency设置为5s，那么实际上 etcd 中的数据变化会需要 6-7s，甚至更长时间。 Kubelet在更新状态失败时，会进行nodeStatusUpdateRetry次重试，默认为 5 次。 Kubelet 会在函数tryUpdateNodeStatus中尝试进行状态更新。Kubelet 使用了 Golang 中的http.Client()方法，但是没有指定超时时间，因此，如果 API Server 过载时，当建立 TCP 连接时可能会出现一些故障。 因此，在nodeStatusUpdateRetry * --node-status-update-frequency时间后才会更新一次节点状态。 同时，Kubernetes 的 controller manager 将尝试每--node-monitor-period时间周期内检查nodeStatusUpdateRetry次。在--node-monitor-grace-period之后，会认为节点 unhealthy，然后会在--pod-eviction-timeout后删除 Pod。 kube proxy 有一个 watcher API，一旦 Pod 被驱逐了，kube proxy 将会通知更新节点的 iptables 规则，将 Pod 从 Service 的 Endpoints 中移除，这样就不会访问到来自故障节点的 Pod 了。 配置对于这些参数的配置，需要根据不通的集群规模场景来进行配置。 社区默认的配置 参数 值 –node-status-update-frequency 10s –node-monitor-period 5s –node-monitor-grace-period 40s –pod-eviction-timeout 5m 快速更新和快速响应 参数 值 –node-status-update-frequency 4s –node-monitor-period 2s –node-monitor-grace-period 20s –pod-eviction-timeout 30s 在这种情况下，Pod 将在 50s 被驱逐，因为该节点在 20s 后被视为Down掉了，--pod-eviction-timeout在 30s 之后发生，但是，这种情况会给 etcd 产生很大的开销，因为每个节点都会尝试每 2s 更新一次状态。 如果环境有1000个节点，那么每分钟将有15000次节点更新操作，这可能需要大型 etcd 容器甚至是 etcd 的专用节点。 如果我们计算尝试次数，则除法将给出5，但实际上每次尝试的 nodeStatusUpdateRetry 尝试将从3到5。 由于所有组件的延迟，尝试总次数将在15到25之间变化。 中等更新和平均响应 参数 值 –node-status-update-frequency 20s –node-monitor-period 5s –node-monitor-grace-period 2m –pod-eviction-timeout 1m 这种场景下会 20s 更新一次 node 状态，controller manager 认为 node 状态不正常之前，会有 2m60⁄205=30 次的 node 状态更新，Node 状态为 down 之后 1m，就会触发驱逐操作。 如果有 1000 个节点，1分钟之内就会有 60s/20s*1000=3000 次的节点状态更新操作。 低更新和慢响应 参数 值 –node-status-update-frequency 1m –node-monitor-period 5s –node-monitor-grace-period 5m –pod-eviction-timeout 1m Kubelet 将会 1m 更新一次节点的状态，在认为不健康之后会有 5m/1m*5=25 次重试更新的机会。Node为不健康的时候，1m 之后 pod开始被驱逐。 可以有不同的组合，例如快速更新和慢反应以满足特定情况。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[istio架构与技术]]></title>
    <url>%2Fposts%2F5a4fga4h.html</url>
    <content type="text"><![CDATA[概念使用云平台可以为组织提供丰富的好处。然而，不可否认的是，采用云可能会给 DevOps 团队带来压力。开发人员必须使用微服务以满足应用的可移植性，同时运营商管理了极其庞大的混合和多云部署。Istio 允许您连接、保护、控制和观测服务。 在较高的层次上，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。它是一个完全开源的服务网格，可以透明地分层到现有的分布式应用程序上。它也是一个平台，包括允许它集成到任何日志记录平台、遥测或策略系统的 API。Istio 的多样化功能集使您能够成功高效地运行分布式微服务架构，并提供保护、连接和监控微服务的统一方法。 Service Mesh• 治理能力独立（Sidecar） • 应用程序无感知 • 服务通信的基础设施层 为什么要使用 Istio？Istio 提供一种简单的方式来为已部署的服务建立网络，该网络具有负载均衡、服务间认证、监控等功能，只需要对服务的代码进行一点或不需要做任何改动。想要让服务支持 Istio，只需要在您的环境中部署一个特殊的 sidecar 代理，使用 Istio 控制平面功能配置和管理代理，拦截微服务之间的所有网络通信： HTTP、gRPC、WebSocket 和 TCP 流量的自动负载均衡。 通过丰富的路由规则、重试、故障转移和故障注入，可以对流量行为进行细粒度控制。 可插入的策略层和配置 API，支持访问控制、速率限制和配额。 对出入集群入口和出口中所有流量的自动度量指标、日志记录和追踪。 通过强大的基于身份的验证和授权，在集群中实现安全的服务间通信。 Istio 旨在实现可扩展性，满足各种部署需求。 核心功能Istio 在服务网络中统一提供了许多关键功能： 流量管理通过简单的规则配置和流量路由，您可以控制服务之间的流量和 API 调用。Istio 简化了断路器、超时和重试等服务级别属性的配置，并且可以轻松设置 A/B测试、金丝雀部署和基于百分比的流量分割的分阶段部署等重要任务。 通过更好地了解您的流量和开箱即用的故障恢复功能，您可以在问题出现之前先发现问题，使调用更可靠，并且使您的网络更加强大——无论您面临什么条件。 安全Istio 的安全功能使开发人员可以专注于应用程序级别的安全性。Istio 提供底层安全通信信道，并大规模管理服务通信的认证、授权和加密。使用Istio，服务通信在默认情况下是安全的，它允许您跨多种协议和运行时一致地实施策略——所有这些都很少或根本不需要应用程序更改。 虽然 Istio 与平台无关，但将其与 Kubernetes（或基础架构）网络策略结合使用，其优势会更大，包括在网络和应用层保护 pod 间或服务间通信的能力。 可观察性Istio 强大的追踪、监控和日志记录可让您深入了解服务网格部署。通过 Istio 的监控功能，可以真正了解服务性能如何影响上游和下游的功能，而其自定义仪表板可以提供对所有服务性能的可视性，并让您了解该性能如何影响您的其他进程。 Istio 的 Mixer 组件负责策略控制和遥测收集。它提供后端抽象和中介，将 Istio 的其余部分与各个基础架构后端的实现细节隔离开来，并为运维提供对网格和基础架构后端之间所有交互的细粒度控制。 所有这些功能可以让您可以更有效地设置、监控和实施服务上的 SLO。当然，最重要的是，您可以快速有效地检测和修复问题。 平台支持Istio 是独立于平台的，旨在运行在各种环境中，包括跨云、内部部署、Kubernetes、Mesos 等。您可以在 Kubernetes 上部署 Istio 或具有 Consul 的 Nomad 上部署。Istio 目前支持： 在 Kubernetes 上部署的服务 使用 Consul 注册的服务 在虚拟机上部署的服务 集成和定制策略执行组件可以扩展和定制，以便与现有的 ACL、日志、监控、配额、审计等方案集成。 架构Istio 服务网格逻辑上分为数据平面和控制平面。 数据平面由一组以 sidecar 方式部署的智能代理（Envoy）组成。这些代理可以调节和控制微服务及 Mixer 之间所有的网络通信。 控制平面负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。 下图显示了构成每个面板的不同组件： EnvoyIstio 使用 Envoy代理的扩展版本，Envoy 是以 C++ 开发的高性能代理，用于调解服务网格中所有服务的所有入站和出站流量。Envoy 的许多内置功能被 Istio 发扬光大，例如： 动态服务发现 负载均衡 TLS 终止 HTTP/2 &amp; gRPC 代理 熔断器 健康检查、基于百分比流量拆分的灰度发布 故障注入 丰富的度量指标 Envoy 被部署为 sidecar，和对应服务在同一个 Kubernetes pod 中。这允许 Istio 将大量关于流量行为的信号作为属性提取出来，而这些属性又可以在 Mixer 中用于执行策略决策，并发送给监控系统，以提供整个网格行为的信息。 Sidecar 代理模型还可以将 Istio 的功能添加到现有部署中，而无需重新构建或重写代码。可以阅读更多来了解为什么我们在设计目标中选择这种方式。 MixerMixer是一个独立于平台的组件，负责在服务网格上执行访问控制和使用策略，并从 Envoy 代理和其他服务收集遥测数据。代理提取请求级属性，发送到 Mixer 进行评估 Mixer 中包括一个灵活的插件模型，使其能够接入到各种主机环境和基础设施后端，从这些细节中抽象出 Envoy 代理和 Istio 管理的服务。 1可以配一些监控 PilotPilot为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。它将控制流量行为的高级路由规则转换为特定于 Envoy 的配置，并在运行时将它们传播到 sidecar。 Pilot 将平台特定的服务发现机制抽象化并将其合成为符合 Envoy 数据平面 API的任何 sidecar 都可以使用的标准格式。这种松散耦合使得 Istio 能够在多种环境下运行（例如，Kubernetes、Consul、Nomad），同时保持用于流量管理的相同操作界面。 1k8s来说的话，比如说pod这样的元数据，传给Envoy CitadelCitadel通过内置身份和凭证管理赋能强大的服务间和最终用户身份验证。可用于升级服务网格中未加密的流量，并为运维人员提供基于服务标识而不是网络控制的强制执行策略的能力。从 0.5 版本开始，Istio 支持基于角色的访问控制，以控制谁可以访问您的服务，而不是基于不稳定的三层或四层网络标识。 1证书安全管理中心，证书生成以及下发 GalleyGalley 将担任 Istio 的配置验证，获取配置，处理和分配组件的任务。它负责将其余的 Istio 组件与从底层平台（例如 Kubernetes）获取用户配置的细节中隔离开来。 和k8s结合 在 Helm 和 Tiller 的环境中使用 helm install 命令进行安装下载istio123456curl -L https://git.io/getLatestIstio | ISTIO_VERSION=1.1.9 sh -mv istio-1.1.9 /opt/k8s/work/cd /opt/k8s/work/cd istio-1.1.9/export PATH=$PWD/bin:$PATHhelm repo add istio.io https://storage.googleapis.com/istio-release/releases/1.1.9/charts/ 安装12345678910111213141516171819helm install install/kubernetes/helm/istio-init --name istio-init --namespace istio-systemkubectl get crds | grep &apos;istio.io\|certmanager.k8s.io&apos; | wc -l53helm install install/kubernetes/helm/istio --name istio --namespace istio-system[root@node1 istio-1.1.9]# kubectl get pods -n istio-systemNAME READY STATUS RESTARTS AGEistio-citadel-b6d6889c4-96fwx 1/1 Running 0 44histio-galley-654c696595-2rbr9 1/1 Running 0 44histio-ingressgateway-6b47b76cc6-2rxbq 1/1 Running 0 44histio-init-crd-10-smj28 0/1 Completed 0 44histio-init-crd-11-wktdb 0/1 Completed 0 44histio-pilot-5c99cfc94-g7t84 2/2 Running 0 44histio-policy-6c5795449-tkzmp 2/2 Running 3 44histio-sidecar-injector-79c88d56cf-lmv9j 1/1 Running 0 44histio-telemetry-64f99d84c7-ksjmh 2/2 Running 2 44hprometheus-d8d46c5b5-sdljw 1/1 Running 0 44h 卸载123helm delete --purge istiohelm delete --purge istio-initkubectl delete -f install/kubernetes/helm/istio-init/files]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s1.14集群部署-helm]]></title>
    <url>%2Fposts%2Fdkgg3h4f.html</url>
    <content type="text"><![CDATA[摘要Helm这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套测试环境，对于单体服务，部署一套测试环境我相信还是非常快的，但是对于微服务架构的应用，要部署一套新的环境，就有点折磨人了，微服务越多、你就会越绝望的。虽然我们线上和测试环境已经都迁移到了kubernetes环境，但是每个微服务也得维护一套yaml文件，而且每个环境下的配置文件也不太一样，部署一套新的环境成本是真的很高。如果我们能使用类似于yum的工具来安装我们的应用的话是不是就很爽歪歪了啊？Helm就相当于kubernetes环境下的yum包管理工具。 用途做为 Kubernetes 的一个包管理工具，Helm具有如下功能： 创建新的 chart chart 打包成 tgz 格式 上传 chart 到 chart 仓库或从仓库中下载 chart 在Kubernetes集群中安装或卸载 chart 管理用Helm安装的 chart 的发布周期 重要概念Helm 有三个重要概念： chart：包含了创建Kubernetes的一个应用实例的必要信息 config：包含了应用发布配置信息 release：是一个 chart 及其配置的一个运行实例 Helm组件Helm 有以下两个组成部分： Helm Client 是用户命令行工具，其主要负责如下： 本地 chart 开发 仓库管理 与 Tiller sever 交互 发送预安装的 chart 查询 release 信息 要求升级或卸载已存在的 release Tiller Server是一个部署在Kubernetes集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下： 监听来自 Helm client 的请求 通过 chart 及其配置构建一次发布 安装 chart 到Kubernetes集群，并跟踪随后的发布 通过与Kubernetes交互升级或卸载 chart 简单的说，client 管理 charts，而 server 管理发布 release 安装我们可以在Helm Realese页面下载二进制文件，这里下载的2.14.1版本，解压后将可执行文件helm拷贝到/usr/local/bin目录下即可，这样Helm客户端就在这台机器上安装完成了。 123helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts另外还需要在每个node节点安装yum install socat -y 123[root@node1 helm]# helm versionClient: &amp;version.Version&#123;SemVer:&quot;v2.14.1&quot;, GitCommit:&quot;5270352a09c7e8b6e8c9593002a73535276507c0&quot;, GitTreeState:&quot;clean&quot;&#125;Server: &amp;version.Version&#123;SemVer:&quot;v2.14.1&quot;, GitCommit:&quot;5270352a09c7e8b6e8c9593002a73535276507c0&quot;, GitTreeState:&quot;clean&quot;&#125; 123456自Kubernetes 1.6版本开始，API Server启用了RBAC授权。而目前的Tiller部署没有定义授权的ServiceAccount，这会导致访问API Server时被拒绝。我们可以采用如下方法，明确为Tiller部署添加授权。kubectl create serviceaccount --namespace kube-system tillerkubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tillerkubectl patch deploy --namespace kube-system tiller-deploy -p &apos;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&apos;kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default 使用我们现在了尝试创建一个 Chart： 1234567891011121314151617181920212223242526272829[root@node1 helm]# helm create hello-helmCreating hello-helm[root@node1 helm]# lshello-helm helm-v2.14.1-linux-amd64.tar.gz linux-amd64[root@node1 helm]# helm install ./hello-helmNAME: virulent-wolverineLAST DEPLOYED: Mon Jun 17 10:56:39 2019NAMESPACE: defaultSTATUS: DEPLOYEDRESOURCES:==&gt; v1/DeploymentNAME READY UP-TO-DATE AVAILABLE AGEvirulent-wolverine-hello-helm 0/1 0 0 1s==&gt; v1/Pod(related)NAME READY STATUS RESTARTS AGEvirulent-wolverine-hello-helm-6f54d6f866-d5t7v 0/1 ContainerCreating 0 1s==&gt; v1/ServiceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEvirulent-wolverine-hello-helm ClusterIP 10.254.123.130 &lt;none&gt; 80/TCP 1sNOTES:1. Get the application URL by running these commands: export POD_NAME=$(kubectl get pods --namespace default -l &quot;app.kubernetes.io/name=hello-helm,app.kubernetes.io/instance=virulent-wolverine&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;) echo &quot;Visit http://127.0.0.1:8080 to use your application&quot; kubectl port-forward $POD_NAME 8080:80 然后我们根据提示执行下面的命令： 12export POD_NAME=$(kubectl get pods --namespace default -l &quot;app.kubernetes.io/name=hello-helm,app.kubernetes.io/instance=virulent-wolverine&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)kubectl port-forward $POD_NAME 8080:80 访问： 1234567891011121314151617181920212223242526[root@node1 ~]# curl 127.0.0.1:8080&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 查看release： 123[root@node1 ~]# helm listNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACEvirulent-wolverine 1 Mon Jun 17 10:56:39 2019 DEPLOYED hello-helm-0.1.0 1.0 default 打包chart： 1helm package hello-helm 删除： 123456[root@node1 ~]# helm delete virulent-wolverinerelease &quot;virulent-wolverine&quot; deleted[root@node1 ~]# helm list[root@node1 ~]# helm list --allNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACEvirulent-wolverine 1 Mon Jun 17 10:56:39 2019 DELETED hello-helm-0.1.0 1.0 default 彻底删除： 1234[root@node1 ~]# helm delete virulent-wolverine --purgerelease &quot;virulent-wolverine&quot; deleted[root@node1 ~]# helm list --all[root@node1 ~]#]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s1.14集群部署-dashboard]]></title>
    <url>%2Fposts%2Fc2556dcf.html</url>
    <content type="text"><![CDATA[修改配置文件1cd /opt/k8s/work/kubernetes/cluster/addons/dashboard 修改 service 定义，指定端口类型为 NodePort，这样外界可以通过地址 NodeIP:NodePort 访问 dashboard； 1234567891011121314151617cat dashboard-service.yamlapiVersion: v1kind: Servicemetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: &quot;true&quot; addonmanager.kubernetes.io/mode: Reconcilespec: type: NodePort # 增加这一行 selector: k8s-app: kubernetes-dashboard ports: - port: 443 targetPort: 8443 修改镜像地址mirrorgooglecontainers/kubernetes-dashboard-amd64:v1.10.1在dashboard-controller.yaml中 执行所有定义文件1kubectl apply -f . 查看分配的 NodePort123456789101112131415161718[root@node1 dashboard]# kubectl get deployment kubernetes-dashboard -n kube-systemNAME READY UP-TO-DATE AVAILABLE AGEkubernetes-dashboard 1/1 1 1 23h[root@node1 dashboard]# kubectl --namespace kube-system get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-8854569d4-g2hth 1/1 Running 4 5d22h 172.30.40.2 node1 &lt;none&gt; &lt;none&gt;kubernetes-dashboard-7848d45466-6pm2q 1/1 Running 0 23h 172.30.200.3 node2 &lt;none&gt; &lt;none&gt;metrics-server-5f7cf7659-59swk 1/1 Running 0 2d5h 172.30.40.3 node1 &lt;none&gt; &lt;none&gt;[root@node1 dashboard]# kubectl get services kubernetes-dashboard -n kube-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes-dashboard NodePort 10.254.234.85 &lt;none&gt; 443:32681/TCP 23h 访问 dashboard1https://192.168.6.101:32681 创建登录 Dashboard 的 token 和 kubeconfig 配置文件dashboard 默认只支持 token 认证（不支持 client 证书认证），所以如果使用 Kubeconfig 文件，需要将 token 写入到该文件。 创建登录 token12345kubectl create sa dashboard-admin -n kube-systemkubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk &apos;&#123;print $1&#125;&apos;)DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system $&#123;ADMIN_SECRET&#125; | grep -E &apos;^token&apos; | awk &apos;&#123;print $2&#125;&apos;)echo $&#123;DASHBOARD_LOGIN_TOKEN&#125; 使用输出的 token 登录 Dashboard。 创建使用 token 的 KubeConfig 文件123456789101112131415161718192021source /opt/k8s/bin/environment.sh# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/cert/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=dashboard.kubeconfig# 设置客户端认证参数，使用上面创建的 Tokenkubectl config set-credentials dashboard_user \ --token=$&#123;DASHBOARD_LOGIN_TOKEN&#125; \ --kubeconfig=dashboard.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=dashboard_user \ --kubeconfig=dashboard.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=dashboard.kubeconfig 用生成的 dashboard.kubeconfig 登录 Dashboard。 为kubernetes dashboard访问用户添加权限控制RoleRole表示是一组规则权限，只能累加，Role可以定义在一个namespace中，只能用于授予对单个命名空间中的资源访问的权限。比如我们新建一个对默认命名空间中Pods具有访问权限的角色： 123456789kind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: namespace: default name: pod-readerrules:- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] ClusterRoleClusterRole具有与Role相同的权限角色控制能力，不同的是ClusterRole是集群级别的，可以用于: 集群级别的资源控制(例如 node 访问权限) 非资源型 endpoints(例如 /healthz 访问) 所有命名空间资源控制(例如 pods) 比如我们要创建一个授权某个特定命名空间或全部命名空间(取决于绑定方式)访问secrets的集群角色： 123456789kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: # &quot;namespace&quot; omitted since ClusterRoles are not namespaced name: secret-readerrules:- apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] RoleBinding和ClusterRoleBindingRoloBinding可以将角色中定义的权限授予用户或用户组，RoleBinding包含一组权限列表(subjects)，权限列表中包含有不同形式的待授予权限资源类型(users、groups、service accounts)，RoleBinding适用于某个命名空间内授权，而 ClusterRoleBinding适用于集群范围内的授权。 比如我们将默认命名空间的pod-reader角色授予用户jane，这样以后该用户在默认命名空间中将具有pod-reader的权限： 1234567891011121314# This role binding allows &quot;jane&quot; to read pods in the &quot;default&quot; namespace.kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: read-pods namespace: defaultsubjects:- kind: User name: jane apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io RoleBinding同样可以引用ClusterRole来对当前 namespace 内用户、用户组或 ServiceAccount 进行授权，这种操作允许集群管理员在整个集群内定义一些通用的 ClusterRole，然后在不同的 namespace 中使用 RoleBinding 来引用 例如，以下 RoleBinding 引用了一个 ClusterRole，这个 ClusterRole 具有整个集群内对 secrets 的访问权限；但是其授权用户 dave 只能访问 development 空间中的 secrets(因为 RoleBinding 定义在 development 命名空间) 1234567891011121314# This role binding allows &quot;dave&quot; to read secrets in the &quot;development&quot; namespace.kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: read-secrets namespace: development # This only grants permissions within the &quot;development&quot; namespace.subjects:- kind: User name: dave apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 最后，使用 ClusterRoleBinding 可以对整个集群中的所有命名空间资源权限进行授权；以下 ClusterRoleBinding 样例展示了授权 manager 组内所有用户在全部命名空间中对 secrets 进行访问 12345678910111213# This cluster role binding allows anyone in the &quot;manager&quot; group to read secrets in any namespace.kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: read-secrets-globalsubjects:- kind: Group name: manager apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 例子 新增一个新的用户sy 该用户只能对命名空间kube-system下面的pods和deployments进行管理 第一步新建一个ServiceAccount： 12[root@node1 dashboard]# kubectl create sa sy -n kube-systemserviceaccount/sy created 然后我们新建一个角色role-sy：(role.yaml) 123456789101112kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: namespace: kube-system name: role-syrules:- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]- apiGroups: [&quot;extensions&quot;, &quot;apps&quot;] resources: [&quot;deployments&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;] 注意上面的rules规则：管理pods和deployments的权限。 然后我们创建一个角色绑定，将上面的角色role-sy绑定到**sy**的ServiceAccount`上：(role-bind.yaml) 12345678910111213kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: role-bind-sy namespace: kube-systemsubjects:- kind: ServiceAccount name: sy namespace: kube-systemroleRef: kind: Role name: role-sy apiGroup: rbac.authorization.k8s.io 分别执行上面两个yaml文件： 1234[root@node1 dashboard]# kubectl create -f role.yamlrole.rbac.authorization.k8s.io/role-sy created[root@node1 dashboard]# kubectl create -f role-bind.yaml rolebinding.rbac.authorization.k8s.io/role-bind-sy created 接下来该怎么做？和前面一样的，我们只需要拿到sy这个ServiceAccount的token就可以登录Dashboard了： 12345[root@node1 dashboard]# kubectl get secret -n kube-system |grep sysy-token-5cmnl kubernetes.io/service-account-token 3 3m2s[root@node1 dashboard]# kubectl get secret sy-token-5cmnl -o jsonpath=&#123;.data.token&#125; -n kube-system |base64 -d# 会生成一串很长的base64后的字符串 这样就可以控制权限了，需要将登录地址改为namespace=kube-system]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s1.14集群部署-metrics-server]]></title>
    <url>%2Fposts%2Fd3554aa2.html</url>
    <content type="text"><![CDATA[介绍metrics-server 通过 kube-apiserver 发现所有节点，然后调用 kubelet APIs（通过 https 接口）获得各节点（Node）和 Pod 的 CPU、Memory 等资源使用情况。 从 Kubernetes 1.12 开始，kubernetes 的安装脚本移除了 Heapster，从 1.13 开始完全移除了对 Heapster 的支持，Heapster 不再被维护。 替代方案如下： 用于支持自动扩缩容的 CPU/memory HPA metrics：metrics-server； 通用的监控方案：使用第三方可以获取 Prometheus 格式监控指标的监控系统，如 Prometheus Operator； 事件传输：使用第三方工具来传输、归档 kubernetes events； Kubernetes Dashboard 还不支持 metrics-server（PR：#3504），如果使用 metrics-server 替代 Heapster，将无法在 dashboard 中以图形展示 Pod 的内存和 CPU 情况，需要通过 Prometheus、Grafana 等监控方案来弥补。 部署12345cd /opt/k8s/work/git clone https://github.com/kubernetes-incubator/metrics-server.gitcd metrics-server/deploy/1.8+/lsaggregated-metrics-reader.yaml auth-delegator.yaml auth-reader.yaml metrics-apiservice.yaml metrics-server-deployment.yaml metrics-server-service.yaml resource-reader.yaml 修改 metrics-server-deployment.yaml 文件，为 metrics-server 添加三个命令行参数：1234567891011121314151617181920212223242526272829303132333435363738394041---apiVersion: v1kind: ServiceAccountmetadata: name: metrics-server namespace: kube-system---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-serverspec: selector: matchLabels: k8s-app: metrics-server template: metadata: name: metrics-server labels: k8s-app: metrics-server spec: serviceAccountName: metrics-server volumes: # mount in tmp so we can safely use from-scratch images and/or read-only containers - name: tmp-dir emptyDir: &#123;&#125; containers: - name: metrics-server image: mirrorgooglecontainers/metrics-server-amd64:v0.3.3 command: - /metrics-server - --metric-resolution=30s - --requestheader-allowed-names=aggregator - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP imagePullPolicy: Always volumeMounts: - name: tmp-dir mountPath: /tmp –metric-resolution=30s：从 kubelet 采集数据的周期； –requestheader-allowed-names=aggregator：允许请求 metrics-server API 的用户名，该名称与 kube-apiserver 的 --proxy-client-cert-file 指定的证书 CN 一致； –kubelet-preferred-address-types：优先使用 InternalIP 来访问 kubelet，这样可以避免节点名称没有 DNS 解析记录时，通过节点名称调用节点 kubelet API 失败的情况（未配置时默认的情况）； 修改apiserver参数：12345cd /etc/systemd/systemvim kube-apiserver.service--requestheader-allowed-names=&quot;aggregator&quot; 重启apiserver 部署 metrics-server：12cd /opt/k8s/work/metrics-server/deploy/1.8+/kubectl create -f . 使用 kubectl top 命令查看集群节点资源使用情况1234[root@node1 1.8+]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% node1 117m 5% 2217Mi 57% node2 147m 7% 2680Mi 69%]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s1.14集群部署-coredns]]></title>
    <url>%2Fposts%2F77sa4nc2.html</url>
    <content type="text"><![CDATA[介绍1.11后CoreDNS 已取代 Kube DNS 作为集群服务发现元件,由于 Kubernetes 需要让 Pod 与 Pod 之间能夠互相通信,然而要能够通信需要知道彼此的 IP 才行,而这种做法通常是通过 Kubernetes API 来获取,但是 Pod IP 会因为生命周期变化而改变,因此这种做法无法弹性使用,且还会增加 API Server 负担,基于此问题 Kubernetes 提供了 DNS 服务来作为查询,让 Pod 能夠以 Service 名称作为域名来查询 IP 位址,因此使用者就再不需要关心实际 Pod IP,而 DNS 也会根据 Pod 变化更新资源记录(Record resources) CoreDNS 是由 CNCF 维护的开源 DNS 方案,该方案前身是 SkyDNS,其采用了 Caddy 的一部分来开发伺服器框架,使其能够建立一套快速灵活的 DNS,而 CoreDNS 每个功能都可以被当作成一個插件的中介软体,如 Log、Cache、Kubernetes 等功能,甚至能够将源记录存储在 Redis、Etcd 中 部署修改配置文件coredns 目录是 cluster/addons/dns： 123456cd /opt/k8s/work/kubernetes/cluster/addons/dns/corednscp coredns.yaml.base coredns.yamlsource /opt/k8s/bin/environment.shsed -i -e &quot;s/__PILLAR__DNS__DOMAIN__/$&#123;CLUSTER_DNS_DOMAIN&#125;/&quot; -e &quot;s/__PILLAR__DNS__SERVER__/$&#123;CLUSTER_DNS_SVC_IP&#125;/&quot; coredns.yaml还需要将镜像修改下，coredns/coredns:1.3.1 创建 coredns1kubectl create -f coredns.yaml 验证12345678910111213141516cat&lt;&lt;EOF | kubectl apply -f -apiVersion: v1kind: Podmetadata: name: busybox namespace: defaultspec: containers: - name: busybox image: busybox:1.28.3 command: - sleep - &quot;3600&quot; imagePullPolicy: IfNotPresent restartPolicy: AlwaysEOF 创建成功后，我们进行检查 12345678910111213141516171819202122232425kubectl get podNAME READY STATUS RESTARTS AGEbusybox 1/1 Running 0 4s[root@node1 coredns]# kubectl exec -ti busybox -- nslookup kubernetesServer: 10.254.0.2Address 1: 10.254.0.2 kube-dns.kube-system.svc.cluster.localName: kubernetesAddress 1: 10.254.0.1 kubernetes.default.svc.cluster.local[root@node1 ~]# kubectl exec -ti busybox ping kubernetes.default.svc.cluster.localPING kubernetes.default.svc.cluster.local (10.254.0.1): 56 data bytes64 bytes from 10.254.0.1: seq=0 ttl=64 time=0.099 ms^C--- kubernetes.default.svc.cluster.local ping statistics ---1 packets transmitted, 1 packets received, 0% packet lossround-trip min/avg/max = 0.099/0.099/0.099 ms注意：用my-svc.my-namespace.svc.cluster.local的方式可以访问服务]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s角色]]></title>
    <url>%2Fposts%2F65aa44f.html</url>
    <content type="text"><![CDATA[查看node节点1234[root@node1 work]# kubectl get nodesNAME STATUS ROLES AGE VERSIONnode1 Ready &lt;none&gt; 41h v1.14.2node2 Ready &lt;none&gt; 41h v1.14.2 设置集群角色12345678910111213# 设置 node1 为 master 角色kubectl label nodes node1 node-role.kubernetes.io/master=# 设置 node2 为 node 角色kubectl label nodes node2 node-role.kubernetes.io/node=[root@node1 ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONnode1 Ready master 42h v1.14.2node2 Ready node 42h v1.14.2 设置taint语法12345kubectl taint node [node] key=value[effect] 其中[effect] 可取值: [ NoSchedule | PreferNoSchedule | NoExecute ] NoSchedule: 一定不能被调度 PreferNoSchedule: 尽量不要调度 NoExecute: 不仅不会调度, 还会驱逐Node上已有的Pod 使用123456789[root@node1 ~]# kubectl taint nodes node1 node-role.kubernetes.io/master=:NoExecutenode/node1 tainted[root@node1 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-ds-kztdz 1/1 Running 0 18hnginx-ds-vbjh9 0/1 Terminating 0 18h[root@node1 ~]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-ds-kztdz 1/1 Running 0 18h 查看taint123456789101112131415161718192021222324[root@node1 ~]# kubectl describe node node1Name: node1Roles: masterLabels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=node1 kubernetes.io/os=linux node-role.kubernetes.io/master=Annotations: node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: trueCreationTimestamp: Tue, 04 Jun 2019 15:28:56 +0800Taints: node-role.kubernetes.io/master:NoExecute node-role.kubernetes.io/master:NoScheduleUnschedulable: falseConditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Thu, 06 Jun 2019 10:08:16 +0800 Tue, 04 Jun 2019 15:28:57 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Thu, 06 Jun 2019 10:08:16 +0800 Tue, 04 Jun 2019 15:28:57 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Thu, 06 Jun 2019 10:08:16 +0800 Tue, 04 Jun 2019 15:28:57 +0800 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Thu, 06 Jun 2019 10:08:16 +0800 Tue, 04 Jun 2019 15:28:57 +0800 KubeletReady kubelet is posting ready statusAddresses: InternalIP: 192.168.6.101 删除taint12[root@node1 ~]# kubectl taint nodes node1 node-role.kubernetes.io/master-node/node1 untainted RBACKubernetes有一个很基本的特性就是它的所有资源对象都是模型化的 API 对象，允许执行 CRUD(Create、Read、Update、Delete)操作(也就是我们常说的增、删、改、查操作)，比如下面的这下资源： Pods ConfigMaps Deployments Nodes Secrets Namespaces 上面这些资源对象的可能存在的操作有： create get delete list update edit watch exec 在更上层，这些资源和 API Group 进行关联，比如Pods属于 Core API Group，而Deployements属于 apps API Group，要在Kubernetes中进行RBAC的管理，除了上面的这些资源和操作以外，我们还需要另外的一些对象： Rule：规则，规则是一组属于不同 API Group 资源上的一组操作的集合 Role 和 ClusterRole：角色和集群角色，这两个对象都包含上面的 Rules 元素，二者的区别在于，在 Role 中，定义的规则只适用于单个命名空间，也就是和 namespace 关联的，而 ClusterRole 是集群范围内的，因此定义的规则不受命名空间的约束。另外 Role 和 ClusterRole 在Kubernetes中都被定义为集群内部的 API 资源，和我们前面学习过的 Pod、ConfigMap 这些类似，都是我们集群的资源对象，所以同样的可以使用我们前面的kubectl相关的命令来进行操作 Subject：主题，对应在集群中尝试操作的对象，集群中定义了3种类型的主题资源： User Account：用户，这是有外部独立服务进行管理的，管理员进行私钥的分配，用户可以使用 KeyStone或者 Goolge 帐号，甚至一个用户名和密码的文件列表也可以。对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部的 API 来进行管理 Group：组，这是用来关联多个账户的，集群中有一些默认创建的组，比如cluster-admin Service Account：服务帐号，通过Kubernetes API 来管理的一些用户帐号，和 namespace 进行关联的，适用于集群内部运行的应用程序，需要通过 API 来完成权限认证，所以在集群内部进行权限操作，我们都需要使用到 ServiceAccount，这也是我们这节课的重点 RoleBinding 和 ClusterRoleBinding：角色绑定和集群角色绑定，简单来说就是把声明的 Subject 和我们的 Role 进行绑定的过程(给某个用户绑定上操作的权限)，二者的区别也是作用范围的区别：RoleBinding 只会影响到当前 namespace 下面的资源操作权限，而 ClusterRoleBinding 会影响到所有的 namespace。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s1.14集群部署-node节点]]></title>
    <url>%2Fposts%2F44qq5gb2.html</url>
    <content type="text"><![CDATA[安装依赖包1234567source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;yum install -y epel-release&quot; ssh root@$&#123;node_ip&#125; &quot;yum install -y conntrack ipvsadm ntp ntpdate ipset jq iptables curl sysstat libseccomp &amp;&amp; modprobe ip_vs &quot; done docker下载和分发 docker 二进制文件到 docker 下载页面 下载最新发布包： 123cd /opt/k8s/workwget https://download.docker.com/linux/static/stable/x86_64/docker-18.09.6.tgztar -xvf docker-18.09.6.tgz 分发二进制文件到所有 worker 节点： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp docker/* root@$&#123;node_ip&#125;:/opt/k8s/bin/ ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot; done 创建和分发 systemd unit 文件1234567891011121314151617181920212223cd /opt/k8s/workcat &gt; docker.service &lt;&lt;&quot;EOF&quot;[Unit]Description=Docker Application Container EngineDocumentation=http://docs.docker.io[Service]WorkingDirectory=##DOCKER_DIR##Environment=&quot;PATH=/opt/k8s/bin:/bin:/sbin:/usr/bin:/usr/sbin&quot;EnvironmentFile=-/run/flannel/dockerExecStart=/opt/k8s/bin/dockerd $DOCKER_NETWORK_OPTIONSExecReload=/bin/kill -s HUP $MAINPIDRestart=on-failureRestartSec=5LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinityDelegate=yesKillMode=process[Install]WantedBy=multi-user.targetEOF 分发 systemd unit 文件到所有 worker 机器: 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shsed -i -e &quot;s|##DOCKER_DIR##|$&#123;DOCKER_DIR&#125;|&quot; docker.servicefor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp docker.service root@$&#123;node_ip&#125;:/etc/systemd/system/ done 配置和分发 docker 配置文件使用国内的仓库镜像服务器以加快 pull image 的速度，同时增加下载的并发数 (需要重启 dockerd 生效)： 123456789101112131415161718cd /opt/k8s/worksource /opt/k8s/bin/environment.shcat &gt; docker-daemon.json &lt;&lt;EOF&#123; &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;https://hub-mirror.c.163.com&quot;], &quot;insecure-registries&quot;: [&quot;docker02:35000&quot;], &quot;max-concurrent-downloads&quot;: 20, &quot;live-restore&quot;: true, &quot;max-concurrent-uploads&quot;: 10, &quot;debug&quot;: true, &quot;data-root&quot;: &quot;$&#123;DOCKER_DIR&#125;/data&quot;, &quot;exec-root&quot;: &quot;$&#123;DOCKER_DIR&#125;/exec&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;100m&quot;, &quot;max-file&quot;: &quot;5&quot; &#125;&#125;EOF 分发 docker 配置文件到所有 worker 节点： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/docker/ $&#123;DOCKER_DIR&#125;/&#123;data,exec&#125;&quot; scp docker-daemon.json root@$&#123;node_ip&#125;:/etc/docker/daemon.json done 启动 docker 服务123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl restart docker&quot; done 检查服务运行状态123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl status docker|grep Active&quot; done 确保状态为 active (running) kubelet创建 kubelet bootstrap kubeconfig 文件123456789101112131415161718192021222324252627282930313233cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_name in $&#123;NODE_NAMES[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot; # 创建 token export BOOTSTRAP_TOKEN=$(kubeadm token create \ --description kubelet-bootstrap-token \ --groups system:bootstrappers:$&#123;node_name&#125; \ --kubeconfig ~/.kube/config) # 设置集群参数 kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/cert/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig # 设置客户端认证参数 kubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig # 设置上下文参数 kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig # 设置默认上下文 kubectl config use-context default --kubeconfig=kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig done 向 kubeconfig 写入的是 token，bootstrap 结束后 kube-controller-manager 为 kubelet 创建 client 和 server 证书； 查看 kubeadm 为各节点创建的 token： 12345[root@node1 ~]# kubeadm token list --kubeconfig ~/.kube/configTOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPSkp5seh.klhbcowm40rkaoh1 &lt;invalid&gt; 2019-06-05T15:24:51+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:node1u2zt2n.3tqw704a4ndqdj1k &lt;invalid&gt; 2019-06-05T15:24:51+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:node2[root@node1 ~]# 分发 bootstrap kubeconfig 文件到所有 worker 节点1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_name in $&#123;NODE_NAMES[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot; scp kubelet-bootstrap-$&#123;node_name&#125;.kubeconfig root@$&#123;node_name&#125;:/etc/kubernetes/kubelet-bootstrap.kubeconfig done 创建和分发 kubelet 参数配置文件创建 kubelet 参数配置文件模板（可配置项参考代码中注释 ）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172cd /opt/k8s/worksource /opt/k8s/bin/environment.shcat &gt; kubelet-config.yaml.template &lt;&lt;EOFkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: &quot;##NODE_IP##&quot;staticPodPath: &quot;&quot;syncFrequency: 1mfileCheckFrequency: 20shttpCheckFrequency: 20sstaticPodURL: &quot;&quot;port: 10250readOnlyPort: 0rotateCertificates: trueserverTLSBootstrap: trueauthentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: &quot;/etc/kubernetes/cert/ca.pem&quot;authorization: mode: WebhookregistryPullQPS: 0registryBurst: 20eventRecordQPS: 0eventBurst: 20enableDebuggingHandlers: trueenableContentionProfiling: truehealthzPort: 10248healthzBindAddress: &quot;##NODE_IP##&quot;clusterDomain: &quot;$&#123;CLUSTER_DNS_DOMAIN&#125;&quot;clusterDNS: - &quot;$&#123;CLUSTER_DNS_SVC_IP&#125;&quot;nodeStatusUpdateFrequency: 10snodeStatusReportFrequency: 1mimageMinimumGCAge: 2mimageGCHighThresholdPercent: 85imageGCLowThresholdPercent: 80volumeStatsAggPeriod: 1mkubeletCgroups: &quot;&quot;systemCgroups: &quot;&quot;cgroupRoot: &quot;&quot;cgroupsPerQOS: truecgroupDriver: cgroupfsruntimeRequestTimeout: 10mhairpinMode: promiscuous-bridgemaxPods: 220podCIDR: &quot;$&#123;CLUSTER_CIDR&#125;&quot;podPidsLimit: -1resolvConf: /etc/resolv.confmaxOpenFiles: 1000000kubeAPIQPS: 1000kubeAPIBurst: 2000serializeImagePulls: falseevictionHard: memory.available: &quot;100Mi&quot;nodefs.available: &quot;10%&quot;nodefs.inodesFree: &quot;5%&quot;imagefs.available: &quot;15%&quot;evictionSoft: &#123;&#125;enableControllerAttachDetach: truefailSwapOn: truecontainerLogMaxSize: 20MicontainerLogMaxFiles: 10systemReserved: &#123;&#125;kubeReserved: &#123;&#125;systemReservedCgroup: &quot;&quot;kubeReservedCgroup: &quot;&quot;enforceNodeAllocatable: [&quot;pods&quot;]EOF address：kubelet 安全端口（https，10250）监听的地址，不能为 127.0.0.1，否则 kube-apiserver、heapster 等不能调用 kubelet 的 API； readOnlyPort=0：关闭只读端口(默认 10255)，等效为未指定； authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口； authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTP 证书认证； authentication.webhook.enabled=true：开启 HTTPs bearer token 认证； 对于未通过 x509 证书和 webhook 认证的请求(kube-apiserver 或其他客户端)，将被拒绝，提示 Unauthorized； authroization.mode=Webhook：kubelet 使用 SubjectAccessReview API 查询 kube-apiserver 某 user、group 是否具有操作资源的权限(RBAC)； featureGates.RotateKubeletClientCertificate、featureGates.RotateKubeletServerCertificate：自动 rotate 证书，证书的有效期取决于 kube-controller-manager 的 –experimental-cluster-signing-duration 参数； 需要 root 账户运行； 为各节点创建和分发 kubelet 配置文件： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; sed -e &quot;s/##NODE_IP##/$&#123;node_ip&#125;/&quot; kubelet-config.yaml.template &gt; kubelet-config-$&#123;node_ip&#125;.yaml.template scp kubelet-config-$&#123;node_ip&#125;.yaml.template root@$&#123;node_ip&#125;:/etc/kubernetes/kubelet-config.yaml done 创建和分发 kubelet systemd unit 文件创建 kubelet systemd unit 文件模板： 12345678910111213141516171819202122232425262728293031323334cd /opt/k8s/worksource /opt/k8s/bin/environment.shcat &gt; kubelet.service.template &lt;&lt;EOF[Unit]Description=Kubernetes KubeletDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=$&#123;K8S_DIR&#125;/kubeletExecStart=/opt/k8s/bin/kubelet \\ --allow-privileged=true \\ --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \\ --cert-dir=/etc/kubernetes/cert \\ --cni-conf-dir=/etc/cni/net.d \\ --container-runtime=docker \\ --container-runtime-endpoint=unix:///var/run/dockershim.sock \\ --root-dir=$&#123;K8S_DIR&#125;/kubelet \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --config=/etc/kubernetes/kubelet-config.yaml \\ --hostname-override=##NODE_NAME## \\ --pod-infra-container-image=registry.cn-beijing.aliyuncs.com/k8s_images/pause-amd64:3.1 \\ --image-pull-progress-deadline=15m \\ --volume-plugin-dir=$&#123;K8S_DIR&#125;/kubelet/kubelet-plugins/volume/exec/ \\ --logtostderr=true \\ --v=2Restart=alwaysRestartSec=5StartLimitInterval=0[Install]WantedBy=multi-user.targetEOF 如果设置了 --hostname-override 选项，则 kube-proxy 也需要设置该选项，否则会出现找不到 Node 的情况； --bootstrap-kubeconfig：指向 bootstrap kubeconfig 文件，kubelet 使用该文件中的用户名和 token 向 kube-apiserver 发送 TLS Bootstrapping 请求； K8S approve kubelet 的 csr 请求后，在 --cert-dir 目录创建证书和私钥文件，然后写入 --kubeconfig 文件； --pod-infra-container-image 不使用 redhat 的 pod-infrastructure:latest 镜像，它不能回收容器的僵尸； 为各节点创建和分发 kubelet systemd unit 文件： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_name in $&#123;NODE_NAMES[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot; sed -e &quot;s/##NODE_NAME##/$&#123;node_name&#125;/&quot; kubelet.service.template &gt; kubelet-$&#123;node_name&#125;.service scp kubelet-$&#123;node_name&#125;.service root@$&#123;node_name&#125;:/etc/systemd/system/kubelet.service done Bootstrap Token Auth 和授予权限kubelet 启动时查找 --kubeletconfig 参数对应的文件是否存在，如果不存在则使用 --bootstrap-kubeconfig 指定的 kubeconfig 文件向 kube-apiserver 发送证书签名请求 (CSR)。 kube-apiserver 收到 CSR 请求后，对其中的 Token 进行认证，认证通过后将请求的 user 设置为 system:bootstrap:&lt;Token ID&gt;，group 设置为 system:bootstrappers，这一过程称为 Bootstrap Token Auth。 如果说kubelet启动失败的话： 创建一个 clusterrolebinding，将 group system:bootstrappers 和 clusterrole system:node-bootstrapper 绑定： 1$ kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers 启动 kubelet 服务12345678source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kubelet/kubelet-plugins/volume/exec/&quot; ssh root@$&#123;node_ip&#125; &quot;/usr/sbin/swapoff -a&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet&quot; done kubelet 启动后使用 –bootstrap-kubeconfig 向 kube-apiserver 发送 CSR 请求，当这个 CSR 被 approve 后，kube-controller-manager 为 kubelet 创建 TLS 客户端证书、私钥和 –kubeletconfig 文件。 注意：kube-controller-manager 需要配置 --cluster-signing-cert-file 和 --cluster-signing-key-file参数，才会为 TLS Bootstrap 创建证书和私钥。 自动 approve CSR 请求创建三个 ClusterRoleBinding，分别用于自动 approve client、renew client、renew server 证书： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556cd /opt/k8s/workcat &gt; csr-crb.yaml &lt;&lt;EOF # Approve all CSRs for the group &quot;system:bootstrappers&quot; kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: auto-approve-csrs-for-group subjects: - kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclient apiGroup: rbac.authorization.k8s.io--- # To let a node of the group &quot;system:nodes&quot; renew its own credentials kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: node-client-cert-renewal subjects: - kind: Group name: system:nodes apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient apiGroup: rbac.authorization.k8s.io---# A ClusterRole which instructs the CSR approver to approve a node requesting a# serving cert matching its client cert.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: approve-node-server-renewal-csrrules:- apiGroups: [&quot;certificates.k8s.io&quot;] resources: [&quot;certificatesigningrequests/selfnodeserver&quot;] verbs: [&quot;create&quot;]--- # To let a node of the group &quot;system:nodes&quot; renew its own server credentials kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: node-server-cert-renewal subjects: - kind: Group name: system:nodes apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: approve-node-server-renewal-csr apiGroup: rbac.authorization.k8s.ioEOFkubectl apply -f csr-crb.yaml auto-approve-csrs-for-group：自动 approve node 的第一次 CSR； 注意第一次 CSR 时，请求的 Group 为 system:bootstrappers； node-client-cert-renewal：自动 approve node 后续过期的 client 证书，自动生成的证书 Group 为 system:nodes; node-server-cert-renewal：自动 approve node 后续过期的 server 证书，自动生成的证书 Group 为 system:nodes; 手动 approve server cert csr基于安全性考虑，CSR approving controllers 不会自动 approve kubelet server 证书签名请求，需要手动 approve： 1234567891011121314151617181920$ kubectl get csrNAME AGE REQUESTOR CONDITIONcsr-5f4vh 9m25s system:bootstrap:82jfrm Approved,Issuedcsr-5r7j7 6m11s system:node:zhangjun-k8s03 Pendingcsr-5rw7s 9m23s system:bootstrap:b1f7np Approved,Issuedcsr-9snww 8m3s system:bootstrap:82jfrm Approved,Issuedcsr-c7z56 6m12s system:node:zhangjun-k8s02 Pendingcsr-j55lh 6m12s system:node:zhangjun-k8s01 Pendingcsr-m29fm 9m25s system:bootstrap:3gzd53 Approved,Issuedcsr-rc8w7 8m3s system:bootstrap:3gzd53 Approved,Issuedcsr-vd52r 8m2s system:bootstrap:b1f7np Approved,Issued$ kubectl certificate approve csr-5r7j7certificatesigningrequest.certificates.k8s.io/csr-5r7j7 approved$ kubectl certificate approve csr-c7z56certificatesigningrequest.certificates.k8s.io/csr-c7z56 approved$ kubectl certificate approve csr-j55lhcertificatesigningrequest.certificates.k8s.io/csr-j55lh approved kubelet 提供的 API 接口 10248: healthz http 服务； 10250: https 服务，访问该端口时需要认证和授权（即使访问 /healthz 也需要）； 未开启只读端口 10255； 从 K8S v1.10 开始，去除了 --cadvisor-port 参数（默认 4194 端口），不支持访问 cAdvisor UI &amp; API。 例如执行 kubectl exec -it nginx-ds-5rmws -- sh 命令时，kube-apiserver 会向 kubelet 发送如下请求： 1POST /exec/default/nginx-ds-5rmws/my-nginx?command=sh&amp;input=1&amp;output=1&amp;tty=1 kubelet 接收 10250 端口的 https 请求，可以访问如下资源： /pods、/runningpods /metrics、/metrics/cadvisor、/metrics/probes /spec /stats、/stats/container /logs /run/、/exec/, /attach/, /portForward/, /containerLogs/ 详情参考：https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/server/server.go#L434:3 由于关闭了匿名认证，同时开启了 webhook 授权，所有访问 10250 端口 https API 的请求都需要被认证和授权。 预定义的 ClusterRole system:kubelet-api-admin 授予访问 kubelet 所有 API 的权限(kube-apiserver 使用的 kubernetes 证书 User 授予了该权限)： 12345678910111213$ kubectl describe clusterrole system:kubelet-api-adminName: system:kubelet-api-adminLabels: kubernetes.io/bootstrapping=rbac-defaultsAnnotations: rbac.authorization.kubernetes.io/autoupdate=truePolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- nodes [] [] [get list watch proxy] nodes/log [] [] [*] nodes/metrics [] [] [*] nodes/proxy [] [] [*] nodes/spec [] [] [*] nodes/stats [] [] [*] kubelet api 认证和授权kubelet 配置了如下认证参数： authentication.anonymous.enabled：设置为 false，不允许匿名访问 10250 端口； authentication.x509.clientCAFile：指定签名客户端证书的 CA 证书，开启 HTTPs 证书认证； authentication.webhook.enabled=true：开启 HTTPs bearer token 认证； 同时配置了如下授权参数： authroization.mode=Webhook：开启 RBAC 授权； kubelet 收到请求后，使用 clientCAFile 对证书签名进行认证，或者查询 bearer token 是否有效。如果两者都没通过，则拒绝请求，提示 Unauthorized： 12345$ curl -s --cacert /etc/kubernetes/cert/ca.pem https://192.168.6.101:10250/metricsUnauthorized$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer 123456&quot; https://192.168.6.101:10250/metricsUnauthorized 通过认证后，kubelet 使用 SubjectAccessReview API 向 kube-apiserver 发送请求，查询证书或 token 对应的 user、group 是否有操作资源的权限(RBAC)； 证书认证和授权12345678910111213141516$ # 权限不足的证书；$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /etc/kubernetes/cert/kube-controller-manager.pem --key /etc/kubernetes/cert/kube-controller-manager-key.pem https://192.168.6.101:10250/metricsForbidden (user=system:kube-controller-manager, verb=get, resource=nodes, subresource=metrics)$ # 使用部署 kubectl 命令行工具时创建的、具有最高权限的 admin 证书；$ curl -s --cacert /etc/kubernetes/cert/ca.pem --cert /opt/k8s/work/admin.pem --key /opt/k8s/work/admin-key.pem https://192.168.6.101:10250/metrics|head# HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.# TYPE apiserver_audit_event_total counterapiserver_audit_event_total 0# HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.# TYPE apiserver_audit_requests_rejected_total counterapiserver_audit_requests_rejected_total 0# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.# TYPE apiserver_client_certificate_expiration_seconds histogramapiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;0&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;1800&quot;&#125; 0 --cacert、--cert、--key 的参数值必须是文件路径，如上面的 ./admin.pem 不能省略 ./，否则返回 401 Unauthorized； bear token 认证和授权创建一个 ServiceAccount，将它和 ClusterRole system:kubelet-api-admin 绑定，从而具有调用 kubelet API 的权限： 12345678910111213141516kubectl create sa kubelet-api-testkubectl create clusterrolebinding kubelet-api-test --clusterrole=system:kubelet-api-admin --serviceaccount=default:kubelet-api-testSECRET=$(kubectl get secrets | grep kubelet-api-test | awk &apos;&#123;print $1&#125;&apos;)TOKEN=$(kubectl describe secret $&#123;SECRET&#125; | grep -E &apos;^token&apos; | awk &apos;&#123;print $2&#125;&apos;)echo $&#123;TOKEN&#125;$ curl -s --cacert /etc/kubernetes/cert/ca.pem -H &quot;Authorization: Bearer $&#123;TOKEN&#125;&quot; https://192.168.6.101:10250/metrics|head# HELP apiserver_audit_event_total Counter of audit events generated and sent to the audit backend.# TYPE apiserver_audit_event_total counterapiserver_audit_event_total 0# HELP apiserver_audit_requests_rejected_total Counter of apiserver requests rejected due to an error in audit logging backend.# TYPE apiserver_audit_requests_rejected_total counterapiserver_audit_requests_rejected_total 0# HELP apiserver_client_certificate_expiration_seconds Distribution of the remaining lifetime on the certificate used to authenticate a request.# TYPE apiserver_client_certificate_expiration_seconds histogramapiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;0&quot;&#125; 0apiserver_client_certificate_expiration_seconds_bucket&#123;le=&quot;1800&quot;&#125; 0 kube-proxy创建 kube-proxy 证书创建证书签名请求： 12345678910111213141516171819cd /opt/k8s/workcat &gt; kube-proxy-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;4Paradigm&quot; &#125; ]&#125;EOF CN：指定该证书的 User 为 system:kube-proxy； 预定义的 RoleBinding system:node-proxier 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限； 该证书只会被 kube-proxy 当做 client 证书使用，所以 hosts 字段为空； 生成证书和私钥： 123456cd /opt/k8s/workcfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxyls kube-proxy* 创建和分发 kubeconfig 文件1234567891011121314151617181920cd /opt/k8s/worksource /opt/k8s/bin/environment.shkubectl config set-cluster kubernetes \ --certificate-authority=/opt/k8s/work/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.kubeconfigkubectl config set-credentials kube-proxy \ --client-certificate=kube-proxy.pem \ --client-key=kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfigkubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfigkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig --embed-certs=true：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl-proxy.kubeconfig 文件中(不加时，写入的是证书文件路径)； 分发 kubeconfig 文件： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_name in $&#123;NODE_NAMES[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot; scp kube-proxy.kubeconfig root@$&#123;node_name&#125;:/etc/kubernetes/ done 创建 kube-proxy 配置文件从 v1.10 开始，kube-proxy 部分参数可以配置文件中配置。可以使用 --write-config-to 选项生成该配置文件，或者参考 源代码的注释。 创建 kube-proxy config 文件模板： 12345678910111213141516171819202122cd /opt/k8s/workcat &gt; kube-proxy-config.yaml.template &lt;&lt;EOFkind: KubeProxyConfigurationapiVersion: kubeproxy.config.k8s.io/v1alpha1clientConnection: burst: 200 kubeconfig: &quot;/etc/kubernetes/kube-proxy.kubeconfig&quot; qps: 100bindAddress: ##NODE_IP##healthzBindAddress: ##NODE_IP##:10256metricsBindAddress: ##NODE_IP##:10249enableProfiling: trueclusterCIDR: $&#123;CLUSTER_CIDR&#125;hostnameOverride: ##NODE_NAME##mode: &quot;ipvs&quot;portRange: &quot;&quot;kubeProxyIPTablesConfiguration: masqueradeAll: falsekubeProxyIPVSConfiguration: scheduler: rr excludeCIDRs: []EOF bindAddress: 监听地址； clientConnection.kubeconfig: 连接 apiserver 的 kubeconfig 文件； clusterCIDR: kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT； hostnameOverride: 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则； mode: 使用 ipvs 模式； 为各节点创建和分发 kube-proxy 配置文件： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor (( i=0; i &lt; 3; i++ )) do echo &quot;&gt;&gt;&gt; $&#123;NODE_NAMES[i]&#125;&quot; sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-proxy-config.yaml.template &gt; kube-proxy-config-$&#123;NODE_NAMES[i]&#125;.yaml.template scp kube-proxy-config-$&#123;NODE_NAMES[i]&#125;.yaml.template root@$&#123;NODE_NAMES[i]&#125;:/etc/kubernetes/kube-proxy-config.yaml done 创建和分发 kube-proxy systemd unit 文件123456789101112131415161718192021cd /opt/k8s/worksource /opt/k8s/bin/environment.shcat &gt; kube-proxy.service &lt;&lt;EOF[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=$&#123;K8S_DIR&#125;/kube-proxyExecStart=/opt/k8s/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy-config.yaml \\ --logtostderr=true \\ --v=2Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 分发 kube-proxy systemd unit 文件： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_name in $&#123;NODE_NAMES[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_name&#125;&quot; scp kube-proxy.service root@$&#123;node_name&#125;:/etc/systemd/system/ done 启动 kube-proxy 服务123456789cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-proxy&quot; ssh root@$&#123;node_ip&#125; &quot;modprobe ip_vs_rr&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy&quot; done 启动服务前必须先创建工作目录； 检查启动结果123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-proxy|grep Active&quot; done 确保状态为 active (running) 查看 ipvs 路由规则123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;/usr/sbin/ipvsadm -ln&quot; done]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s1.14集群部署-controller、schedule]]></title>
    <url>%2Fposts%2F544ccaa2.html</url>
    <content type="text"><![CDATA[kube-controller-manager 集群创建 kube-controller-manager 证书和私钥创建证书签名请求： 123456789101112131415161718192021222324cd /opt/k8s/workcat &gt; kube-controller-manager-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;system:kube-controller-manager&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.6.101&quot;, &quot;192.168.6.102&quot; ], &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;system:kube-controller-manager&quot;, &quot;OU&quot;: &quot;4Paradigm&quot; &#125; ]&#125;EOF hosts 列表包含所有 kube-controller-manager 节点 IP； CN 和 O 均为 system:kube-controller-manager，kubernetes 内置的 ClusterRoleBindings system:kube-controller-manager 赋予 kube-controller-manager 工作所需的权限。 生成证书和私钥： 12345cd /opt/k8s/workcfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager 将生成的证书和私钥分发到所有 master 节点： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kube-controller-manager*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/ done 创建和分发 kubeconfig 文件kube-controller-manager 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-controller-manager 证书： 1234567891011121314151617181920cd /opt/k8s/worksource /opt/k8s/bin/environment.shkubectl config set-cluster kubernetes \ --certificate-authority=/opt/k8s/work/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-controller-manager.kubeconfigkubectl config set-credentials system:kube-controller-manager \ --client-certificate=kube-controller-manager.pem \ --client-key=kube-controller-manager-key.pem \ --embed-certs=true \ --kubeconfig=kube-controller-manager.kubeconfigkubectl config set-context system:kube-controller-manager \ --cluster=kubernetes \ --user=system:kube-controller-manager \ --kubeconfig=kube-controller-manager.kubeconfigkubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig 分发 kubeconfig 到所有 master 节点： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kube-controller-manager.kubeconfig root@$&#123;node_ip&#125;:/etc/kubernetes/ done 创建 kube-controller-manager systemd unit 模板文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152cd /opt/k8s/worksource /opt/k8s/bin/environment.shcat &gt; kube-controller-manager.service.template &lt;&lt;EOF[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]WorkingDirectory=$&#123;K8S_DIR&#125;/kube-controller-managerExecStart=/opt/k8s/bin/kube-controller-manager \\ --profiling \\ --cluster-name=kubernetes \\ --controllers=*,bootstrapsigner,tokencleaner \\ --kube-api-qps=1000 \\ --kube-api-burst=2000 \\ --leader-elect \\ --use-service-account-credentials\\ --concurrent-service-syncs=2 \\ --bind-address=##NODE_IP## \\ --secure-port=10252 \\ --tls-cert-file=/etc/kubernetes/cert/kube-controller-manager.pem \\ --tls-private-key-file=/etc/kubernetes/cert/kube-controller-manager-key.pem \\ --port=0 \\ --authentication-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\ --client-ca-file=/etc/kubernetes/cert/ca.pem \\ --requestheader-allowed-names=&quot;&quot; \\ --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\ --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-username-headers=X-Remote-User \\ --authorization-kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\ --cluster-signing-cert-file=/etc/kubernetes/cert/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/cert/ca-key.pem \\ --experimental-cluster-signing-duration=8760h \\ --horizontal-pod-autoscaler-sync-period=10s \\ --concurrent-deployment-syncs=10 \\ --concurrent-gc-syncs=30 \\ --node-cidr-mask-size=24 \\ --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\ --pod-eviction-timeout=6m \\ --terminated-pod-gc-threshold=10000 \\ --root-ca-file=/etc/kubernetes/cert/ca.pem \\ --service-account-private-key-file=/etc/kubernetes/cert/ca-key.pem \\ --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\ --logtostderr=true \\ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOF --port=0：关闭监听非安全端口（http），同时 --address 参数无效，--bind-address 参数有效； --secure-port=10252、--bind-address=0.0.0.0: 在所有网络接口监听 10252 端口的 https /metrics 请求； --kubeconfig：指定 kubeconfig 文件路径，kube-controller-manager 使用它连接和验证 kube-apiserver； --authentication-kubeconfig 和 --authorization-kubeconfig：kube-controller-manager 使用它连接 apiserver，对 client 的请求进行认证和授权。kube-controller-manager 不再使用 --tls-ca-file对请求 https metrics 的 Client 证书进行校验。如果没有配置这两个 kubeconfig 参数，则 client 连接 kube-controller-manager https 端口的请求会被拒绝(提示权限不足)。 --cluster-signing-*-file：签名 TLS Bootstrap 创建的证书； --experimental-cluster-signing-duration：指定 TLS Bootstrap 证书的有效期； --root-ca-file：放置到容器 ServiceAccount 中的 CA 证书，用来对 kube-apiserver 的证书进行校验； --service-account-private-key-file：签名 ServiceAccount 中 Token 的私钥文件，必须和 kube-apiserver 的 --service-account-key-file 指定的公钥文件配对使用； --service-cluster-ip-range ：指定 Service Cluster IP 网段，必须和 kube-apiserver 中的同名参数一致； --leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态； --controllers=*,bootstrapsigner,tokencleaner：启用的控制器列表，tokencleaner 用于自动清理过期的 Bootstrap token； --horizontal-pod-autoscaler-*：custom metrics 相关参数，支持 autoscaling/v2alpha1； --tls-cert-file、--tls-private-key-file：使用 https 输出 metrics 时使用的 Server 证书和秘钥； --use-service-account-credentials=true: kube-controller-manager 中各 controller 使用 serviceaccount 访问 kube-apiserver； 创建和分发 kube-controller-mananger systemd unit 文件替换模板文件中的变量，为各节点创建 systemd unit 文件： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor (( i=0; i &lt; 2; i++ )) do sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-controller-manager.service.template &gt; kube-controller-manager-$&#123;NODE_IPS[i]&#125;.service donels kube-controller-manager*.service NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP； 分发到所有 master 节点： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kube-controller-manager-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-controller-manager.service done 文件重命名为 kube-controller-manager.service; 启动 kube-controller-manager 服务1234567source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-controller-manager&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl restart kube-controller-manager&quot; done 启动服务前必须先创建工作目录； 检查服务运行状态123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-controller-manager|grep Active&quot; done 确保状态为 active (running)，否则查看日志，确认原因： 1journalctl -u kube-controller-manager 查看当前的 leader123456789101112[root@node1 ~]# kubectl get endpoints kube-controller-manager --namespace=kube-system -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: &apos;&#123;&quot;holderIdentity&quot;:&quot;node1_3e3a8815-8698-11e9-87d5-005056b16e40&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2019-06-04T07:14:15Z&quot;,&quot;renewTime&quot;:&quot;2019-06-05T07:22:40Z&quot;,&quot;leaderTransitions&quot;:2&#125;&apos; creationTimestamp: &quot;2019-06-04T07:00:39Z&quot; name: kube-controller-manager namespace: kube-system resourceVersion: &quot;124731&quot; selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager uid: 75e30eec-8696-11e9-b371-005056b1d2de kube-scheduler 集群创建 kube-scheduler 证书和私钥创建证书签名请求： 123456789101112131415161718192021222324cd /opt/k8s/workcat &gt; kube-scheduler-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;system:kube-scheduler&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.6.101&quot;, &quot;192.168.6.102&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;system:kube-scheduler&quot;, &quot;OU&quot;: &quot;4Paradigm&quot; &#125; ]&#125;EOF hosts 列表包含所有 kube-scheduler 节点 IP； CN 和 O 均为 system:kube-scheduler，kubernetes 内置的 ClusterRoleBindings system:kube-scheduler 将赋予 kube-scheduler 工作所需的权限； 生成证书和私钥： 123456cd /opt/k8s/workcfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-schedulerls kube-scheduler*pem 将生成的证书和私钥分发到所有 master 节点： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kube-scheduler*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/ done 创建和分发 kubeconfig 文件kube-scheduler 使用 kubeconfig 文件访问 apiserver，该文件提供了 apiserver 地址、嵌入的 CA 证书和 kube-scheduler 证书： 1234567891011121314151617181920cd /opt/k8s/worksource /opt/k8s/bin/environment.shkubectl config set-cluster kubernetes \ --certificate-authority=/opt/k8s/work/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-scheduler.kubeconfigkubectl config set-credentials system:kube-scheduler \ --client-certificate=kube-scheduler.pem \ --client-key=kube-scheduler-key.pem \ --embed-certs=true \ --kubeconfig=kube-scheduler.kubeconfigkubectl config set-context system:kube-scheduler \ --cluster=kubernetes \ --user=system:kube-scheduler \ --kubeconfig=kube-scheduler.kubeconfigkubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig 分发 kubeconfig 到所有 master 节点： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kube-scheduler.kubeconfig root@$&#123;node_ip&#125;:/etc/kubernetes/ done 创建 kube-scheduler 配置文件1234567891011121314151617cd /opt/k8s/workcat &gt;kube-scheduler.yaml.template &lt;&lt;EOFapiVersion: kubescheduler.config.k8s.io/v1alpha1kind: KubeSchedulerConfigurationbindTimeoutSeconds: 600clientConnection: burst: 200 kubeconfig: &quot;/etc/kubernetes/kube-scheduler.kubeconfig&quot; qps: 100enableContentionProfiling: falseenableProfiling: truehardPodAffinitySymmetricWeight: 1healthzBindAddress: ##NODE_IP##:10251leaderElection: leaderElect: truemetricsBindAddress: ##NODE_IP##:10251EOF --kubeconfig：指定 kubeconfig 文件路径，kube-scheduler 使用它连接和验证 kube-apiserver； --leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态； 替换模板文件中的变量： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor (( i=0; i &lt; 3; i++ )) do sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-scheduler.yaml.template &gt; kube-scheduler-$&#123;NODE_IPS[i]&#125;.yaml donels kube-scheduler*.yaml NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP； 分发 kube-scheduler 配置文件到所有 master 节点： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kube-scheduler-$&#123;node_ip&#125;.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/kube-scheduler.yaml done 重命名为 kube-scheduler.yaml; 创建 kube-scheduler systemd unit 模板文件1234567891011121314151617181920212223242526272829303132cd /opt/k8s/workcat &gt; kube-scheduler.service.template &lt;&lt;EOF[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]WorkingDirectory=$&#123;K8S_DIR&#125;/kube-schedulerExecStart=/opt/k8s/bin/kube-scheduler \\ --config=/etc/kubernetes/kube-scheduler.yaml \\ --bind-address=##NODE_IP## \\ --secure-port=10259 \\ --port=0 \\ --tls-cert-file=/etc/kubernetes/cert/kube-scheduler.pem \\ --tls-private-key-file=/etc/kubernetes/cert/kube-scheduler-key.pem \\ --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\ --client-ca-file=/etc/kubernetes/cert/ca.pem \\ --requestheader-allowed-names=&quot;&quot; \\ --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\ --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-username-headers=X-Remote-User \\ --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\ --logtostderr=true \\ --v=2Restart=alwaysRestartSec=5StartLimitInterval=0[Install]WantedBy=multi-user.targetEOF 为各节点创建和分发 kube-scheduler systemd unit 文件替换模板文件中的变量，为各节点创建 systemd unit 文件： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor (( i=0; i &lt; 2; i++ )) do sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-scheduler.service.template &gt; kube-scheduler-$&#123;NODE_IPS[i]&#125;.service donels kube-scheduler*.service 分发 systemd unit 文件到所有 master 节点： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kube-scheduler-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-scheduler.service done 重命名为 kube-scheduler.service； 启动 kube-scheduler 服务1234567source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-scheduler&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl restart kube-scheduler&quot; done 启动服务前必须先创建工作目录； 检查服务运行状态123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-scheduler|grep Active&quot; done 确保状态为 active (running) 查看当前的 leader123456789101112[root@node1 ~]# kubectl get endpoints kube-scheduler --namespace=kube-system -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: &apos;&#123;&quot;holderIdentity&quot;:&quot;node1_b23eda23-8698-11e9-b281-005056b16e40&quot;,&quot;leaseDurationSeconds&quot;:15,&quot;acquireTime&quot;:&quot;2019-06-04T07:17:00Z&quot;,&quot;renewTime&quot;:&quot;2019-06-05T07:31:12Z&quot;,&quot;leaderTransitions&quot;:1&#125;&apos; creationTimestamp: &quot;2019-06-04T07:07:02Z&quot; name: kube-scheduler namespace: kube-system resourceVersion: &quot;125460&quot; selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler uid: 5a3888a1-8697-11e9-b371-005056b1d2de]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s1.14集群部署-apiserver集群]]></title>
    <url>%2Fposts%2F863q77b5.html</url>
    <content type="text"><![CDATA[nginx代理基于 nginx 代理的 kube-apiserver 高可用方案1234- 控制节点的 kube-controller-manager、kube-scheduler 是多实例部署，所以只要有一个实例正常，就可以保证高可用；- 集群内的 Pod 使用 K8S 服务域名 kubernetes 访问 kube-apiserver， kube-dns 会自动解析出多个 kube-apiserver 节点的 IP，所以也是高可用的；- 在每个节点起一个 nginx 进程，后端对接多个 apiserver 实例，nginx 对它们做健康检查和负载均衡；- kubelet、kube-proxy、controller-manager、scheduler 通过本地的 nginx（监听 127.0.0.1）访问 kube-apiserver，从而实现 kube-apiserver 的高可用； 下载和编译 nginx下载源码： 123cd /opt/k8s/workwget http://nginx.org/download/nginx-1.15.3.tar.gztar -xzvf nginx-1.15.3.tar.gz 配置编译参数： 123cd /opt/k8s/work/nginx-1.15.3mkdir nginx-prefix./configure --with-stream --without-http --prefix=$(pwd)/nginx-prefix --without-http_uwsgi_module --without-http_scgi_module --without-http_fastcgi_module --with-stream：开启 4 层透明转发(TCP Proxy)功能； --without-xxx：关闭所有其他功能，这样生成的动态链接二进制程序依赖最小； 编译和安装： 12cd /opt/k8s/work/nginx-1.15.3make &amp;&amp; make install 安装和部署 nginx创建目录结构： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; mkdir -p /opt/k8s/kube-nginx/&#123;conf,logs,sbin&#125; done 拷贝二进制程序： 123456789cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp /opt/k8s/work/nginx-1.15.3/nginx-prefix/sbin/nginx root@$&#123;node_ip&#125;:/opt/k8s/kube-nginx/sbin/kube-nginx ssh root@$&#123;node_ip&#125; &quot;chmod a+x /opt/k8s/kube-nginx/sbin/*&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p /opt/k8s/kube-nginx/&#123;conf,logs,sbin&#125;&quot; done 重命名二进制文件为 kube-nginx； 配置 nginx，开启 4 层透明转发功能： 12345678910111213141516171819202122cd /opt/k8s/workcat &gt; kube-nginx.conf &lt;&lt;EOFworker_processes 1;events &#123; worker_connections 1024;&#125;stream &#123; upstream backend &#123; hash $remote_addr consistent; server 192.168.6.101:6443 max_fails=3 fail_timeout=30s; server 192.168.6.102:6443 max_fails=3 fail_timeout=30s; &#125; server &#123; listen 127.0.0.1:8443; proxy_connect_timeout 1s; proxy_pass backend; &#125;&#125;EOF 分发配置文件： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kube-nginx.conf root@$&#123;node_ip&#125;:/opt/k8s/kube-nginx/conf/kube-nginx.conf done 配置 systemd unit 文件，启动服务配置 kube-nginx systemd unit 文件： 12345678910111213141516171819202122cd /opt/k8s/workcat &gt; kube-nginx.service &lt;&lt;EOF[Unit]Description=kube-apiserver nginx proxyAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=forkingExecStartPre=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -tExecStart=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginxExecReload=/opt/k8s/kube-nginx/sbin/kube-nginx -c /opt/k8s/kube-nginx/conf/kube-nginx.conf -p /opt/k8s/kube-nginx -s reloadPrivateTmp=trueRestart=alwaysRestartSec=5StartLimitInterval=0LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 分发 systemd unit 文件： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kube-nginx.service root@$&#123;node_ip&#125;:/etc/systemd/system/ done 启动 kube-nginx 服务： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-nginx &amp;&amp; systemctl restart kube-nginx&quot; done apiserver集群准备工作下载最新版本二进制文件12345cd /opt/k8s/workwget https://dl.k8s.io/v1.14.2/kubernetes-server-linux-amd64.tar.gztar -xzvf kubernetes-server-linux-amd64.tar.gzcd kubernetestar -xzvf kubernetes-src.tar.gz 将二进制文件拷贝到所有 master 节点： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kubernetes/server/bin/&#123;apiextensions-apiserver,cloud-controller-manager,kube-apiserver,kube-controller-manager,kube-proxy,kube-scheduler,kubeadm,kubectl,kubelet,mounter&#125; root@$&#123;node_ip&#125;:/opt/k8s/bin/ ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot; done 创建 kubernetes 证书和私钥创建证书签名请求： 12345678910111213141516171819202122232425262728293031cd /opt/k8s/worksource /opt/k8s/bin/environment.shcat &gt; kubernetes-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.6.101&quot;, &quot;192.168.6.102&quot;, &quot;$&#123;CLUSTER_KUBERNETES_SVC_IP&#125;&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local.&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;4Paradigm&quot; &#125; ]&#125;EOF hosts 字段指定授权使用该证书的 IP 和域名列表，这里列出了 master 节点 IP、kubernetes 服务的 IP 和域名； kubernetes 服务 IP 是 apiserver 自动创建的，一般是 --service-cluster-ip-range 参数指定的网段的第一个IP，后续可以通过下面命令获取： 123[root@node1 ~]# kubectl get svc kubernetesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 24h 生成证书和私钥： 12345cfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetesls kubernetes*pem 将生成的证书和私钥文件拷贝到所有 master 节点： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/kubernetes/cert&quot; scp kubernetes*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/ done 创建加密配置文件123456789101112131415cd /opt/k8s/worksource /opt/k8s/bin/environment.shcat &gt; encryption-config.yaml &lt;&lt;EOFkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: $&#123;ENCRYPTION_KEY&#125; - identity: &#123;&#125;EOF 将加密配置文件拷贝到 master 节点的 /etc/kubernetes 目录下： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp encryption-config.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/ done 创建审计策略文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191cd /opt/k8s/worksource /opt/k8s/bin/environment.shcat &gt; audit-policy.yaml &lt;&lt;EOFapiVersion: audit.k8s.io/v1beta1kind: Policyrules: # The following requests were manually identified as high-volume and low-risk, so drop them. - level: None resources: - group: &quot;&quot; resources: - endpoints - services - services/status users: - &apos;system:kube-proxy&apos; verbs: - watch - level: None resources: - group: &quot;&quot; resources: - nodes - nodes/status userGroups: - &apos;system:nodes&apos; verbs: - get - level: None namespaces: - kube-system resources: - group: &quot;&quot; resources: - endpoints users: - &apos;system:kube-controller-manager&apos; - &apos;system:kube-scheduler&apos; - &apos;system:serviceaccount:kube-system:endpoint-controller&apos; verbs: - get - update - level: None resources: - group: &quot;&quot; resources: - namespaces - namespaces/status - namespaces/finalize users: - &apos;system:apiserver&apos; verbs: - get # Don&apos;t log HPA fetching metrics. - level: None resources: - group: metrics.k8s.io users: - &apos;system:kube-controller-manager&apos; verbs: - get - list # Don&apos;t log these read-only URLs. - level: None nonResourceURLs: - &apos;/healthz*&apos; - /version - &apos;/swagger*&apos; # Don&apos;t log events requests. - level: None resources: - group: &quot;&quot; resources: - events # node and pod status calls from nodes are high-volume and can be large, don&apos;t log responses for expected updates from nodes - level: Request omitStages: - RequestReceived resources: - group: &quot;&quot; resources: - nodes/status - pods/status users: - kubelet - &apos;system:node-problem-detector&apos; - &apos;system:serviceaccount:kube-system:node-problem-detector&apos; verbs: - update - patch - level: Request omitStages: - RequestReceived resources: - group: &quot;&quot; resources: - nodes/status - pods/status userGroups: - &apos;system:nodes&apos; verbs: - update - patch # deletecollection calls can be large, don&apos;t log responses for expected namespace deletions - level: Request omitStages: - RequestReceived users: - &apos;system:serviceaccount:kube-system:namespace-controller&apos; verbs: - deletecollection # Secrets, ConfigMaps, and TokenReviews can contain sensitive &amp; binary data, # so only log at the Metadata level. - level: Metadata omitStages: - RequestReceived resources: - group: &quot;&quot; resources: - secrets - configmaps - group: authentication.k8s.io resources: - tokenreviews # Get repsonses can be large; skip them. - level: Request omitStages: - RequestReceived resources: - group: &quot;&quot; - group: admissionregistration.k8s.io - group: apiextensions.k8s.io - group: apiregistration.k8s.io - group: apps - group: authentication.k8s.io - group: authorization.k8s.io - group: autoscaling - group: batch - group: certificates.k8s.io - group: extensions - group: metrics.k8s.io - group: networking.k8s.io - group: policy - group: rbac.authorization.k8s.io - group: scheduling.k8s.io - group: settings.k8s.io - group: storage.k8s.io verbs: - get - list - watch # Default level for known APIs - level: RequestResponse omitStages: - RequestReceived resources: - group: &quot;&quot; - group: admissionregistration.k8s.io - group: apiextensions.k8s.io - group: apiregistration.k8s.io - group: apps - group: authentication.k8s.io - group: authorization.k8s.io - group: autoscaling - group: batch - group: certificates.k8s.io - group: extensions - group: metrics.k8s.io - group: networking.k8s.io - group: policy - group: rbac.authorization.k8s.io - group: scheduling.k8s.io - group: settings.k8s.io - group: storage.k8s.io # Default level for all other requests. - level: Metadata omitStages: - RequestReceivedEOF 分发审计策略文件： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp audit-policy.yaml root@$&#123;node_ip&#125;:/etc/kubernetes/audit-policy.yaml done 创建后续访问 metrics-server 使用的证书创建证书签名请求: 12345678910111213141516171819cat &gt; proxy-client-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;aggregator&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;4Paradigm&quot; &#125; ]&#125;EOF CN 名称为 aggregator，需要与 metrics-server 的 --requestheader-allowed-names 参数配置一致，否则访问会被 metrics-server 拒绝； 生成证书和私钥： 12345cfssl gencert -ca=/etc/kubernetes/cert/ca.pem \ -ca-key=/etc/kubernetes/cert/ca-key.pem \ -config=/etc/kubernetes/cert/ca-config.json \ -profile=kubernetes proxy-client-csr.json | cfssljson -bare proxy-clientls proxy-client*.pem 将生成的证书和私钥文件拷贝到所有 master 节点： 123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp proxy-client*.pem root@$&#123;node_ip&#125;:/etc/kubernetes/cert/ done 创建 kube-apiserver systemd unit 模板文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374cd /opt/k8s/worksource /opt/k8s/bin/environment.shcat &gt; kube-apiserver.service.template &lt;&lt;EOF[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=$&#123;K8S_DIR&#125;/kube-apiserverExecStart=/opt/k8s/bin/kube-apiserver \\ --advertise-address=##NODE_IP## \\ --default-not-ready-toleration-seconds=360 \\ --default-unreachable-toleration-seconds=360 \\ --feature-gates=DynamicAuditing=true \\ --max-mutating-requests-inflight=2000 \\ --max-requests-inflight=4000 \\ --default-watch-cache-size=200 \\ --delete-collection-workers=2 \\ --encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\ --etcd-cafile=/etc/kubernetes/cert/ca.pem \\ --etcd-certfile=/etc/kubernetes/cert/kubernetes.pem \\ --etcd-keyfile=/etc/kubernetes/cert/kubernetes-key.pem \\ --etcd-servers=$&#123;ETCD_ENDPOINTS&#125; \\ --bind-address=##NODE_IP## \\ --secure-port=6443 \\ --tls-cert-file=/etc/kubernetes/cert/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/cert/kubernetes-key.pem \\ --insecure-port=0 \\ --audit-dynamic-configuration \\ --audit-log-maxage=15 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-mode=batch \\ --audit-log-truncate-enabled \\ --audit-log-batch-buffer-size=20000 \\ --audit-log-batch-max-size=2 \\ --audit-log-path=$&#123;K8S_DIR&#125;/kube-apiserver/audit.log \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --profiling \\ --anonymous-auth=false \\ --client-ca-file=/etc/kubernetes/cert/ca.pem \\ --enable-bootstrap-token-auth \\ --requestheader-allowed-names=&quot;&quot; \\ --requestheader-client-ca-file=/etc/kubernetes/cert/ca.pem \\ --requestheader-extra-headers-prefix=&quot;X-Remote-Extra-&quot; \\ --requestheader-group-headers=X-Remote-Group \\ --requestheader-username-headers=X-Remote-User \\ --service-account-key-file=/etc/kubernetes/cert/ca.pem \\ --authorization-mode=Node,RBAC \\ --runtime-config=api/all=true \\ --enable-admission-plugins=NodeRestriction \\ --allow-privileged=true \\ --apiserver-count=3 \\ --event-ttl=168h \\ --kubelet-certificate-authority=/etc/kubernetes/cert/ca.pem \\ --kubelet-client-certificate=/etc/kubernetes/cert/kubernetes.pem \\ --kubelet-client-key=/etc/kubernetes/cert/kubernetes-key.pem \\ --kubelet-https=true \\ --kubelet-timeout=10s \\ --proxy-client-cert-file=/etc/kubernetes/cert/proxy-client.pem \\ --proxy-client-key-file=/etc/kubernetes/cert/proxy-client-key.pem \\ --service-cluster-ip-range=$&#123;SERVICE_CIDR&#125; \\ --service-node-port-range=$&#123;NODE_PORT_RANGE&#125; \\ --logtostderr=true \\ --v=2Restart=on-failureRestartSec=10Type=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF --advertise-address：apiserver 对外通告的 IP（kubernetes 服务后端节点 IP）； --default-*-toleration-seconds：设置节点异常相关的阈值； --max-*-requests-inflight：请求相关的最大阈值； --etcd-*：访问 etcd 的证书和 etcd 服务器地址； --experimental-encryption-provider-config：指定用于加密 etcd 中 secret 的配置； --bind-address： https 监听的 IP，不能为 127.0.0.1，否则外界不能访问它的安全端口 6443； --secret-port：https 监听端口； --insecure-port=0：关闭监听 http 非安全端口(8080)； --tls-*-file：指定 apiserver 使用的证书、私钥和 CA 文件； --audit-*：配置审计策略和审计日志文件相关的参数； --client-ca-file：验证 client (kue-controller-manager、kube-scheduler、kubelet、kube-proxy 等)请求所带的证书； --enable-bootstrap-token-auth：启用 kubelet bootstrap 的 token 认证； --requestheader-*：kube-apiserver 的 aggregator layer 相关的配置参数，proxy-client &amp; HPA 需要使用； --requestheader-client-ca-file：用于签名 --proxy-client-cert-file 和 --proxy-client-key-file 指定的证书；在启用了 metric aggregator 时使用； 如果 --requestheader-allowed-names 不为空，则--proxy-client-cert-file 证书的 CN 必须位于 allowed-names 中，默认为 aggregator; --service-account-key-file：签名 ServiceAccount Token 的公钥文件，kube-controller-manager 的 --service-account-private-key-file 指定私钥文件，两者配对使用； --runtime-config=api/all=true： 启用所有版本的 APIs，如 autoscaling/v2alpha1； --authorization-mode=Node,RBAC、--anonymous-auth=false： 开启 Node 和 RBAC 授权模式，拒绝未授权的请求； --enable-admission-plugins：启用一些默认关闭的 plugins； --allow-privileged：运行执行 privileged 权限的容器； --apiserver-count=3：指定 apiserver 实例的数量； --event-ttl：指定 events 的保存时间； --kubelet-*：如果指定，则使用 https 访问 kubelet APIs；需要为证书对应的用户(上面 kubernetes*.pem 证书的用户为 kubernetes) 用户定义 RBAC 规则，否则访问 kubelet API 时提示未授权； --proxy-client-*：apiserver 访问 metrics-server 使用的证书； --service-cluster-ip-range： 指定 Service Cluster IP 地址段； --service-node-port-range： 指定 NodePort 的端口范围； 如果 kube-apiserver 机器没有运行 kube-proxy，则还需要添加 --enable-aggregator-routing=true 参数； 关于 --requestheader-XXX 相关参数，参考： https://github.com/kubernetes-incubator/apiserver-builder/blob/master/docs/concepts/auth.md https://docs.bitnami.com/kubernetes/how-to/configure-autoscaling-custom-metrics/ 注意：requestheader-client-ca-file 指定的 CA 证书，必须具有 client auth and server auth； 为各节点创建和分发 kube-apiserver systemd unit 文件替换模板文件中的变量，为各节点生成 systemd unit 文件： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor (( i=0; i &lt; 2; i++ )) do sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; kube-apiserver.service.template &gt; kube-apiserver-$&#123;NODE_IPS[i]&#125;.service donels kube-apiserver*.service NODE_NAMES 和 NODE_IPS 为相同长度的 bash 数组，分别为节点名称和对应的 IP； 分发生成的 systemd unit 文件： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kube-apiserver-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/kube-apiserver.service done 文件重命名为 kube-apiserver.service; 启动 kube-apiserver 服务1234567source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;K8S_DIR&#125;/kube-apiserver&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl restart kube-apiserver&quot; done 启动服务前必须先创建工作目录； 检查 kube-apiserver 运行状态123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl status kube-apiserver |grep &apos;Active:&apos;&quot; done 确保状态为 active (running) 打印 kube-apiserver 写入 etcd 的数据1234567source /opt/k8s/bin/environment.shETCDCTL_API=3 etcdctl \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \ --cacert=/opt/k8s/work/ca.pem \ --cert=/opt/k8s/work/etcd.pem \ --key=/opt/k8s/work/etcd-key.pem \ get /registry/ --prefix --keys-only 授予 kube-apiserver 访问 kubelet API 的权限在执行 kubectl exec、run、logs 等命令时，apiserver 会将请求转发到 kubelet 的 https 端口。这里定义 RBAC 规则，授权 apiserver 使用的证书（kubernetes.pem）用户名（CN：kuberntes）访问 kubelet API 的权限： 1kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s1.14集群部署-flannel网络、kubectl工具]]></title>
    <url>%2Fposts%2F66ae7fg23.html</url>
    <content type="text"><![CDATA[flannel网络介绍Flannel是CoreOS团队针对Kubernetes设计的一个网络规划服务，简单来说，它的功能是让集群中的不同节点主机创建的Docker容器都具有全集群唯一的虚拟IP地址。 flannel在k8s工作kubernetes 要求集群内各节点(包括 master 节点)能通过 Pod 网段互联互通。flannel 使用 vxlan 技术为各节点创建一个可以互通的 Pod 网络，使用的端口为 UDP 8472（需要开放该端口，如公有云 AWS 等）。 flanneld 第一次启动时，从 etcd 获取配置的 Pod 网段信息，为本节点分配一个未使用的地址段，然后创建 flannedl.1 网络接口（也可能是其它名称，如 flannel1 等）。 flannel 将分配给自己的 Pod 网段信息写入 /run/flannel/docker 文件，docker 后续使用这个文件中的环境变量设置 docker0 网桥，从而从这个地址段为本节点的所有 Pod 容器分配 IP。 部署下载和分发 flanneld 二进制文件从 flannel 的 release 页面 下载最新版本的安装包： 1234cd /opt/k8s/workmkdir flannelwget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gztar -xzvf flannel-v0.11.0-linux-amd64.tar.gz -C flannel 分发二进制文件到集群所有节点： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp flannel/&#123;flanneld,mk-docker-opts.sh&#125; root@$&#123;node_ip&#125;:/opt/k8s/bin/ ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot; done 创建 flannel 证书和私钥flanneld 从 etcd 集群存取网段分配信息，而 etcd 集群启用了双向 x509 证书认证，所以需要为 flanneld 生成证书和私钥。 创建证书签名请求： 1234567891011121314151617181920cd /opt/k8s/workcat &gt; flanneld-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;flanneld&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;4Paradigm&quot; &#125; ]&#125;EOF 该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空； 生成证书和私钥： 12345cfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneldls flanneld*pem 将生成的证书和私钥分发到所有节点（master 和 worker）： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/flanneld/cert&quot; scp flanneld*.pem root@$&#123;node_ip&#125;:/etc/flanneld/cert done 在etcd 写入集群 Pod 网段信息注意：本步骤只需执行一次。 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shetcdctl \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \ --ca-file=/opt/k8s/work/ca.pem \ --cert-file=/opt/k8s/work/flanneld.pem \ --key-file=/opt/k8s/work/flanneld-key.pem \ mk $&#123;FLANNEL_ETCD_PREFIX&#125;/config &apos;&#123;&quot;Network&quot;:&quot;&apos;$&#123;CLUSTER_CIDR&#125;&apos;&quot;, &quot;SubnetLen&quot;: 21, &quot;Backend&quot;: &#123;&quot;Type&quot;: &quot;vxlan&quot;&#125;&#125;&apos; flanneld 当前版本 (v0.11.0) 不支持 etcd v3，故使用 etcd v2 API 写入配置 key 和网段数据； 写入的 Pod 网段 ${CLUSTER_CIDR} 地址段（如 /16）必须小于 SubnetLen，必须与 kube-controller-manager 的 --cluster-cidr 参数值一致； 创建 flanneld 的 systemd123456789101112131415161718192021222324252627282930cd /opt/k8s/worksource /opt/k8s/bin/environment.shcat &gt; flanneld.service &lt;&lt; EOF[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyExecStart=/opt/k8s/bin/flanneld \\ -etcd-cafile=/etc/kubernetes/cert/ca.pem \\ -etcd-certfile=/etc/flanneld/cert/flanneld.pem \\ -etcd-keyfile=/etc/flanneld/cert/flanneld-key.pem \\ -etcd-endpoints=$&#123;ETCD_ENDPOINTS&#125; \\ -etcd-prefix=$&#123;FLANNEL_ETCD_PREFIX&#125; \\ -iface=$&#123;IFACE&#125; \\ -ip-masqExecStartPost=/opt/k8s/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/dockerRestart=alwaysRestartSec=5StartLimitInterval=0[Install]WantedBy=multi-user.targetRequiredBy=docker.serviceEOF mk-docker-opts.sh 脚本将分配给 flanneld 的 Pod 子网段信息写入 /run/flannel/docker 文件，后续 docker 启动时使用这个文件中的环境变量配置 docker0 网桥； flanneld 使用系统缺省路由所在的接口与其它节点通信，对于有多个网络接口（如内网和公网）的节点，可以用 -iface 参数指定通信接口; flanneld 运行时需要 root 权限； -ip-masq: flanneld 为访问 Pod 网络外的流量设置 SNAT 规则，同时将传递给 Docker 的变量 --ip-masq（/run/flannel/docker 文件中）设置为 false，这样 Docker 将不再创建 SNAT 规则； Docker 的 --ip-masq 为 true 时，创建的 SNAT 规则比较“暴力”：将所有本节点 Pod 发起的、访问非 docker0 接口的请求做 SNAT，这样访问其他节点 Pod 的请求来源 IP 会被设置为 flannel.1 接口的 IP，导致目的 Pod 看不到真实的来源 Pod IP。 flanneld 创建的 SNAT 规则比较温和，只对访问非 Pod 网段的请求做 SNAT。 分发 flanneld systemd unit1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp flanneld.service root@$&#123;node_ip&#125;:/etc/systemd/system/ done 启动 flanneld 服务123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable flanneld &amp;&amp; systemctl restart flanneld&quot; done 检查启动结果123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl status flanneld|grep Active&quot; done 确保状态为 active (running) 检查分配给各 flanneld 的 Pod 网段信息查看集群 Pod 网段(/16)： 1234567source /opt/k8s/bin/environment.shetcdctl \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \ --ca-file=/etc/kubernetes/cert/ca.pem \ --cert-file=/etc/flanneld/cert/flanneld.pem \ --key-file=/etc/flanneld/cert/flanneld-key.pem \ get $&#123;FLANNEL_ETCD_PREFIX&#125;/config 输出： {&quot;Network&quot;:&quot;172.30.0.0/16&quot;, &quot;SubnetLen&quot;: 21, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;}} 查看已分配的 Pod 子网段列表(/24): 1234567source /opt/k8s/bin/environment.shetcdctl \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \ --ca-file=/etc/kubernetes/cert/ca.pem \ --cert-file=/etc/flanneld/cert/flanneld.pem \ --key-file=/etc/flanneld/cert/flanneld-key.pem \ ls $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets 输出（结果视部署情况而定）： 12345678[root@node1 ~]# etcdctl \&gt; --endpoints=$&#123;ETCD_ENDPOINTS&#125; \&gt; --ca-file=/etc/kubernetes/cert/ca.pem \&gt; --cert-file=/etc/flanneld/cert/flanneld.pem \&gt; --key-file=/etc/flanneld/cert/flanneld-key.pem \&gt; ls $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets/kubernetes/network/subnets/172.30.168.0-21/kubernetes/network/subnets/172.30.48.0-21 查看某一 Pod 网段对应的节点 IP 和 flannel 接口地址: 1234567source /opt/k8s/bin/environment.shetcdctl \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; \ --ca-file=/etc/kubernetes/cert/ca.pem \ --cert-file=/etc/flanneld/cert/flanneld.pem \ --key-file=/etc/flanneld/cert/flanneld-key.pem \ get $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets/172.30.168.0-21 输出： 12345678[root@node1 ~]# etcdctl \&gt; --endpoints=$&#123;ETCD_ENDPOINTS&#125; \&gt; --ca-file=/etc/kubernetes/cert/ca.pem \&gt; --cert-file=/etc/flanneld/cert/flanneld.pem \&gt; --key-file=/etc/flanneld/cert/flanneld-key.pem \&gt; get $&#123;FLANNEL_ETCD_PREFIX&#125;/subnets/172.30.168.0-21&#123;&quot;PublicIP&quot;:&quot;192.168.6.101&quot;,&quot;BackendType&quot;:&quot;vxlan&quot;,&quot;BackendData&quot;:&#123;&quot;VtepMAC&quot;:&quot;62:58:f9:a2:16:73&quot;&#125;&#125;[root@node1 ~]# 验证各节点能通过 Pod 网段互通在各节点上部署 flannel 后，检查是否创建了 flannel 接口(名称可能为 flannel0、flannel.0、flannel.1 等)： 123456source /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh $&#123;node_ip&#125; &quot;/usr/sbin/ip addr show flannel.1|grep -w inet&quot; done 输出： 123456789[root@node1 ~]# for node_ip in $&#123;NODE_IPS[@]&#125;&gt; do&gt; echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;&gt; ssh $&#123;node_ip&#125; &quot;/usr/sbin/ip addr show flannel.1|grep -w inet&quot;&gt; done&gt;&gt;&gt; 192.168.6.101 inet 172.30.168.0/32 scope global flannel.1&gt;&gt;&gt; 192.168.6.102 inet 172.30.48.0/32 scope global flannel.1 在各节点上 ping 所有 flannel 接口 IP，确保能通 kubectl下载和分发 kubectl 二进制文件下载和解压： 123cd /opt/k8s/workwget https://dl.k8s.io/v1.14.2/kubernetes-client-linux-amd64.tar.gztar -xzvf kubernetes-client-linux-amd64.tar.gz 分发到所有使用 kubectl 的节点： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp kubernetes/client/bin/kubectl root@$&#123;node_ip&#125;:/opt/k8s/bin/ ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot; done 创建 admin 证书和私钥kubectl 与 apiserver https 安全端口通信，apiserver 对提供的证书进行认证和授权。 kubectl 作为集群的管理工具，需要被授予最高权限，这里创建具有最高权限的 admin 证书。 创建证书签名请求： 1234567891011121314151617181920cd /opt/k8s/workcat &gt; admin-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;admin&quot;, &quot;hosts&quot;: [], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;4Paradigm&quot; &#125; ]&#125;EOF O 为 system:masters，kube-apiserver 收到该证书后将请求的 Group 设置为 system:masters； 预定义的 ClusterRoleBinding cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予所有 API的权限； 该证书只会被 kubectl 当做 client 证书使用，所以 hosts 字段为空； 生成证书和私钥： 12345cd /opt/k8s/workcfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes admin-csr.json | cfssljson -bare admin 创建 kubeconfig 文件kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书； 12345678910111213141516171819202122232425cd /opt/k8s/worksource /opt/k8s/bin/environment.sh# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/opt/k8s/work/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kubectl.kubeconfig# 设置客户端认证参数kubectl config set-credentials admin \ --client-certificate=/opt/k8s/work/admin.pem \ --client-key=/opt/k8s/work/admin-key.pem \ --embed-certs=true \ --kubeconfig=kubectl.kubeconfig# 设置上下文参数kubectl config set-context kubernetes \ --cluster=kubernetes \ --user=admin \ --kubeconfig=kubectl.kubeconfig# 设置默认上下文kubectl config use-context kubernetes --kubeconfig=kubectl.kubeconfig --certificate-authority：验证 kube-apiserver 证书的根证书； --client-certificate、--client-key：刚生成的 admin 证书和私钥，连接 kube-apiserver 时使用； --embed-certs=true：将 ca.pem 和 admin.pem 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(不加时，写入的是证书文件路径，后续拷贝 kubeconfig 到其它机器时，还需要单独拷贝证书文件，不方便。)； 分发 kubeconfig 文件分发到所有使用 kubectl 命令的节点： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p ~/.kube&quot; scp kubectl.kubeconfig root@$&#123;node_ip&#125;:~/.kube/config done 保存的文件名为 ~/.kube/config；]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s1.14集群部署-etcd集群]]></title>
    <url>%2Fposts%2F7dqa4nb2.html</url>
    <content type="text"><![CDATA[简介ETCD 是一个高可用的分布式键值数据库，可用于服务发现。ETCD 采用 raft 一致性算法，基于 Go 语言实现。 1234567特点简单：安装配置使用简单，提供 HTTP API 安全：支持 SSL 证书 可靠：采用 raft 算法，实现分布式系统数据的可用性和一致性 kubernetes 使用 etcd 存储所有运行数据 下载和分发 etcd 二进制文件123cd /opt/k8s/workwget https://github.com/coreos/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gztar -xvf etcd-v3.3.13-linux-amd64.tar.gz 分发二进制文件到集群所有节点： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp etcd-v3.3.13-linux-amd64/etcd* root@$&#123;node_ip&#125;:/opt/k8s/bin ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot; done 创建 etcd 证书和私钥创建证书签名请求： 123456789101112131415161718192021222324cd /opt/k8s/workcat &gt; etcd-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;192.168.6.101&quot;, &quot;192.168.6.102&quot; ], &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;4Paradigm&quot; &#125; ]&#125;EOF 生成证书和私钥： 123456cd /opt/k8s/workcfssl gencert -ca=/opt/k8s/work/ca.pem \ -ca-key=/opt/k8s/work/ca-key.pem \ -config=/opt/k8s/work/ca-config.json \ -profile=kubernetes etcd-csr.json | cfssljson -bare etcdls etcd*pem 分发生成的证书和私钥到各 etcd 节点： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/etcd/cert&quot; scp etcd*.pem root@$&#123;node_ip&#125;:/etc/etcd/cert/ done 创建 etcd 的 systemd123456789101112131415161718192021222324252627282930313233343536373839404142434445cd /opt/k8s/worksource /opt/k8s/bin/environment.shcat &gt; etcd.service.template &lt;&lt;EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=$&#123;ETCD_DATA_DIR&#125;ExecStart=/opt/k8s/bin/etcd \\ --data-dir=$&#123;ETCD_DATA_DIR&#125; \\ --wal-dir=$&#123;ETCD_WAL_DIR&#125; \\ --name=##NODE_NAME## \\ --cert-file=/etc/etcd/cert/etcd.pem \\ --key-file=/etc/etcd/cert/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/cert/ca.pem \\ --peer-cert-file=/etc/etcd/cert/etcd.pem \\ --peer-key-file=/etc/etcd/cert/etcd-key.pem \\ --peer-trusted-ca-file=/etc/kubernetes/cert/ca.pem \\ --peer-client-cert-auth \\ --client-cert-auth \\ --listen-peer-urls=https://##NODE_IP##:2380 \\ --initial-advertise-peer-urls=https://##NODE_IP##:2380 \\ --listen-client-urls=https://##NODE_IP##:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://##NODE_IP##:2379 \\ --initial-cluster-token=etcd-cluster-0 \\ --initial-cluster=$&#123;ETCD_NODES&#125; \\ --initial-cluster-state=new \\ --auto-compaction-mode=periodic \\ --auto-compaction-retention=1 \\ --max-request-bytes=33554432 \\ --quota-backend-bytes=6442450944 \\ --heartbeat-interval=250 \\ --election-timeout=2000Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF WorkingDirectory、--data-dir：指定工作目录和数据目录为 ${ETCD_DATA_DIR}，需在启动服务前创建这个目录； --wal-dir：指定 wal 目录，为了提高性能，一般使用 SSD 或者和 --data-dir 不同的磁盘； --name：指定节点名称，当 --initial-cluster-state 值为 new 时，--name 的参数值必须位于 --initial-cluster 列表中； --cert-file、--key-file：etcd server 与 client 通信时使用的证书和私钥； --trusted-ca-file：签名 client 证书的 CA 证书，用于验证 client 证书； --peer-cert-file、--peer-key-file：etcd 与 peer 通信使用的证书和私钥； --peer-trusted-ca-file：签名 peer 证书的 CA 证书，用于验证 peer 证书； 替换模板文件中的变量，为各节点创建 systemd unit 文件： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor (( i=0; i &lt; 2; i++ )) do sed -e &quot;s/##NODE_NAME##/$&#123;NODE_NAMES[i]&#125;/&quot; -e &quot;s/##NODE_IP##/$&#123;NODE_IPS[i]&#125;/&quot; etcd.service.template &gt; etcd-$&#123;NODE_IPS[i]&#125;.service donels *.service 分发生成的 systemd unit 文件： 1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp etcd-$&#123;node_ip&#125;.service root@$&#123;node_ip&#125;:/etc/systemd/system/etcd.service done 启动 etcd 服务12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p $&#123;ETCD_DATA_DIR&#125; $&#123;ETCD_WAL_DIR&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl daemon-reload &amp;&amp; systemctl enable etcd &amp;&amp; systemctl restart etcd &quot; &amp; done 检查结果1234567cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;systemctl status etcd|grep Active&quot; done 确保状态为 active (running)，否则查看日志，确认原因： 1journalctl -u etcd 验证服务状态1234567891011cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ETCDCTL_API=3 /opt/k8s/bin/etcdctl \ --endpoints=https://$&#123;node_ip&#125;:2379 \ --cacert=/etc/kubernetes/cert/ca.pem \ --cert=/etc/etcd/cert/etcd.pem \ --key=/etc/etcd/cert/etcd-key.pem endpoint health done 结果显示12345678910111213[root@node1 ~]# for node_ip in $&#123;NODE_IPS[@]&#125;&gt; do&gt; echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot;&gt; ETCDCTL_API=3 /opt/k8s/bin/etcdctl \&gt; --endpoints=https://$&#123;node_ip&#125;:2379 \&gt; --cacert=/etc/kubernetes/cert/ca.pem \&gt; --cert=/etc/etcd/cert/etcd.pem \&gt; --key=/etc/etcd/cert/etcd-key.pem endpoint health&gt; done&gt;&gt;&gt; 192.168.6.101https://192.168.6.101:2379 is healthy: successfully committed proposal: took = 2.45561ms&gt;&gt;&gt; 192.168.6.102https://192.168.6.102:2379 is healthy: successfully committed proposal: took = 3.898134ms 查看当前的 leader123456source /opt/k8s/bin/environment.shETCDCTL_API=3 /opt/k8s/bin/etcdctl \ -w table --cacert=/etc/kubernetes/cert/ca.pem \ --cert=/etc/etcd/cert/etcd.pem \ --key=/etc/etcd/cert/etcd-key.pem \ --endpoints=$&#123;ETCD_ENDPOINTS&#125; endpoint status 结果为： 1234567891011[root@node1 ~]# ETCDCTL_API=3 /opt/k8s/bin/etcdctl \&gt; -w table --cacert=/etc/kubernetes/cert/ca.pem \&gt; --cert=/etc/etcd/cert/etcd.pem \&gt; --key=/etc/etcd/cert/etcd-key.pem \&gt; --endpoints=$&#123;ETCD_ENDPOINTS&#125; endpoint status+----------------------------+------------------+---------+---------+-----------+-----------+------------+| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |+----------------------------+------------------+---------+---------+-----------+-----------+------------+| https://192.168.6.101:2379 | 77fff77a6e7d24c5 | 3.3.13 | 864 kB | true | 8 | 41721 || https://192.168.6.102:2379 | e82e7402173c61e | 3.3.13 | 872 kB | false | 8 | 41721 |+----------------------------+------------------+---------+---------+-----------+-----------+------------+ 可以看到6.101为leader]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s1.14集群部署-cfssl证书]]></title>
    <url>%2Fposts%2F8d664qf5.html</url>
    <content type="text"><![CDATA[k8s证书的三种方式 cfssl easyrsa openssl 本文使用cfssl签发证书 安装 cfssl 工具集123456789101112mkdir -p /opt/k8s/cert &amp;&amp; cd /opt/k8swget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64mv cfssl_linux-amd64 /opt/k8s/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64mv cfssljson_linux-amd64 /opt/k8s/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /opt/k8s/bin/cfssl-certinfochmod +x /opt/k8s/bin/*export PATH=/opt/k8s/bin:$PATH 创建根证书 (CA)CA 证书是集群所有节点共享的，只需要创建一个 CA 证书，后续创建的所有证书都由它签名。 创建配置文件CA 配置文件用于配置根证书的使用场景 (profile) 和具体参数 (usage，过期时间、服务端认证、客户端认证、加密等)，后续在签名其它证书时需要指定特定场景。 123456789101112131415161718192021cd /opt/k8s/workcat &gt; ca-config.json &lt;&lt;EOF&#123; &quot;signing&quot;: &#123; &quot;default&quot;: &#123; &quot;expiry&quot;: &quot;87600h&quot; &#125;, &quot;profiles&quot;: &#123; &quot;kubernetes&quot;: &#123; &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; &#125; &#125; &#125;&#125;EOF signing：表示该证书可用于签名其它证书，生成的 ca.pem 证书中 CA=TRUE； server auth：表示 client 可以用该该证书对 server 提供的证书进行验证； client auth：表示 server 可以用该该证书对 client 提供的证书进行验证； 创建证书签名请求文件12345678910111213141516171819cd /opt/k8s/workcat &gt; ca-csr.json &lt;&lt;EOF&#123; &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: &#123; &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 &#125;, &quot;names&quot;: [ &#123; &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;BeiJing&quot;, &quot;L&quot;: &quot;BeiJing&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;4Paradigm&quot; &#125; ]&#125;EOF 生成 CA 证书和私钥12cd /opt/k8s/workcfssl gencert -initca ca-csr.json | cfssljson -bare ca 分发证书文件将生成的 CA 证书、秘钥文件、配置文件拷贝到所有节点的 /etc/kubernetes/cert 目录下： 12345678cd /opt/k8s/worksource /opt/k8s/bin/environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; ssh root@$&#123;node_ip&#125; &quot;mkdir -p /etc/kubernetes/cert&quot; scp ca*.pem ca-config.json root@$&#123;node_ip&#125;:/etc/kubernetes/cert done]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s1.14集群部署-系统初始化]]></title>
    <url>%2Fposts%2F9c775ab5.html</url>
    <content type="text"><![CDATA[K8s环境准备本次安装版本 Kubernetes 1.14.2 Docker 18.09.6-ce Etcd 3.3.13 Flanneld 0.11.0 机器123192.168.6.101 node1192.168.6.102 node2 其中node1，node2做master集群，也都是node节点 主机名123456hostnamectl set-hostname node1hostnamectl set-hostname node2cat &gt;&gt; /etc/hosts &lt;&lt;EOF192.168.6.101 node1192.168.6.102 node2EOF 免秘钥1234注意：在node1上操作即可ssh-keygen -t rsassh-copy-id root@node1ssh-copy-id root@node2 初始化安装依赖包以下操作均在所有机器操作 1yum install -y epel-release conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget unzip net-tools 关闭防火墙1234systemctl stop firewalldsystemctl disable firewalldiptables -F &amp;&amp; iptables -X &amp;&amp; iptables -F -t nat &amp;&amp; iptables -X -t natiptables -P FORWARD ACCEPT 关闭 swap 分区12swapoff -ased -i &apos;/ swap / s/^\(.*\)$/#\1/g&apos; /etc/fstab 关闭 SELinux12setenforce 0sed -i &apos;s/^SELINUX=.*/SELINUX=disabled/&apos; /etc/selinux/config 加载内核并优化12345678910111213141516171819modprobe ip_vs_rrmodprobe br_netfiltercat &gt; kubernetes.conf &lt;&lt;EOFnet.bridge.bridge-nf-call-iptables=1net.bridge.bridge-nf-call-ip6tables=1net.ipv4.ip_forward=1net.ipv4.tcp_tw_recycle=0vm.swappiness=0 # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它vm.overcommit_memory=1 # 不检查物理内存是否够用vm.panic_on_oom=0 # 开启 OOMfs.inotify.max_user_instances=8192fs.inotify.max_user_watches=1048576fs.file-max=52706963fs.nr_open=52706963net.ipv6.conf.all.disable_ipv6=1net.netfilter.nf_conntrack_max=2310720EOFcp kubernetes.conf /etc/sysctl.d/kubernetes.confsysctl -p /etc/sysctl.d/kubernetes.conf ntp1ntpdate ntp1.aliyun.com 创建相关目录1mkdir -p /opt/k8s/&#123;bin,work&#125; /etc/&#123;kubernetes,etcd&#125;/cert 升级内核CentOS 7.x 系统自带的 3.10.x 内核存在一些 Bugs，导致运行的 Docker、Kubernetes 不稳定，例如： 高版本的 docker(1.13 以后) 启用了 3.10 kernel 实验支持的 kernel memory account 功能(无法关闭)，当节点压力大如频繁启动和停止容器时会导致 cgroup memory leak； 网络设备引用计数泄漏，会导致类似于报错：”kernel:unregister_netdevice: waiting for eth0 to become free. Usage count = 1”; 解决方案如下： 升级内核到 4.4.X 以上； 或者，手动编译内核，disable CONFIG_MEMCG_KMEM 特性； 或者，安装修复了该问题的 Docker 18.09.1 及以上的版本。但由于 kubelet 也会设置 kmem（它 vendor 了 runc），所以需要重新编译 kubelet 并指定 GOFLAGS=”-tags=nokmem”； 123git clone --branch v1.14.1 --single-branch --depth 1 https://github.com/kubernetes/kubernetescd kubernetesKUBE_GIT_VERSION=v1.14.1 ./build/run.sh make kubelet GOFLAGS=&quot;-tags=nokmem&quot; 这里采用升级内核的解决办法： 12345rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm# 安装完成后检查 /boot/grub2/grub.cfg 中对应内核 menuentry 中是否包含 initrd16 配置，如果没有，再安装一次！yum --enablerepo=elrepo-kernel install -y kernel-lt# 设置开机从新内核启动grub2-set-default 0 安装内核源文件（可选，在升级完内核并重启机器后执行）: 12# yum erase kernel-headersyum --enablerepo=elrepo-kernel install kernel-lt-devel-$(uname -r) kernel-lt-headers-$(uname -r) 设置配置参数脚本脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[root@node1 ~]# cat environment.sh#!/usr/bin/bash# 生成 EncryptionConfig 所需的加密 keyexport ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)# 集群各机器 IP 数组export NODE_IPS=(192.168.6.101 192.168.6.102)# 集群各 IP 对应的主机名数组export NODE_NAMES=(node1 node2)# etcd 集群服务地址列表export ETCD_ENDPOINTS=&quot;https://192.168.6.101:2379,https://192.168.6.102:2379&quot;# etcd 集群间通信的 IP 和端口export ETCD_NODES=&quot;node1=https://192.168.6.101:2380,node2=https://192.168.6.102:2380&quot;# kube-apiserver 的反向代理(kube-nginx)地址端口export KUBE_APISERVER=&quot;https://127.0.0.1:8443&quot;# 节点间互联网络接口名称export IFACE=&quot;ens160&quot;# etcd 数据目录export ETCD_DATA_DIR=&quot;/data/k8s/etcd/data&quot;# etcd WAL 目录，建议是 SSD 磁盘分区，或者和 ETCD_DATA_DIR 不同的磁盘分区export ETCD_WAL_DIR=&quot;/data/k8s/etcd/wal&quot;# k8s 各组件数据目录export K8S_DIR=&quot;/data/k8s/k8s&quot;# docker 数据目录export DOCKER_DIR=&quot;/data/k8s/docker&quot;## 以下参数一般不需要修改# TLS Bootstrapping 使用的 Token，可以使用命令 head -c 16 /dev/urandom | od -An -t x | tr -d &apos; &apos; 生成BOOTSTRAP_TOKEN=&quot;4d8a35f48da304e4433ba0bda5b8ffd1&quot;# 最好使用 当前未用的网段 来定义服务网段和 Pod 网段# 服务网段，部署前路由不可达，部署后集群内路由可达(kube-proxy 保证)SERVICE_CIDR=&quot;10.254.0.0/16&quot;# Pod 网段，建议 /16 段地址，部署前路由不可达，部署后集群内路由可达(flanneld 保证)CLUSTER_CIDR=&quot;172.30.0.0/16&quot;# 服务端口范围 (NodePort Range)export NODE_PORT_RANGE=&quot;30000-32767&quot;# flanneld 网络配置前缀export FLANNEL_ETCD_PREFIX=&quot;/kubernetes/network&quot;# kubernetes 服务 IP (一般是 SERVICE_CIDR 中第一个IP)export CLUSTER_KUBERNETES_SVC_IP=&quot;10.254.0.1&quot;# 集群 DNS 服务 IP (从 SERVICE_CIDR 中预分配)export CLUSTER_DNS_SVC_IP=&quot;10.254.0.2&quot;# 集群 DNS 域名（末尾不带点号）export CLUSTER_DNS_DOMAIN=&quot;cluster.local&quot;# 将二进制目录 /opt/k8s/bin 加到 PATH 中export PATH=/opt/k8s/bin:$PATH 分发到所有节点1234567source environment.shfor node_ip in $&#123;NODE_IPS[@]&#125; do echo &quot;&gt;&gt;&gt; $&#123;node_ip&#125;&quot; scp environment.sh root@$&#123;node_ip&#125;:/opt/k8s/bin/ ssh root@$&#123;node_ip&#125; &quot;chmod +x /opt/k8s/bin/*&quot; done]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tomcat报SEVERE Error listenerStart]]></title>
    <url>%2Fposts%2F1b8b1cc1.html</url>
    <content type="text"><![CDATA[问题启动tomcat时报错，错误信息如下： 1234org.apache.catalina.core.StandardContext startInternalSEVERE: Error listenerStartorg.apache.catalina.core.StandardContext startInternalSEVERE: Context [/projectname] startup failed due to previous errors 方法在WEB-INF/classes目录下新建一个文件叫logging.properties，内容如下 123456handlers = org.apache.juli.FileHandler, java.util.logging.ConsoleHandler org.apache.juli.FileHandler.level = FINE org.apache.juli.FileHandler.directory = $&#123;catalina.base&#125;/logs org.apache.juli.FileHandler.prefix = error-debug. java.util.logging.ConsoleHandler.level = FINE java.util.logging.ConsoleHandler.formatter=java.util.logging.SimpleFormatter 之后，重启tomcat查看日志，就可以看到是由于数据库连接或者jdk版本不兼容等原因导致的]]></content>
      <categories>
        <category>中间件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s强制删除]]></title>
    <url>%2Fposts%2F267edcc5.html</url>
    <content type="text"><![CDATA[可使用kubectl中的强制删除命令12345# 删除PODkubectl delete pod PODNAME --force --grace-period=0# 删除NAMESPACEkubectl delete namespace NAMESPACENAME --force --grace-period=0 有时候这种方法也删除不掉，可能是之前删除顺序有问题，没有删干净pod，就删除命名空间，导致删除不掉 直接从ETCD中删除源数据12345# 删除default namespace下的pod名为pod-to-be-deleted-0ETCDCTL_API=3 etcdctl del /registry/pods/default/pod-to-be-deleted-0# 删除需要删除的NAMESPACEetcdctl del /registry/namespaces/NAMESPACENAME 添加别名上面直接etcd删除，是证书直接能找到时候，如果证书配置方式不一样，就需要手动配一下！ 配置别名etcdctl3，添加证书等参数 1234567alias etcdctl3=&apos;docker run --rm -it \--net host -e ETCDCTL_API=3 \-v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:3.3.10 etcdctl \--cert /etc/kubernetes/pki/etcd/peer.crt \--key /etc/kubernetes/pki/etcd/peer.key \--cacert /etc/kubernetes/pki/etcd/ca.crt \--endpoints https://192.168.3.101:2379,https://192.168.3.102:2379,https://192.168.3.103:2379&apos; 查询都有哪些daemonsets 1234tcdctl3 get /registry/daemonsets/ --prefix --keys-only/registry/daemonsets/default/testpod/registry/daemonsets/kube-system/calico-node/registry/daemonsets/kube-system/kube-proxy 与kubectl查看的结果一致 12345kubectl get daemonsets --all-namespaces NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEdefault testpod 3 3 3 3 3 &lt;none&gt; 91mkube-system calico-node 3 3 3 3 3 beta.kubernetes.io/os=linux 116mkube-system kube-proxy 3 3 3 3 3 &lt;none&gt; 122m 在etcd中查询default namespace中的pod 1234etcdctl3 get /registry/pods/default --prefix --keys-only /registry/pods/default/testpod-5wtb7/registry/pods/default/testpod-646d8/registry/pods/default/testpod-t7ps7 kubectl命令看到结果与etcd中一致 12345kubectl get pods -l app=fortestNAME READY STATUS RESTARTS AGEtestpod-5wtb7 1/1 Running 0 93mtestpod-646d8 1/1 Running 0 93mtestpod-t7ps7 1/1 Running 0 93m 在etcd中删除pod testpod-t7ps7 12etcdctl3 del /registry/pods/default/testpod-t7ps7 1 再次查看pod，发现testpod-t7ps7已经没有了 12345kubectl get podsNAME READY STATUS RESTARTS AGEtestpod-5wtb7 1/1 Running 0 96mtestpod-646d8 1/1 Running 0 96mtestpod-qczvt 1/1 Running 0 17s]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[fabric问题汇总]]></title>
    <url>%2Fposts%2F9c5623c7.html</url>
    <content type="text"><![CDATA[安装部署fabric测试项目安装问题12npm install probuf的时候报的错fabric依赖的sdk需要依赖c++的编译库，windows也许windows-tools，linux也需要支持的gc++,gc fabric-ca-server 存储私钥么1从mysql中不存储私钥 启动order遇到问题123Failed to initialize local MSP: the supplied identity is not valid: x509: certificate signed by unknown authority原因：实体的证书不是组织的证书签发的 docker-compose创建报错123[blocksProvider] DeliverBlocks -&gt; ERRO 039 [vaccinechannel] Got error &amp;&#123;FORBIDDEN&#125;解决办法： 需要在组织的msp中增加config.yaml 应用过程中问题Peer或者Orderer不通当不通的时候，先确认域名对应的IP是否正确，然后用telnet检查服务端口： 12ping peer0.org1.example.comtelnet peer0.org1.example.com 7051 如果不通，检查一下/etc/hosts中是否设置了域名和IP的对应关系是否正确。 如果还是不通，看一下系统有没有防火墙，7051端口有没有被防火墙禁止。 目标Peer上的Docker没有启动，导致合约实例化失败实例化合约时出错： 1./peer.sh chaincode instantiate -o orderer.example.com:7050 --tls true --cafile ./tlsca.example.com-cert.pem -C mychannel -n demo -v 0.0.1 -c &apos;&#123;&quot;Args&quot;:[&quot;init&quot;]&#125;&apos; -P &quot;OR(&apos;Org1MSP.member&apos;,&apos;Org2MSP.member&apos;)&quot; 错误如下： 1Error: Error endorsing chaincode: rpc error: code = Unknown desc = error starting container: Post http://unix.sock/containers/create?name=dev-peer1.org1.example.com-demo-0.0.1: dial unix /var/run/docker.sock: connect: no such file or directory 这是目标peer上的docker没有启动造成的。 genesisblock中admin证书错误导致orderer panic: x509: ECDSA verification failureorderer在启动的时候报错，直接panic： 123456789-----END CERTIFICATE-----2018-06-22 14:27:30.462 UTC [orderer/commmon/multichannel] newLedgerResources -&gt; CRIT 04d Error creating channelconfig bundle: initializing channelconfig failed: could not create channel Consortiums sub-group config: setting up the MSP manager failed: the supplied identity is not valid: x509: certificate signed by unknown authority (possibly because of &quot;x509: ECDSA verification failure&quot; while trying to verify candidate authority certificate &quot;ca.org1.example.com&quot;)panic: Error creating channelconfig bundle: initializing channelconfig failed: could not create channel Consortiums sub-group config: setting up the MSP manager failed: the supplied identity is not valid: x509: certificate signed by unknown authority (possibly because of &quot;x509: ECDSA verification failure&quot; while trying to verify candidate authority certificate &quot;ca.org1.example.com&quot;)goroutine 1 [running]:github.com/hyperledger/fabric/vendor/github.com/op/go-logging.(*Logger).Panicf(0xc4201ee120, 0x108668e, 0x27, 0xc42026af50, 0x1, 0x1) /w/workspace/fabric-binaries-x86_64/gopath/src/github.com/hyperledger/fabric/vendor/github.com/op/go-logging/logger.go:194 +0x134github.com/hyperledger/fabric/orderer/common/multichannel.(*Registrar).newLedgerResources(0xc42010a380, 0xc420138840, 0xc420138840) /w/workspace/fabric-binaries-x86_64/gopath/src/github.com/hyperledger/fabric/orderer/common/multichannel/registrar.go:253 +0x391 怀疑是创世块的原因，用下面的命令将创始块解开： 1./bin/configtxgen -profile TwoOrgsOrdererGenesis -outputBlock ./genesisblock 发现比较奇怪的地方，Org1的Admin证书有两个： 123456789101112&quot;groups&quot;: &#123; &quot;Org1MSP&quot;: &#123; &quot;mod_policy&quot;: &quot;Admins&quot;, ... &quot;mod_policy&quot;: &quot;Admins&quot;, &quot;value&quot;: &#123; &quot;config&quot;: &#123; &quot;admins&quot;: [ &quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNHVENDQWIrZ0F3SUJBZ0lRVXRxQWxlZENzWkErWStWdlZMUTZQakFLQmdncWhrak9QUVFEQWpCek1Rc3cKQ1FZRFZRUUdFd0pWVXpFVE1CRUdBMVVFQ0JNS1EyRnNhV1p2Y201cFlURVdNQlFHQTFVRUJ4TU5VMkZ1SUVaeQpZVzVqYVhOamJ6RVpNQmNHQTFVRUNoTVFiM0puTVM1bGVHRnRjR3hsTG1OdmJURWNNQm9HQTFVRUF4TVRZMkV1CmIzSm5NUzVsZUdGdGNHeGxMbU52YlRBZUZ3MHhPREEyTWpFd05qVTNNekJhRncweU9EQTJNVGd3TmpVM016QmEKTUZzeEN6QUpCZ05WQkFZVEFsVlRNUk13RVFZRFZRUUlFd3BEWVd4cFptOXlibWxoTVJZd0ZBWURWUVFIRXcxVApZVzRnUm5KaGJtTnBjMk52TVI4d0hRWURWUVFEREJaQlpHMXBia0J2Y21jeExtVjRZVzF3YkdVdVkyOXRNRmt3CkV3WUhLb1pJemowQ0FRWUlLb1pJemowREFRY0RRZ0FFRVp3cUhTVmxxRGNKNC9aVSt0YnB5RVBSTkl5ellMdTMKRGlRVUZOMklBZm5vVGhjTjRmY3Y4c2dsdXUxcnpJYUVHSFRFLzd0TC9EdEg2U3Fjd2tOQkthTk5NRXN3RGdZRApWUjBQQVFIL0JBUURBZ2VBTUF3R0ExVWRFd0VCL3dRQ01BQXdLd1lEVlIwakJDUXdJb0FnbkpjYVVLVFlseVJxCjcyckk4QXNINHNVZHB0ZytWY3IvbHkxZlp3QndrOEF3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUloQUsvRXh6NlYKRVYwUFl4M1BQbitPMysvODQrdXFEVkZ2Q1ZRUEVNcU1yV3dkQWlBNVVqTDcyb2drTHB3UUtGZ1ptdTJqRmtPWApSVnhpY0htLzZCR3htelFRc1E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==&quot;, &quot;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUNGVENDQWJ5Z0F3SUJBZ0lRU3E0VzJ1SEVqbHdXZHdGY21WNUlpekFLQmdncWhrak9QUVFEQWpCek1Rc3cKQ1FZRFZRUUdFd0pWVXpFVE1CRUdBMVVFQ0JNS1EyRnNhV1p2Y201cFlURVdNQlFHQTFVRUJ4TU5VMkZ1SUVaeQpZVzVqYVhOamJ6RVpNQmNHQTFVRUNoTVFiM0puTVM1bGVHRnRjR3hsTG1OdmJURWNNQm9HQTFVRUF4TVRZMkV1CmIzSm5NUzVsZUdGdGNHeGxMbU52YlRBZUZ3MHhPREEyTWpFd056VXdNVEZhRncweU9EQTJNVGd3TnpVd01URmEKTUZneEN6QUpCZ05WQkFZVEFsVlRNUk13RVFZRFZRUUlFd3BEWVd4cFptOXlibWxoTVJZd0ZBWURWUVFIRXcxVApZVzRnUm5KaGJtTnBjMk52TVJ3d0dnWURWUVFERXhOallTNXZjbWN4TG1WNFlXMXdiR1V1WTI5dE1Ga3dFd1lICktvWkl6ajBDQVFZSUtvWkl6ajBEQVFjRFFnQUVxNHl6K0tqSTR2ZmtObzQ0bWp0Q25HQ2cwLzA3L2Y5VW1sZlEKMlpSZWtHN2lyVm1QY0N6YnRVVEcvTFJjbndVemgyaFMvZkg5cGxvZEM4a1pwSlpXQzZOTk1Fc3dEZ1lEVlIwUApBUUgvQkFRREFnZUFNQXdHQTFVZEV3RUIvd1FDTUFBd0t3WURWUjBqQkNRd0lvQWdPc1NNQ2VqcnBOMnBhNEZSCnBOMVE2eXJkVHJleXNGY0Q1Ym9TcVNzSnFLNHdDZ1lJS29aSXpqMEVBd0lEUndBd1JBSWdCQWo1Q3l2cEFhU0kKaTh4anpVVHZxbUt5dmxSOFFPeExBUTAvVi9jRGpTNENJRVg3V1lnZzYwTFUwTy9LNEpmVVpiQmoyNHRBbTkxcgpkQmczN21IZHZVcSsKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=&quot; ], ... 将上面的两大行字符串分别用base64解码得到证书，然后用openssl命令查看： 12echo &quot;LS0tLS1CRUdJTiBDRVJUSU....tLS0tCg==&quot; |base64 -D &gt;a.certopenssl x509 -in a.cert -text 第一个证书正确： 123... Subject: C=US, ST=California, L=San Francisco, CN=Admin@org1.example.com... 查看第二行： 12echo &quot;LS0tLS1CRUdJTi....tLS0tLQo=&quot; |base64 -D &gt;b.certopenssl x509 -in b.cert -text 发现第二个证书是CA证书，不是用户证书！ 1Subject: C=US, ST=California, L=San Francisco, CN=ca.org1.example.com 检查生成genesisblock时使用的configtx.yaml文件，发现configtx.yaml中配置的msp目录： 1MSPDir: ./certs/peerOrganizations/org1.example.com/msp msp的admincerts子目录中，多出了一个ca证书： 12$ ls ./certs/peerOrganizations/org1.example.com/msp/admincerts/Admin@org1.example.com-cert.pem ca.org1.example.com-cert.pem 把多出的ca证书删除。 残留数据导致orderer启动失败启动orderer的时候报错，orderer直接panic： 122018-06-21 11:01:47.892 CST [orderer/commmon/multichannel] newLedgerResources -&gt; CRIT 052 Error creating channelconfig bundle: initializing channelconfig failed: could not create channel Orderer sub-group config: setting up the MSP manager failed: the supplied identity is not valid: x509: certificate signed by unknown authority (possibly because of &quot;x509: ECDSA verification failure&quot; while trying to verify candidate authority certificate &quot;ca.example.com&quot;)panic: Error creating channelconfig bundle: initializing channelconfig failed: could not create channel Orderer sub-group config: setting up the MSP manager failed: the supplied identity is not valid: x509: certificate signed by unknown authority (possibly because of &quot;x509: ECDSA verification failure&quot; while trying to verify candidate authority certificate &quot;ca.example.com&quot;) 排查发现，部署orderer的机器上以前部署过orderer，并且orderer.yaml中配置的数据路径/opt/app/fabric/orderer/data中残留了以前的数据。 将/opt/app/fabric/orderer/data中的文件都删除后，问题解决。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Prometheus添加验证登录]]></title>
    <url>%2Fposts%2F239bdf86.html</url>
    <content type="text"><![CDATA[prometheus添加nginx验证Prometheus默认开箱即食，并没有设置认证方式，如果你使用Grafana那就另当别论。 如果你想直接访问Prometheus并且需要设置个认证，那么通过Nginx反向代理是一个不错的选择。 本文通过Nginx反向代理增加401认证方式来实现。 安装apache-htpasswd工具1yum -y install httpd-tools 加密认证密码1htpasswd -cs /usr/local/nginx/conf/401htpasswd sy 设置Nginx反向代理及401认证12345678910111213cd /usr/local/nginx/conf/vhostvi demo.confserver &#123; listen 80; server_name 192.168.10.177; location / &#123; auth_basic &quot;Prometheus&quot;; auth_basic_user_file /usr/local/nginx/conf/401htpasswd; proxy_pass http://localhost:9090/; &#125;&#125; 启动nginx；访问192.168.10.177 正常输入密码后，会看到Prometheus页面，如果提示403则表示账号密码不正确，或者路径配错。 加密node_exporternode_exporter是Prometheus的一个扩展程序，也是通过go语言编写，同样是开箱即食，主要用来采集服务器上的数据（CPU、内存等等）。 所以为了安全考虑，也需要加密一下。 Nginx配置如下12345678910server &#123; listen 19100; server_name 你的远程主机IP; location / &#123; proxy_pass http://localhost:9100/; auth_basic &quot;Prometheus&quot;; auth_basic_user_file /usr/local/nginx/conf/401htpasswd; &#125;&#125; prometheus配置12345678910在Prometheus配置文件下面添加以下内容，username是远程服务器认证账号，password为加密密码，此处IP为远程服务器的IP地址，不需要加http。- job_name: server static_configs: - targets: [&apos;IP:19100&apos;] labels: instance: name basic_auth: username: sy password: 123456]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[nginx基础整理]]></title>
    <url>%2Fposts%2F8d1c645b.html</url>
    <content type="text"><![CDATA[安装安装依赖 prce(重定向支持)和openssl(https支持，如果不需要https可以不安装。) 123yum install -y pcre-devel yum -y install gcc make gcc-c++ wgetyum -y install openssl openssl-devel 下载nginx的所有版本在这里 123456789#创建存放源码包的目录[root@nginx ~]# mkdir tools[root@nginx ~]# cd tools/ #下载Nginx源码包[root@nginx tools]# wget http://nginx.org/download/nginx-1.14.2.tar.gz[root@nginx tools]# lsnginx-1.14.2.tar.gz #解压Nginx源码包[root@nginx tools]# tar -xf nginx-1.14.2.tar.gz 编译安装然后进入目录编译安装 123 #创建安装的目录[root@nginx nginx-1.14.2]# mkdir -p /application/nginx[root@nginx nginx-1.14.2]# ./configure --prefix=/application/nginx 如果没有error信息，就可以执行下边的安装了： 12makemake install nginx测试运行下面命令会出现两个结果，一般情况nginx会安装在/usr/local/nginx目录中 12345cd /usr/local/nginx/sbin/./nginx -t# nginx: the configuration file /usr/local/nginx/conf/nginx.conf syntax is ok# nginx: configuration file /usr/local/nginx/conf/nginx.conf test is successful 设置全局nginx命令1vi ~/.bash_profile 将下面内容添加到 ~/.bash_profile 文件中 12PATH=$PATH:$HOME/bin:/usr/local/nginx/sbin/export PATH 运行命令 source ~/.bash_profile 让配置立即生效。你就可以全局运行 nginx 命令了。 开机自启动开机自启动方法一： 编辑 vi /lib/systemd/system/nginx.service 文件，没有创建一个 touch nginx.service 然后将如下内容根据具体情况进行修改后，添加到nginx.service文件中： 12345678910111213141516[Unit]Description=nginxAfter=network.target remote-fs.target nss-lookup.target[Service]Type=forkingPIDFile=/var/run/nginx.pidExecStartPre=/usr/local/nginx/sbin/nginx -t -c /usr/local/nginx/conf/nginx.confExecStart=/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.confExecReload=/bin/kill -s HUP $MAINPIDExecStop=/bin/kill -s QUIT $MAINPIDPrivateTmp=true[Install]WantedBy=multi-user.target [Unit]:服务的说明 Description:描述服务 After:描述服务类别 [Service]服务运行参数的设置 Type=forking是后台运行的形式 ExecStart为服务的具体运行命令 ExecReload为重启命令 ExecStop为停止命令 PrivateTmp=True表示给服务分配独立的临时空间 注意：[Service]的启动、重启、停止命令全部要求使用绝对路径。 [Install]运行级别下服务安装的相关设置，可设置为多用户，即系统运行级别为3。 保存退出。 设置开机启动，使配置生效： 1234567891011121314# 启动nginx服务systemctl start nginx.service# 停止开机自启动systemctl disable nginx.service# 查看服务当前状态systemctl status nginx.service# 查看所有已启动的服务systemctl list-units --type=service# 重新启动服务systemctl restart nginx.service# 设置开机自启动systemctl enable nginx.service# 输出下面内容表示成功了Created symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /usr/lib/systemd/system/nginx.service. 123456789systemctl is-enabled servicename.service # 查询服务是否开机启动systemctl enable *.service # 开机运行服务systemctl disable *.service # 取消开机运行systemctl start *.service # 启动服务systemctl stop *.service # 停止服务systemctl restart *.service # 重启服务systemctl reload *.service # 重新加载服务配置文件systemctl status *.service # 查询服务运行状态systemctl --failed # 显示启动失败的服务 注：*代表某个服务的名字，如http的服务名为httpd 开机自启动方法二： 1234vi /etc/rc.local# 在 rc.local 文件中，添加下面这条命令/usr/local/nginx/sbin/nginx start 如果开机后发现自启动脚本没有执行，你要去确认一下rc.local这个文件的访问权限是否是可执行的，因为rc.local默认是不可执行的。修改rc.local访问权限，增加可执行权限： 12# /etc/rc.local是/etc/rc.d/rc.local的软连接，chmod +x /etc/rc.d/rc.local 参数说明 参数 说明 –prefix=&lt;path&gt; Nginx安装路径。如果没有指定，默认为 /usr/local/nginx。 –sbin-path=&lt;path&gt; Nginx可执行文件安装路径。只能安装时指定，如果没有指定，默认为&lt;prefix&gt;/sbin/nginx。 –conf-path=&lt;path&gt; 在没有给定-c选项下默认的nginx.conf的路径。如果没有指定，默认为&lt;prefix&gt;/conf/nginx.conf。 –pid-path=&lt;path&gt; 在nginx.conf中没有指定pid指令的情况下，默认的nginx.pid的路径。如果没有指定，默认为 &lt;prefix&gt;/logs/nginx.pid。 –lock-path=&lt;path&gt; nginx.lock文件的路径。 –error-log-path=&lt;path&gt; 在nginx.conf中没有指定error_log指令的情况下，默认的错误日志的路径。如果没有指定，默认为 &lt;prefix&gt;/- logs/error.log。 –http-log-path=&lt;path&gt; 在nginx.conf中没有指定access_log指令的情况下，默认的访问日志的路径。如果没有指定，默认为 &lt;prefix&gt;/- logs/access.log。 –user=&lt;user&gt; 在nginx.conf中没有指定user指令的情况下，默认的nginx使用的用户。如果没有指定，默认为 nobody。 –group=&lt;group&gt; 在nginx.conf中没有指定user指令的情况下，默认的nginx使用的组。如果没有指定，默认为 nobody。 –builddir=DIR 指定编译的目录 –with-rtsig_module 启用 rtsig 模块 –with-select_module –without-select_module 允许或不允许开启SELECT模式，如果 configure 没有找到更合适的模式，比如：kqueue(sun os),epoll (linux kenel 2.6+), rtsig(- 实时信号)或者/dev/poll(一种类似select的模式，底层实现与SELECT基本相 同，都是采用轮训方法) SELECT模式将是默认安装模式 –with-poll_module –without-poll_module Whether or not to enable the poll module. This module is enabled by, default if a more suitable method such as kqueue, epoll, rtsig or /dev/poll is not discovered by configure. –with-http_ssl_module Enable ngx_http_ssl_module. Enables SSL support and the ability to handle HTTPS requests. Requires OpenSSL. On Debian, this is libssl-dev. 开启HTTP SSL模块，使NGINX可以支持HTTPS请求。这个模块需要已经安装了OPENSSL，在DEBIAN上是libssl –with-http_realip_module 启用 ngx_http_realip_module –with-http_addition_module 启用 ngx_http_addition_module –with-http_sub_module 启用 ngx_http_sub_module –with-http_dav_module 启用 ngx_http_dav_module –with-http_flv_module 启用 ngx_http_flv_module –with-http_stub_status_module 启用 “server status” 页 –without-http_charset_module 禁用 ngx_http_charset_module –without-http_gzip_module 禁用 ngx_http_gzip_module. 如果启用，需要 zlib 。 –without-http_ssi_module 禁用 ngx_http_ssi_module –without-http_userid_module 禁用 ngx_http_userid_module –without-http_access_module 禁用 ngx_http_access_module –without-http_auth_basic_module 禁用 ngx_http_auth_basic_module –without-http_autoindex_module 禁用 ngx_http_autoindex_module –without-http_geo_module 禁用 ngx_http_geo_module –without-http_map_module 禁用 ngx_http_map_module –without-http_referer_module 禁用 ngx_http_referer_module –without-http_rewrite_module 禁用 ngx_http_rewrite_module. 如果启用需要 PCRE 。 –without-http_proxy_module 禁用 ngx_http_proxy_module –without-http_fastcgi_module 禁用 ngx_http_fastcgi_module –without-http_memcached_module 禁用 ngx_http_memcached_module –without-http_limit_zone_module 禁用 ngx_http_limit_zone_module –without-http_empty_gif_module 禁用 ngx_http_empty_gif_module –without-http_browser_module 禁用 ngx_http_browser_module –without-http_upstream_ip_hash_module 禁用 ngx_http_upstream_ip_hash_module –with-http_perl_module 启用 ngx_http_perl_module –with-perl_modules_path=PATH 指定 perl 模块的路径 –with-perl=PATH 指定 perl 执行文件的路径 –http-log-path=PATH Set path to the http access log –http-client-body-temp-path=PATH Set path to the http client request body temporary files –http-proxy-temp-path=PATH Set path to the http proxy temporary files –http-fastcgi-temp-path=PATH Set path to the http fastcgi temporary files –without-http 禁用 HTTP server –with-mail 启用 IMAP4/POP3/SMTP 代理模块 –with-mail_ssl_module 启用 ngx_mail_ssl_module –with-cc=PATH 指定 C 编译器的路径 –with-cpp=PATH 指定 C 预处理器的路径 –with-cc-opt=OPTIONS Additional parameters which will be added to the variable CFLAGS. With the use of the system library PCRE in FreeBSD, it is necessary to indicate –with-cc-opt=”-I /usr/local/include”. If we are using select() and it is necessary to increase the number of file descriptors, then this also can be assigned here: –with-cc-opt=”-D FD_SETSIZE=2048”. –with-ld-opt=OPTIONS Additional parameters passed to the linker. With the use of the system library PCRE in - FreeBSD, it is necessary to indicate –with-ld-opt=”-L /usr/local/lib”. –with-cpu-opt=CPU 为特定的 CPU 编译，有效的值包括：pentium, pentiumpro, pentium3, pentium4, athlon, opteron, amd64, sparc32, sparc64, ppc64 –without-pcre 禁止 PCRE 库的使用。同时也会禁止 HTTP rewrite 模块。在 “location” 配置指令中的正则表达式也需要 PCRE 。 –with-pcre=DIR 指定 PCRE 库的源代码的路径。 –with-pcre-opt=OPTIONS Set additional options for PCRE building. –with-md5=DIR Set path to md5 library sources. –with-md5-opt=OPTIONS Set additional options for md5 building. –with-md5-asm Use md5 assembler sources. –with-sha1=DIR Set path to sha1 library sources. –with-sha1-opt=OPTIONS Set additional options for sha1 building. –with-sha1-asm Use sha1 assembler sources. –with-zlib=DIR Set path to zlib library sources. –with-zlib-opt=OPTIONS Set additional options for zlib building. –with-zlib-asm=CPU Use zlib assembler sources optimized for specified CPU, valid values are: pentium, pentiumpro –with-openssl=DIR Set path to OpenSSL library sources –with-openssl-opt=OPTIONS Set additional options for OpenSSL building –with-debug 启用调试日志 –add-module=PATH Add in a third-party module found in directory PATH 配置在Centos 默认配置文件在 /usr/local/nginx-1.5.1/conf/nginx.conf 我们要在这里配置一些文件。nginx.conf是主配置文件，由若干个部分组成，每个大括号{}表示一个部分。每一行指令都由分号结束;，标志着一行的结束。 常用正则 正则 说明 正则 说明 . 匹配除换行符以外的任意字符 $ 匹配字符串的结束 ? 重复0次或1次 {n} 重复n次 + 重复1次或更多次 {n,} 重复n次或更多次 * 重复0次或更多次 [c] 匹配单个字符c \d 匹配数字 [a-z] 匹配a-z小写字母的任意一个 ^ 匹配字符串的开始 - - 全局变量 变量 说明 变量 说明 $args 这个变量等于请求行中的参数，同$query_string $remote_port 客户端的端口。 $content_length 请求头中的Content-length字段。 $remote_user 已经经过Auth Basic Module验证的用户名。 $content_type 请求头中的Content-Type字段。 $request_filename 当前请求的文件路径，由root或alias指令与URI请求生成。 $document_root 当前请求在root指令中指定的值。 $scheme HTTP方法（如http，https）。 $host 请求主机头字段，否则为服务器名称。 $server_protocol 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。 $http_user_agent 客户端agent信息 $server_addr 服务器地址，在完成一次系统调用后可以确定这个值。 $http_cookie 客户端cookie信息 $server_name 服务器名称。 $limit_rate 这个变量可以限制连接速率。 $server_port 请求到达服务器的端口号。 $request_method 客户端请求的动作，通常为GET或POST。 $request_uri 包含请求参数的原始URI，不包含主机名，如：/foo/bar.php?arg=baz。 $remote_addr 客户端的IP地址。 $uri 不带请求参数的当前URI，$uri不包含主机名，如/foo/bar.html。 $document_uri 与$uri相同。 - - 例如请求：http://localhost:3000/test1/test2/test.php $host：localhost$server_port：3000$request_uri：/test1/test2/test.php$document_uri：/test1/test2/test.php$document_root：/var/www/html$request_filename：/var/www/html/test1/test2/test.php 符号参考 符号 说明 符号 说明 符号 说明 k,K 千字节 m,M 兆字节 ms 毫秒 s 秒 m 分钟 h 小时 d 日 w 周 M 一个月, 30天 例如，”8k”，”1m” 代表字节数计量。例如，”1h 30m”，”1y 6M”。代表 “1小时 30分”，”1年零6个月”。 配置文件nginx 的配置系统由一个主配置文件和其他一些辅助的配置文件构成。这些配置文件均是纯文本文件，全部位于 nginx 安装目录下的 conf 目录下。 指令由 nginx 的各个模块提供，不同的模块会提供不同的指令来实现配置。 指令除了 Key-Value 的形式，还有作用域指令。 nginx.conf 中的配置信息，根据其逻辑上的意义，对它们进行了分类，也就是分成了多个作用域，或者称之为配置指令上下文。不同的作用域含有一个或者多个配置项。 下面的这些上下文指令是用的比较多： Directive Description Contains Directive main nginx 在运行时与具体业务功能（比如 http 服务或者 email 服务代理）无关的一些参数，比如工作进程数，运行的身份等。 user, worker_processes, error_log, events, http, mail http 与提供 http 服务相关的一些配置参数。例如：是否使用 keepalive 啊，是否使用 gzip 进行压缩等。 server server http 服务上支持若干虚拟主机。每个虚拟主机一个对应的 server 配置项，配置项里面包含该虚拟主机相关的配置。在提供 mail 服务的代理时，也可以建立若干 server. 每个 server 通过监听的地址来区分。 listen, server_name, access_log, location, protocol, proxy, smtp_auth, xclient location http 服务中，某些特定的 URL 对应的一系列配置项。 index, root mail 实现 email 相关的 SMTP/IMAP/POP3 代理时，共享的一些配置项（因为可能实现多个代理，工作在多个监听地址上）。 server, http, imap_capabilities include 以便增强配置文件的可读性，使得部分配置文件可以重新使用。 - valid_referers 用来校验Http请求头Referer是否有效。 - try_files 用在server部分，不过最常见的还是用在location部分，它会按照给定的参数顺序进行尝试，第一个被匹配到的将会被使用。 - if 当在location块中使用if指令，在某些情况下它并不按照预期运行，一般来说避免使用if指令。 - 例如我们再 nginx.conf 里面引用两个配置 vhost/example.com.conf 和 vhost/gitlab.com.conf 它们都被放在一个我自己新建的目录 vhost 下面。nginx.conf 配置如下： 12345678910111213141516171819202122232425262728293031323334353637worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 80; server_name localhost; location / &#123; root html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; include vhost/example.com.conf; include vhost/gitlab.com.conf;&#125; 简单的配置: example.com.conf 1234567server &#123; #侦听的80端口 listen 80; server_name baidu.com app.baidu.com; # 这里指定域名 index index.html index.htm; # 这里指定默认入口页面 root /home/www/app.baidu.com; # 这里指定目录&#125; 内置预定义变量Nginx提供了许多预定义的变量，也可以通过使用set来设置变量。你可以在if中使用预定义变量，也可以将它们传递给代理服务器。以下是一些常见的预定义变量，更多详见 变量名称 值 $args_name 在请求中的name参数 $args 所有请求参数 $query_string $args的别名 $content_length 请求头Content-Length的值 $content_type 请求头Content-Type的值 $host 如果当前有Host，则为请求头Host的值；如果没有这个头，那么该值等于匹配该请求的server_name的值 $remote_addr 客户端的IP地址 $request 完整的请求，从客户端收到，包括Http请求方法、URI、Http协议、头、请求体 $request_uri 完整请求的URI，从客户端来的请求，包括参数 $scheme 当前请求的协议 $uri 当前请求的标准化URI 反向代理反向代理是一个Web服务器，它接受客户端的连接请求，然后将请求转发给上游服务器，并将从服务器得到的结果返回给连接的客户端。下面简单的反向代理的例子： 123456789101112server &#123; listen 80; server_name localhost; client_max_body_size 1024M; # 允许客户端请求的最大单文件字节数 location / &#123; proxy_pass http://localhost:8080; proxy_set_header Host $host:$server_port; proxy_set_header X-Forwarded-For $remote_addr; # HTTP的请求端真实的IP proxy_set_header X-Forwarded-Proto $scheme; # 为了正确地识别实际用户发出的协议是 http 还是 https &#125;&#125; 复杂的配置: gitlab.com.conf。 1234567891011121314151617181920server &#123; #侦听的80端口 listen 80; server_name git.example.cn; location / &#123; proxy_pass http://localhost:3000; #以下是一些反向代理的配置可删除 proxy_redirect off; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header Host $host; client_max_body_size 10m; #允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数 proxy_connect_timeout 300; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 300; #后端服务器数据回传时间(代理发送超时) proxy_read_timeout 300; #连接成功后，后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置 proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2） &#125;&#125; 代理到上游服务器的配置中，最重要的是proxy_pass指令。以下是代理模块中的一些常用指令： 指令 说明 proxy_connect_timeout Nginx从接受请求至连接到上游服务器的最长等待时间 proxy_send_timeout 后端服务器数据回传时间(代理发送超时) proxy_read_timeout 连接成功后，后端服务器响应时间(代理接收超时) proxy_cookie_domain 替代从上游服务器来的Set-Cookie头的domain属性 proxy_cookie_path 替代从上游服务器来的Set-Cookie头的path属性 proxy_buffer_size 设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers proxy_buffers缓冲区，网页平均在多少k以下 proxy_set_header 重写发送到上游服务器头的内容，也可以通过将某个头部的值设置为空字符串，而不发送某个头部的方法实现 proxy_ignore_headers 这个指令禁止处理来自代理服务器的应答。 proxy_intercept_errors 使nginx阻止HTTP应答代码为400或者更高的应答。 负载均衡upstream指令启用一个新的配置区段，在该区段定义一组上游服务器。这些服务器可能被设置不同的权重，也可能出于对服务器进行维护，标记为down。 123456789101112131415161718192021222324252627282930313233upstream gitlab &#123; ip_hash; # upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.122.11:8081 ; server 127.0.0.1:82 weight=3; server 127.0.0.1:83 weight=3 down; server 127.0.0.1:84 weight=3; max_fails=3 fail_timeout=20s; server 127.0.0.1:85 weight=4;; keepalive 32;&#125;server &#123; #侦听的80端口 listen 80; server_name git.example.cn; location / &#123; proxy_pass http://gitlab; #在这里设置一个代理，和upstream的名字一样 #以下是一些反向代理的配置可删除 proxy_redirect off; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; client_max_body_size 10m; #允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数 proxy_connect_timeout 300; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 300; #后端服务器数据回传时间(代理发送超时) proxy_read_timeout 300; #连接成功后，后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k;# 缓冲区，网页平均在32k以下的话，这样设置 proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2） proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传 &#125;&#125; 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 负载均衡： upstream模块能够使用3种负载均衡算法：轮询、IP哈希、最少连接数。 轮询： 默认情况下使用轮询算法，不需要配置指令来激活它，它是基于在队列中谁是下一个的原理确保访问均匀地分布到每个上游服务器；IP哈希： 通过ip_hash指令来激活，Nginx通过IPv4地址的前3个字节或者整个IPv6地址作为哈希键来实现，同一个IP地址总是能被映射到同一个上游服务器；最少连接数： 通过least_conn指令来激活，该算法通过选择一个活跃数最少的上游服务器进行连接。如果上游服务器处理能力不同，可以通过给server配置weight权重来说明，该算法将考虑到不同服务器的加权最少连接数。 RR简单配置 ，这里我配置了2台服务器，当然实际上是一台，只是端口不一样而已，而8081的服务器是不存在的，也就是说访问不到，但是我们访问 http://localhost 的时候，也不会有问题，会默认跳转到http://localhost:8080具体是因为Nginx会自动判断服务器的状态，如果服务器处于不能访问（服务器挂了），就不会跳转到这台服务器，所以也避免了一台服务器挂了影响使用的情况，由于Nginx默认是RR策略，所以我们不需要其他更多的设置 1234567891011121314upstream test &#123; server localhost:8080; server localhost:8081;&#125;server &#123; listen 81; server_name localhost; client_max_body_size 1024M; location / &#123; proxy_pass http://test; proxy_set_header Host $host:$server_port; &#125;&#125; 负载均衡的核心代码为 1234upstream test &#123; server localhost:8080; server localhost:8081;&#125; 权重指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 例如 1234upstream test &#123; server localhost:8080 weight=9; server localhost:8081 weight=1;&#125; 那么10次一般只会有1次会访问到8081，而有9次会访问到8080 ip_hash上面的2种方式都有一个问题，那就是下一个请求来的时候请求可能分发到另外一个服务器，当我们的程序不是无状态的时候（采用了session保存数据），这时候就有一个很大的很问题了，比如把登录信息保存到了session中，那么跳转到另外一台服务器的时候就需要重新登录了，所以很多时候我们需要一个客户只访问一个服务器，那么就需要用iphash了，iphash的每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 12345upstream test &#123; ip_hash; server localhost:8080; server localhost:8081;&#125; fair这是个第三方模块，按后端服务器的响应时间来分配请求，响应时间短的优先分配。 12345upstream backend &#123; fair; server localhost:8080; server localhost:8081;&#125; url_hash这是个第三方模块，按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 123456upstream backend &#123; hash $request_uri; hash_method crc32; server localhost:8080; server localhost:8081;&#125; 以上5种负载均衡各自适用不同情况下使用，所以可以根据实际情况选择使用哪种策略模式，不过fair和url_hash需要安装第三方模块才能使用 server指令可选参数： weight：设置一个服务器的访问权重，数值越高，收到的请求也越多； fail_timeout：在这个指定的时间内服务器必须提供响应，如果在这个时间内没有收到响应，那么服务器将会被标记为down状态； max_fails：设置在fail_timeout时间之内尝试对一个服务器连接的最大次数，如果超过这个次数，那么服务器将会被标记为down; down：标记一个服务器不再接受任何请求； backup：一旦其他服务器宕机，那么有该标记的机器将会接收请求。 keepalive指令： Nginx服务器将会为每一个worker进行保持同上游服务器的连接。 屏蔽ip在nginx的配置文件nginx.conf中加入如下配置，可以放到http, server, location, limit_except语句块，需要注意相对路径，本例当中nginx.conf，blocksip.conf在同一个目录中。 1include blockip.conf; 在blockip.conf里面输入内容，如： 1234567891011121314deny 165.91.122.67;deny IP; # 屏蔽单个ip访问allow IP; # 允许单个ip访问deny all; # 屏蔽所有ip访问allow all; # 允许所有ip访问deny 123.0.0.0/8 # 屏蔽整个段即从123.0.0.1到123.255.255.254访问的命令deny 124.45.0.0/16 # 屏蔽IP段即从123.45.0.1到123.45.255.254访问的命令deny 123.45.6.0/24 # 屏蔽IP段即从123.45.6.1到123.45.6.254访问的命令# 如果你想实现这样的应用，除了几个IP外，其他全部拒绝allow 1.1.1.1; allow 1.1.1.2;deny all; 第三方模块安装方法1./configure --prefix=/你的安装目录 --add-module=/第三方模块目录 重定向 permanent 永久性重定向。请求日志中的状态码为301 redirect 临时重定向。请求日志中的状态码为302 重定向整个网站1234server &#123; server_name old-site.com return 301 $scheme://new-site.com$request_uri;&#125; 重定向单页12345server &#123; location = /oldpage.html &#123; return 301 http://example.org/newpage.html; &#125;&#125; 重定向整个子路径123location /old-site &#123; rewrite ^/old-site/(.*) http://example.org/new-site/$1 permanent;&#125; 性能内容缓存允许浏览器基本上永久地缓存静态内容。 Nginx将为您设置Expires和Cache-Control头信息。 1234location /static &#123; root /data; expires max;&#125; 如果要求浏览器永远不会缓存响应（例如用于跟踪请求），请使用-1。 1234location = /empty.gif &#123; empty_gif; expires -1;&#125; Gzip压缩123456789101112131415gzip on;gzip_buffers 16 8k;gzip_comp_level 6;gzip_http_version 1.1;gzip_min_length 256;gzip_proxied any;gzip_vary on;gzip_types text/xml application/xml application/atom+xml application/rss+xml application/xhtml+xml image/svg+xml text/javascript application/javascript application/x-javascript text/x-json application/json application/x-web-app-manifest+json text/css text/plain text/x-component font/opentype application/x-font-ttf application/vnd.ms-fontobject image/x-icon;gzip_disable &quot;msie6&quot;; 打开文件缓存1234open_file_cache max=1000 inactive=20s;open_file_cache_valid 30s;open_file_cache_min_uses 2;open_file_cache_errors on; SSL缓存12ssl_session_cache shared:SSL:10m;ssl_session_timeout 10m; 上游Keepalive123456789101112upstream backend &#123; server 127.0.0.1:8080; keepalive 32;&#125;server &#123; ... location /api/ &#123; proxy_pass http://backend; proxy_http_version 1.1; proxy_set_header Connection &quot;&quot;; &#125;&#125; 监控使用ngxtop实时解析nginx访问日志，并且将处理结果输出到终端，功能类似于系统命令top。所有示例都读取nginx配置文件的访问日志位置和格式。如果要指定访问日志文件和/或日志格式，请使用-f和-a选项。 注意：在nginx配置中/usr/local/nginx/conf/nginx.conf日志文件必须是绝对路径。 12345678910111213141516171819202122# 安装 ngxtoppip install ngxtop# 实时状态ngxtop# 状态为404的前10个请求的路径：ngxtop top request_path --filter &apos;status == 404&apos;# 发送总字节数最多的前10个请求ngxtop --order-by &apos;avg(bytes_sent) * count&apos;# 排名前十位的IP，例如，谁攻击你最多ngxtop --group-by remote_addr# 打印具有4xx或5xx状态的请求，以及status和http refererngxtop -i &apos;status &gt;= 400&apos; print request status http_referer# 由200个请求路径响应发送的平均正文字节以&apos;foo&apos;开始：ngxtop avg bytes_sent --filter &apos;status == 200 and request_path.startswith(&quot;foo&quot;)&apos;# 使用“common”日志格式从远程机器分析apache访问日志ssh remote tail -f /var/log/apache2/access.log | ngxtop -f common 常见使用场景跨域问题在工作中，有时候会遇到一些接口不支持跨域，这时候可以简单的添加add_headers来支持cors跨域。配置如下： 123456789101112131415server &#123; listen 80; server_name api.xxx.com; add_header &apos;Access-Control-Allow-Origin&apos; &apos;*&apos;; add_header &apos;Access-Control-Allow-Credentials&apos; &apos;true&apos;; add_header &apos;Access-Control-Allow-Methods&apos; &apos;GET,POST,HEAD&apos;; location / &#123; proxy_pass http://127.0.0.1:3000; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; &#125; &#125; 上面更改头信息，还有一种，使用 rewrite 指令重定向URI来解决跨域问题。 1234567891011121314151617181920212223242526272829303132333435363738upstream test &#123; server 127.0.0.1:8080; server localhost:8081;&#125;server &#123; listen 80; server_name api.xxx.com; location / &#123; root html; #去请求../html文件夹里的文件 index index.html index.htm; #首页响应地址 &#125; # 用于拦截请求，匹配任何以 /api/开头的地址， # 匹配符合以后，停止往下搜索正则。 location ^~/api/&#123; # 代表重写拦截进来的请求，并且只能对域名后边的除去传递的参数外的字符串起作用， # 例如www.a.com/proxy/api/msg?meth=1&amp;par=2重写，只对/proxy/api/msg重写。 # rewrite后面的参数是一个简单的正则 ^/api/(.*)$， # $1代表正则中的第一个()，$2代表第二个()的值，以此类推。 rewrite ^/api/(.*)$ /$1 break; # 把请求代理到其他主机 # 其中 http://www.b.com/ 写法和 http://www.b.com写法的区别如下 # 如果你的请求地址是他 http://server/html/test.jsp # 配置一： http://www.b.com/ 后面有“/” # 将反向代理成 http://www.b.com/html/test.jsp 访问 # 配置一： http://www.b.com 后面没有有“/” # 将反向代理成 http://www.b.com/test.jsp 访问 proxy_pass http://test; # 如果 proxy_pass URL 是 http://a.xx.com/platform/ 这种情况 # proxy_cookie_path应该设置成 /platform/ / (注意两个斜杠之间有空格)。 proxy_cookie_path /platfrom/ /; # http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_pass_header # 设置 Cookie 头通过 proxy_pass_header Set-Cookie; &#125; &#125; 跳转到带www的域上面123456789101112131415server &#123; listen 80; # 配置正常的带www的域名 server_name www.wangchujiang.com; root /home/www/wabg/download; location / &#123; try_files $uri $uri/ /index.html =404; &#125;&#125;server &#123; # 这个要放到下面， # 将不带www的 wangchujiang.com 永久性重定向到 https://www.wangchujiang.com server_name wangchujiang.com; rewrite ^(.*) https://www.wangchujiang.com$1 permanent;&#125; 代理转发12345678910111213141516171819202122232425262728293031323334upstream server-api&#123; # api 代理服务地址 server 127.0.0.1:3110; &#125;upstream server-resource&#123; # 静态资源 代理服务地址 server 127.0.0.1:3120;&#125;server &#123; listen 3111; server_name localhost; # 这里指定域名 root /home/www/server-statics; # 匹配 api 路由的反向代理到API服务 location ^~/api/ &#123; rewrite ^/(.*)$ /$1 break; proxy_pass http://server-api; &#125; # 假设这里验证码也在API服务中 location ^~/captcha &#123; rewrite ^/(.*)$ /$1 break; proxy_pass http://server-api; &#125; # 假设你的图片资源全部在另外一个服务上面 location ^~/img/ &#123; rewrite ^/(.*)$ /$1 break; proxy_pass http://server-resource; &#125; # 路由在前端，后端没有真实路由，在路由不存在的 404状态的页面返回 /index.html # 这个方式使用场景，你在写React或者Vue项目的时候，没有真实路由 location / &#123; try_files $uri $uri/ /index.html =404; # ^ 空格很重要 &#125;&#125; 监控状态信息通过 nginx -V 来查看是否有 with-http_stub_status_module 该模块。 nginx -V 这里 V 是大写的，如果是小写的 v 即 nginx -v，则不会出现有哪些模块，只会出现 nginx 的版本 1234location /nginx_status &#123; stub_status on; access_log off;&#125; 通过 http://127.0.0.1/nginx_status 访问出现下面结果。 1234Active connections: 3server accepts handled requests 7 7 5 Reading: 0 Writing: 1 Waiting: 2 主动连接(第 1 行) 当前与http建立的连接数，包括等待的客户端连接：3 服务器接受处理的请求(第 2~3 行) 接受的客户端连接总数目：7处理的客户端连接总数目：7客户端总的请求数目：5 读取其它信(第 4 行) 当前，nginx读请求连接当前，nginx写响应返回给客户端目前有多少空闲客户端请求连接 代理转发连接替换1234location ^~/api/upload &#123; rewrite ^/(.*)$ /wfs/v1/upload break; proxy_pass http://wfs-api;&#125; ssl配置超文本传输安全协议（缩写：HTTPS，英语：Hypertext Transfer Protocol Secure）是超文本传输协议和SSL/TLS的组合，用以提供加密通讯及对网络服务器身份的鉴定。HTTPS连接经常被用于万维网上的交易支付和企业信息系统中敏感信息的传输。HTTPS不应与在RFC 2660中定义的安全超文本传输协议（S-HTTP）相混。HTTPS 目前已经是所有注重隐私和安全的网站的首选，随着技术的不断发展，HTTPS 网站已不再是大型网站的专利，所有普通的个人站长和博客均可以自己动手搭建一个安全的加密的网站。 创建SSL证书，如果你购买的证书，就可以直接下载 1234567891011sudo mkdir /etc/nginx/ssl# 创建了有效期100年，加密强度为RSA2048的SSL密钥key和X509证书文件。sudo openssl req -x509 -nodes -days 36500 -newkey rsa:2048 -keyout /etc/nginx/ssl/nginx.key -out /etc/nginx/ssl/nginx.crt# 上面命令，会有下面需要填写内容Country Name (2 letter code) [AU]:USState or Province Name (full name) [Some-State]:New YorkLocality Name (eg, city) []:New York CityOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Bouncy Castles, Inc.Organizational Unit Name (eg, section) []:Ministry of Water SlidesCommon Name (e.g. server FQDN or YOUR name) []:your_domain.comEmail Address []:admin@your_domain.com 创建自签证书 123456789101112首先，创建证书和私钥的目录# mkdir -p /etc/nginx/cert# cd /etc/nginx/cert创建服务器私钥，命令会让你输入一个口令：# openssl genrsa -des3 -out nginx.key 2048创建签名请求的证书（CSR）：# openssl req -new -key nginx.key -out nginx.csr在加载SSL支持的Nginx并使用上述私钥时除去必须的口令：# cp nginx.key nginx.key.org# openssl rsa -in nginx.key.org -out nginx.key最后标记证书使用上述私钥和CSR：# openssl x509 -req -days 365 -in nginx.csr -signkey nginx.key -out nginx.crt 查看目前nginx编译选项 1sbin/nginx -V 输出下面内容 1234nginx version: nginx/1.7.8built by gcc 4.4.7 20120313 (Red Hat 4.4.7-4) (GCC)TLS SNI support enabledconfigure arguments: --prefix=/usr/local/nginx-1.7.8 --with-http_ssl_module --with-http_spdy_module --with-http_stub_status_module --with-pcre 如果依赖的模块不存在，可以进入安装目录，输入下面命令重新编译安装。 1./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module 运行完成之后还需要make (不用make install) 1234# 备份nginx的二进制文件cp -rf /usr/local/nginx/sbin/nginx /usr/local/nginx/sbin/nginx.bak# 覆盖nginx的二进制文件cp -rf objs/nginx /usr/local/nginx/sbin/ HTTPS server 1234567891011121314151617181920212223242526server &#123; listen 443 ssl; server_name localhost; ssl_certificate /etc/nginx/ssl/nginx.crt; ssl_certificate_key /etc/nginx/ssl/nginx.key; # 禁止在header中出现服务器版本，防止黑客利用版本漏洞攻击 server_tokens off; # 设置ssl/tls会话缓存的类型和大小。如果设置了这个参数一般是shared，buildin可能会参数内存碎片，默认是none，和off差不多，停用缓存。如shared:SSL:10m表示我所有的nginx工作进程共享ssl会话缓存，官网介绍说1M可以存放约4000个sessions。 ssl_session_cache shared:SSL:1m; # 客户端可以重用会话缓存中ssl参数的过期时间，内网系统默认5分钟太短了，可以设成30m即30分钟甚至4h。 ssl_session_timeout 5m; # 选择加密套件，不同的浏览器所支持的套件（和顺序）可能会不同。 # 这里指定的是OpenSSL库能够识别的写法，你可以通过 openssl -v cipher &apos;RC4:HIGH:!aNULL:!MD5&apos;（后面是你所指定的套件加密算法） 来看所支持算法。 ssl_ciphers HIGH:!aNULL:!MD5; # 设置协商加密算法时，优先使用我们服务端的加密套件，而不是客户端浏览器的加密套件。 ssl_prefer_server_ciphers on; location / &#123; root html; index index.html index.htm; &#125;&#125; 强制将http重定向到https1234567server &#123; listen 80; server_name example.com; rewrite ^ https://$http_host$request_uri? permanent; # 强制将http重定向到https # 在错误页面和“服务器”响应头字段中启用或禁用发射nginx版本。 防止黑客利用版本漏洞攻击 server_tokens off;&#125; 两个虚拟主机纯静态-html 支持 1234567891011121314151617181920http &#123; server &#123; listen 80; server_name www.domain1.com; access_log logs/domain1.access.log main; location / &#123; index index.html; root /var/www/domain1.com/htdocs; &#125; &#125; server &#123; listen 80; server_name www.domain2.com; access_log logs/domain2.access.log main; location / &#123; index index.html; root /var/www/domain2.com/htdocs; &#125; &#125;&#125; 虚拟主机标准配置1234567891011http &#123; server &#123; listen 80 default; server_name _ *; access_log logs/default.access.log main; location / &#123; index index.html; root /var/www/default/htdocs; &#125; &#125;&#125; 爬虫过滤根据 User-Agent 过滤请求，通过一个简单的正则表达式，就可以过滤不符合要求的爬虫请求(初级爬虫)。 ~* 表示不区分大小写的正则匹配 1234567location / &#123; if ($http_user_agent ~* &quot;python|curl|java|wget|httpclient|okhttp&quot;) &#123; return 503; &#125; # 正常处理 # ...&#125; 防盗链12345678location ~* \.(gif|jpg|png|swf|flv)$ &#123; root html valid_referers none blocked *.nginxcn.com; if ($invalid_referer) &#123; rewrite ^/ www.nginx.cn #return 404; &#125;&#125; 虚拟目录配置alias指定的目录是准确的，root是指定目录的上级目录，并且该上级目录要含有location指定名称的同名目录。 12345678location /img/ &#123; alias /var/www/image/;&#125;# 访问/img/目录里面的文件时，ningx会自动去/var/www/image/目录找文件location /img/ &#123; root /var/www/image;&#125;# 访问/img/目录下的文件时，nginx会去/var/www/image/img/目录下找文件。] 防盗图配置123456location ~ \/public\/(css|js|img)\/.*\.(js|css|gif|jpg|jpeg|png|bmp|swf) &#123; valid_referers none blocked *.jslite.io; if ($invalid_referer) &#123; rewrite ^/ http://wangchujiang.com/piratesp.png; &#125;&#125; 屏蔽.git等文件123location ~ (.git|.gitattributes|.gitignore|.svn) &#123; deny all;&#125; 域名路径加不加需要都能正常访问12345http://wangchujiang.com/api/index.php?a=1&amp;name=wcj ^ 有后缀http://wangchujiang.com/api/index?a=1&amp;name=wcj ^ 没有后缀 nginx rewrite规则如下： 12345678910rewrite ^/(.*)/$ /index.php?/$1 permanent;if (!-d $request_filename)&#123; set $rule_1 1$rule_1;&#125;if (!-f $request_filename)&#123; set $rule_1 2$rule_1;&#125;if ($rule_1 = &quot;21&quot;)&#123; rewrite ^/ /index.php last;&#125; 错误问题1The plain HTTP request was sent to HTTPS port 解决办法，fastcgi_param HTTPS $https if_not_empty 添加这条规则， 1234567891011121314server &#123; listen 443 ssl; # 注意这条规则 server_name my.domain.com; fastcgi_param HTTPS $https if_not_empty; fastcgi_param HTTPS on; ssl_certificate /etc/ssl/certs/your.pem; ssl_certificate_key /etc/ssl/private/your.key; location / &#123; # Your config here... &#125;&#125;]]></content>
      <categories>
        <category>中间件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[区块链浏览器部署]]></title>
    <url>%2Fposts%2F984b410.html</url>
    <content type="text"><![CDATA[Hyperledger ExplorerHyperledger Explorer是一个简单，功能强大，易于使用，高度可维护的开源浏览器，用于查看底层区块链网络上的活动 。 postgresql安装创建用户 1useradd postgres 123456789101112131415161718注意：更新yum源，163或者阿里的yum源都可以添加RPM yum install https://download.postgresql.org/pub/repos/yum/9.5/redhat/rhel-7-x86_64/pgdg-centos95-9.5-3.noarch.rpm安装PostgreSQL 9.5 yum install postgresql95-server postgresql95-contrib初始化数据库 /usr/pgsql-9.5/bin/postgresql95-setup initdb设置开机自启动 systemctl enable postgresql-9.5.service启动服务 systemctl start postgresql-9.5.service查看服务运行状态 systemctl status postgresql-9.5.service postgreSQL 安装完成后，会建立一下‘postgres’用户，用于执行PostgreSQL，数据库中也会建立一个’postgres’用户，默认密码为自动生成，需要在系统中改一下。 修改用户密码1234su - postgres 切换用户，执行后提示符会变为 &apos;-bash-4.2$&apos;psql -U postgres 登录数据库，执行后提示符变为 &apos;postgres=#&apos;ALTER USER postgres WITH PASSWORD &apos;gooagoo&apos; 设置postgres用户密码\q 退出数据库 开启远程访问123vi /var/lib/pgsql/9.5/data/postgresql.conf修改#listen_addresses = &apos;localhost&apos; 为 listen_addresses=&apos;*&apos;当然，此处‘*’也可以改为任何你想开放的服务器IP 信任远程连接12345vi /var/lib/pgsql/9.5/data/pg_hba.conf 修改如下内容，信任指定服务器连接 # IPv4 local connections: host all all 127.0.0.1/32 trust host all all 10.211.55.6/32（需要连接的服务器IP） trust 重启1systemctl restart postgresql-9.5.service blockchain-explorer克隆存储库12345678git clone https://github.com/hyperledger/blockchain-explorer.gitcd blockchain-explorer也可以从我的百度链接上面下载：链接：https://pan.baidu.com/s/1VsxMlk5qo_5hUKsJ03DTlA 提取码：9s5f cp -apr blockchain-explorer /var/lib/pgsql/ 数据库设置1234567891011121314su - postgrescd blockchain-explorer/app修改explorerconfig.json以更新postgresql属性postgreSQL主机，端口，数据库，用户名，密码详细信息。“postgreSQL”：&#123; &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;port&quot;: &quot;5432&quot;, &quot;database&quot;: &quot;fabricexplorer&quot;, &quot;username&quot;: &quot;postgres&quot;, &quot;passwd&quot;: &quot;gooagoo&quot;&#125; 运行create database脚本12cd blockchain-explorer/app/persistence/fabric/postgreSQL/db./createdb.sh 安装node1234567tar xvf node-v11.10.0-linux-x64.tarcd node-v11.10.0-linux-x64./configure &amp;&amp; make &amp;&amp; make install装cnpm淘宝源npm install -g cnpm --registry=https://registry.npm.taobao.org npm测试12345678910cnpm installcd blockchain-explorer/app/testcnpm installcnpm run testcd client/cnpm installcnpm test -- -u --coveragecnpm run build由于config.json还没有写配置，所以node test会测试不成功 fabric的网络配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586vim blockchain-explorer/app/platform/fabric/config.json&#123; &quot;network-configs&quot;: &#123; &quot;network-1&quot;: &#123; &quot;version&quot;: &quot;1.0&quot;, &quot;clients&quot;: &#123; &quot;client-1&quot;: &#123; &quot;tlsEnable&quot;: false, &quot;organization&quot;: &quot;1D8L291SQ3QRQ80AB2M1029FB60010HCMSP&quot;, &quot;channel&quot;: &quot;vaccine&quot;, &quot;credentialStore&quot;: &#123; &quot;path&quot;: &quot;./tmp/credentialStore_Org1/credential&quot;, &quot;cryptoStore&quot;: &#123; &quot;path&quot;: &quot;./tmp/credentialStore_Org1/crypto&quot; &#125; &#125; &#125; &#125;, &quot;channels&quot;: &#123; &quot;vaccine&quot;: &#123; &quot;peers&quot;: &#123; &quot;peer0.syj.vaccine.com&quot;: &#123;&#125; &#125;, &quot;connection&quot;: &#123; &quot;timeout&quot;: &#123; &quot;peer&quot;: &#123; &quot;endorser&quot;: &quot;6000&quot;, &quot;eventHub&quot;: &quot;6000&quot;, &quot;eventReg&quot;: &quot;6000&quot; &#125; &#125; &#125; &#125; &#125;, &quot;organizations&quot;: &#123; &quot;1D8L291SQ3QRQ80AB2M1029FB60010HCMSP&quot;: &#123; &quot;mspid&quot;: &quot;1D8L291SQ3QRQ80AB2M1029FB60010HCMSP&quot;, &quot;fullpath&quot;: false, &quot;adminPrivateKey&quot;: &#123; &quot;path&quot;: &quot;/data/fabric/fabric-ca-files/vaccine-org/syj.vaccine.com/admin/msp/keystore&quot; &#125;, &quot;signedCert&quot;: &#123; &quot;path&quot;: &quot;/data/fabric/fabric-ca-files/vaccine-org/syj.vaccine.com/admin/msp/signcerts&quot; &#125; &#125;, &quot;1D9K2HVDM752IN0AB2M105R9Q5001125MSP&quot;: &#123; &quot;mspid&quot;: &quot;1D9K2HVDM752IN0AB2M105R9Q5001125MSP&quot;, &quot;fullpath&quot;: false, &quot;adminPrivateKey&quot;: &#123; &quot;path&quot;: &quot;/data/fabric/fabric-ca-files/vaccine-org/czsrmyy.czsjkzx.hbsjkzx.vaccine.com/admin/msp/keystore&quot; &#125;, &quot;signedCert&quot;: &#123; &quot;path&quot;: &quot;/data/fabric/fabric-ca-files/vaccine-org/czsrmyy.czsjkzx.hbsjkzx.vaccine.com/admin/msp/signcerts&quot; &#125; &#125;, &quot;OrdererMSP&quot;: &#123; &quot;mspid&quot;: &quot;OrdererMSP&quot;, &quot;adminPrivateKey&quot;: &#123; &quot;path&quot;: &quot;/data/fabric/fabric-ca-files/vaccine-order/vaccine.syj.vaccine.com/admin/msp/keystore&quot; &#125; &#125; &#125;, &quot;peers&quot;: &#123; &quot;peer0.syj.vaccine.com&quot;: &#123; &quot;url&quot;: &quot;grpc://peer0.syj.vaccine.com:7051&quot;, &quot;eventUrl&quot;: &quot;grpc://peer0.syj.vaccine.com:7053&quot;, &quot;grpcOptions&quot;: &#123; &quot;ssl-target-name-override&quot;: &quot;peer0.syj.vaccine.com&quot; &#125; &#125;, &quot;peer0.czsrmyy.czsjkzx.hbsjkzx.vaccine.com&quot;: &#123; &quot;url&quot;: &quot;grpc://peer0.czsrmyy.czsjkzx.hbsjkzx.vaccine.com:7351&quot; &#125; &#125;, &quot;orderers&quot;: &#123; &quot;orderer1.vaccine.syj.vaccine.com&quot;: &#123; &quot;url&quot;: &quot;grpc://orderer1.vaccine.syj.vaccine.com:7050&quot; &#125; &#125; &#125;, &quot;network-2&quot;: &#123;&#125; &#125;, &quot;configtxgenToolPath&quot;: &quot;/data/fabric/bin&quot;, &quot;license&quot;: &quot;Apache-2.0&quot;&#125; 完成之后，在执行上面的，cnpm test 汉化1234cd /var/lib/pgsql/blockchain-explorer/client/src将components替换成我百度链接的包之后还需要重新，cnpm build一下 注意1234由于区块链浏览器默认需要有锚节点，才能显示所有节点，所以在部署了第一个组织之后，需要在升级下锚节点./bin/configtxgen -profile TwoOrgsChannel -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID vaccine -asOrg Org1MSPdocker exec -it cli-vaccine peer channel update -o orderer1.vaccine.syj.vaccine.com:7050 -c vaccine -f ./channel-artifacts/Org1MSPanchors.tx 启动12cd blockchain-explorer/./start.sh 访问1ip:8080]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python监控es状态]]></title>
    <url>%2Fposts%2F71abe607.html</url>
    <content type="text"><![CDATA[需求用python写一个监控es状态的脚本 实例监控es的机器1234567891011配置文件[root@Ops-script monitor_elasticsearch]# cat escluster_ip.ini[mail]name = sy@xxx.com[tax_es]cluster_ip = 192.168.50.7:9200,192.168.50.8:9200,192.168.50.9:9200name_pass = none,none[analysis_es]cluster_ip = 192.168.50.24:9200,192.168.50.25:9200name_pass = none,none 脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@Ops-script monitor_elasticsearch]# cat monitor_elastic_cluster.pyimport urllib,socket,time,jsonimport logging,requests,configparser,os# 定义日志格式读取配置文件cur_path = os.path.dirname(os.path.realpath(__file__))logging.basicConfig(format=&apos;[ %(asctime)s ] -- %(message)s&apos;, datefmt=&apos;%a, %d %b %Y %H:%M:%S&apos;, filename=cur_path+&quot;/scripts.log&quot;, level=logging.INFO)config_path = os.path.join(cur_path,&quot;escluster_ip.ini&quot;)conf = configparser.ConfigParser()conf.read(config_path)esgroup = conf.sections()def GetClusterIp(): # 按集群名循环检查,集群名为配置文件中的标题 for group in esgroup: # 获取到 mail 中的邮件地址不再继续循环 if group == &quot;mail&quot;: mailname = conf.get(group,&quot;name&quot;) continue # 获取到一个集群 ip 后进行 get 请求 iplist = conf.get(group,&quot;cluster_ip&quot;).split(&quot;,&quot;) for esip in iplist: url = &quot;http://%s/_cat/health&quot;%esip try: # 超时时间为 3 秒, request 的请求值不是200的将全部置为400 # 如果是请求成功,则获取 status 的状态和 status_num 的百分比 response = requests.get(url,timeout=3) request = response.status_code status = response.text.split()[3] status_num = response.text.split()[-1] except: request = 400 # 请求值为 200 后,检查 status 值非 green 状态发送报警邮件,并不在继续检查本集群内的剩余节点 if request == 200: if status != &quot;green&quot;: mailtitle = &quot;elasticsearch集群 [ %s ], ip为:%s ,查状态: %s, 状态百分比: %s&quot;%(group,esip,status,status_num) mailtxt = &quot;elasticsearch集群 [ %s ]\n\nip为: [ %s ]\n\n检查状态为: %s\n\n状态百分比: %s&quot;%(group,esip,status,status_num) command = &quot;echo -e %s%s%s | mail -s %s%s%s %s&quot;%(&apos;&quot;&apos;,mailtxt,&apos;&quot;&apos;,&apos;&quot;&apos;,mailtitle,&apos;&quot;&apos;,mailname) logging.info(mailtitle) os.system(command) break # 请求值不是 200 ,报警后继续检查集群内剩余 ip else: mailtitle = &quot;elasticsearch集群 [ %s ], ip为:%s, 请求超时,请检查端口&quot;%(group,esip) mailtxt = &quot;elasticsearch集群 [ %s ]\n\nip: [ %s ]\n\n请求超时,请检查端口&quot;%(group,esip) command = &quot;echo -e %s%s%s | mail -s %s%s%s %s&quot;%(&apos;&quot;&apos;,mailtxt,&apos;&quot;&apos;,&apos;&quot;&apos;,mailtitle,&apos;&quot;&apos;,mailname) logging.info(mailtitle) os.system(command)if __name__ == &quot;__main__&quot;: GetClusterIp()]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[钉钉定时发送值班人员]]></title>
    <url>%2Fposts%2F59cdc228.html</url>
    <content type="text"><![CDATA[需求每天运维人员都需要去做些基础服务，就需要值班人员去轮班解决，现在需要写一个定时发送值班人员的脚本 前提需要自己在钉钉群，申请个机器人，申请过程这里不赘述了，下面是脚本 脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[root@Ops-script dingding]# mkdir /home/monitor/dingding[root@Ops-script dingding]# touch groupkey helpkey[root@Ops-script dingding]# cat send_dingding.sh #!/bin/bashgroupfiles=&quot;/home/monitor/dingding/groupkey&quot;helpfiles=&quot;/home/monitor/dingding/helpkey&quot;Date=`date +%Y-%m-%d\ %H:%M:%S`url=&quot;https://oapi.dingtalk.com/robot/send?access_token=c2123f81820fccfadfc47bbd629d26e7613ae49f1a053edc6e81f5864c550e30&quot;group=(&quot;a:xx;&quot; &quot;b:xx;&quot; &quot;c:xx;&quot;)opshelp=(&quot;a:xx;&quot; &quot;b:xx;&quot; &quot;c:xx;&quot;)groupkey=`sed -n &quot;1p&quot; $groupfiles`helpkeys=`awk &apos;NR==1&#123;print $1&#125;&apos; $helpfiles`helpkey=`awk &apos;NR==1&#123;print $2&#125;&apos; $helpfiles`# 每日值班人for crew in $&#123;group[@]&#125;;do if echo $crew | grep -q $groupkey ;then values=`echo $crew | awk -F&apos;:&apos; &apos;&#123;print $2&#125;&apos;` onduty_mess=&quot;今日运维值班人: [ $values ]&quot; fidone# 修改缓存文件内的运维值班人员 key , 使得下次人员自动更换if [ $groupkey == &quot;a&quot; ];then echo &quot;b&quot; &gt; $groupfileselif [ $groupkey == &quot;b&quot; ];then echo &quot;c&quot; &gt; $groupfileselif [ $groupkey == &quot;c&quot; ];then echo &quot;a&quot; &gt; $groupfilesfi# 修改缓存文件内的运维上线人员 key , 使得下次人员自动更换if [ $helpkeys == 7 ];then helpsum=1else helpsum=$(($helpkeys+1))fiif [ $helpsum == 1 ];then if [ $helpkey == &quot;a&quot; ];then echo &quot;$helpsum b&quot; &gt; $helpfiles elif [ $helpkey == &quot;b&quot; ];then echo &quot;$helpsum c&quot; &gt; $helpfiles elif [ $helpkey == &quot;c&quot; ];then echo &quot;$helpsum a&quot; &gt; $helpfiles fielse echo &quot;$helpsum $helpkey&quot; &gt; $helpfilesfi# 每周支持上线人for opsdit in $&#123;opshelp[@]&#125;;do if [ $helpsum == 1 ];then helpkey=`awk &apos;NR==1&#123;print $2&#125;&apos; $helpfiles` fi if echo $opsdit | grep -q $helpkey ;then helpvalues=`echo $opsdit | awk -F&apos;:&apos; &apos;&#123;print $2&#125;&apos;` help_mess=&quot;本周版本上线运维支持: [ $helpvalues ]&quot; fidonecurl -XPOST -s -L -H &quot;Content-Type:application/json&quot; -H &quot;charset:utf-8&quot; $url -d &quot; &#123; \&quot;msgtype\&quot;: \&quot;text\&quot;, \&quot;text\&quot;: &#123; \&quot;content\&quot;: \&quot;大家好~\n$onduty_mess\n$help_mess\&quot; &#125; &#125;&quot;]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker存储驱动]]></title>
    <url>%2Fposts%2F12280181.html</url>
    <content type="text"><![CDATA[摘要Docker最开始采用AUFS作为文件系统，也得益于AUFS分层的概念，实现了多个Container可以共享同一个image。但由于AUFS未并入Linux内核，且只支持Ubuntu，考虑到兼容性问题，在Docker 0.7版本中引入了存储驱动。 目前，Docker支持的存储驱动：aufs，devicemapper，btrfs，zfs，overlay和overlay2。 就如Docker官网上说的，没有单一的驱动适合所有的应用场景，要根据不同的场景选择合适的存储驱动，才能有效的提高Docker的性能。如何选择适合的存储驱动，要先了解存储驱动原理才能更好的判断 写时复制（CoW）所有驱动都用到的技术——写时复制（CoW）。CoW就是copy-on-write，表示只在需要写时才去复制，这个是针对已有文件的修改场景。比如基于一个image启动多个Container，如果为每个Container都去分配一个image一样的文件系统，那么将会占用大量的磁盘空间。而CoW技术可以让所有的容器共享image的文件系统，所有数据都从image中读取，只有当要对文件进行写操作时，才从image里把要写的文件复制到自己的文件系统进行修改。所以无论有多少个容器共享同一个image，所做的写操作都是对从image中复制到自己的文件系统中的复本上进行，并不会修改image的源文件，且多个容器操作同一个文件，会在每个容器的文件系统里生成一个复本，每个容器修改的都是自己的复本，相互隔离，相互不影响。使用CoW可以有效的提高磁盘的利用率。 用时分配（allocate-on-demand）而写时分配是用在原本没有这个文件的场景，只有在要新写入一个文件时才分配空间，这样可以提高存储资源的利用率。比如启动一个容器，并不会为这个容器预分配一些磁盘空间，而是当有新文件写入时，才按需分配新空间。 AUFSAUFS（AnotherUnionFS）是一种Union FS，是文件级的存储驱动。AUFS能透明覆盖一或多个现有文件系统的层状文件系统，把多层合并成文件系统的单层表示。简单来说就是支持将不同目录挂载到同一个虚拟文件系统下的文件系统。这种文件系统可以一层一层地叠加修改文件。无论底下有多少层都是只读的，只有最上层的文件系统是可写的。当需要修改一个文件时，AUFS创建该文件的一个副本，使用CoW将文件从只读层复制到可写层进行修改，结果也保存在可写层。在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示： OverlayOverlay是Linux内核3.18后支持的，也是一种Union FS，和AUFS的多层不同的是Overlay只有两层：一个upper文件系统和一个lower文件系统，分别代表Docker的镜像层和容器层。当需要修改一个文件时，使用CoW将文件从只读的lower复制到可写的upper进行修改，结果也保存在upper层。在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示： Device mapperDevice mapper是Linux内核2.6.9后支持的，提供的一种从逻辑设备到物理设备的映射框架机制，在该机制下，用户可以很方便的根据自己的需要制定实现存储资源的管理策略。前面讲的AUFS和OverlayFS都是文件级存储，而Device mapper是块级存储，所有的操作都是直接对块进行操作，而不是文件。Device mapper驱动会先在块设备上创建一个资源池，然后在资源池上创建一个带有文件系统的基本设备，所有镜像都是这个基本设备的快照，而容器则是镜像的快照。所以在容器里看到文件系统是资源池上基本设备的文件系统的快照，并不有为容器分配空间。当要写入一个新文件时，在容器的镜像内为其分配新的块并写入数据，这个叫用时分配。当要修改已有文件时，再使用CoW为容器快照分配块空间，将要修改的数据复制到在容器快照中新的块里再进行修改。Device mapper 驱动默认会创建一个100G的文件包含镜像和容器。每一个容器被限制在10G大小的卷内，可以自己配置调整。结构如下图所示： BtrfsBtrfs被称为下一代写时复制文件系统，并入Linux内核，也是文件级级存储，但可以像Device mapper一直接操作底层设备。Btrfs把文件系统的一部分配置为一个完整的子文件系统，称之为subvolume 。那么采用 subvolume，一个大的文件系统可以被划分为多个子文件系统，这些子文件系统共享底层的设备空间，在需要磁盘空间时便从底层设备中分配，类似应用程序调用 malloc()分配内存一样。为了灵活利用设备空间，Btrfs 将磁盘空间划分为多个chunk 。每个chunk可以使用不同的磁盘空间分配策略。比如某些chunk只存放metadata，某些chunk只存放数据。这种模型有很多优点，比如Btrfs支持动态添加设备。用户在系统中增加新的磁盘之后，可以使用Btrfs的命令将该设备添加到文件系统中。Btrfs把一个大的文件系统当成一个资源池，配置成多个完整的子文件系统，还可以往资源池里加新的子文件系统，而基础镜像则是子文件系统的快照，每个子镜像和容器都有自己的快照，这些快照则都是subvolume的快照。 当写入一个新文件时，为在容器的快照里为其分配一个新的数据块，文件写在这个空间里，这个叫用时分配。而当要修改已有文件时，使用CoW复制分配一个新的原始数据和快照，在这个新分配的空间变更数据，变结束再更新相关的数据结构指向新子文件系统和快照，原来的原始数据和快照没有指针指向，被覆盖。 ZFSZFS 文件系统是一个革命性的全新的文件系统，它从根本上改变了文件系统的管理方式，ZFS 完全抛弃了“卷管理”，不再创建虚拟的卷，而是把所有设备集中到一个存储池中来进行管理，用“存储池”的概念来管理物理存储空间。过去，文件系统都是构建在物理设备之上的。为了管理这些物理设备，并为数据提供冗余，“卷管理”的概念提供了一个单设备的映像。而ZFS创建在虚拟的，被称为“zpools”的存储池之上。每个存储池由若干虚拟设备（virtual devices，vdevs）组成。这些虚拟设备可以是原始磁盘，也可能是一个RAID1镜像设备，或是非标准RAID等级的多磁盘组。于是zpool上的文件系统可以使用这些虚拟设备的总存储容量。 下面看一下在Docker里ZFS的使用。首先从zpool里分配一个ZFS文件系统给镜像的基础层，而其他镜像层则是这个ZFS文件系统快照的克隆，快照是只读的，而克隆是可写的，当容器启动时则在镜像的最顶层生成一个可写层。如下图所示： 当要写一个新文件时，使用按需分配，一个新的数据快从zpool里生成，新的数据写入这个块，而这个新空间存于容器（ZFS的克隆）里。当要修改一个已存在的文件时，使用写时复制，分配一个新空间并把原始数据复制到新空间完成修改。 overlay2OverlayFS将Linux主机上的两个单独目录分层，并将它们显示为一个目录。这些目录称为层，统一过程称为联合安装。OverlayFS指向一个upper文件系统和一个lower文件系统，分别代表Docker的镜像层和容器层。用统一视图将整合的目录公开。 该overlay2驱动程序原生支持多达128个较低的OverlayFS层。此功能为与层相关的Docker命令（如docker build和docker commit）提供了更好的性能，并且大量减少了inode的消耗。 对比 存储驱动 简介 优点 缺点 存储级别 场景 aufs 最古老的联合文件系统，没有被内核收录，只支持ubuntu 允许容器共享可执行文件和共享内存，历史悠久，使用广泛 会导致一些严重的内核崩溃，多层，在CoW时如果文件大且在低层会慢一些 文件级存储 大并发少IO devicemapper 自动创建的稀疏文件的loop挂载后，自动创建块设备 精简配置和写时复制（CoW）快照技术，只复制修改的块 不支持共享存储，多个容器读同一个文件复制多份，容器启停可能会有磁盘溢出 块级存储 IO密集场景 btrfs 和devicemapper一样操作底层设备 非常快，支持动态添加设备 设备之间不共享可执行内存 文件级块存储 不适合高密度容器的paas平台 zfs 支持多个容器共享一个缓存块，适合大内存场景 CoW使碎片化问题更严重，文件在磁盘上物理地址不连续，顺序读性能差 所有设备集中到一个共享池里面进行管理 Paas平台和高密度场景 overlay 联合文件系统，内核版本3.18.0开始合并到内核中，只有两层 非常快速的联合文件系统。还支持页面缓存共享，这意味着访问同一文件的多个容器可以共享单个页面缓存条目（或条目），如aufs一样高效 会导致过多的inode消耗，不管修改内容大小都会复制整个文件，修改大文件消耗时间长 文件级存储 大并发少IO overlay2 内核版本4.0有附加功能，避免过多的inode消耗 文件级存储 大并发少IO AUFS VS OverlayAUFS和Overlay都是联合文件系统，但AUFS有多层，而Overlay只有两层，所以在做写时复制操作时，如果文件比较大且存在比较低的层，则AUSF可能会慢一些。而且Overlay并入了linux kernel mainline，AUFS没有，所以可能会比AUFS快。但Overlay还太年轻，要谨慎在生产使用。而AUFS做为docker的第一个存储驱动，已经有很长的历史，比较的稳定，且在大量的生产中实践过，有较强的社区支持。目前开源的DC/OS指定使用Overlay。 Overlay VS Device mapperOverlay是文件级存储，Device mapper是块级存储，当文件特别大而修改的内容很小，Overlay不管修改的内容大小都会复制整个文件，对大文件进行修改显示要比小文件要消耗更多的时间，而块级无论是大文件还是小文件都只复制需要修改的块，并不是整个文件，在这种场景下，显然device mapper要快一些。因为块级的是直接访问逻辑盘，适合IO密集的场景。而对于程序内部复杂，大并发但少IO的场景，Overlay的性能相对要强一些。 Device mapper VS Btrfs Driver VS ZFSDevice mapper和Btrfs都是直接对块操作，都不支持共享存储，表示当有多个容器读同一个文件时，需要生活多个复本，所以这种存储驱动不适合在高密度容器的PaaS平台上使用。而且在很多容器启停的情况下可能会导致磁盘溢出，造成主机不能工作。Device mapper不建议在生产使用。Btrfs在docker build可以很高效。 ZFS最初是为拥有大量内存的Salaris服务器设计的，所在在使用时对内存会有影响，适合内存大的环境。ZFS的COW使碎片化问题更加严重，对于顺序写生成的大文件，如果以后随机的对其中的一部分进行了更改，那么这个文件在硬盘上的物理地址就变得不再连续，未来的顺序读会变得性能比较差。ZFS支持多个容器共享一个缓存块，适合PaaS和高密度的用户场景。 IO性能对比 测试工具：IOzone（是一个文件系统的benchmark工具，可以测试不同的操作系统中文件系统的读写性能） 测试场景：从4K到1G文件的顺序和随机IO性能 测试方法：基于不同的存储驱动启动容器，在容器内安装IOzone，执行命令： 1./iozone -a -n 4k -g 1g -i 0 -i 1 -i 2 -f /root/test.rar -Rb ./iozone.xls 测试项的定义和解释 Write：测试向一个新文件写入的性能。 Re-write：测试向一个已存在的文件写入的性能。 Read：测试读一个已存在的文件的性能。 Re-Read：测试读一个最近读过的文件的性能。 Random Read：测试读一个文件中的随机偏移量的性能。 Random Write：测试写一个文件中的随机偏移量的性能。 通过以上的性能数据可以看到：AUFS在读的方面性能相比Overlay要差一些，但在写的方面性能比Overlay要好。device mapper在512M以上文件的读写性能都非常的差，但在512M以下的文件读写性能都比较好。btrfs在512M以上的文件读写性能都非常好，但在512M以下的文件读写性能相比其他的存储驱动都比较差。ZFS整体的读写性能相比其他的存储驱动都要差一些。 简单的测试了一些数据，对测试出来的数据原理还需要进一步的解析。 参数devicemapper12345678&#123; &quot;storage-driver&quot;: &quot;devicemapper&quot;, &quot;storage-opts&quot;: [ &quot;dm.thinpooldev=/dev/mapper/thin-pool&quot;, &quot;dm.use_deferred_deletion=true&quot;, &quot;dm.use_deferred_removal=true&quot; ]&#125; overlay2overlay2需要使用4.0以上版本的内核，如果使用的是RHEL或CentOS，需要3.10.0-514以上版本的内核 12345# 查看是否开启overlaylsmod |grep over# 开启overlay支持modprobe overlay 配置 1234567&#123; &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;storage-opts&quot;: [ &quot;overlay2.override_kernel_check=true&quot; #&quot;overlay2.size=1G&quot;, # xfs文件系统 ]&#125;]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes节点资源耗尽状态的处理]]></title>
    <url>%2Fposts%2Fa54106c5.html</url>
    <content type="text"><![CDATA[最近发现测试环境的k8s集群，总有node利用不上，pod漂移过去之后，启动不了，故仔细排查了一下缘由！ 问题现象12345678[root@master35 scripts]# ./list_pod.sh | grep imisimis-866d46c464-nvz4b 0/1 ContainerCreating 0 3m &lt;none&gt; node149发现有的pod无法启动，刚开始describe查了下原因，看到，一直在拉镜像状态中，但是3分钟了，也不至于镜像拉不下来啊！查看了下node149的状态，发现Warning: “EvictionThresholdMet Attempting to reclaim nodefs”发现大概应该是由于磁盘原因造成的，也可以看下kubelet日志，也会报这个类似的错误 原因分析12345678[root@node149 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/cl-root 36G 30G 6G 86% /devtmpfs 7.8G 0 7.8G 0% /devtmpfs 7.8G 0 7.8G 0% /dev/shmtmpfs 7.8G 9.3M 7.8G 1% /runtmpfs 7.8G 0 7.8G 0% /sys/fs/cgroup/dev/sda1 1014M 186M 829M 19% /boot 1由于这是测试环境，所以docker的目录，默认在/var/lib/docker，没有单独挂载别的目录，这样的话，也没加定时任务清理磁盘，/ 磁盘就会越来越满，现在看是用了86% 由于某些原因，我们的那个portal pod必须运行于该node上（通过nodeSelector选定node的方式）。在无法扩充根分区size的情况下，为了临时恢复pod运行，我们只能进一步“压榨”node了。于是我们的思路是：通过调整node的eviction threshold值来让node恢复healthy。 解决方案每个node上的kubelet都负责定期采集资源占用数据，并与预设的 threshold值进行比对，如果超过 threshold值，kubelet就会尝试杀掉一些Pod以回收相关资源，对Node进行保护。kubelet关注的资源指标threshold大约有如下几种： 12345- memory.available- nodefs.available- nodefs.inodesFree- imagefs.available- imagefs.inodesFree 每种threshold又分为eviction-soft和eviction-hard两组值。soft和hard的区别在于前者在到达threshold值时会给pod一段时间优雅退出，而后者则崇尚“暴力”，直接杀掉pod，没有任何优雅退出的机会。这里还要提一下nodefs和imagefs的区别： 12nodefs: 指node自身的存储，存储daemon的运行日志等，一般指root分区/；imagefs: 指docker daemon用于存储image和容器可写层(writable layer)的磁盘； 解决步骤我们需要为kubelet重新设定nodefs.available的threshold值。怎么做呢？ kubelet是运行于每个kubernetes node上的daemon，它在system boot时由systemd拉起: 123root@master35 ~# ps -ef|grep kubeletroot 5718 5695 0 16:38 pts/3 00:00:00 grep --color=auto kubeletroot 13640 1 4 10:25 ? 00:17:25 /usr/bin/kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --require-kubeconfig=true --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --cluster-dns=10.96.0.10 --cluster-domain=cluster.local --authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt --cadvisor-port=0 查看一下kubelet service的状态： 123456789101112131415[root@master35 scripts]# systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Thu 2018-07-19 21:04:35 CST; 8 months 29 days ago Docs: http://kubernetes.io/docs/ Main PID: 1921 (kubelet) Tasks: 19 Memory: 54.9M CGroup: /system.slice/kubelet.service └─1921 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --pod-manifest-path=...Apr 14 09:26:16 master35 kubelet[1921]: W0414 09:26:16.673359 1921 reflector.go:341] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: watch o...(56737582)Apr 15 06:36:48 master35 kubelet[1921]: W0415 06:36:48.938194 1921 reflector.go:341] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: watch o...(56940044) 我们定义一个新的Environment var，比如就叫：KUBELET_EVICTION_POLICY_ARGS 在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf 123Environment=&quot;KUBELET_EVICTION_POLICY_ARGS=--eviction-hard=nodefs.available&lt;5%&quot;ExecStart=ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_EXTRA_ARGS $KUBELET_EVICTION_POLICY_ARGS 这样控制，node的磁盘策略为&lt;5%的硬盘就可以用，不像之前默认的15%就用不了了！]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[wordpress快速安装]]></title>
    <url>%2Fposts%2F5842d4c0.html</url>
    <content type="text"><![CDATA[yum安装lnmp环境安装前准备123456789# 配置阿里云 yum 仓库$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo$ wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo$ yum clean all$ yum makecache# 配置时间同步$ vim /etc/crontab00 00 * * * root /sbin/ntpdate ntp.aliyun.com &amp;&gt;/dev/null 配置 nginx repo1234567891011121314$ vim /etc/yum.repos.d/nginx.repo[nginx-stable]name=nginx stable repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.key[nginx-mainline]name=nginx mainline repobaseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/gpgcheck=1enabled=0gpgkey=https://nginx.org/keys/nginx_signing.key yum安装lnmp1$ yum -y install nginx mariadb-server php php-bcmath php-fpm php-gd php-json php-mbstring php-mcrypt php-mysqlnd php-opcache php-pdo php-pdo_dblib php-pgsql php-recode php-snmp php-soap php-xml php-pecl-zip 启动php和mariadb123456# 启动 PHP-FPM$ systemctl enable php-fpm$ systemctl start php-fpm# 启动 mariadb$ systemctl enable mariadb.service$ systemctl start mariadb.service 创建 wordpress 数据库12345678# 连接数据库，默认密码为空mysql -uroot -p# 创建wordpress数据库名为 wpcreate database wp;# 创建数据库用户，用户名: blog 密码：123456grant all privileges on wp.* to &apos;blog&apos;@&apos;127.0.0.1&apos; identified by &apos;123456&apos;;# 刷新授权flush privileges; 配置nginx虚拟主机123456789101112131415161718192021$ vim /etc/nginx/conf.d/blog.confserver &#123; listen 80; server_name 10.100.4.169; index index.html index.php; # 访问日志目录 access_log /var/log/nginx/blog_access.log main; # 网站根目录 root /data/www; location / &#123; root /data/www; &#125; location ~ \.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125; 配置wordpress下载最新版wordpress1$ wget https://cn.wordpress.org/latest-zh_CN.tar.gz 配置wordpress连接数据库1234567891011121314151617181920212223$ tar xf latest-zh_CN.tar.gz$ mv wordpress/ /data/www$ cd /data/www/$ cp wp-config-sample.php wp-config.php$ vim wp-config.php// ** MySQL 设置 - 具体信息来自您正在使用的主机 ** ///** WordPress数据库的名称 */define(&apos;DB_NAME&apos;, &apos;wp&apos;);/** MySQL数据库用户名 */define(&apos;DB_USER&apos;, &apos;blog&apos;);/** MySQL数据库密码 */define(&apos;DB_PASSWORD&apos;, &apos;123456&apos;);/** MySQL主机 */define(&apos;DB_HOST&apos;, &apos;127.0.0.1&apos;);/** 创建数据表时默认的文字编码 */define(&apos;DB_CHARSET&apos;, &apos;utf8&apos;);/** 数据库整理类型。如不确定请勿更改 */define(&apos;DB_COLLATE&apos;, &apos;&apos;); 启动 nginx123$ systemctl enable nginx$ systemctl start nginx$ ps -ef|grep nginx 访问wordpressnginx 启动后我们就可以在浏览器通过 IP 地址访问 WordPress 了，首先会让我们给博客起个名字，名设置管理员的账号密码，点击安装 WordPress 就完成了。]]></content>
      <categories>
        <category>中间件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[多级缓存]]></title>
    <url>%2Fposts%2F9c74e32c.html</url>
    <content type="text"><![CDATA[每一级缓存的意义时效性高的数据：采取DB和redis缓存双写方案 时效性不高的数据：采取nginx本地缓存+redis分布式缓存+tomcat堆缓存的多级缓存架构 a: nginx本地缓存，抗的是热数据的高并发访问。利用nginx本地缓存，将热数据锁定在nginx的本地缓存内，那么对这些热数据的大量访问，就直接走nginx就可以，不需要走后续的各种网络开销了。 b: redis分布式大规模缓存，抗的是很高的离散访问，支撑海量的数据，高并发的访问，高可用的服务。最完整的数据和缓存。 c: tomcat jvm堆内存缓存，主要是抗redis大规模灾难的，如果redis出现了大规模的宕机，导致nginx大量流量直接涌入数据生产服务，那么最后的tomcat堆内存缓存至少可以再抗一下，不至于让数据库直接裸奔， 同时tomcat jvm堆内存缓存，也可以抗住redis没有cache住的最后那少量的部分缓存。 RedisRedis Cluster通过master的水平扩容，来横向扩展读写吞吐量，还有支撑更多的海量数据 redis cluster 高可用性：redis cluster 提供主备切换。slave做master的热备，一旦master故障。slave提升为master，对外提供服务，保证集群的高可用性。并且，当master恢复后，会作为 slave加入到集群中。 redis cluster水平扩容master的水平扩容，来横向扩展读写吞吐量，还有支撑更多的海量数据 slave 自动迁移为redis cluster 添加冗余slave redis性能（需根据机器配置测试） redis单机，读吞吐是5w/s，写吞吐2w/s 扩展redis更多master，那么如果有5台master，不就读吞吐可以达到总量25w/s QPS，写可以达到10w/s QPS redis单机，内存，6G-8G，内存不易过大fork类操作的时候很耗时，会导致请求延时的问题。扩容到5台master，能支撑的总的缓存数据量就是30G Cache Aside模式12（1）读的时候，先读缓存，缓存没有的话，那么就读数据库，然后取出数据后放入缓存，同时返回响应（2）更新的时候，先删除缓存，然后再更新数据库 DB和缓存双写不一致问题以及解决方案 缓存不一致场景一： 1234 解决思路：先删除缓存，再修改数据库，如果删除缓存成功了，如果修改数据库失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中 缓存不一致场景二： 12345678910111213141516 解决思路： a：更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的队列中 b:读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中 c:一个队列对应一个工作线程 d:每个工作线程串行拿到对应的操作，然后一条一条的执行这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成 e: 多个更新缓存请求处理：这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值 f:多个读请求，进行读请求过滤：对一个商品的库存的数据库更新操作已经在内存队列中了然后对这个商品的库存的读取操作，要求读取数据库的库存数据，然后更新到缓存中，多个读这多个读，其实只要有一个读请求操作压到队列里就可以了其他的读操作，全部都wait那个读请求的操作，刷新缓存，就可以读到缓存中的最新数据了如果读请求发现redis缓存中没有数据，就会发送读请求给库存服务，但是此时缓存中为空，可能是因为写请求先删除了缓存，也可能是数据库里压根儿没这条数据如果是数据库中压根儿没这条数据的场景，那么就不应该将读请求操作给压入队列中，而是直接返回空就可以了 大value缓存的全量更新效率低下问题缓存数据的维度化拆分 缓存数据生产服务工作流程 （1）监听多个kafka topic，每个kafka topic对应一个服务（简化一下，监听一个kafka topic） （2）如果一个服务发生了数据变更，那么就发送一个消息到kafka topic中 （3）缓存数据生产服务监听到了消息以后，就发送请求到对应的服务中调用接口以及拉取数据，此时是从mysql中查询的 （4）缓存数据生产服务拉取到了数据之后，会将数据在本地缓存中写入一份，就是ehcache中 ​ 同时会将数据在redis中写入一份 缓存并发重建冲突解决方案 重建缓存：比如数据在所有的缓存中都不存在了（LRU算法弄掉了），就需要重新查询数据写入缓存，重建缓存 123456缓存重建存在的问题一：缓存数据生产服务在多个机器节点上部署了多个实例 若没有缓存数据。12:00的时候发来一个读请求 12:01发来一个读请求（此时12:00的读请求由于网络延迟还未执行完）。12:01请求比12:00的请求执行速度快。更新了生产服务的数据并将数据写入缓存。写完后。12:00的请求将数据写入了缓存。那么此时生产服务的最新数据是12：01的，但是缓存中是服务数据是12:00的。数据不一致。解决思路：对请求的数据ID 进行hash，让对同一个数据的请求落在同一个服务实例上 123456789缓存重建存在的问题二：生产服务发送的变更消息到kafka。由于问题一解决方案中的hash算法与kafka分区策略不一致。数据变更的消息所到的缓存服务实例，跟请求分发到的那个缓存服务实例也许就不在一台机器上了 （1）变更缓存重建以及空缓存请求重建，更新redis之前，都需要先获取对应商品id的分布式锁（2）拿到分布式锁之后，需要根据时间版本去比较一下，如果自己的版本新于redis中的版本，那么就更新，否则就不更新（3）如果拿不到分布式锁，那么就等待，不断轮询等待，直到自己获取到分布式的锁 缓存雪崩问题12345678910111213缓存雪崩产生场景：1、redis集群彻底崩溃2、缓存服务大量对redis的请求hang住，占用资源3、缓存服务大量的请求打到源头服务去查询mysql，直接打死mysql4、源头服务因为mysql被打死也崩溃，对源服务的请求也hang住，占用资源5、缓存服务大量的资源全部耗费在访问redis和源服务无果，最后自己被拖死，无法提供服务6、nginx无法访问缓存服务，redis和源服务，只能基于本地缓存提供服务，但是缓存过期后，没有数据提供解决思路 1、对redis访问做资源隔离 2、若redis集群崩溃，对redis进行熔断 3、对源服务的访问做限流 4、限流失败后采用stubbed fallback降级机制 缓存穿透问题每次如果从生产查询到的数据是空，就说明这个数据根本就不存在 那么如果这个数据不存在的话，我们不要不往redis和ehcache等缓存中写入数据，我们呢，给写入一个空的数据，比如说空的productInfo的json串 因为我们有一个异步监听数据变更的机制在里面，也就是说，如果数据变更的话，某个数据本来是没有的，可能会导致缓存穿透，所以我们给了个空数据 但是现在这个数据有了，我们接收到这个变更的消息过后，就可以将数据再次从生产服务中查询出来 然后设置到各级缓存中去了 缓存失效问题设置随机的缓存失效时间]]></content>
      <categories>
        <category>中间件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[fabric分布式部署]]></title>
    <url>%2Fposts%2Fe0275bab.html</url>
    <content type="text"><![CDATA[Kafka模式简介Hyperledger Fabric采用kafka方式实现排序（orderer）服务的集群，kafka模块被认为是半中心化结构。顺便提一下，去中心化的BFT（拜占庭容错）排序（orderer）服务集群方式目前还在开发，还没有规定发布时间，将在1.x周期内发布，可以关注跟踪FAB-33的更新。 Kafka模式由排序（orderer）服务、kafka集群和zookeeper集群组成。每个排序(orderer)服务相互之间不通信，只与kafka集群通信，kafka集群与zookeeper相互连接。 Fabric网络中的各节点（Peer）收到客户端发送的交易请求时，把交易信息发送给与其连接的排序（orderer）服务，交由排序（orderer）服务集群进行排序处理。 配置 orderer1.example.com,kafka1,zookeeper1 192.168.3.98 orderer1.example.com,kafka1,zookeeper1 192.168.3.97 orderer1.example.com,kafka1,zookeeper1 192.168.3.94 peer0.org1.example.com 192.168.10.174 peer1.org1.example.com 192.168.10.173 peer0.org2.example.com 192.168.3.93 安装在六台机器上安装依赖工具docker、go、fabric源码 docker就不多说了，17,03以上就可以，go可以yum安装 123456789101112131415161718192021222324252627安装fabric源码:git clone https://github.com/hyperledger/fabric.gitcd fabricgit checkout v1.4.0拉镜像：# mkdir -p /etc/docker# tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;&#123;&quot;registry-mirrors&quot;: [&quot;https://8w1wqmsz.mirror.aliyuncs.com&quot;]&#125;EOF# systemctl daemon-reload# systemctl restart docker# docker pull hyperledger/fabric-peer:latest# docker pull hyperledger/fabric-orderer:latest# docker pull hyperledger/fabric-tools:latest# docker pull hyperledger/fabric-ccenv:latest# docker pull hyperledger/fabric-baseos:latest# docker pull hyperledger/fabric-kafka:latest# docker pull hyperledger/fabric-zookeeper:latest# docker pull hyperledger/fabric-couchdb:latest# docker pull hyperledger/fabric-ca:latest 部署创建创世快123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339# cd $GOPATH/src/github.com/hyperledger/fabric# mkdir kafkapeer# cd kafkapeer# cat configtx.yaml# Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#---################################################################################## Section: Organizations## - This section defines the different organizational identities which will# be referenced later in the configuration.#################################################################################Organizations: # SampleOrg defines an MSP using the sampleconfig. It should never be used # in production but may be used as a template for other definitions - &amp;OrdererOrg # DefaultOrg defines the organization which is used in the sampleconfig # of the fabric.git development environment Name: OrdererOrg # ID to load the MSP definition as ID: OrdererMSP # MSPDir is the filesystem path which contains the MSP configuration MSPDir: crypto-config/ordererOrganizations/example.com/msp # Policies defines the set of policies at this level of the config tree # For organization policies, their canonical path is usually # /Channel/&lt;Application|Orderer&gt;/&lt;OrgName&gt;/&lt;PolicyName&gt; Policies: Readers: Type: Signature Rule: &quot;OR(&apos;OrdererMSP.member&apos;)&quot; Writers: Type: Signature Rule: &quot;OR(&apos;OrdererMSP.member&apos;)&quot; Admins: Type: Signature Rule: &quot;OR(&apos;OrdererMSP.admin&apos;)&quot; - &amp;Org1 # DefaultOrg defines the organization which is used in the sampleconfig # of the fabric.git development environment Name: Org1MSP # ID to load the MSP definition as ID: Org1MSP MSPDir: crypto-config/peerOrganizations/org1.example.com/msp # Policies defines the set of policies at this level of the config tree # For organization policies, their canonical path is usually # /Channel/&lt;Application|Orderer&gt;/&lt;OrgName&gt;/&lt;PolicyName&gt; Policies: Readers: Type: Signature Rule: &quot;OR(&apos;Org1MSP.admin&apos;, &apos;Org1MSP.peer&apos;, &apos;Org1MSP.client&apos;)&quot; Writers: Type: Signature Rule: &quot;OR(&apos;Org1MSP.admin&apos;, &apos;Org1MSP.client&apos;)&quot; Admins: Type: Signature Rule: &quot;OR(&apos;Org1MSP.admin&apos;)&quot; AnchorPeers: # AnchorPeers defines the location of peers which can be used # for cross org gossip communication. Note, this value is only # encoded in the genesis block in the Application section context - Host: peer0.org1.example.com Port: 7051 - &amp;Org2 # DefaultOrg defines the organization which is used in the sampleconfig # of the fabric.git development environment Name: Org2MSP # ID to load the MSP definition as ID: Org2MSP MSPDir: crypto-config/peerOrganizations/org2.example.com/msp # Policies defines the set of policies at this level of the config tree # For organization policies, their canonical path is usually # /Channel/&lt;Application|Orderer&gt;/&lt;OrgName&gt;/&lt;PolicyName&gt; Policies: Readers: Type: Signature Rule: &quot;OR(&apos;Org2MSP.admin&apos;, &apos;Org2MSP.peer&apos;, &apos;Org2MSP.client&apos;)&quot; Writers: Type: Signature Rule: &quot;OR(&apos;Org2MSP.admin&apos;, &apos;Org2MSP.client&apos;)&quot; Admins: Type: Signature Rule: &quot;OR(&apos;Org2MSP.admin&apos;)&quot; AnchorPeers: # AnchorPeers defines the location of peers which can be used # for cross org gossip communication. Note, this value is only # encoded in the genesis block in the Application section context - Host: peer0.org2.example.com Port: 7051################################################################################## SECTION: Capabilities## - This section defines the capabilities of fabric network. This is a new# concept as of v1.1.0 and should not be utilized in mixed networks with# v1.0.x peers and orderers. Capabilities define features which must be# present in a fabric binary for that binary to safely participate in the# fabric network. For instance, if a new MSP type is added, newer binaries# might recognize and validate the signatures from this type, while older# binaries without this support would be unable to validate those# transactions. This could lead to different versions of the fabric binaries# having different world states. Instead, defining a capability for a channel# informs those binaries without this capability that they must cease# processing transactions until they have been upgraded. For v1.0.x if any# capabilities are defined (including a map with all capabilities turned off)# then the v1.0.x peer will deliberately crash.#################################################################################Capabilities: # Channel capabilities apply to both the orderers and the peers and must be # supported by both. Set the value of the capability to true to require it. Global: &amp;ChannelCapabilities # V1.1 for Global is a catchall flag for behavior which has been # determined to be desired for all orderers and peers running v1.0.x, # but the modification of which would cause incompatibilities. Users # should leave this flag set to true. V1_1: true # Orderer capabilities apply only to the orderers, and may be safely # manipulated without concern for upgrading peers. Set the value of the # capability to true to require it. Orderer: &amp;OrdererCapabilities # V1.1 for Order is a catchall flag for behavior which has been # determined to be desired for all orderers running v1.0.x, but the # modification of which would cause incompatibilities. Users should # leave this flag set to true. V1_1: true # Application capabilities apply only to the peer network, and may be safely # manipulated without concern for upgrading orderers. Set the value of the # capability to true to require it. Application: &amp;ApplicationCapabilities # V1.1 for Application is a catchall flag for behavior which has been # determined to be desired for all peers running v1.0.x, but the # modification of which would cause incompatibilities. Users should # leave this flag set to true. V1_2: true################################################################################## SECTION: Application## - This section defines the values to encode into a config transaction or# genesis block for application related parameters#################################################################################Application: &amp;ApplicationDefaults # Organizations is the list of orgs which are defined as participants on # the application side of the network Organizations: # Policies defines the set of policies at this level of the config tree # For Application policies, their canonical path is # /Channel/Application/&lt;PolicyName&gt; Policies: Readers: Type: ImplicitMeta Rule: &quot;ANY Readers&quot; Writers: Type: ImplicitMeta Rule: &quot;ANY Writers&quot; Admins: Type: ImplicitMeta Rule: &quot;MAJORITY Admins&quot; # Capabilities describes the application level capabilities, see the # dedicated Capabilities section elsewhere in this file for a full # description Capabilities: &lt;&lt;: *ApplicationCapabilities################################################################################## SECTION: Orderer## - This section defines the values to encode into a config transaction or# genesis block for orderer related parameters#################################################################################Orderer: &amp;OrdererDefaults # Orderer Type: The orderer implementation to start # Available types are &quot;solo&quot; and &quot;kafka&quot; OrdererType: kafka Addresses: - orderer0.example.com:7050 - orderer1.example.com:7050 - orderer2.example.com:7050 # Batch Timeout: The amount of time to wait before creating a batch BatchTimeout: 2s # Batch Size: Controls the number of messages batched into a block BatchSize: # Max Message Count: The maximum number of messages to permit in a batch MaxMessageCount: 10 # Absolute Max Bytes: The absolute maximum number of bytes allowed for # the serialized messages in a batch. AbsoluteMaxBytes: 98 MB # Preferred Max Bytes: The preferred maximum number of bytes allowed for # the serialized messages in a batch. A message larger than the preferred # max bytes will result in a batch larger than preferred max bytes. PreferredMaxBytes: 512 KB Kafka: # Brokers: A list of Kafka brokers to which the orderer connects. Edit # this list to identify the brokers of the ordering service. # NOTE: Use IP:port notation. Brokers: - kafka0:9092 - kafka1:9092 - kafka2:9092 - kafka3:9092 # Organizations is the list of orgs which are defined as participants on # the orderer side of the network Organizations: # Policies defines the set of policies at this level of the config tree # For Orderer policies, their canonical path is # /Channel/Orderer/&lt;PolicyName&gt; Policies: Readers: Type: ImplicitMeta Rule: &quot;ANY Readers&quot; Writers: Type: ImplicitMeta Rule: &quot;ANY Writers&quot; Admins: Type: ImplicitMeta Rule: &quot;MAJORITY Admins&quot; # BlockValidation specifies what signatures must be included in the block # from the orderer for the peer to validate it. BlockValidation: Type: ImplicitMeta Rule: &quot;ANY Writers&quot; # Capabilities describes the orderer level capabilities, see the # dedicated Capabilities section elsewhere in this file for a full # description Capabilities: &lt;&lt;: *OrdererCapabilities################################################################################## CHANNEL## This section defines the values to encode into a config transaction or# genesis block for channel related parameters.#################################################################################Channel: &amp;ChannelDefaults # Policies defines the set of policies at this level of the config tree # For Channel policies, their canonical path is # /Channel/&lt;PolicyName&gt; Policies: # Who may invoke the &apos;Deliver&apos; API Readers: Type: ImplicitMeta Rule: &quot;ANY Readers&quot; # Who may invoke the &apos;Broadcast&apos; API Writers: Type: ImplicitMeta Rule: &quot;ANY Writers&quot; # By default, who may modify elements at this config level Admins: Type: ImplicitMeta Rule: &quot;MAJORITY Admins&quot; # Capabilities describes the channel level capabilities, see the # dedicated Capabilities section elsewhere in this file for a full # description Capabilities: &lt;&lt;: *ChannelCapabilities################################################################################## Profile## - Different configuration profiles may be encoded here to be specified# as parameters to the configtxgen tool#################################################################################Profiles: TwoOrgsOrdererGenesis: &lt;&lt;: *ChannelDefaults Orderer: &lt;&lt;: *OrdererDefaults Organizations: - *OrdererOrg Consortiums: SampleConsortium: Organizations: - *Org1 - *Org2 TwoOrgsChannel: Consortium: SampleConsortium Application: &lt;&lt;: *ApplicationDefaults Organizations: - *Org1 - *Org2 1234bin目录从fabric源码里面拷过来，这样方便生成块# mkdir channel-artifacts# ./bin/configtxgen -profile TwoOrgsOrdererGenesis -outputBlock ./channel-artifacts/genesis.block# ./bin/configtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/mychannel.tx -channelID mychannel 完事之后，把kafka目录，拷到所有机器上 部署kafka、zk1234567891011121314151617181920212223242526zk的yaml文件：# cat docker-compose-zookeeper.yaml# Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#version: &apos;2&apos;services: zookeeper0: container_name: zookeeper0 hostname: zookeeper0 image: hyperledger/fabric-zookeeper restart: always environment: - ZOO_MY_ID=1 - ZOO_SERVERS=server.1=zookeeper0:2888:3888 server.2=zookeeper1:2888:3888 server.3=zookeeper2:2888:3888 ports: - 2181:2181 - 2888:2888 - 3888:3888 dns: - &quot;192.168.3.94&quot; 12345678910111213141516171819202122232425262728293031323334kafka的yaml文件：# cat docker-compose-kafka.yaml# Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#version: &apos;2&apos;services: kafka0: container_name: kafka0 hostname: kafka0 image: hyperledger/fabric-kafka restart: always environment: - KAFKA_MESSAGE_MAX_BYTES=103809024 # 99 * 1024 * 1024 B - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024 # 99 * 1024 * 1024 B - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false environment: - KAFKA_BROKER_ID=1 - KAFKA_MIN_INSYNC_REPLICAS=2 - KAFKA_DEFAULT_REPLICATION_FACTOR=3 - KAFKA_ZOOKEEPER_CONNECT=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181 ports: - 9092:9092 dns: - &quot;192.168.3.94&quot; 部署orderer1234567891011121314151617181920212223242526272829303132333435363738394041424344因为咱们的fabric证书，没生成tls，所以下面的配置文件需要把tls去掉，zk，kafka都各自按照上述步骤配置在三台不同机器上，orderer也一样cat docker-compose-orderer.yaml# Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#version: &apos;2&apos;services: orderer0.example.com: container_name: orderer0.example.com image: hyperledger/fabric-orderer environment: - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] - ORDERER_KAFKA_RETRY_LONGINTERVAL=10s - ORDERER_KAFKA_RETRY_LONGTOTAL=100s - ORDERER_KAFKA_RETRY_SHORTINTERVAL=1s - ORDERER_KAFKA_RETRY_SHORTTOTAL=30s - ORDERER_KAFKA_VERBOSE=true working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer volumes: - ./channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - ./crypto-config/ordererOrganizations/example.com/orderers/orderer0.example.com/msp:/var/hyperledger/orderer/msp - ./crypto-config/ordererOrganizations/example.com/orderers/orderer0.example.com/tls/:/var/hyperledger/orderer/tls ports: - 7050:7050 dns: - &quot;192.168.3.94&quot; 部署peer1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# cat docker-compose-peer.yaml# All elements in this file should depend on the docker-compose-base.yaml# Provided fabric peer nodeversion: &apos;2&apos;services: peer1.org1.example.com: container_name: peer1.org1.example.com hostname: peer1.org1.example.com image: hyperledger/fabric-peer environment: - CORE_PEER_ID=peer1.org1.example.com - CORE_PEER_ADDRESS=peer1.org1.example.com:7051 - CORE_PEER_CHAINCODELISTENADDRESS=peer1.org1.example.com:7052 - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1.org1.example.com:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock # the following setting starts chaincode containers on the same # bridge network as the peers # https://docs.docker.com/compose/networking/ #- CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_PEER_GOSSIP_USELEADERELECTION=true - CORE_PEER_GOSSIP_ORGLEADER=false - CORE_PEER_PROFILE_ENABLED=true - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: peer node start volumes: - /var/run/:/host/var/run/ - ./crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/msp:/etc/hyperledger/fabric/msp - ./crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls:/etc/hyperledger/fabric/tls ports: - 7051:7051 - 7052:7052 - 7053:7053 dns: - &quot;192.168.3.94&quot; cli: container_name: cli image: hyperledger/fabric-tools tty: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock # - CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_PEER_ID=cli - CORE_PEER_ADDRESS=peer1.org1.example.com:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls/ca.crt - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer volumes: - /var/run/:/host/var/run/ - ./chaincode/go/:/opt/gopath/src/github.com/hyperledger/fabric/kafkapeer/chaincode/go - ./crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ - ./channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts dns: - &quot;192.168.3.94&quot; 启动1234docker-compose -f docker-compose-zookeeper.yaml up -ddocker-compose -f docker-compose-kafka.yaml up -ddocker-compose -f docker-compose-orderer.yaml up -ddocker-compose -f docker-compose-peer.yaml up -d 创建channel123进入cli：peer channel create -o orderer0.example.com:7050 -c mychannel -f ./channel-artifacts/mychannel.txpeer channel join -b mychannel.block 之后切换变量，批量加就可以了]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tornado-电话报警]]></title>
    <url>%2Fposts%2Faa774e96.html</url>
    <content type="text"><![CDATA[需求某些时候邮件，钉钉的报警我们在家里，或者周末是很少去观看的，这时候如果服务器出了问题，运维人员是没法第一时间排查到，所以短信和电话报警就很有必要去做。 已有阿里云的语音短信报警接口，故做了个端口电话报警。 电话报警脚本[root@aa phone_send]# cat send_model.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445# -*- coding:utf-8 -*-import requestsimport tornado.ioloopimport tornado.webphonenumber = &quot;xxxxxxx,xxxxxxx&quot;portdic = &#123; &quot;9876&quot;:&quot;服务类型MQ,端口9876&quot;, &quot;2181&quot;:&quot;服务类型ZK,端口2181&quot;, &quot;3306&quot;:&quot;服务类型数据库,端口3306&quot;, &quot;27017&quot;:&quot;服务类型数据库,端口27017&quot;, &quot;1908&quot;:&quot;服务类型spada,薛亮应用&quot;, &quot;53&quot;:&quot;服务类型dns,端口53&quot;, &quot;9200&quot;:&quot;服务类型es,端口9200&quot;, &quot;6379&quot;:&quot;服务类型redis,端口6379&quot;, &quot;80&quot;:&quot;服务类型nginx,端口80&quot;&#125;statusdic = &#123; &quot;PROBLEM&quot;:&quot;服务发生故障&quot;, &quot;OK&quot;:&quot;故障恢复&quot;&#125;class MainHandler(tornado.web.RequestHandler): def get(self): status = self.get_argument(&apos;status&apos;) endpoint = self.get_argument(&apos;endpoint&apos;) metric = self.get_argument(&apos;metric&apos;) tags = self.get_argument(&apos;tags&apos;) statusok = statusdic.get(status) port = tags.split(&quot;:&quot;)[1] p_endpoint = endpoint.split(&quot;.&quot;) del(p_endpoint[0]) portmes = portdic.get(port) if portmes == None: portmes = &quot;端口&quot; + port # 短信 requests.get(&quot;http://域名/send_sms/%s,%s,%s,%s/%s&quot;%(statusok,endpoint,metric,portmes,phonenumber)) # 电话 requests.get(&quot;http://域名/send_phone/%s%s%s%s/%s&quot;%(statusok,p_endpoint,metric,portmes,phonenumber)) message = status + endpoint + metric + tags print(status,endpoint,metric,tags,port)application = tornado.web.Application([(r&quot;/message&quot;, MainHandler), ])if __name__ == &quot;__main__&quot;: application.listen(8868) tornado.ioloop.IOLoop.instance().start() 执行1python3 send_model.py callback接口http://ip:8868/message openfalcon监控做模板]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tornado-hello]]></title>
    <url>%2Fposts%2F6fd83cc1.html</url>
    <content type="text"><![CDATA[概念Tornado是一个Python Web框架和异步网络库，最初是在FriendFeed上开发的。通过使用非阻塞网络I / O，Tornado可以扩展到数万个开放连接，使其成为长轮询， WebSockets和其他需要与每个用户建立长期连接的应用程序的理想选择 。 安装1pip3 install tornado 简单的web12345678910111213import tornado.ioloopimport tornado.webclass MainHandler(tornado.web.RequestHandler): def get(self): self.write(&quot;Hello, world&quot;)if __name__ == &quot;__main__&quot;: application = tornado.web.Application([ (r&quot;/index&quot;, MainHandler), ]) application.listen(8888) tornado.ioloop.IOLoop.current().start() 访问http://ip:8888/index]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[fabric-ca部署]]></title>
    <url>%2Fposts%2F86326b76.html</url>
    <content type="text"><![CDATA[部署一个fabric-ca创建一个由两个组织org1.example.com和org2.example.com组成的的联盟1234567891011121314151617181920还有一个组织example.com用来部署orderer。组织example.com部署了一个solo模式的orderer。（多个orderer的部署方式，以后探讨）orderer.example.com组织org1.example.com部署了两个peer:peer0.org1.example.compeer1.org1.example.com组织org2.example.com部署了一个peer:peer0.org2.example.com每个组织都要有一个Admin用户，每个组件(peer/orderer)也需要一个账号，因此需要通过FabricCA创建7个用户：example.com: Admin@example.com orderer.example.comorg1.example.com: Admin@org1.example.com peer0.org1.example.com peer1.org1.example.com org2.example.com: Admin@org2.example.com peer0.org2.example.com这里只创建了Admin用户和每个组件的账号，普通用户的创建方式相同，只是普通用户的证书不需要添加到目标组件的admincerts目录中。或者说一个用户的证书如果被添加到了对应组织或组件的msp/admincerts目录中，那么这个用户就称为对应的管理员。 启动fabric-ca123456789101112131415161718192021fabirc-ca的编译：$ go get -u github.com/hyperledger/fabric-ca$ cd $GOPATH/src/github.com/hyperledger/fabric-ca$ make fabric-ca-server$ make fabric-ca-client$ ls bin/fabric-ca-client fabric-ca-server这里将fabric-ca部署在/opt/app/fabric-ca/server目录中：mkdir -p /opt/app/fabric-ca/servercp -rf $GOPATH/src/github.com/hyperledger/fabric-ca/bin/* /opt/app/fabric-ca/serverln -s /opt/app/fabric-ca/server/fabric-ca-client /usr/bin/fabric-ca-client直接启动ca，fabric-ca admin的名称为admin，密码为pass。(这里只是演示，生产中使用，你需要根据实际的情况配置)cd /opt/app/fabric-ca/server./fabric-ca-server start -b admin:pass &amp;如果有删除联盟和删除用户的需求，需要用下面的方式启动：cd /opt/app/fabric-ca/server./fabric-ca-server start -b admin:pass --cfg.affiliations.allowremove --cfg.identities.allowremove &amp; 生成fabric-ca admin的凭证123456789101112mkdir /root/fabric-deploycd ~/fabric-deploymkdir fabric-ca-files 生成fabric-ca admin的凭证，用-H参数指定client目录：mkdir -p `pwd`/fabric-ca-files/adminfabric-ca-client enroll -u http://admin:pass@localhost:7054 -H `pwd`/fabric-ca-files/admin也可以用环境变量FABRIC_CA_CLIENT_HOME指定了client的工作目录，生成的用户凭证将存放在这个目录中。export FABRIC_CA_CLIENT_HOME=`pwd`/fabric-ca-files/adminmkdir -p $FABRIC_CA_CLIENT_HOMEfabric-ca-client enroll -u http://admin:pass@localhost:7054 创建联盟123456789101112131415161718192021222324252627282930上面的启动方式默认会创建两个组织：$ fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation list2018/05/07 02:36:46 [INFO] [::1]:56148 GET /affiliations 200 0 &quot;OK&quot;affiliation: . affiliation: org2 affiliation: org2.department1 affiliation: org1 affiliation: org1.department1 affiliation: org1.department2为了查看信息的时候，看到的输出比较简洁，用下面的命令将其删除：fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation remove --force org1fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation remove --force org2执行下面命令创建联盟：fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com.examplefabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com.example.org1fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com.example.org2注意：联盟是有层级的。创建联盟如下：$ fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation list2018/04/28 15:19:34 [INFO] 127.0.0.1:38160 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org1 affiliation: com.example.org2 为每个组织准备msp12345678910111213141516171819202122232425262728293031323334353637就是从Fabric-CA中，读取出用来签署用户的根证书等。为example.com准备msp，将ca证书等存放example.com组织的目录中:mkdir -p ./fabric-ca-files/example.com/mspfabric-ca-client getcacert -M `pwd`/fabric-ca-files/example.com/msp //-M需要指定绝对路径命令执行结束后，会在fabric-ca-files/example.com/msp得到文件：$ tree fabric-ca-files/example.com/msp/example.com/msp/|-- cacerts| `-- localhost-7054.pem|-- intermediatecerts| `-- localhost-7054.pem|-- keystore`-- signcerts注意通过getcacert得到msp目录中只有CA证书，而且这里没有使用中间CA，fabric-ca-files/example.com/msp/intermediatecerts/localhost-7054.pem是一个空文件。同样的方式为org1.example.com获取msp:mkdir -p fabric-ca-files/org1.example.com/mspfabric-ca-client getcacert -M `pwd`/fabric-ca-files/org1.example.com/msp为org2.example.com准备msp:mkdir -p ./fabric-ca-files/org2.example.com/mspfabric-ca-client getcacert -M `pwd`/fabric-ca-files/org2.example.com/msp这里是用getcacert为每个组织准备需要的ca文件，在生成创始块的时候会用到。在1.1.0版本的fabric-ca中，只会生成用户在操作区块链的时候用到的证书和密钥，不会生成用来加密grpc通信的证书。这里复用之前用cryptogen生成的tls证书，需要将验证tls证书的ca添加到msp目录中，如下：cp -rf certs/ordererOrganizations/example.com/msp/tlscacerts fabric-ca-files/example.com/msp/cp -rf certs/peerOrganizations/org1.example.com/msp/tlscacerts/ fabric-ca-files/org1.example.com/msp/cp -rf certs/peerOrganizations/org2.example.com/msp/tlscacerts/ fabric-ca-files/org2.example.com/msp/如果在你的环境中，各个组件域名的证书，是由第三方CA签署的，就将第三方CA的根证书添加到msp/tlscacerts目录中。组织的msp目录中，包含都是CA根证书，分别是TLS加密的根证书，和用于身份验证的根证书。另外还需要admin用户的证书，后面的操作中会添加。 注册example.com的管理员Admin@example.com可以直接用命令行（命令比较长，这里用\\截断了）： 123fabric-ca-client register --id.name Admin@example.com --id.type client --id.affiliation &quot;com.example.org1&quot; \ --id.attrs &apos;&quot;hf.Registrar.Roles=client,orderer,peer,user&quot;,&quot;hf.Registrar.DelegateRoles=client,orderer,peer,user&quot;,\ hf.Registrar.Attributes=*,hf.GenCRL=true,hf.Revoker=true,hf.AffiliationMgr=true,hf.IntermediateCA=true,role=admin:ecert&apos; 也可以将命令行参数写在fabric-ca admin的配置文件fabric-ca-files/admin/fabric-ca-client-config.yaml中。 12$ ls fabric-ca-files/admin/admin/fabric-ca-client-config.yaml msp 为了演示清楚，这里使用修改配置文件的方式，将fabric-ca-files/admin/fabric-ca-client-config.yaml其中的id部分修改为： 1234567891011121314151617181920212223id: name: Admin@example.com type: client affiliation: com.example maxenrollments: 0 attributes: - name: hf.Registrar.Roles value: client,orderer,peer,user - name: hf.Registrar.DelegateRoles value: client,orderer,peer,user - name: hf.Registrar.Attributes value: &quot;*&quot; - name: hf.GenCRL value: true - name: hf.Revoker value: true - name: hf.AffiliationMgr value: true - name: hf.IntermediateCA value: true - name: role value: admin ecert: true 注意最后一行role属性，是我们自定义的属性，对于自定义的属性，要设置certs，在配置文件中需要单独设置ecert属性为true或者false。如果在命令行中，添加后缀:ecert表示true，例如: 1fabric-ca-client register --id.affiliation &quot;com.example.org1&quot; --id.attrs &quot;role=admin:ecert&quot; 直接执行下面的命令，即可完成用户`Admin@example.com`注册，注意这时候的注册使用fabricCA的admin账号完成的： 1fabric-ca-client register -H `pwd`/fabric-ca-files/admin --id.secret=password 如果不用--id.secret指定密码，会自动生成密码。 其它配置的含义是用户名为`Admin@example.com，类型是client，它能够管理com.example.*`下的用户，如下: 1234567891011--id.name Admin@example.com //用户名--id.type client //类型为client--id.affiliation &quot;com.example&quot; //权利访问hf.Registrar.Roles=client,orderer,peer,user //能够管理的用户类型hf.Registrar.DelegateRoles=client,orderer,peer,user //可以授权给子用户管理的用户类型hf.Registrar.Attributes=* //可以为子用户设置所有属性hf.GenCRL=true //可以生成撤销证书列表hf.Revoker=true //可以撤销用户hf.AffiliationMgr=true //能够管理联盟hf.IntermediateCA=true //可以作为中间CArole=admin:ecert //自定义属性 完成注册之后，还需生成Admin@example.com凭证： 1234$ mkdir -p ./fabric-ca-files/example.com/admin$ fabric-ca-client enroll -u http://Admin@example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/example.com/admin$ ls ./fabric-ca-files/example.com/adminfabric-ca-client-config.yaml msp/ 这时候可以用Admin@example.com的身份查看联盟： 123456$ fabric-ca-client affiliation list -H `pwd`/fabric-ca-files/example.com/admin2018/04/28 15:35:10 [INFO] 127.0.0.1:38172 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org1 affiliation: com.example.org2 最后将Admin@example.com的证书复制到example.com/msp/admincerts/中： 12mkdir fabric-ca-files/example.com/msp/admincerts/cp fabric-ca-files/example.com/admin/msp/signcerts/cert.pem fabric-ca-files/example.com/msp/admincerts/ 注册org1.example.com的管理员Admin@org1.example.com为org1.example.com的管理员Admin@org1.example.com准备一个目录: 12cd ~/fabric-deploymkdir -p ./fabric-ca-files/org1.example.com/admin 将fabric-ca-files/admin/fabric-ca-client-config.yaml其中的id部分修改为： 1234567891011121314151617181920212223id: name: Admin@org1.example.com type: client affiliation: com.example.org1 maxenrollments: 0 attributes: - name: hf.Registrar.Roles value: client,orderer,peer,user - name: hf.Registrar.DelegateRoles value: client,orderer,peer,user - name: hf.Registrar.Attributes value: &quot;*&quot; - name: hf.GenCRL value: true - name: hf.Revoker value: true - name: hf.AffiliationMgr value: true - name: hf.IntermediateCA value: true - name: role value: admin ecert: true 注册： 1fabric-ca-client register -H `pwd`/fabric-ca-files/admin --id.secret=password 生成凭证： 123$ fabric-ca-client enroll -u http://Admin@org1.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org1.example.com/admin$ ls ./fabric-ca-files/org1.example.com/adminfabric-ca-client-config.yaml msp/ 查看联盟： 12345$ fabric-ca-client affiliation list -H `pwd`/fabric-ca-files/org1.example.com/admin2018/05/04 15:42:53 [INFO] 127.0.0.1:51298 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org1 注意与`Admin@example.com`的区别，这里只能看到组织com.example.org1 将Admin@org1.example.com的证书复制到org1.example.com的msp/admincerts中： 12mkdir fabric-ca-files/org1.example.com/msp/admincerts/cp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/msp/admincerts/ 在`Admin@org1.example.com目录`中也需要创建msp/admincerts目录，通过peer命令操作fabric的时候会要求admincerts存在： 12mkdir fabric-ca-files/org1.example.com/admin/msp/admincerts/ # 注意是org1.example.com/admin目录cp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/admin/msp/admincerts/ 另外，这里没有使用中间CA，将intermediatecerts中的空文件删除，否则peer会提示Warning： 1rm fabric-ca-files/org1.example.com/admin/msp/intermediatecerts/* 注册org2.example.com的管理员Admin@org2.example.com为org2.example.com的管理员Admin@org2.example.com准备一个目录: 12cd ~/fabric-deploymkdir -p ./fabric-ca-files/org2.example.com/admin 将fabric-ca-files/admin/fabric-ca-client-config.yaml其中的id部分修改为： 1234567891011121314151617181920212223id: name: Admin@org2.example.com type: client affiliation: com.example.org2 maxenrollments: 0 attributes: - name: hf.Registrar.Roles value: client,orderer,peer,user - name: hf.Registrar.DelegateRoles value: client,orderer,peer,user - name: hf.Registrar.Attributes value: &quot;*&quot; - name: hf.GenCRL value: true - name: hf.Revoker value: true - name: hf.AffiliationMgr value: true - name: hf.IntermediateCA value: true - name: role value: admin ecert: true 注册： 1fabric-ca-client register -H `pwd`/fabric-ca-files/admin --id.secret=password 生成凭证： 123$ fabric-ca-client enroll -u http://Admin@org2.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org2.example.com/admin$ ls ./fabric-ca-files/org2.example.com/adminfabric-ca-client-config.yaml msp/ 查看联盟： 12345$ fabric-ca-client affiliation list -H `pwd`/fabric-ca-files/org2.example.com/admin2018/05/02 16:49:00 [INFO] 127.0.0.1:50828 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org2 Admin@org2.example.com只能看到组织com.example.org2。 将Admin@org2.example.com的证书复制到org2.example.com的msp/admincerts中： 12mkdir fabric-ca-files/org2.example.com/msp/admincerts/cp fabric-ca-files/org2.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org2.example.com/msp/admincerts/ 在Admin@org2.example.com中也需要创建msp/admincerts目录，通过peer命令操作fabric的时候会要求admincerts存在： 12mkdir fabric-ca-files/org2.example.com/admin/msp/admincerts/cp fabric-ca-files/org2.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org2.example.com/admin/msp/admincerts/ 另外，这里没有使用中间CA，将intermediatecerts中的空文件删除，否则peer会提示Warning： 1rm fabric-ca-files/org2.example.com/admin/msp/intermediatecerts/* 各个组织分别使用自己的Admin账户创建其它账号example.com、org1.example.com、org2.example.com三个组织这时候可以分别使用自己的Admin账号创建子账号。 orderer.example.com使用`Admin@example.com注册账号orderer.example.com。注意这时候指定的目录是fabric-ca-files/example.com`/admin/。 修改fabric-ca-files/example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: orderer.example.com type: orderer affiliation: com.example maxenrollments: 0 attributes: - name: role value: orderer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/example.com/ordererfabric-ca-client enroll -u http://orderer.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/example.com/orderer 将`Admin@example.com`的证书复制到fabric-ca-files/example.com/orderer/msp/admincerts： 12mkdir fabric-ca-files/example.com/orderer/msp/admincertscp fabric-ca-files/example.com/admin/msp/signcerts/cert.pem fabric-ca-files/example.com/orderer/msp/admincerts/ peer0.org1.example.com使用`Admin@org1.example.com注册账号peer0.org1.example.com。这时候指定的目录是fabric-ca-files/org1.example.com`/admin/。 修改fabric-ca-files/org1.example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: peer0.org1.example.com type: peer affiliation: com.example.org1 maxenrollments: 0 attributes: - name: role value: peer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/org1.example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/org1.example.com/peer0fabric-ca-client enroll -u http://peer0.org1.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org1.example.com/peer0 将`Admin@org1.example.com`的证书复制到fabric-ca-files/org1.example.com/peer0/msp/admincerts： 12mkdir fabric-ca-files/org1.example.com/peer0/msp/admincertscp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/peer0/msp/admincerts/ peer1.org1.example.com使用`Admin@org1.example.com注册账号peer1.org1.example.com。这时候指定的目录是fabric-ca-files/org1.example.com`/admin/。 修改fabric-ca-files/org1.example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: peer1.org1.example.com type: peer affiliation: com.example.org1 maxenrollments: 0 attributes: - name: role value: peer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/org1.example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/org1.example.com/peer1fabric-ca-client enroll -u http://peer1.org1.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org1.example.com/peer1 将`Admin@org1.example.com`的证书复制到fabric-ca-files/org1.example.com/peer1/msp/admincerts： 12mkdir fabric-ca-files/org1.example.com/peer1/msp/admincertscp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/peer1/msp/admincerts/ peer0.org2.example.com使用`Admin@org2.example.com注册账号peer0.org2.example.com。这时候指定的目录是fabric-ca-files/org2.example.com`/admin/。 修改fabric-ca-files/org2.example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: peer0.org2.example.com type: peer affiliation: com.example.org2 maxenrollments: 0 attributes: - name: role value: peer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/org2.example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/org2.example.com/peer0fabric-ca-client enroll -u http://peer0.org2.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org2.example.com/peer0 将`Admin@org2.example.com`的证书复制到fabric-ca-files/org2.example.com/peer0/msp/admincerts： 12mkdir fabric-ca-files/org2.example.com/peer0/msp/admincertscp fabric-ca-files/org2.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org2.example.com/peer0/msp/admincerts/ 注意： 之前发现直接这么生成的证书，会少东西，需要在每个组织的msp目录下面配置下config.yaml 1234567891011[root@localhost msp]# pwd/data/fabric/fabric-ca-files/gzyb.vaccine.com/msp[root@localhost msp]# cat config.yaml NodeOUs: Enable: true ClientOUIdentifier: Certificate: cacerts/localhost-7054.pem OrganizationalUnitIdentifier: client PeerOUIdentifier: Certificate: cacerts/localhost-7054.pem OrganizationalUnitIdentifier: peer]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Prometheus监控mongo]]></title>
    <url>%2Fposts%2F440gqa86.html</url>
    <content type="text"><![CDATA[安装mongo插件123456789yum -y install glidegit clone git@github.com:dcu/mongodb_exporter.git $GOPATH/src/github.com/dcu/mongodb_exporter也可以去github上，下载源码，在编译安装cd $GOPATH/src/github.com/dcu/mongodb_exportermake build./mongodb_exporter -h注意：go环境需要提前安装好，可以编译也可以yum安装如果不会编译，可以通知本人，给你发编译后的包 启动1nohup ./mongodb_exporter --mongodb.uri &quot;mongodb://readonly:readonly@192.168.50.7:27017&quot; &amp; 上面是单点的，集群的话192.168.50.7:27017,192.168.50.8:27017 这样就可以 prometheus配置123456- job_name: mongo_exporter static_configs: - targets: - &apos;192.168.50.7:9001&apos; labels: service: mongo 导入模板https://grafana.com/dashboards/2583]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Prometheus监控elasticsearch]]></title>
    <url>%2Fposts%2F550bdf86.html</url>
    <content type="text"><![CDATA[安装监控插件12345wget https://github.com/justwatchcom/elasticsearch_exporter/releases/download/v1.0.4rc1/elasticsearch_exporter-1.0.4rc1.linux-amd64.tar.gz tar -zxvf elasticsearch_exporter-1.0.4rc1.linux-amd64.tar.gz cd elasticsearch_exporter-1.0.4rc1.linux-amd64/ nohup ./elasticsearch_exporter --web.listen-address &quot;:9109&quot; --es.uri http://192.168.50.153:9200 &amp; 启动成功后，可以访问 http://192.168.50.153:9109/metrics ，看抓取的信息 监控图表 指标 解析 ##搜索和索引性能 elasticsearch_indices_search_query_total 查询总数 吞吐量 elasticsearch_indices_search_query_time_seconds 查询总时间 性能 elasticsearch_indices_search_fetch_total 提取总数 elasticsearch_indices_search_fetch_time_seconds 花费在提取上的总时间 ##索引请求 elasticsearch_indices_indexing_index_total 索引的文件总数 elasticsearch_indices_indexing_index_time_seconds_total 索引文档总时间 elasticsearch_indices_indexing_delete_total 索引的文件删除总数 elasticsearch_indices_indexing_delete_time_seconds_total 索引的文件删除总时间 elasticsearch_indices_refresh_total 索引刷新总数 elasticsearch_indices_refresh_time_seconds_total 刷新指数的总时间 elasticsearch_indices_flush_total 索引刷新总数到磁盘 elasticsearch_indices_flush_time_seconds 将索引刷新到磁盘上的总时间 累计flush时间 ##JVM内存和垃圾回收 elasticsearch_jvm_gc_collection_seconds_sum GC run time in seconds垃圾回收时间 elasticsearch_jvm_gc_collection_seconds_count Count of JVM GC runs垃圾搜集数 elasticsearch_jvm_memory_committed_bytes JVM memory currently committed by area最大使用内存限制 elasticsearch_jvm_memory_max_bytes 配置的最大jvm值 elasticsearch_jvm_memory_pool_max_bytes JVM内存最大池数 elasticsearch_jvm_memory_pool_peak_max_bytes 最大的JVM内存峰值 elasticsearch_jvm_memory_pool_peak_used_bytes 池使用的JVM内存峰值 elasticsearch_jvm_memory_pool_used_bytes 目前使用的JVM内存池 elasticsearch_jvm_memory_used_bytes JVM memory currently used by area 内存使用量 ##集群健康和节点可用性 elasticsearch_cluster_health_status 集群状态，green（ 所有的主分片和副本分片都正常运行）、yellow（所有的主分片都正常运行，但不是所有的副本分片都正常运行）red（有主分片没能正常运行）值为1的即为对应状态 elasticsearch_cluster_health_number_of_data_nodes node节点的数量 elasticsearch_cluster_health_number_of_in_flight_fetch 正在进行的碎片信息请求的数量 elasticsearch_cluster_health_number_of_nodes 集群内所有的节点 elasticsearch_cluster_health_number_of_pending_tasks 尚未执行的集群级别更改 elasticsearch_cluster_health_initializing_shards 正在初始化的分片数 elasticsearch_cluster_health_unassigned_shards 未分配分片数 elasticsearch_cluster_health_active_primary_shards 活跃的主分片总数 elasticsearch_cluster_health_active_shards 活跃的分片总数（包括复制分片） elasticsearch_cluster_health_relocating_shards 当前节点正在迁移到其他节点的分片数量，通常为0，集群中有节点新加入或者退出时该值会增加 ##资源饱和度 elasticsearch_thread_pool_completed_count 线程池操作完成（bulk、index、search、force_merge） elasticsearch_thread_pool_active_count 线程池线程活动（bulk、index、search、force_merge） elasticsearch_thread_pool_largest_count 线程池最大线程数（bulk、index、search、force_merge） elasticsearch_thread_pool_queue_count 线程池中的排队线程数（bulk、index、search、force_merge） elasticsearch_thread_pool_rejected_count 线程池的被拒绝线程数（bulk、index、search、force_merge） elasticsearch_indices_fielddata_memory_size_bytes fielddata缓存的大小（字节） elasticsearch_indices_fielddata_evictions 来自fielddata缓存的驱逐次数 elasticsearch_indices_filter_cache_evictions 来自过滤器缓存的驱逐次数（仅版本2.x） elasticsearch_indices_filter_cache_memory_size_bytes 过滤器高速缓存的大小（字节）（仅版本2.x） elasticsearch_cluster_health_number_of_pending_tasks 待处理任务数 elasticsearch_indices_get_time_seconds elasticsearch_indices_get_missing_total 丢失的文件的GET请求总数 elasticsearch_indices_get_missing_time_seconds 花费在文档丢失的GET请求上的总时间 elasticsearch_indices_get_exists_time_seconds elasticsearch_indices_get_exists_total elasticsearch_indices_get_total ##主机级别的系统和网络指标 elasticsearch_process_cpu_percent Percent CPU used by process CPU使用率 elasticsearch_filesystem_data_free_bytes Free space on block device in bytes 磁盘可用空间 elasticsearch_process_open_files_count Open file descriptors ES进程打开的文件描述符 elasticsearch_transport_rx_packets_total Count of packets receivedES节点之间网络入流量 elasticsearch_transport_tx_packets_total Count of packets sentES节点之间网络出流量 prometheus配置123456789101112131415161718- job_name: &apos;elasticsearch&apos; scrape_interval: 60s scrape_timeout: 30s metrics_path: &quot;/metrics&quot; static_configs: - targets: - &apos;192.168.50.153:9109&apos; labels: service: elasticsearch relabel_configs: - source_labels: [__address__] regex: &apos;(.*)\:9109&apos; target_label: &apos;instance&apos; replacement: &apos;$1&apos; - source_labels: [__address__] regex: &apos;.*\.(.*)\.lan.*&apos; target_label: &apos;environment&apos; replacement: &apos;$1&apos; 之后运行重读prometheus配置命令 1./reload-prometheus.sh grafana模板1https://grafana.com/dashboards/2322 报警配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283groups:- name: elasticsearchStatsAlert rules: - alert: Elastic_Cluster_Health_RED expr: elasticsearch_cluster_health_status&#123;color=&quot;red&quot;&#125;==1 for: 1m labels: severity: critical annotations: summary: &quot;Instance &#123;&#123; $labels.instance &#125;&#125;: not all primary and replica shards are allocated in elasticsearch cluster &#123;&#123; $labels.cluster &#125;&#125;&quot; description: &quot;Instance &#123;&#123; $labels.instance &#125;&#125;: not all primary and replica shards are allocated in elasticsearch cluster &#123;&#123; $labels.cluster &#125;&#125;.&quot; - alert: Elastic_Cluster_Health_Yellow expr: elasticsearch_cluster_health_status&#123;color=&quot;yellow&quot;&#125;==1 for: 1m labels: severity: critical annotations: summary: &quot; Instance &#123;&#123; $labels.instance &#125;&#125;: not all primary and replica shards are allocated in elasticsearch cluster &#123;&#123; $labels.cluster &#125;&#125;&quot; description: &quot;Instance &#123;&#123; $labels.instance &#125;&#125;: not all primary and replica shards are allocated in elasticsearch cluster &#123;&#123; $labels.cluster &#125;&#125;.&quot; - alert: Elasticsearch_JVM_Heap_Too_High expr: elasticsearch_jvm_memory_used_bytes&#123;area=&quot;heap&quot;&#125; / elasticsearch_jvm_memory_max_bytes&#123;area=&quot;heap&quot;&#125; &gt; 0.8 for: 1m labels: severity: critical annotations: summary: &quot;ElasticSearch node &#123;&#123; $labels.instance &#125;&#125; heap usage is high &quot; description: &quot;The heap in &#123;&#123; $labels.instance &#125;&#125; is over 80% for 15m.&quot; - alert: Elasticsearch_health_up expr: elasticsearch_cluster_health_up !=1 for: 1m labels: severity: critical annotations: summary: &quot; ElasticSearch node: &#123;&#123; $labels.instance &#125;&#125; last scrape of the ElasticSearch cluster health failed&quot; description: &quot;ElasticSearch node: &#123;&#123; $labels.instance &#125;&#125; last scrape of the ElasticSearch cluster health failed&quot; - alert: Elasticsearch_Too_Few_Nodes_Running expr: elasticsearch_cluster_health_number_of_nodes &lt; 12 for: 1m labels: severity: critical annotations: summary: &quot;There are only &#123;&#123;$value&#125;&#125; &lt; 12 ElasticSearch nodes running &quot; description: &quot;lasticSearch running on less than 12 nodes(total 14)&quot; - alert: Elasticsearch_Count_of_JVM_GC_Runs expr: rate(elasticsearch_jvm_gc_collection_seconds_count&#123;&#125;[5m])&gt;5 for: 1m labels: severity: critical annotations: summary: &quot;ElasticSearch node &#123;&#123; $labels.instance &#125;&#125;: Count of JVM GC runs &gt; 5 per sec and has a value of &#123;&#123; $value &#125;&#125; &quot; description: &quot;ElasticSearch node &#123;&#123; $labels.instance &#125;&#125;: Count of JVM GC runs &gt; 5 per sec and has a value of &#123;&#123; $value &#125;&#125;&quot; - alert: Elasticsearch_GC_Run_Time expr: rate(elasticsearch_jvm_gc_collection_seconds_sum[5m])&gt;0.3 for: 1m labels: severity: critical annotations: summary: &quot; ElasticSearch node &#123;&#123; $labels.instance &#125;&#125;: GC run time in seconds &gt; 0.3 sec and has a value of &#123;&#123; $value &#125;&#125;&quot; description: &quot;ElasticSearch node &#123;&#123; $labels.instance &#125;&#125;: GC run time in seconds &gt; 0.3 sec and has a value of &#123;&#123; $value &#125;&#125;&quot; - alert: Elasticsearch_json_parse_failures expr: elasticsearch_cluster_health_json_parse_failures&gt;0 for: 1m labels: severity: critical annotations: summary: &quot; ElasticSearch node &#123;&#123; $labels.instance &#125;&#125;: json parse failures &gt; 0 and has a value of &#123;&#123; $value &#125;&#125;&quot; description: &quot;ElasticSearch node &#123;&#123; $labels.instance &#125;&#125;: json parse failures &gt; 0 and has a value of &#123;&#123; $value &#125;&#125;&quot; - alert: Elasticsearch_breakers_tripped expr: rate(elasticsearch_breakers_tripped&#123;&#125;[5m])&gt;0 for: 1m labels: severity: critical annotations: summary: &quot; ElasticSearch node &#123;&#123; $labels.instance &#125;&#125;: breakers tripped &gt; 0 and has a value of &#123;&#123; $value &#125;&#125;&quot; description: &quot;ElasticSearch node &#123;&#123; $labels.instance &#125;&#125;: breakers tripped &gt; 0 and has a value of &#123;&#123; $value &#125;&#125;&quot; - alert: Elasticsearch_health_timed_out expr: elasticsearch_cluster_health_timed_out&gt;0 for: 1m labels: severity: critical annotations: summary: &quot; ElasticSearch node &#123;&#123; $labels.instance &#125;&#125;: Number of cluster health checks timed out &gt; 0 and has a value of &#123;&#123; $value &#125;&#125;&quot; description: &quot;ElasticSearch node &#123;&#123; $labels.instance &#125;&#125;: Number of cluster health checks timed out &gt; 0 and has a value of &#123;&#123; $value &#125;&#125;&quot;]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Prometheus删除数据指标]]></title>
    <url>%2Fposts%2F354cfft6.html</url>
    <content type="text"><![CDATA[概述有的时候我们可能希望从 Prometheus 中删除一些不需要的数据指标，或者只是单纯的想要释放一些磁盘空间。Prometheus 中的时间序列只能通过 HTTP API 来进行管理。 默认情况下，管理时间序列的 API 是被禁用的，要启用它，我们需要在 Prometheus 的启动参数中添加--web.enable-admin-api这个参数，比如我们前面的文章中通过 Kubernetes Pod 来部署的，则同样需要添加上这个参数 12345678command:- &quot;/bin/prometheus&quot;args:- &quot;--config.file=/etc/prometheus/prometheus.yml&quot;- &quot;--storage.tsdb.path=/prometheus&quot;- &quot;--storage.tsdb.retention=24h&quot;- &quot;--web.enable-admin-api&quot; # 控制对admin HTTP API的访问，其中包括删除时间序列等功能- &quot;--web.enable-lifecycle&quot; # 支持热更新，直接执行localhost:9090/-/reload立即生效 删除时间序列指标控制管理 API 启用后，可以使用下面的语法来删除与某个标签匹配的所有时间序列指标： 1curl -X POST -g &apos;http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=&#123;kubernetes_name=&quot;redis&quot;&#125;&apos; 上面命令就可以用于删除具有标签kubernetes_name=&quot;redis&quot;的时间序列指标。 如果要删除一些 job 任务或者 instance 的数据指标，则可以使用下面的命令： 12curl -X POST -g &apos;http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=&#123;job=&quot;kubernetes-service-endpoints&quot;&#125;&apos;curl -X POST -g &apos;http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=&#123;instance=&quot;10.244.2.158:9090&quot;&#125;&apos; 要从 Prometheus 中删除所有的数据，可以使用如下命令： 1curl -X POST -g &apos;http://localhost:9090/api/v1/admin/tsdb/delete_series?match[]=&#123;__name__=~&quot;.+&quot;&#125;&apos; 不过需要注意的是上面的 API 调用并不会立即删除数据，实际数据任然还存在磁盘上，会在后面进行数据清理。 要确定何时删除旧数据，可以使用--storage.tsdb.retention参数进行配置（默认情况下，Prometheus 会将数据保留15天）。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Prometheus监控]]></title>
    <url>%2Fposts%2Fe2e612d1.html</url>
    <content type="text"><![CDATA[在Kubernetes上快速部署Prometheus创建一个新的命名空间12345678[root@prometheus]# cat monitor_namespace.yaml apiVersion: v1kind: Namespacemetadata: name: monitor labels: name: monitor[root@prometheus]#kubectl create -f monitor_namespace.yaml rbac文件12345678910111213141516171819202122232425262728293031323334353637383940414243[root@prometheus]# cat rbac-setup.yaml apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: [&quot;&quot;] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- apiGroups: - extensions resources: - ingresses verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- nonResourceURLs: [&quot;/metrics&quot;] verbs: [&quot;get&quot;]---apiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: monitor---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: monitor [root@prometheus]#kubectl create -f rbac-setup.yaml prometheus-deploy文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373[root@prometheus]# cat configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: monitordata: #被引用到/etc/prometheus/prometheus.yml prometheus.yml: | global: #每15s采集一次数据和15s做一次告警检测 scrape_interval: 15s evaluation_interval: 15s #指定加载的告警规则文件 rule_files: - /etc/prometheus/rules.yml #将报警送至何地进行报警 alerting: alertmanagers: - static_configs: - targets: [&quot;192.168.50.60:9093&quot;] #指定prometheus要监控的目标 scrape_configs: - job_name: &apos;k8s-node&apos; scrape_interval: 10s static_configs: - targets: - &apos;192.168.50.61:31672&apos; #自定义获取监控数据,每个 job_name 都是独立的 - job_name: &apos;tomcat-pods&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_service_annotation_prometheus_io_jvm_scrape] regex: true;true action: keep - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_app_metrics_patn] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__meta_kubernetes_pod_ip, __meta_kubernetes_service_annotation_prometheus_io_app_metrics_port] action: replace target_label: __address__ regex: (.+);(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_pod_host_ip] action: replace target_label: kubernetes_host_ip - job_name: &apos;kubernetes-apiservers&apos; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: &apos;kubernetes-nodes&apos; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics - job_name: &apos;kubernetes-cadvisor&apos; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: &apos;kubernetes-service-endpoints&apos; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: &apos;kubernetes-services&apos; kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: &apos;kubernetes-ingresses&apos; kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path] regex: (.+);(.+);(.+) replacement: $&#123;1&#125;://$&#123;2&#125;$&#123;3&#125; target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: kubernetes_name - job_name: &apos;kubernetes-pods&apos; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # 监控规则文件,被引用到/etc/prometheus/rules.yml rules.yml: | groups: - name: test-rule rules: ############# Node监控 ############# - alert: k8s-node状态异常 expr: up&#123;job=&quot;k8s-node&quot;&#125; != 1 for: 3m labels: team: k8s-node annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点状态异常&quot; description: &quot;可能是重启了&quot; - alert: k8s-node节点CPU使用率 expr: (1 - avg(irate(node_cpu_seconds_total&#123;job=&quot;k8s-node&quot;,mode=&quot;idle&quot;&#125;[1m])) by (instance)) * 100 &gt; 95 for: 1m labels: team: k8s-node annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点CPU使用率超过95%&quot; description: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点当前CPU使用率为: &#123;&#123; $value &#125;&#125;&quot; - alert: k8s-node节点磁盘使用率 expr: (node_filesystem_size_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125; - node_filesystem_avail_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125;) / node_filesystem_size_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125; * 100 &gt; 85 for: 1m labels: team: k8s-node annotations: description: &quot;Node服务器[[ &#123;&#123;$labels.instance&#125;&#125; ]] 的 &#123;&#123;mountpoint&#125;&#125; 磁盘空间使用率超过85%&quot; summary: &quot;磁盘 &#123;&#123;$labels.device&#125;&#125; 当前使用率为: &#123;&#123; $value &#125;&#125;&quot; - alert: k8s-node节点内存使用率 expr: (node_memory_MemTotal_bytes&#123;job=&quot;k8s-node&quot;&#125; - (node_memory_Buffers_bytes&#123;job=&quot;k8s-node&quot;&#125; + node_memory_Cached_bytes&#123;job=&quot;k8s-node&quot;&#125; + node_memory_MemFree_bytes&#123;job=&quot;k8s-node&quot;&#125;)) / node_memory_MemTotal_bytes&#123;job=&quot;k8s-node&quot;&#125; * 100 for: 1m labels: team: k8s-node annotations: description: &quot;Node服务器[[ &#123;&#123;$labels.instance&#125;&#125; ]] 内存使用率超过95%&quot; summary: &quot;&#123;&#123;$labels.instance&#125;&#125; 当前内存使用率为: &#123;&#123; $value &#125;&#125;&quot; ############ Pod 监控 ############ - alert: 监控k8s的pod状态异常 expr: up&#123;kubernetes_namespace=&quot;monitor&quot;&#125; != 1 for: 3m labels: team: &quot;kube-state-metrics&quot; annotations: description: &quot;&#123;&#123;$labels.kubernetes_namespace&#125;&#125; 内的 pod 状态有变动&quot; summary: &quot;此 Pod 用于获取 k8s 监控数据, 绑定在一个节点上&quot; - alert: 应用的 pod 状态有变动 expr: kube_pod_container_status_ready&#123;namespace=&quot;product&quot;&#125; != 1 for: 3m labels: status: &quot;product 命名空间内的 pod &#123;&#123;$labels.pod&#125;&#125;有变动&quot; annotations: description: &quot;Deployment &#123;&#123;$labels.container&#125;&#125; 内的 pod 状态有变动&quot; summary: &quot;可能是重启或者在升级版本,如果频繁重启,请跟踪排查问题&quot; - alert: 以下应用的 pod 重启次数已经超过15,请查看原因 expr: kube_pod_container_status_restarts_total&#123;namespace=&quot;product&quot;&#125; &gt; 15 for: 3m labels: status: &quot;product 命名空间内的 pod &#123;&#123;$labels.pod&#125;&#125; 重启次数太多&quot; annotations: description: &quot;Deployment &#123;&#123;$labels.container&#125;&#125; 内的 pod 重启次数太多&quot; summary: &quot;重启次数太多,可能是因为 pod 内应用有问题&quot; ########### Java 监控 ############ - alert: jvm线程数过高 expr: jvm_threads_current&#123;job=&quot;tomcat-pods&quot;&#125;&gt;2000 for: 1m labels: status: &quot;空间内 jvm 的变动情况&quot; annotations: description: &quot;&#123;&#123;$labels.kubernetes_pod_name&#125;&#125;: Jvm线程数过高&quot; summary: &apos;&#123;&#123; $labels.kubernetes_pod_name &#125;&#125; : 当前你线程值为: &#123;&#123; $value &#125;&#125;&apos; [root@prometheus]# cat prometheus.deploy.yml ---apiVersion: apps/v1beta2kind: Deploymentmetadata: labels: name: prometheus-deployment name: prometheus namespace: monitorspec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: containers: - image: prom/prometheus:v2.6.0 name: prometheus command: - &quot;/bin/prometheus&quot; args: - &quot;--config.file=/etc/prometheus/prometheus.yml&quot; - &quot;--storage.tsdb.path=/home/prometheus&quot; - &quot;--storage.tsdb.retention=168h&quot; - &quot;--web.enable-lifecycle&quot; ports: - containerPort: 9090 protocol: TCP volumeMounts: - mountPath: &quot;/home/prometheus&quot; name: data - mountPath: &quot;/etc/prometheus&quot; name: config-volume - mountPath: &quot;/etc/localtime&quot; readOnly: false name: localtime resources: requests: cpu: 100m memory: 2048Mi limits: cpu: 500m memory: 3180Mi serviceAccountName: prometheus nodeSelector: nodetype: prometheus volumes: - name: data hostPath: path: &quot;/opt/prometheus/data&quot; - name: config-volume configMap: name: prometheus-config - name: localtime hostPath: path: &quot;/etc/localtime&quot; type: File [root@prometheus]# cat prometheus.svc.yml ---kind: ServiceapiVersion: v1metadata: labels: app: prometheus name: prometheus namespace: monitorspec: type: NodePort ports: - port: 9090 targetPort: 9090 nodePort: 30003 selector: app: prometheus [root@prometheus]#kubectl create -f configmap.yaml[root@prometheus]#kubectl create -f prometheus.deploy.yml[root@prometheus]#kubectl create -f prometheus.svc.yml注：需要在本地创建/opt/prometheus/data作为prometheus数据路径，另需要给data目录赋予777权限 热重读配置文件congfigmap有热重启功能，这样每次改完配置文件都不需要重启prometheus的pod来重读配置了 123456789- &quot;--web.enable-lifecycle&quot;在prometheus.deploy.yml的配置文件里面加上这段话就可以了[root@prometheus]# cat reload-prometheus.sh #!/bin/bashkubectl apply -f configmap.yamlsleep 60curl -XPOST http://192.168.50.60:30003/-/reload可以写个脚本，每次修改完配置文件的配置之后，执行一下脚本就可以同步生效了！ 安装kube-state-metrics123[root@prometheus]# git clone https://github.com/kubernetes/kube-state-metrics.git之后把默认的命名空间改成monitor，进入kube-state-metrics目录[root@prometheus]#kubectl create -f ./ 安装grafana12345678910111213141516创建grafana的数据目录mkdir /opt/grafana/data启动脚本[root@grafana]# cat start_grafana.sh #!/bin/bashdocker stop `docker ps -a |awk &apos;/grafana/&#123;print $1&#125;&apos;`docker rm `docker ps -a |awk &apos;/grafana/&#123;print $1&#125;&apos;`docker run -d \ --name=grafana \ --restart=always \ -p 3000:3000 \ -m 4096m \ -v /opt/grafana/data:/var/lib/grafana \ -v /opt/grafana/log:/var/log/grafana \ grafana/grafana:5.4.3 1、安装完之后，需要添加source，source直接点prometheus，链接就是http://192.168.50.60:30003之前创建的prometheus界面 2、添加模板dashboad（列出几个常用的） 点import导入，有俩种方式，直接填官网模板，或者导入json https://grafana.com/dashboards/9276 node的cpu、内存等 https://grafana.com/dashboards/3146 pod https://grafana.com/dashboards/8588 deployment 安装alertmanager创建配置文件、目录1234567891011121314151617181920212223242526272829303132创建alert数据目录mkdir /opt/alert/data注意：需要alertmanager.yml配置，此配置钉钉和邮件可同时放松[root@docker60 alert]# cat alertmanager.yml global: resolve_timeout: 5mroute: group_by: [&apos;alertname&apos;] group_wait: 10s group_interval: 10s repeat_interval: 6m receiver: defaultreceivers:- name: &apos;default&apos; email_configs: - to: &quot;&quot; send_resolved: true from: &quot;&quot; smarthost: &quot;smtp.xxx.com:25&quot; auth_username: &quot;&quot; auth_password: &quot;&quot; webhook_configs: - url: &apos;http://192.168.50.60:8060/dingtalk/ops_dingding/send&apos; send_resolved: trueinhibit_rules: - source_match: severity: &apos;critical&apos; target_match: severity: &apos;warning&apos; equal: [&apos;alertname&apos;] 启动脚本1234567891011121314[root@alert]# cat start_alert.sh#!/bin/bashdocker stop `docker ps -a |awk &apos;/alertmanager/&#123;print $1&#125;&apos;`docker rm `docker ps -a |awk &apos;/alertmanager/&#123;print $1&#125;&apos;`docker run -d \ --name alertmanager \ --restart=always \ -p 9093:9093 \ -v /etc/localtime:/etc/localtime:ro \ -v /opt/alert/alertmanager.yml:/etc/alertmanager/alertmanager.yml \ -v /opt/alert/data:/alertmanager \ prom/alertmanager:v0.15.3 安装dingding插件1234567891011121、安装go （这里就不叙述了）2、假设go的路径是/usr/local/gomkdir -pv /usr/local/go/src/github.com/timonwong3、下载dingding插件git clone https://github.com/timonwong/prometheus-webhook-dingtalk.git4、添加dingding机器人在dingding群里面添加即可5、启动dingding[root@alert]# cat start_dingding.sh cd /usr/local/go/src/github.com/timonwong/prometheus-webhook-dingtalkkill -9 `ps -ef | grep prometheus-webhook-dingtalk | grep -v grep | awk &apos;&#123;print $2&#125;&apos;`nohup ./prometheus-webhook-dingtalk --ding.profile=&quot;ops_dingding=https://oapi.dingtalk.com/robot/send?access_token=xxxx&quot; 2&gt;&amp;1 1&gt;dingding.log &amp;]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jira-安装及破解]]></title>
    <url>%2Fposts%2F6924c90a.html</url>
    <content type="text"><![CDATA[安装jiraJIRA是Atlassian公司出品的项目与事务跟踪工具，被广泛应用于缺陷跟踪、客户服务、需求收集、流程审批、任务跟踪、项目跟踪和敏捷管理等工作领域。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263下载包：wget https://product-downloads.atlassian.com/software/jira/downloads/atlassian-jira-software-7.13.0-x64.bin[root@YZSJHL82-204 ~]# chmod +x atlassian-jira-software-7.13.0-x64.bin[root@YZSJHL82-204 ~]# ./atlassian-jira-software-7.13.0-x64.binUnpacking JRE ...Starting Installer ...十月 23, 2018 4:38:25 下午 java.util.prefs.FileSystemPreferences$1 run信息: Created user preferences directory.十月 23, 2018 4:38:25 下午 java.util.prefs.FileSystemPreferences$2 run信息: Created system preferences directory in java.home.This will install JIRA Software 7.4.1 on your computer.OK [o, Enter], Cancel [c]o #按o安装Choose the appropriate installation or upgrade option.Please choose one of the following:Express Install (use default settings) [1], Custom Install (recommended for advanced users) [2, Enter], Upgrade an existing JIRA installation [3]2 #2为自定义安装Where should JIRA Software be installed?[/opt/atlassian/jira]/usr/local/atlassina/jira #自定义安装目录Default location for JIRA Software data[/var/atlassian/application-data/jira]/usr/local/atlassina/jira_data #自定义数据目录Configure which ports JIRA Software will use.JIRA requires two TCP ports that are not being used by any otherapplications on this machine. The HTTP port is where you will access JIRAthrough your browser. The Control port is used to startup and shutdown JIRA.Use default ports (HTTP: 8080, Control: 8005) - Recommended [1, Enter], Set custom value for HTTP and Control ports [2]2 #2为自定义端口HTTP Port Number[8080] #8080为默认端口8050 #http连接端口Control Port Number[8005]8040 #控制端口JIRA can be run in the background.You may choose to run JIRA as a service, which means it will startautomatically whenever the computer restarts.Install JIRA as Service?Yes [y, Enter], No [n]y #是否开机自启Details on where JIRA Software will be installed and the settings that will be used.Installation Directory: /usr/local/atlassina/jira Home Directory: /usr/local/atlassina/jira_data HTTP Port: 8050 RMI Port: 8040 Install as service: Yes Install [i, Enter], Exit [e]i #确认已选配置Extracting files ...Please wait a few moments while JIRA Software is configured.Installation of JIRA Software 7.4.1 is completeStart JIRA Software 7.4.1 now?Yes [y, Enter], No [n]y #启动Please wait a few moments while JIRA Software starts up.Launching JIRA Software ...Installation of JIRA Software 7.4.1 is completeYour installation of JIRA Software 7.4.1 is now ready and can be accessedvia your browser.JIRA Software 7.4.1 can be accessed at http://localhost:8050Finishing installation ... 浏览器访问jira，地址为：http://IP:8050 请自行修改IP和端口。如果可以访问，说明安装成功。 配置数据库及密码在mySQL上创建用户及库做授权123create database jira_new;grant all privileges on *.* to jira@&apos;10.4.82.204&apos; identified by &apos;jira&apos;;flush privileges; 在授权完用户我们不可以马上填写信息，需要添加MySQL的一个jra包，否则下一步会提示找不到mysql的驱动 wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.zip1234567停止jira[root@YZSJHL82-204 ~]# /etc/init.d/jira stop上传软件包[root@YZSJHL82-204 ~]# cp mysql-connector-java-5.1.46-bin.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/启动jira[root@YZSJHL82-204 ~]# /etc/init.d/jira start注意防火墙 安装完数据库插件即可下一步: 设置jira主题 因为第一次安装，我们需要去jira官网注册用户，获取授权码 (免费30天，安装后更换破解即可) 保存好服务器ID，进入atlassian官网获取试用许可证，下边附上注册地址： 注册官网：https://my.atlassian.com 或使用以下地址： https://id.atlassian.com/signup?application=mac&amp;continue=https://my.atlassian.com 登陆账号后，选择New Evaluation License 设置管理员用户:官网注册的账号只可以免费试用30天，所以当我们安装完需要尽快进行破解 破解jirahttps://download.csdn.net/download/lbwahoo/100308071234567停止jira[root@YZSJHL82-204 ~]# /etc/init.d/jira stop进入安装目录下的atlassian-jira/WEB-INF/lib/目录下，用破解包atlassian-extras-3.2.jar替换原来的包。并将mysql连接驱动复制到此目录下。[root@YZSJHL82-204 ~]# cp atlassian-extras-3.2.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/启动jira[root@YZSJHL82-204 ~]# /etc/init.d/jira start注意防火墙 配置数据库连接地址12/var/atlassian/application-data/jira/dbconfig.xml#此路径为默认路径]]></content>
      <categories>
        <category>版本管理工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python删除mongo表]]></title>
    <url>%2Fposts%2F7c0c3da0.html</url>
    <content type="text"><![CDATA[PyMongoPython 要连接 MongoDB 需要 MongoDB 驱动，这里我们使用 PyMongo 驱动来连接。 pip 安装pip 是一个通用的 Python 包管理工具，提供了对 Python 包的查找、下载、安装、卸载的功能。 安装 pymongo: 1$ python3 -m pip3 install pymongo 也可以指定安装的版本: 1$ python3 -m pip3 install pymongo==3.5.1 更新 pymongo 命令： 1$ python3 -m pip3 install --upgrade pymongo easy_install 安装旧版的 Python 可以使用 easy_install 来安装，easy_install 也是 Python 包管理工具。 1$ python -m easy_install pymongo 更新 pymongo 命令： 1$ python -m easy_install -U pymongo 创建数据库创建数据库需要使用 MongoClient 对象，并且指定连接的 URL 地址和要创建的数据库名。 如下实例中，我们创建的数据库 aa : 123456#!/usr/bin/python3 import pymongo myclient = pymongo.MongoClient(&quot;mongodb://localhost:27017/&quot;)mydb = myclient[&quot;aa&quot;] 删除表12345678910111213141516171819202122232425262728293031323334#!/usr/bin/env python#-*- coding: utf-8 -*-from pymongo import MongoClientfrom datetime import datetimedef delete(year,month,day): try: client = MongoClient(&apos;mongodb://192.168.50.223:27017,192.168.50.224:27017,192.168.50.225:27017&apos;) db_auth = client.admin db_auth.authenticate(&quot;root&quot;, &quot;passwd&quot;) db = client.gag_bill old_count = db.billInfo.count() print (&quot;old_count = %d&quot; % (old_count)) db.billInfo.remove(&#123;&quot;cTimeStamp&quot;:&#123;&quot;$lte&quot;:datetime(year,month,day,0,0,0,000)&#125;&#125;) new_count = db.billInfo.count() client.close() print (&quot;del_data = %d&quot; %(old_count-new_count)) print (&quot;new_count = %d&quot; % (new_count)) except Exception as e: print (e)if __name__ == &apos;__main__&apos;: starttime = datetime.now() print (&quot;start_time = %s&quot; % (starttime)) year = starttime.year month = starttime.month day = starttime.day-4 delete(year,month,day) endtime = datetime.now() print (&quot;end_time = %s&quot; % (endtime)) runtime = (endtime - starttime).seconds print (&quot;run_time = %d seconds&quot; % (runtime))]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mongo副本集配置及一些常用命令]]></title>
    <url>%2Fposts%2F24c2f149.html</url>
    <content type="text"><![CDATA[在每台机器都配置下mongo配置文件123456789101112[root@mangodb1 conf]# cat mongod.conf dbpath= /data/mongodb/data/logpath=/data/mongodb/logs/mongod.loglogappend=truefork=truemaxConns=2000bind_ip=127.0.0.1,10.92.160.5（IP或者主机名）directoryperdb=true#auth=truepidfilepath=/data/mongodb/logs/mongod.pidreplSet = rep#keyFile = /data/mongodb/conf/mongokey 配置mongo副本集1234config = &#123; _id:&quot;rep&quot;,members:[&#123;_id:0,host:&quot;10.92.160.5:27017&quot;&#125;,&#123;_id:1,host:&quot;10.92.160.6:27017&quot;&#125;,&#123;_id:2,host:&quot;10.92.160.7:27017&quot;&#125;]&#125;rs.initiate(config);rs.status(); （查看集群状态的） 常用命令基本命令1234show dbs 看库use 库db.setSlaveOk()show tables 看表 创建admin用户12use admindb.addUser(&quot;root&quot;,&quot;123456&quot;) 导出表加个-c1/data/mongodb/bin/mongoexport -uroot -p123456 --authenticationDatabase admin -d gag_shop -c organizationManagerAuthorities -o organizationManagerAuthorities.json 导入1/data/mongodb/bin/mongoimport -uroot -p123456 --authenticationDatabase admin -d open /root/userInterfaceAuthority.json 查表某个字段信息12db.表名字.findOne(&#123;&#125;)db.terminalMonitorInfo.findOne(&#123;&quot;_id&quot; : &quot;086273F59379&quot;&#125;) 清除日志123db.runCommand( &#123; dropDatabase: 1 &#125; ) 清楚日志，需谨慎，必须得进指定的库里面或者echo &quot;db.runCommand(&#123;dropDatabase:1&#125;)&quot; | /home/mongodb/bin/mongo -uprivate -pPrivate 127.0.0.1:27017/gag_log 导出命令1/data/server/mongodb/bin/mongoexport -uroot -pNTA3NAa579 --authenticationDatabase admin -d gag_base -c sysAuthority -q &quot;&#123;&quot;_id&quot; : /new_pro/&#125;&quot; -o new_pro.txt 导出命令]]></content>
      <categories>
        <category>中间件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s拉取Harbor私有镜像]]></title>
    <url>%2Fposts%2F5trce44e.html</url>
    <content type="text"><![CDATA[secret介绍1Secret是用来保存小片敏感数据的k8s资源，例如密码，token，或者秘钥。这类数据当然也可以存放在Pod或者镜像中，但是放在Secret中是为了更方便的控制如何使用数据，并减少暴露的风险。 创建先在服务器上登录 Harbor 仓库 123docker login registry.sy.com输入账号密码 查看登录的秘钥数据 12登录成功后会在当前用户下生成 .docker/config.json 文件cat ~/.docker/config.json 12再对上面的 config.json 进行base64加密cat ~/.docker/config.json |base64 -w 0 创建 secret.yaml 文件 1234567apiVersion: v1kind: Secretmetadata: name: logintype: kubernetes.io/dockerconfigjsondata: .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJ0Y2xvdWQuaHViIjogewoJCQkiYXV0aCI6ICJZV1J0YVc0NlNHRnlZbTl5TVRJek5EVT0iCgkJfQoJfSwKCSJIdHRwSGVhZGVycyI6IHsKCQkiVXNlci1BZ2VudCI6ICJEb2NrZXItQ2xpZW50LzE3LjA5LjAtY2UgKGxpbnV4KSIKCX0KfQ== 1kubectl create -f secret.yaml]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s的亲和性]]></title>
    <url>%2Fposts%2Foigg664a.html</url>
    <content type="text"><![CDATA[摘要一般情况下我们部署的 POD 是通过集群自动调度选择某个节点的，默认情况下调度器考虑的是资源足够，并且负载尽量平均，但是有的时候我们需要能够更加细粒度的去控制 POD 的调度，比如我们内部的一些服务 gitlab 之类的也是跑在Kubernetes集群上的，我们就不希望对外的一些服务和内部的服务跑在同一个节点上了，害怕内部服务对外部的服务产生影响；有的时候呢我们两个服务直接交流比较频繁，又希望能够将这两个服务的 POD 调度到同样的节点上。这就需要用到 Kubernetes 里面的一个概念：亲和性，亲和性主要分为两类：nodeAffinity和podAffinity。 nodeSelector我们知道label是kubernetes中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，比如最常见的一个就是 service 通过匹配 label 去选择 POD 的。而 POD 的调度也可以根据节点的 label 进行特定的部署。 我们可以通过下面的命令查看我们的 node 的 label： 1234[root@node1 ~]# kubectl get nodes --show-labelsNAME STATUS ROLES AGE VERSION LABELSnode1 Ready &lt;none&gt; 10d v1.14.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linuxnode2 Ready &lt;none&gt; 10d v1.14.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux 123456[root@node1 ~]# kubectl label nodes node1 source=synode/node1 labeled[root@node1 ~]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELSnode1 Ready &lt;none&gt; 10d v1.14.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux,source=synode2 Ready &lt;none&gt; 10d v1.14.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux 我们可以通过上面的--show-labels参数可以查看上述标签是否生效。当 node 被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在 POD 的 spec 字段中添加nodeSelector字段，里面是我们需要被调度的节点的 label。例如，下面是我们之前的一个默认的 busybox POD 的 YAML 文件： 1234567891011121314apiVersion: v1kind: Podmetadata: labels: app: busybox-pod name: test-busyboxspec: containers: - command: - sleep - &quot;3600&quot; image: busybox imagePullPolicy: Always name: test-busybox 然后我需要让上面的 POD 被调度到140的节点上，那么最简单的方法就是去匹配140上面的 label，如下： 12345678910111213141516apiVersion: v1kind: Podmetadata: labels: app: busybox-pod name: test-busyboxspec: containers: - command: - sleep - &quot;3600&quot; image: busybox imagePullPolicy: Always name: test-busybox nodeSelector: source: sy nodeAffinitynodeAffinity就是节点亲和性，相对应的是Anti-Affinity，就是反亲和性，这种方法比上面的nodeSelector更加灵活，它可以进行一些简单的逻辑组合了，不只是简单的相等匹配。 调度可以分成软策略和硬策略两种方式，软策略就是如果你没有满足调度要求的节点的话，POD 就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有的话也无所谓了的策略；而硬策略就比较强硬了，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然我就不干的策略。 nodeAffinity就有两上面两种策略：preferredDuringSchedulingIgnoredDuringExecution和requiredDuringSchedulingIgnoredDuringExecution，前面的就是软策略，后面的就是硬策略。 如下例子：（test-node-affinity.yaml） 123456789101112131415161718192021222324252627apiVersion: v1kind: Podmetadata: name: with-node-affinity labels: app: node-affinity-podspec: containers: - name: with-node-affinity image: nginx affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: NotIn values: - node1 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: source operator: In values: - sy 上面这个 POD 首先是要求 POD 不能运行在node1这个节点上，如果有个节点满足source=sy的话就优先调度到这个节点上，同样的我们可以使用descirbe命令查看具体的调度情况是否满足我们的要求。这里的匹配逻辑是 label 的值在某个列表中，现在Kubernetes提供的操作符有下面的几种： In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Gt：label 的值大于某个值 Lt：label 的值小于某个值 Exists：某个 label 存在 DoesNotExist：某个 label 不存在 如果nodeSelectorTerms下面有多个选项的话，满足任何一个条件就可以了；如果matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度 POD。 podAffinity上面两种方式都是让 POD 去选择节点的，有的时候我们也希望能够根据 POD 之间的关系进行调度，Kubernetes在1.4版本引入的podAffinity概念就可以实现我们这个需求。 和nodeAffinity类似，podAffinity也有requiredDuringSchedulingIgnoredDuringExecution和 preferredDuringSchedulingIgnoredDuringExecution两种调度策略，唯一不同的是如果要使用互斥性，我们需要使用podAntiAffinity字段。 如下例子，我们希望with-pod-affinity和busybox-pod能够就近部署，而不希望和node-affinity-pod部署在同一个拓扑域下面：（test-pod-affinity.yaml） 12345678910111213141516171819202122232425262728293031apiVersion: v1kind: Podmetadata: name: with-pod-affinity labels: app: pod-affinity-podspec: containers: - name: with-pod-affinity image: nginx affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - busybox-pod topologyKey: kubernetes.io/hostname podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - node-affinity-pod topologyKey: kubernetes.io/hostname 上面这个例子中的 POD 需要调度到某个指定的主机上，至少有一个节点上运行了这样的 POD：这个 POD 有一个app=busybox-pod的 label。podAntiAffinity则是希望最好不要调度到这样的节点：这个节点上运行了某个 POD，而这个 POD 有app=node-affinity-pod的 label。 亲和性/反亲和性调度策略比较如下： 调度策略 匹配标签 操作符 拓扑域支持 调度目标 nodeAffinity 主机 In, NotIn, Exists, DoesNotExist, Gt, Lt 否 指定主机 podAffinity POD In, NotIn, Exists, DoesNotExist 是 POD与指定POD同一拓扑域 podAnitAffinity POD In, NotIn, Exists, DoesNotExist 是 POD与指定POD不在同一拓扑域 污点（Taints）与容忍（tolerations）对于nodeAffinity无论是硬策略还是软策略方式，都是调度 POD 到预期节点上，而Taints恰好与之相反，如果一个节点标记为 Taints ，除非 POD 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度pod。 比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 POD，则污点就很有用了，POD 不会再被调度到 taint 标记过的节点 1kubectl taint nodes node1 key=value:NoSchedule 如果仍然希望某个 POD 调度到 taint 节点上，则必须在 Spec 中做出Toleration定义，才能调度到该节点，举例如下： 12345tolerations:- key: &quot;key&quot;operator: &quot;Equal&quot;value: &quot;value&quot;effect: &quot;NoSchedule&quot; effect 共有三个可选项，可按实际需求进行设置： NoSchedule：POD 不会被调度到标记为 taints 节点。 PreferNoSchedule：NoSchedule 的软策略版本。 NoExecute：该选项意味着一旦 Taint 生效，如该节点内正在运行的 POD 没有对应 Tolerate 设置，会直接被逐出。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s的ConfigMap]]></title>
    <url>%2Fposts%2Fdfjg544e.html</url>
    <content type="text"><![CDATA[介绍我们经常都需要为我们的应用程序配置一些特殊的数据，比如密钥、Token 、数据库连接地址或者其他私密的信息。你的应用可能会使用一些特定的配置文件进行配置，比如settings.py文件，或者我们可以在应用的业务逻辑中读取环境变量或者某些标志来处理配置信息。 当然你可以直接将这些应用配置信息直接硬编码到你的应用程序中去，对于一个小型的应用，这或许是可以接受的，但是，对于一个相对较大的应用程序或者微服务的话，硬编码就会变得难以管理了。比如你现在有10个微服务，都连接了数据库A，如果现在需要更改数据库A的连接地址的话，就需要修改10个地方，显然这是难以忍受的。 当然，我们可以使用环境变量和统一的配置文件来解决这个问题，当我们想改变配置的时候，只需要更改环境变量或者配置文件就可以了，但是对于微服务来说的话，这也是比较麻烦的一件事情，Docker 允许我们在 Dockerfile 中指定环境变量，但是如果我们需要在不同的容器中引用相同的数据呢，如果我们的应用程序是运行在集群上的时候，对于配置主机的环境变量也是难以管理的了 （秘钥用secret） configMap用法configMap应用场景 生成为容器内的环境变量 设置容器的启动命令的启动参数 (需要设置为环境变量） 以Volume的形式挂载为容器内部的文件或目录 configMap创建1ConfigMap以一个或多个key:value的形式保存在Kubernetes系统中供应用使用，既可以用于表示一个变量的值(例如apploglevel=info)，也可以用于表示一个完整配置文件的内容(server.xml=&lt;?xml..&gt;..) ConfigMap创建方式 通过直接在命令行中指定configmap参数创建，即–from-literal 通过指定文件创建，即将一个配置文件创建为一个ConfigMap –from-file=&lt;文件&gt; 通过指定目录创建，即将一个目录下的所有配置文件创建为一个ConfigMap，–from-file=&lt;目录&gt; 事先写好标准的configmap的yaml文件，然后kubectl create -f 创建 –from-literal方式创建创建configMap 1kubectl create configmap nginx-config --from-literal=nginx_port=80 --from-literal=server_name=sy.com 替换 1kubectl create configmap nginx-config --from-literal=nginx_port=8080 -o yaml --dry-run | kubectl replace -f - –from-file方式创建注意Pod使用configMap 使用env方式 当configMap修改时，内部环境变量不会动态改变 使用volumeMount方式 修改configMap环境变量随之变化]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kubectl在shell中的自动补全]]></title>
    <url>%2Fposts%2F4ddce2e2.html</url>
    <content type="text"><![CDATA[在zsh上设置1source &lt;(kubectl completion zsh) 可以导入到.zshrc中实现自动加载： 1kubectl completion zsh &gt;&gt; ~/.zshrc 键入-n以后，按tab，自动弹出可用的ns： 12➜ admin kubectl -ndefault demo-echo demo-webshell kong kube-public kube-system 在linux上设置1234yum install bash-completionsource /usr/share/bash-completion/bash_completionecho &apos;source &lt;(kubectl completion bash)&apos; &gt;&gt;~/.bashrckubectl completion bash &gt;/etc/bash_completion.d/kubectl 在mas上设置-bash12345brew install bash-completion@2export BASH_COMPLETION_COMPAT_DIR=/usr/local/etc/bash_completion.d[[ -r /usr/local/etc/profile.d/bash_completion.sh ]] &amp;&amp; . /usr/local/etc/profile.d/bash_completion.shecho &apos;source &lt;(kubectl completion bash)&apos; &gt;&gt;~/.bashrckubectl completion bash &gt;/usr/local/etc/bash_completion.d/kubectl]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s基本命令]]></title>
    <url>%2Fposts%2F5b3b5a45.html</url>
    <content type="text"><![CDATA[pods12345$ kubectl get pods -n pro$ kubectl get pods --all-namespaces -n pro$ kubectl get pod aa -o wide -n pro$ kubectl get pod aa -o yaml -n pro$ kubectl describe pod aa -n pro 不用grep列出node上的pod1kubectl get pods --field-selector spec.nodeName=node04 --all-namespaces POD升级和历史列出部署历史记录1$ kubectl rollout history deployment/DEPLOYMENT_NAME 跳转到特定修订版1$ kubectl rollout undo deployment/DEPLOYMENT_NAME --to-revision=N service查看服务1$ kubectl get services 将POD作为服务公开（创建端点）1$ kubectl expose deployment/aa --port=2000 --type=NodePort login1kubectl exec -ti 1 bash -n product log1kubectl logs -f 1 -n product]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch基础入门]]></title>
    <url>%2Fposts%2F8b341200.html</url>
    <content type="text"><![CDATA[什么是 ElasticSearchElasticSearch是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口。Elasticsearch 是用 Java 开发的，并作为 Apache 许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 基础概念索引：含有相同属性的文档集合 类型：索引可以定义一个或多个类型，文档必须属于一个类型 文档：可以被索引的基础数据单位 分片：每个索引都有多个分片，每个分片都是 Lucene 索引 备份：拷贝一份分片就完成分片的备份 应用场景 海量数据分析引擎 站内搜索引擎 数据仓库 安装和配置依赖环境JDK 和 NodeJS 安装jdk，和node这里就不写了，源码安装即可！ 下载登陆 elasticSearch 官网下载文件。 安装1234567891011121314151617181920212223242526272829303132333435363738394041#增加 elk 组groupadd elk#增加 elk 用户并附加到 es 组useradd elk -g elk -p elkwget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.2.4.tar.gztar -zxvf elasticsearch-6.2.4.tar.gz -C /data/elkmkdir /data/elk/dataelastic/ulimit -n 65536vim /etc/sysctl.confvm.max_map_count=262144#给目录权限chown -R elk:elk elasticsearch-6.2.4#使用es用户su elk配置文件修改如下： path.data: /data/elk/dataelastic/ cluster.name: gag-elk node.name: gag-elk-node-92 network.host: 192.168.50.92 http.port: 9200 transport.tcp.port: 9300 node.master: true node.data: true discovery.zen.ping.unicast.hosts: [&quot;192.168.50.92&quot;,&quot;192.168.50.93&quot;,&quot;192.168.50.94&quot;] discovery.zen.minimum_master_nodes: 2 gateway.recover_after_nodes: 2 gateway.expected_nodes: 2 gateway.recover_after_time: 1m #检测到副本数不够的时候，1分钟之后才开始同步 xpack.security.enabled: false thread_pool.index.size: 8 thread_pool.index.queue_size: 8000 thread_pool.bulk.size: 8 thread_pool.bulk.queue_size: 8000另俩台机器也这么操作,就可以配置es集群 启动1bin/elasticsearch 或 bin/elasticsearch -d # -d 表示后台启动 下载插件1234567wget https://github.com/shenshengkun/elasticsearch-head/archive/master.zipunzip master.zipcd elasticsearch-head-masternpm installnpm run start也可以用谷歌浏览器，搜索扩展程序ElasticSearch Head，就可以直接使用head插件 Elasticsearch常用命令123curl -XDELETE &apos;http://host.IP.address:9200/logstash-*&apos; 删除索引(后面为索引名称)curl -XGET &apos;host.IP.address:9200/_cat/health?v&amp;pretty&apos; 查看集群状态curl -XGET &apos;host.IP.address:9200/_cat/indices?v&amp;pretty&apos; 查看索引]]></content>
      <categories>
        <category>elk</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[rancher升级]]></title>
    <url>%2Fposts%2Fdf5a27ca.html</url>
    <content type="text"><![CDATA[停掉 rancher先把之前的rancher-server停掉，然后在/etc/hosts上加上ip还有主机名的解析，否则升级完主机不识别 下载1.6.8镜像123docker pull privatecloud.docker.domain/privatecloud3.0/rancher_server:1.6.8在官网下载就有，我这个是自己的私有仓库 运行rancherserver1docker run -d -v /data/rancherdata:/var/lib/mysql --restart=always -p 48080:8080 privatecloud.docker.domain/privatecloud3.0/rancher_server:1.6.8 获取rancher的代理镜像1234567891011121314151617181920docker login privatecloud.docker.domain 从镜像库拉取镜像docker pull privatecloud.docker.domain/privatecloud3.0/rancher_agent:v1.2.6docker pull privatecloud.docker.domain/privatecloud3.0/rancher_network-manager:v0.7.8docker pull privatecloud.docker.domain/privatecloud3.0/rancher_net:v0.11.9docker pull privatecloud.docker.domain/privatecloud3.0/rancher_dns:v0.15.3docker pull privatecloud.docker.domain/privatecloud3.0/rancher_metadata:v0.9.4docker pull privatecloud.docker.domain/privatecloud3.0/rancher_healthcheck:v0.3.3docker pull privatecloud.docker.domain/privatecloud3.0/rancher_scheduler:v0.8.2docker pull privatecloud.docker.domain/privatecloud3.0/rancher_net:holder重命名镜像：docker tag privatecloud.docker.domain/privatecloud3.0/rancher_agent:v1.2.6 rancher/agent:v1.2.6docker tag privatecloud.docker.domain/privatecloud3.0/rancher_network-manager:v0.7.8 rancher/network-manager:v0.7.8docker tag privatecloud.docker.domain/privatecloud3.0/rancher_net:v0.11.9 rancher/net:v0.11.9docker tag privatecloud.docker.domain/privatecloud3.0/rancher_dns:v0.15.3 rancher/dns:v0.15.3docker tag privatecloud.docker.domain/privatecloud3.0/rancher_metadata:v0.9.4 rancher/metadata:v0.9.4docker tag privatecloud.docker.domain/privatecloud3.0/rancher_healthcheck:v0.3.3 rancher/healthcheck:v0.3.3docker tag privatecloud.docker.domain/privatecloud3.0/rancher_scheduler:v0.8.2 rancher/scheduler:v0.8.2docker tag privatecloud.docker.domain/privatecloud3.0/rancher_net:holder rancher/net:holder 升级12进入http://rancherserverip:48080/然后点升级即可]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-sqlite3基础]]></title>
    <url>%2Fposts%2F8df26b6b.html</url>
    <content type="text"><![CDATA[摘要SQLite，是一款轻型的数据库，是遵守ACID的关系型数据库管理系统，它包含在一个相对小的C库中。它是D.RichardHipp建立的公有领域项目。它的设计目标是嵌入式的，而且目前已经在很多嵌入式产品中使用了它，它占用资源非常的低，在嵌入式设备中，可能只需要几百K的内存就够了。它能够支持Windows/Linux/Unix等等主流的操作系统，同时能够跟很多程序语言相结合，比如 Tcl、C#、PHP、Java、Python等，还有ODBC接口，同样比起Mysql、PostgreSQL这两款开源的世界著名数据库管理系统来讲，它的处理速度比他们都快。SQLite第一个Alpha版本诞生于2000年5月。 至2015年已经有15个年头，SQLite也迎来了一个版本 SQLite 3已经发布。 安装SQLite3 可使用 sqlite3 模块与 Python 进行集成。sqlite3 模块是由 Gerhard Haring 编写的。它提供了一个与 PEP 249 描述的 DB-API 2.0 规范兼容的 SQL 接口。您不需要单独安装该模块，因为 Python 2.5.x 以上版本默认自带了该模块。 为了使用 sqlite3 模块，您首先必须创建一个表示数据库的连接对象，然后您可以有选择地创建光标对象，这将帮助您执行所有的 SQL 语句。 Python sqlite3 模块 API以下是重要的 sqlite3 模块程序，可以满足您在 Python 程序中使用 SQLite 数据库的需求。如果您需要了解更多细节，请查看 Python sqlite3 模块的官方文档。 sqlite3.connect(database [,timeout ,other optional arguments])123该 API 打开一个到 SQLite 数据库文件 database 的链接。您可以使用 &quot;:memory:&quot; 来在 RAM 中打开一个到 database 的数据库连接，而不是在磁盘上打开。如果数据库成功打开，则返回一个连接对象。当一个数据库被多个连接访问，且其中一个修改了数据库，此时 SQLite 数据库被锁定，直到事务提交。timeout 参数表示连接等待锁定的持续时间，直到发生异常断开连接。timeout 参数默认是 5.0（5 秒）。如果给定的数据库名称 filename 不存在，则该调用将创建一个数据库。如果您不想在当前目录中创建数据库，那么您可以指定带有路径的文件名，这样您就能在任意地方创建数据库。 connection.cursor([cursorClass])1该例程创建一个 cursor，将在 Python 数据库编程中用到。该方法接受一个单一的可选的参数 cursorClass。如果提供了该参数，则它必须是一个扩展自 sqlite3.Cursor 的自定义的 cursor 类。 cursor.execute(sql [, optional parameters])12该例程执行一个 SQL 语句。该 SQL 语句可以被参数化（即使用占位符代替 SQL 文本）。sqlite3 模块支持两种类型的占位符：问号和命名占位符（命名样式）。例如：cursor.execute(&quot;insert into people values (?, ?)&quot;, (who, age)) connection.execute(sql [, optional parameters])1该例程是上面执行的由光标（cursor）对象提供的方法的快捷方式，它通过调用光标（cursor）方法创建了一个中间的光标对象，然后通过给定的参数调用光标的 execute 方法。 cursor.executemany(sql, seq_of_parameters)1该例程对 seq_of_parameters 中的所有参数或映射执行一个 SQL 命令。 connection.executemany(sql[, parameters])1该例程是一个由调用光标（cursor）方法创建的中间的光标对象的快捷方式，然后通过给定的参数调用光标的 executemany 方法。 cursor.executescript(sql_script)1该例程一旦接收到脚本，会执行多个 SQL 语句。它首先执行 COMMIT 语句，然后执行作为参数传入的 SQL 脚本。所有的 SQL 语句应该用分号（;）分隔。 connection.executescript(sql_script)1该例程是一个由调用光标（cursor）方法创建的中间的光标对象的快捷方式，然后通过给定的参数调用光标的 executescript 方法。 connection.total_changes()1该例程返回自数据库连接打开以来被修改、插入或删除的数据库总行数。 connection.commit()1该方法提交当前的事务。如果您未调用该方法，那么自您上一次调用 commit() 以来所做的任何动作对其他数据库连接来说是不可见的。 connection.rollback()1该方法回滚自上一次调用 commit() 以来对数据库所做的更改。 connection.close()1该方法关闭数据库连接。请注意，这不会自动调用 commit()。如果您之前未调用 commit() 方法，就直接关闭数据库连接，您所做的所有更改将全部丢失！ cursor.fetchone()1该方法获取查询结果集中的下一行，返回一个单一的序列，当没有更多可用的数据时，则返回 None。 cursor.fetchmany([size=cursor.arraysize])1该方法获取查询结果集中的下一行组，返回一个列表。当没有更多的可用的行时，则返回一个空的列表。该方法尝试获取由 size 参数指定的尽可能多的行。 cursor.fetchall()1该例程获取查询结果集中所有（剩余）的行，返回一个列表。当没有可用的行时，则返回一个空的列表。 操作方式连接数据库下面的 Python 代码显示了如何连接到一个现有的数据库。如果数据库不存在，那么它就会被创建，最后将返回一个数据库对象。 1234567891011#!/usr/bin/pythonimport sqlite3conn = sqlite3.connect(&apos;test.db&apos;)print &quot;Opened database successfully&quot;;在这里，您也可以把数据库名称复制为特定的名称 :memory:，这样就会在 RAM 中创建一个数据库。现在，让我们来运行上面的程序，在当前目录中创建我们的数据库 test.db。您可以根据需要改变路径。保存上面代码到 sqlite.py 文件中，并按如下所示执行。如果数据库成功创建，那么会显示下面所示的消息：$chmod +x sqlite.py$./sqlite.pyOpen database successfully 创建表下面的 Python 代码段将用于在先前创建的数据库中创建一个表： 12345678910111213141516171819#!/usr/bin/pythonimport sqlite3conn = sqlite3.connect(&apos;test.db&apos;)print &quot;Opened database successfully&quot;;conn.execute(&apos;&apos;&apos;CREATE TABLE COMPANY (ID INT PRIMARY KEY NOT NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL);&apos;&apos;&apos;)print &quot;Table created successfully&quot;;conn.close()上述程序执行时，它会在 test.db 中创建 COMPANY 表，并显示下面所示的消息：Opened database successfullyTable created successfully INSERT 操作下面的 Python 程序显示了如何在上面创建的 COMPANY 表中创建记录： 12345678910111213141516171819202122232425#!/usr/bin/pythonimport sqlite3conn = sqlite3.connect(&apos;test.db&apos;)print &quot;Opened database successfully&quot;;conn.execute(&quot;INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY) \ VALUES (1, &apos;Paul&apos;, 32, &apos;California&apos;, 20000.00 )&quot;);conn.execute(&quot;INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY) \ VALUES (2, &apos;Allen&apos;, 25, &apos;Texas&apos;, 15000.00 )&quot;);conn.execute(&quot;INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY) \ VALUES (3, &apos;Teddy&apos;, 23, &apos;Norway&apos;, 20000.00 )&quot;);conn.execute(&quot;INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY) \ VALUES (4, &apos;Mark&apos;, 25, &apos;Rich-Mond &apos;, 65000.00 )&quot;);conn.commit()print &quot;Records created successfully&quot;;conn.close()上述程序执行时，它会在 COMPANY 表中创建给定记录，并会显示以下两行：Opened database successfullyRecords created successfully SELECT 操作下面的 Python 程序显示了如何从前面创建的 COMPANY 表中获取并显示记录： 123456789101112131415161718192021222324252627282930313233343536373839#!/usr/bin/pythonimport sqlite3conn = sqlite3.connect(&apos;test.db&apos;)print &quot;Opened database successfully&quot;;cursor = conn.execute(&quot;SELECT id, name, address, salary from COMPANY&quot;)for row in cursor: print &quot;ID = &quot;, row[0] print &quot;NAME = &quot;, row[1] print &quot;ADDRESS = &quot;, row[2] print &quot;SALARY = &quot;, row[3], &quot;\n&quot;print &quot;Operation done successfully&quot;;conn.close()上述程序执行时，它会产生以下结果：Opened database successfullyID = 1NAME = PaulADDRESS = CaliforniaSALARY = 20000.0ID = 2NAME = AllenADDRESS = TexasSALARY = 15000.0ID = 3NAME = TeddyADDRESS = NorwaySALARY = 20000.0ID = 4NAME = MarkADDRESS = Rich-MondSALARY = 65000.0Operation done successfully UPDATE 操作下面的 Python 代码显示了如何使用 UPDATE 语句来更新任何记录，然后从 COMPANY 表中获取并显示更新的记录： 1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/usr/bin/pythonimport sqlite3conn = sqlite3.connect(&apos;test.db&apos;)print &quot;Opened database successfully&quot;;conn.execute(&quot;UPDATE COMPANY set SALARY = 25000.00 where ID=1&quot;)conn.commit()print &quot;Total number of rows updated :&quot;, conn.total_changescursor = conn.execute(&quot;SELECT id, name, address, salary from COMPANY&quot;)for row in cursor: print &quot;ID = &quot;, row[0] print &quot;NAME = &quot;, row[1] print &quot;ADDRESS = &quot;, row[2] print &quot;SALARY = &quot;, row[3], &quot;\n&quot;print &quot;Operation done successfully&quot;;conn.close()上述程序执行时，它会产生以下结果：Opened database successfullyTotal number of rows updated : 1ID = 1NAME = PaulADDRESS = CaliforniaSALARY = 25000.0ID = 2NAME = AllenADDRESS = TexasSALARY = 15000.0ID = 3NAME = TeddyADDRESS = NorwaySALARY = 20000.0ID = 4NAME = MarkADDRESS = Rich-MondSALARY = 65000.0Operation done successfully DELETE 操作下面的 Python 代码显示了如何使用 DELETE 语句删除任何记录，然后从 COMPANY 表中获取并显示剩余的记录： 123456789101112131415161718192021222324252627282930313233343536373839#!/usr/bin/pythonimport sqlite3conn = sqlite3.connect(&apos;test.db&apos;)print &quot;Opened database successfully&quot;;conn.execute(&quot;DELETE from COMPANY where ID=2;&quot;)conn.commit()print &quot;Total number of rows deleted :&quot;, conn.total_changescursor = conn.execute(&quot;SELECT id, name, address, salary from COMPANY&quot;)for row in cursor: print &quot;ID = &quot;, row[0] print &quot;NAME = &quot;, row[1] print &quot;ADDRESS = &quot;, row[2] print &quot;SALARY = &quot;, row[3], &quot;\n&quot;print &quot;Operation done successfully&quot;;conn.close()上述程序执行时，它会产生以下结果：Opened database successfullyTotal number of rows deleted : 1ID = 1NAME = PaulADDRESS = CaliforniaSALARY = 20000.0ID = 3NAME = TeddyADDRESS = NorwaySALARY = 20000.0ID = 4NAME = MarkADDRESS = Rich-MondSALARY = 65000.0Operation done successfully]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-xml、yaml]]></title>
    <url>%2Fposts%2F6f65a2ef.html</url>
    <content type="text"><![CDATA[摘要本文记录一些python中xml和yaml模块 xmlxml是实现不同语言或程序之间进行数据交换的协议，跟json差不多，但json使用起来更简单，不过，古时候，在json还没诞生的黑暗年代，大家只能选择用xml呀，至今很多传统公司如金融行业的很多系统的接口还主要是xml。 xml的格式如下，就是通过&lt;&gt;节点来区别数据结构的: 1234567891011121314151617181920212223&lt;?xml version=&quot;1.0&quot;?&gt;&lt;data&gt; &lt;country name=&quot;Liechtenstein&quot;&gt; &lt;rank updated=&quot;yes&quot;&gt;2&lt;/rank&gt; &lt;year&gt;2008&lt;/year&gt; &lt;gdppc&gt;141100&lt;/gdppc&gt; &lt;neighbor name=&quot;Austria&quot; direction=&quot;E&quot;/&gt; &lt;neighbor name=&quot;Switzerland&quot; direction=&quot;W&quot;/&gt; &lt;/country&gt; &lt;country name=&quot;Singapore&quot;&gt; &lt;rank updated=&quot;yes&quot;&gt;5&lt;/rank&gt; &lt;year&gt;2011&lt;/year&gt; &lt;gdppc&gt;59900&lt;/gdppc&gt; &lt;neighbor name=&quot;Malaysia&quot; direction=&quot;N&quot;/&gt; &lt;/country&gt; &lt;country name=&quot;Panama&quot;&gt; &lt;rank updated=&quot;yes&quot;&gt;69&lt;/rank&gt; &lt;year&gt;2011&lt;/year&gt; &lt;gdppc&gt;13600&lt;/gdppc&gt; &lt;neighbor name=&quot;Costa Rica&quot; direction=&quot;W&quot;/&gt; &lt;neighbor name=&quot;Colombia&quot; direction=&quot;E&quot;/&gt; &lt;/country&gt;&lt;/data&gt; xml协议在各个语言里的都 是支持的，在python中可以用以下模块操作xml： 123# print(root.iter(&apos;year&apos;)) #全文搜索# print(root.find(&apos;country&apos;)) #在root的子节点找，只找一个# print(root.findall(&apos;country&apos;)) #在root的子节点找，找所有 1234567891011121314151617181920212223242526272829303132333435363738import xml.etree.ElementTree as ET #导入模块，名字太长了，把这个模块名重命名为ET tree = ET.parse(&quot;xmltest.xml&quot;) #parse解析，用ET模块下的parse这个方法把xml文件解析开，解析开拿到一个tree,tree就是一个对象root = tree.getroot() #这个对象可以调用方法，getroot就是根的意思print(root.tag) #root这个对象有一个属性tag，tag的值就是根标签的名字#遍历xml文档for child in root: print(&apos;========&gt;&apos;,child.tag,child.attrib,child.attrib[&apos;name&apos;]) for i in child: print(i.tag,i.attrib,i.text) #只遍历year 节点for node in root.iter(&apos;year&apos;): print(node.tag,node.text)#---------------------------------------import xml.etree.ElementTree as ET tree = ET.parse(&quot;xmltest.xml&quot;)root = tree.getroot() #修改for node in root.iter(&apos;year&apos;): new_year=int(node.text)+1 node.text=str(new_year) node.set(&apos;updated&apos;,&apos;yes&apos;) node.set(&apos;version&apos;,&apos;1.0&apos;)tree.write(&apos;test.xml&apos;) #删除nodefor country in root.findall(&apos;country&apos;): rank = int(country.find(&apos;rank&apos;).text) if rank &gt; 50: root.remove(country) tree.write(&apos;output.xml&apos;) 12345678910111213#在country内添加（append）节点year2import xml.etree.ElementTree as ETtree = ET.parse(&quot;a.xml&quot;)root=tree.getroot()for country in root.findall(&apos;country&apos;): for year in country.findall(&apos;year&apos;): if int(year.text) &gt; 2000: year2=ET.Element(&apos;year2&apos;) year2.text=&apos;新年&apos; year2.attrib=&#123;&apos;update&apos;:&apos;yes&apos;&#125; country.append(year2) #往country节点下添加子节点tree.write(&apos;a.xml.swap&apos;) 自己创建xml文件 12345678910111213141516import xml.etree.ElementTree as ET new_xml = ET.Element(&quot;namelist&quot;)name = ET.SubElement(new_xml,&quot;name&quot;,attrib=&#123;&quot;enrolled&quot;:&quot;yes&quot;&#125;)age = ET.SubElement(name,&quot;age&quot;,attrib=&#123;&quot;checked&quot;:&quot;no&quot;&#125;)sex = ET.SubElement(name,&quot;sex&quot;)sex.text = &apos;33&apos;name2 = ET.SubElement(new_xml,&quot;name&quot;,attrib=&#123;&quot;enrolled&quot;:&quot;no&quot;&#125;)age = ET.SubElement(name2,&quot;age&quot;)age.text = &apos;19&apos; et = ET.ElementTree(new_xml) #生成文档对象et.write(&quot;test.xml&quot;, encoding=&quot;utf-8&quot;,xml_declaration=True) ET.dump(new_xml) #打印生成的格式 yaml首先安装yaml模块1pip3 install pyyaml 编写yaml配置文件 yaml_example.yaml12345678910 1 name: junxi 2 age: 18 3 spouse: 4 name: Rui 5 age: 18 6 children: 7 - name: Chen You 8 age: 3 9 - name: Ruo Xi10 age: 2 编写解析yaml文件的python程序 yaml_example.py1234567891 import yaml2 3 f = open(&apos;yaml_example.yaml&apos;)4 content = yaml.load(f)5 print(type(content))6 print(&apos;before modification:&apos;, content)7 content[&apos;age&apos;] = 178 content[&apos;children&apos;][1][&apos;age&apos;] = 19 print(&apos;after modification&apos;, content) 程序输出的结果为: 1231 &lt;type &apos;dict&apos;&gt;2 (&apos;before modification:&apos;, &#123;&apos;age&apos;: 18, &apos;spouse&apos;: &#123;&apos;age&apos;: 18, &apos;name&apos;: &apos;Rui&apos;&#125;, &apos;name&apos;: &apos;junxi&apos;, &apos;children&apos;: [&#123;&apos;age&apos;: 3, &apos;name&apos;: &apos;Chen You&apos;&#125;, &#123;&apos;age&apos;: 2, &apos;name&apos;: &apos;Ruo Xi&apos;&#125;]&#125;)3 (&apos;after modification&apos;, &#123;&apos;age&apos;: 17, &apos;spouse&apos;: &#123;&apos;age&apos;: 18, &apos;name&apos;: &apos;Rui&apos;&#125;, &apos;name&apos;: &apos;junxi&apos;, &apos;children&apos;: [&#123;&apos;age&apos;: 3, &apos;name&apos;: &apos;Chen You&apos;&#125;, &#123;&apos;age&apos;: 1, &apos;name&apos;: &apos;Ruo Xi&apos;&#125;]&#125;)]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-序列化]]></title>
    <url>%2Fposts%2F79185f44.html</url>
    <content type="text"><![CDATA[摘要本文记录一些python中序列化的模块 序列化之前我们学习过用eval内置方法可以将一个字符串转成python对象，不过，eval方法是有局限性的，对于普通的数据类型，json.loads和eval都能用，但遇到特殊类型的时候，eval就不管用了,所以eval的重点还是通常用来执行一个字符串表达式，并返回表达式的值。 1234import jsonx=&quot;[null,true,false,1]&quot;print(eval(x)) #报错，无法解析null类型，而json就可以print(json.loads(x)) 什么是序列化？我们把对象(变量)从内存中变成可存储或传输的过程称之为序列化，在Python中叫pickling，在其他语言中也被称之为serialization，marshalling，flattening等等，都是一个意思。 为什么要序列化？持久保存状态需知一个软件/程序的执行就在处理一系列状态的变化，在编程语言中，’状态’会以各种各样有结构的数据类型(也可简单的理解为变量)的形式被保存在内存中。 内存是无法永久保存数据的，当程序运行了一段时间，我们断电或者重启程序，内存中关于这个程序的之前一段时间的数据（有结构）都被清空了。 在断电或重启程序之前将程序当前内存中所有的数据都保存下来（保存到文件中），以便于下次程序执行能够从文件中载入之前的数据，然后继续执行，这就是序列化。 具体的来说，你玩使命召唤闯到了第13关，你保存游戏状态，关机走人，下次再玩，还能从上次的位置开始继续闯关。或如，虚拟机状态的挂起等。 跨平台数据交互序列化之后，不仅可以把序列化后的内容写入磁盘，还可以通过网络传输到别的机器上，如果收发的双方约定好实用一种序列化的格式，那么便打破了平台/语言差异化带来的限制，实现了跨平台数据交互。 反过来，把变量内容从序列化的对象重新读到内存里称之为反序列化，即unpickling。 如何序列化之json和pickle： json如果我们要在不同的编程语言之间传递对象，就必须把对象序列化为标准格式，比如XML，但更好的方法是序列化为JSON，因为JSON表示出来就是一个字符串，可以被所有语言读取，也可以方便地存储到磁盘或者通过网络传输。JSON不仅是标准格式，并且比XML更快，而且可以直接在Web页面中读取，非常方便。 JSON表示的对象就是标准的JavaScript语言的对象，JSON和Python内置的数据类型对应如下： 12345678910111213141516import json dic=&#123;&apos;name&apos;:&apos;alvin&apos;,&apos;age&apos;:23,&apos;sex&apos;:&apos;male&apos;&#125;print(type(dic))#&lt;class &apos;dict&apos;&gt; j=json.dumps(dic)print(type(j))#&lt;class &apos;str&apos;&gt; f=open(&apos;序列化对象&apos;,&apos;w&apos;)f.write(j) #-------------------等价于json.dump(dic,f)f.close()#-----------------------------反序列化&lt;br&gt;import jsonf=open(&apos;序列化对象&apos;)data=json.loads(f.read())# 等价于data=json.load(f) 注意点123456789import json#dct=&quot;&#123;&apos;1&apos;:111&#125;&quot;#json 不认单引号#dct=str(&#123;&quot;1&quot;:111&#125;)#报错,因为生成的数据还是单引号:&#123;&apos;one&apos;: 1&#125;dct=&apos;&#123;&quot;1&quot;:&quot;111&quot;&#125;&apos;print(json.loads(dct))#conclusion:# 无论数据是怎样创建的，只要满足json格式，就可以json.loads出来,不一定非要dumps的数据才能loads pickle12345678910111213141516171819202122import pickle dic=&#123;&apos;name&apos;:&apos;alvin&apos;,&apos;age&apos;:23,&apos;sex&apos;:&apos;male&apos;&#125; print(type(dic))#&lt;class &apos;dict&apos;&gt; j=pickle.dumps(dic)print(type(j))#&lt;class &apos;bytes&apos;&gt; f=open(&apos;序列化对象_pickle&apos;,&apos;wb&apos;)#注意是w是写入str,wb是写入bytes,j是&apos;bytes&apos;f.write(j) #-------------------等价于pickle.dump(dic,f) f.close()#-------------------------反序列化import picklef=open(&apos;序列化对象_pickle&apos;,&apos;rb&apos;) data=pickle.loads(f.read())# 等价于data=pickle.load(f) print(data[&apos;age&apos;]) Pickle的问题和所有其他编程语言特有的序列化问题一样，就是它只能用于Python，并且可能不同版本的Python彼此都不兼容，因此，只能用Pickle保存那些不重要的数据，不能成功地反序列化也没关系。 shelveshelve模块比pickle模块简单，只有一个open函数，返回类似字典的对象，可读可写;key必须为字符串，而值可以是python所支持的数据类型 123456789import shelvef=shelve.open(r&apos;sheve.txt&apos;)# f[&apos;stu1_info&apos;]=&#123;&apos;name&apos;:&apos;egon&apos;,&apos;age&apos;:18,&apos;hobby&apos;:[&apos;piao&apos;,&apos;smoking&apos;,&apos;drinking&apos;]&#125;# f[&apos;stu2_info&apos;]=&#123;&apos;name&apos;:&apos;gangdan&apos;,&apos;age&apos;:53&#125;# f[&apos;school_info&apos;]=&#123;&apos;website&apos;:&apos;http://www.pypy.org&apos;,&apos;city&apos;:&apos;beijing&apos;&#125;print(f[&apos;stu1_info&apos;][&apos;hobby&apos;])f.close()]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-configparser]]></title>
    <url>%2Fposts%2Fcf57e5aa.html</url>
    <content type="text"><![CDATA[摘要本文记录一下python中处理配置文件的模块 ConfigParser用于对特定的配置进行操作，当前模块的名称在 python 3.x 版本中变更为 configparser。 12345678910111213# 注释1; 注释2[section1]k1 = v1k2:v2user=egonage=18is_admin=truesalary=31[section2]k1 = v1 读取1234567891011121314151617181920212223242526272829303132import configparserconfig=configparser.ConfigParser()config.read(&apos;a.cfg&apos;)#查看所有的标题res=config.sections() #[&apos;section1&apos;, &apos;section2&apos;]print(res)#查看标题section1下所有key=value的keyoptions=config.options(&apos;section1&apos;)print(options) #[&apos;k1&apos;, &apos;k2&apos;, &apos;user&apos;, &apos;age&apos;, &apos;is_admin&apos;, &apos;salary&apos;]#查看标题section1下所有key=value的(key,value)格式item_list=config.items(&apos;section1&apos;)print(item_list) #[(&apos;k1&apos;, &apos;v1&apos;), (&apos;k2&apos;, &apos;v2&apos;), (&apos;user&apos;, &apos;egon&apos;), (&apos;age&apos;, &apos;18&apos;), (&apos;is_admin&apos;, &apos;true&apos;), (&apos;salary&apos;, &apos;31&apos;)]#查看标题section1下user的值=&gt;字符串格式val=config.get(&apos;section1&apos;,&apos;user&apos;)print(val) #egon#查看标题section1下age的值=&gt;整数格式val1=config.getint(&apos;section1&apos;,&apos;age&apos;)print(val1) #18#查看标题section1下is_admin的值=&gt;布尔值格式val2=config.getboolean(&apos;section1&apos;,&apos;is_admin&apos;)print(val2) #True#查看标题section1下salary的值=&gt;浮点型格式val3=config.getfloat(&apos;section1&apos;,&apos;salary&apos;)print(val3) #31.0 改写123456789101112131415161718192021222324252627282930import configparserconfig=configparser.ConfigParser()config.read(&apos;a.cfg&apos;,encoding=&apos;utf-8&apos;)#删除整个标题section2config.remove_section(&apos;section2&apos;)#删除标题section1下的某个k1和k2config.remove_option(&apos;section1&apos;,&apos;k1&apos;)config.remove_option(&apos;section1&apos;,&apos;k2&apos;)#判断是否存在某个标题print(config.has_section(&apos;section1&apos;))#判断标题section1下是否有userprint(config.has_option(&apos;section1&apos;,&apos;&apos;))#添加一个标题config.add_section(&apos;egon&apos;)#在标题egon下添加name=egon,age=18的配置config.set(&apos;egon&apos;,&apos;name&apos;,&apos;egon&apos;)config.set(&apos;egon&apos;,&apos;age&apos;,18) #报错,必须是字符串#最后将修改的内容写入文件,完成最终的修改config.write(open(&apos;a.cfg&apos;,&apos;w&apos;)) 例子12345678910111213#类似这样的配置文件，一块一块的分类[DEFAULT]ServerAliveInterval = 45Compression = yesCompressionLevel = 9ForwardX11 = yes [bitbucket.org]User = hg [topsecret.server.com]Port = 50022ForwardX11 = no 1234567891011121314151617#生成类似格式的文件import configparserconfig = configparser.ConfigParser()config[&quot;DEFAULT&quot;] = &#123;&apos;ServerAliveInterval&apos;: &apos;45&apos;, &apos;Compression&apos;: &apos;yes&apos;, &apos;CompressionLevel&apos;: &apos;9&apos;&#125;config[&apos;bitbucket.org&apos;] = &#123;&#125;config[&apos;bitbucket.org&apos;][&apos;User&apos;] = &apos;hg&apos;config[&apos;topsecret.server.com&apos;] = &#123;&#125;topsecret = config[&apos;topsecret.server.com&apos;]topsecret[&apos;Host Port&apos;] = &apos;50022&apos; # mutates the parsertopsecret[&apos;ForwardX11&apos;] = &apos;no&apos; # same hereconfig[&apos;DEFAULT&apos;][&apos;ForwardX11&apos;] = &apos;yes&apos;with open(&apos;example.ini&apos;, &apos;w&apos;) as configfile: config.write(configfile) 1234567891011121314151617#删除（创建一个新文件，并删除bitbucket.org）import configparserconfig = configparser.ConfigParser()config.sections()config.read(&apos;example.ini&apos;)rec = config.remove_section(&quot;bitbucket.org&quot;)#删除该项config.write(open(&quot;example.cfg&quot;,&quot;w&quot;))[DEFAULT]compressionlevel = 9compression = yesserveraliveinterval = 45forwardx11 = yes[topsecret.server.com]host port = 50022forwardx11 = no]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-执行系统命令]]></title>
    <url>%2Fposts%2Fa58affda.html</url>
    <content type="text"><![CDATA[摘要可以执行shell命令的相关模块和函数有： os.system os.spawn* os.popen* –废弃 popen2.* –废弃 commands.* –废弃，3.x中被移除 subprocess commands123456import commandsresult = commands.getoutput(&apos;cmd&apos;)result = commands.getstatus(&apos;cmd&apos;)result = commands.getstatusoutput(&apos;cmd&apos;)(status, out) = commands.getstatusoutput(&apos;cmd&apos; + &apos; 2&gt;&amp;1&apos;) 以上执行shell命令的相关的模块和函数的功能均在 subprocess 模块中实现，并提供了更丰富的功能。 subprocesscall执行命令，返回状态码 12ret = subprocess.call([&quot;ls&quot;, &quot;-l&quot;], shell=False)ret = subprocess.call(&quot;ls -l&quot;, shell=True) shell = True ，允许 shell 命令是字符串形式 check_call执行命令，如果执行状态码是 0 ，则返回0，否则抛异常 12subprocess.check_call([&quot;ls&quot;, &quot;-l&quot;])subprocess.check_call(&quot;exit 1&quot;, shell=True) check_output执行命令，如果状态码是 0 ，则返回执行结果，否则抛异常 12subprocess.check_output([&quot;echo&quot;, &quot;Hello World!&quot;])subprocess.check_output(&quot;exit 1&quot;, shell=True) subprocess.Popen(…)用于执行复杂的系统命令 参数： args：shell命令，可以是字符串或者序列类型（如：list，元组） bufsize：指定缓冲。0 无缓冲,1 行缓冲,其他 缓冲区大小,负值 系统缓冲 stdin, stdout, stderr：分别表示程序的标准输入、输出、错误句柄 preexec_fn：只在Unix平台下有效，用于指定一个可执行对象（callable object），它将在子进程运行之前被调用 close_sfs：在windows平台下，如果close_fds被设置为True，则新创建的子进程将不会继承父进程的输入、输出、错误管道。所以不能将close_fds设置为True同时重定向子进程的标准输入、输出与错误(stdin, stdout, stderr)。 shell：同上 cwd：用于设置子进程的当前目录 env：用于指定子进程的环境变量。如果env = None，子进程的环境变量将从父进程中继承。 universal_newlines：不同系统的换行符不同，True -&gt; 同意使用 \n startupinfo与createionflags只在windows下有效将被传递给底层的CreateProcess()函数，用于设置子进程的一些属性，如：主窗口的外观，进程的优先级等等 123import subprocessret1 = subprocess.Popen([&quot;mkdir&quot;,&quot;t1&quot;])ret2 = subprocess.Popen(&quot;mkdir t2&quot;, shell=True) 终端输入的命令分为两种： 输入即可得到输出，如：ifconfig 输入进行某环境，依赖再输入，如：python 123import subprocessobj = subprocess.Popen(&quot;mkdir t3&quot;, shell=True, cwd=&apos;/home/dev&apos;,) 12345678910111213141516import subprocessobj = subprocess.Popen([&quot;python&quot;], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)obj.stdin.write(&apos;print 1 \n &apos;)obj.stdin.write(&apos;print 2 \n &apos;)obj.stdin.write(&apos;print 3 \n &apos;)obj.stdin.write(&apos;print 4 \n &apos;)obj.stdin.close()cmd_out = obj.stdout.read()obj.stdout.close()cmd_error = obj.stderr.read()obj.stderr.close()print cmd_outprint cmd_error 12345678910import subprocessobj = subprocess.Popen([&quot;python&quot;], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)obj.stdin.write(&apos;print 1 \n &apos;)obj.stdin.write(&apos;print 2 \n &apos;)obj.stdin.write(&apos;print 3 \n &apos;)obj.stdin.write(&apos;print 4 \n &apos;)out_error_list = obj.communicate()print out_error_list 1234567891011121314151617181920212223242526272829303132333435import subprocessobj = subprocess.Popen([&quot;python&quot;], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)out_error_list = obj.communicate(&apos;print &quot;hello&quot;&apos;)print out_error_listimport subprocess&apos;&apos;&apos;sh-3.2# ls /Users/egon/Desktop |grep txt$mysql.txttt.txt事物.txt&apos;&apos;&apos;res1=subprocess.Popen(&apos;ls /Users/jieli/Desktop&apos;,shell=True,stdout=subprocess.PIPE)res=subprocess.Popen(&apos;grep txt$&apos;,shell=True,stdin=res1.stdout, stdout=subprocess.PIPE)print(res.stdout.read().decode(&apos;utf-8&apos;))#等同于上面,但是上面的优势在于,一个数据流可以和另外一个数据流交互,可以通过爬虫得到结果然后交给grepres1=subprocess.Popen(&apos;ls /Users/jieli/Desktop |grep txt$&apos;,shell=True,stdout=subprocess.PIPE)print(res1.stdout.read().decode(&apos;utf-8&apos;))#windows下:# dir | findstr &apos;test*&apos;# dir | findstr &apos;txt$&apos;import subprocessres1=subprocess.Popen(r&apos;dir C:\Users\Administrator\PycharmProjects\test\函数备课&apos;,shell=True,stdout=subprocess.PIPE)res=subprocess.Popen(&apos;findstr test*&apos;,shell=True,stdin=res1.stdout, stdout=subprocess.PIPE)print(res.stdout.read().decode(&apos;gbk&apos;)) #subprocess使用当前系统默认编码，得到结果为bytes类型，在windows下需要用gbk解码 系统命令例子安装cmdb 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980#!/usr/bin/env python2# -*- coding: utf-8 -*-import osimport subprocessimport argparseimport timedef base(cmd): if subprocess.call(cmd, shell=True): raise Exception(&quot;&#123;&#125; 执行失败&quot;.format(cmd))def install_docker(): base(&quot;sudo yum install -y yum-utils device-mapper-persistent-data lvm2&quot;) base(&quot;sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo&quot;) base(&quot;sudo yum makecache fast&quot;) base(&quot;sudo yum -y install docker-ce&quot;) if(not os.path.exists(&quot;/etc/docker&quot;)): base(&quot;mkdir -p /etc/docker&quot;) with open(&quot;/etc/docker/daemon.json&quot;, &quot;w&quot;) as f: f.write(&apos;&#123;\n &quot;registry-mirrors&quot;: [&quot;https://9f4w4icn.mirror.aliyuncs.com&quot;] \n&#125;&apos;) base(&quot;sudo systemctl daemon-reload&quot;) base(&quot;sudo systemctl start docker&quot;)def create_dir(): if (not os.path.exists(&quot;/var/cmdb/db&quot;)): base(&quot;sudo mkdir -p /var/cmdb/db&quot;) if (not os.path.exists(&quot;/var/cmdb/es&quot;)): base(&quot;sudo mkdir -p /var/cmdb/es&quot;)def run_db_container(): base(&quot;sudo docker run --name cmdb-db -d -e MYSQL_ROOT_PASSWORD=cmdbcmdb -v /var/cmdb/db:/var/lib/mysql mysql:5.7.21&quot;)def run_es_container(): base(&quot;sudo docker run --name cmdb-es -d -v /var/cmdb/es:/usr/share/elasticsearch/data elasticsearch:5.6.8&quot;)def init_db(): base(&quot;sudo docker run -it --rm --link cmdb-db -e DB_HOST=cmdb-db -e ENV=PRO -e DB_PORT=3306 -e DB_USERNAME=root -e DB_PASSWORD=cmdbcmdb -e DB_NAME=cmdb mingmingtang/cmdb init-db&quot;)def run_cmdb_container(site_url, email_host, email_port, email_username, email_password): base(&quot;sudo docker run -d --name cmdb --link cmdb-db --link cmdb-es -p 80:80 -e ENV=PRO -e SITE_URL=&#123;&#125; -e DB_HOST=cmdb-db -e DB_PORT=3306 -e DB_USERNAME=root -e DB_PASSWORD=cmdbcmdb -e DB_NAME=cmdb -e ELASTICSEARCH_HOSTS=cmdb-es -e EMAIL_HOST=&#123;&#125; -e EMAIL_PORT=&#123;&#125; -e EMAIL_USERNAME=&#123;&#125; -e EMAIL_PASSWORD=&#123;&#125; mingmingtang/cmdb start&quot;.format(site_url, email_host, email_port, email_username, email_password))def input_para(help): value = &quot;&quot; while(not value): value = raw_input(help) return valueif __name__ == &apos;__main__&apos;: if(os.geteuid() != 0): raise(&quot;请以root权限运行&quot;) # parser = argparse.ArgumentParser() # parser.add_argument(&quot;--siteurl&quot;, type=str, help=&quot;E.g: http://cmdb.xxx.com, http://172.17.100.1&quot;) # parser.add_argument(&quot;--emailhost&quot;, type=str, help=&quot;E.g: http://cmdb.xxx.com, http://172.17.100.1&quot;) # parser.add_argument(&quot;--emailport&quot;, type=str, help=&quot;E.g: http://cmdb.xxx.com, http://172.17.100.1&quot;) # parser.add_argument(&quot;--emailusername&quot;, type=str, help=&quot;E.g: http://cmdb.xxx.com, http://172.17.100.1&quot;) # parser.add_argument(&quot;--emailpassword&quot;, type=str, help=&quot;E.g: http://cmdb.xxx.com, http://172.17.100.1&quot;) # args = parser.parse_args() # SITE_URL = args.SITE_URL site_url = input_para(&quot;请输入网站域名或IP（http://cmdb.xxx.com）：&quot;) email_host = input_para(&quot;网站邮箱服务器（smtp.163.com）：&quot;) email_port = input_para(&quot;邮箱服务器端口（25）：&quot;) email_username = input_para(&quot;邮箱用户名（cmdb@163.com）：&quot;) email_password = input_para(&quot;邮箱密码|独立授权码（P@ssw0rd）：&quot;) print(&quot;开始安装docker&quot;) install_docker() print(&quot;开始创建目录&quot;) create_dir() print(&quot;开始运行mysql容器&quot;) run_db_container() print(&quot;开始运行elasticsearch容器&quot;) run_es_container() print(&quot;等待数据库启动完成(10s)&quot;) time.sleep(10) print(&quot;开始初始化数据库&quot;) init_db() print(&quot;开始运行cmdb&quot;) run_cmdb_container(site_url, email_host, email_port, email_username, email_password) print(&quot;完成！&quot;)]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-argparse、optparse]]></title>
    <url>%2Fposts%2F7961baec.html</url>
    <content type="text"><![CDATA[摘要本文记录一些python中optparse和argparse模块的常用方法 optparseoptparse模块主要用来为脚本传递命令参数，采用预先定义好的选项来解析命令行参数。 首先需要引入optparser模块，然后执行初始化，实例化一个OptionParser对象(可以带参，也可以不带参数)，再为命令行添加选项 12345678import optparseusage=&quot;python %prog -H &lt;target host&gt; -p/-P &lt;target ports&gt;&quot; #用于显示帮助信息parser=optparse.OptionParser(usage) #创建对象实例parser.add_option(&apos;-H&apos;,dest=&apos;Host&apos;,type=&apos;string&apos;,help=&apos;target host&apos;) ##需要的命令行参数parser.add_option(&apos;-P&apos;,&apos;-p&apos;,dest=&apos;Ports&apos;,type=&apos;string&apos;,help=&apos;target ports&apos; default=&quot;20,21&quot;) ## -p/-P 都可以(options,args)=parser.parse_args()print(options.Host)print(options.Ports) 12345678910from optparse import OptionParserusage=&quot;show something usefull-- for example: how to use this program&quot;parser = OptionParser(usage) #带参的话会把参数变量的内容作为帮助信息输出parser.add_option(&quot;-f&quot;,&quot;--file&quot;,dest=&quot;filename&quot;,help=&quot;read picture from File&quot;,metavar=&quot;FILE&quot;,action = &quot;store&quot;,type=&quot;string&quot;)parser.add_option(&quot;-s&quot;,&quot;--save&quot;,dest=&quot;save_mold&quot;,help=&quot;save image to file or not&quot;,default = True)(options,args)=parser.parse_args()print options.filenameprint options.save_mold 12345678910各个参数的含义：dest：用于保存输入的临时变量，其值通过options的属性进行访问，存储的内容是dest之前输入的参数，多个参数用逗号分隔type: 用于检查命令行参数传入的参数的数据类型是否符合要求，有 string，int，float 等类型help：用于生成帮助信息default: 给dest的默认值，如果用户没有在命令行参数给dest分配值，则使用默认值action: 用于指导程序在遇到命令行参数时候该如何处理，有三种值可选： store,store_false和store_true,默认值是store store：读取参数，如果参数类型符合type的要求，则将参数值传递给dest变量，作为options的一个属性供使用。 store_true/store_false: 一般作为一个标记使用，分别设置dest变量的值为True和Falsemetavar: 占位字符串，用于在输出帮助信息时，代替当前命令选项的附加参数的值进行输出，只在帮助信息里有用，注意其和default的区别 argparse1234567891011import argparseparser = argparse.ArgumentParser(description=&apos;Process some integers.&apos;)parser.add_argument(&apos;integers&apos;, metavar=&apos;N&apos;, type=int, nargs=&apos;+&apos;, help=&apos;an integer for the accumulator&apos;)parser.add_argument(&apos;--sum&apos;, dest=&apos;accumulate&apos;, action=&apos;store_const&apos;, const=sum, default=max, help=&apos;sum the integers (default: find the max)&apos;)args = parser.parse_args()print(args.accumulate(args.integers)) 1234567891011121314151617181920212223$ python prog.py -husage: prog.py [-h] [--sum] N [N ...]Process some integers.positional arguments: N an integer for the accumulatoroptional arguments: -h, --help show this help message and exit --sum sum the integers (default: find the max)### $ python prog.py 1 2 3 44$ python prog.py 1 2 3 4 --sum10### $ python prog.py a b cusage: prog.py [-h] [--sum] N [N ...]prog.py: error: argument N: invalid int value: &apos;a&apos; 使用步骤12345678910# 导入模块1:import argparse# 创建解析对象2:parser = argparse.ArgumentParser()# 添加参数项3:parser.add_argument()# 解析参数项4:args = parser.parse_args()# 使用5:args.xxx ArgumentParser对象参数解析这部分并非重点内容，只要知道前几个参数的用法即可 12class argparse.ArgumentParser(prog=None, usage=None, description=None, epilog=None, parents=[], formatter_class=argparse.HelpFormatter, prefix_chars=&apos;-&apos;, fromfile_prefix_chars=None, argument_default=None, conflict_handler=&apos;error&apos;, add_help=True, allow_abbrev=True)Create a new ArgumentParser object. All parameters should be passed as keyword arguments. Each parameter has its own more detailed description below, but in short they are: prog - The name of the program (default: sys.argv[0]) usage - The string describing the program usage (default: generated from arguments added to parser) description - Text to display before the argument help (default: none) epilog - Text to display after the argument help (default: none) parents - A list of ArgumentParser objects whose arguments should also be included formatter_class - A class for customizing the help output prefix_chars - The set of characters that prefix optional arguments (default: ‘-‘) fromfile_prefix_chars - The set of characters that prefix files from which additional arguments should be read (default: None) argument_default - The global default value for arguments (default: None) conflict_handler - The strategy for resolving conflicting optionals (usually unnecessary) add_help - Add a -h/–help option to the parser (default: True) allow_abbrev - Allows long options to be abbreviated if the abbreviation is unambiguous. (default: True) progprog参数相当于python中的sys.argv[0]，默认为当前执行脚本的脚本名，可以进行自定义。 1parser = argparse.ArgumentParser(prog=&apos;myprogram&apos;) usageusage参数用来提示参数项使用方式，默认会自动生成，可以进行自定义 1parser = argparse.ArgumentParser(prog=&apos;PROG&apos;, usage=&apos;%(prog)s [options]&apos;) descriptiondescription用来添加帮助菜单中的描述信息，默认为空。 1parser = argparse.ArgumentParser(description=&apos;A foo that bars&apos;) epilogepilog用来添加帮助菜单中的程序结尾描述信息，默认为空。 123parser = argparse.ArgumentParser( description=&apos;A foo that bars&apos;, epilog=&quot;And that&apos;s how you&apos;d foo a bar&quot;) parents有时候，有些参数项可能作为一组通用参数项来分享给不通的程序，而无需重复的来定义这些参数项，这时候就可以用到parents参数，子对象会继承parents的所有参数项。 123456789101112131415# 首先定义一个用来继承的parents，需要关闭add_help参数parent_parser = argparse.ArgumentParser(add_help=False)parent_parser.add_argument(&apos;--parent&apos;, type=int)# 子对象foo_parser = argparse.ArgumentParser(parents=[parent_parser])foo_parser.add_argument(&apos;foo&apos;)foo_parser.parse_args([&apos;--parent&apos;, &apos;2&apos;, &apos;XXX&apos;])Namespace(foo=&apos;XXX&apos;, parent=2)# 子对象bar_parser = argparse.ArgumentParser(parents=[parent_parser])bar_parser.add_argument(&apos;--bar&apos;)bar_parser.parse_args([&apos;--bar&apos;, &apos;YYY&apos;])Namespace(bar=&apos;YYY&apos;, parent=None) formatter_class使用这个参数的几个选项可以把帮助信息中的格式进行修改。 RawDescriptionHelpFormatter定制描述信息缩进格式 123456789101112131415161718192021parser = argparse.ArgumentParser( prog=&apos;PROG&apos;, formatter_class=argparse.RawDescriptionHelpFormatter, description=textwrap.dedent(&apos;&apos;&apos;\ Please do not mess up this text! -------------------------------- I have indented it exactly the way I want it &apos;&apos;&apos;))usage: PROG [-h]Please do not mess up this text!-------------------------------- I have indented it exactly the way I want itoptional arguments: -h, --help show this help message and exit ArgumentDefaultsHelpFormatter自动增加defaults值显示 1234567891011121314parser = argparse.ArgumentParser( prog=&apos;PROG&apos;, formatter_class=argparse.ArgumentDefaultsHelpFormatter)parser.add_argument(&apos;--foo&apos;, type=int, default=42, help=&apos;FOO!&apos;)parser.add_argument(&apos;bar&apos;, nargs=&apos;*&apos;, default=[1, 2, 3], help=&apos;BAR!&apos;)usage: PROG [-h] [--foo FOO] [bar [bar ...]]positional arguments: bar BAR! (default: [1, 2, 3])optional arguments: -h, --help show this help message and exit --foo FOO FOO! (default: 42) MetavarTypeHelpFormatter用参数类型来替代帮助信息 1234567891011121314parser = argparse.ArgumentParser( prog=&apos;PROG&apos;, formatter_class=argparse.MetavarTypeHelpFormatter)parser.add_argument(&apos;--foo&apos;, type=int)parser.add_argument(&apos;bar&apos;, type=float)usage: PROG [-h] [--foo int] floatpositional arguments: floatoptional arguments: -h, --help show this help message and exit --foo int prefix_charsprefix_chars可以定义选项参数的前缀，默认为[-/–]。 123parser = argparse.ArgumentParser(prog=&apos;PROG&apos;, prefix_chars=&apos;-+&apos;)parser.add_argument(&apos;+f&apos;)parser.add_argument(&apos;++bar&apos;) fromfile_prefix_charsfromfile_prefix_chars可以指定文件为添加参数项值。 123456with open(&apos;args.txt&apos;, &apos;w&apos;) as fp: fp.write(&apos;-f\nbar&apos;)parser = argparse.ArgumentParser(fromfile_prefix_chars=&apos;@&apos;)parser.add_argument(&apos;-f&apos;)parser.parse_args([&apos;-f&apos;, &apos;foo&apos;, &apos;@args.txt&apos;])Namespace(f=&apos;bar&apos;) argument_default正常来说给一个参数项设定默认值可以通过add_argument()或者set_defaults()方法来添加，但是只能对每个参数单独制定，但是使用argument_default参数可以给所有没有默认值的参数一个默认值。 1Generally, argument defaults are specified either by passing a default to add_argument() or by calling the set_defaults() methods with a specific set of name-value pairs. Sometimes however, it may be useful to specify a single parser-wide default for arguments. This can be accomplished by passing the argument_default= keyword argument to ArgumentParser. For example, to globally suppress attribute creation on parse_args() calls, we supply argument_default=SUPPRESS: 这个参数没看懂，贴一下官方文档吧 1parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS) allow_abbrev通常情况，当你传递一个参数项的时候，可以传递一个缩写的参数项，allow_abbrev参数可以关闭缩写参数项（python3）。 12345678910111213parser = argparse.ArgumentParser(prog=&apos;PROG&apos;, allow_abbrev=False)parser.add_argument(&apos;--foobar&apos;, action=&apos;store_true&apos;)parser.add_argument(&apos;--foonley&apos;, action=&apos;store_false&apos;)parser.parse_args([&apos;--foon&apos;])usage: PROG [-h] [--foobar] [--foonley]PROG: error: unrecognized arguments: --foon# 不关闭的情况parser.parse_args([&apos;--foob&apos;])Namespace(foobar=True, foonley=True)parser.parse_args([&apos;--foon&apos;])Namespace(foobar=False, foonley=False) conflict_handler正常来说你指定了一个参数项，是不能在此指定来覆盖已经指定的参数项的，conflict_handler可以开放这个功能。 123456789101112131415161718&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;-f&apos;, &apos;--foo&apos;, help=&apos;old foo help&apos;)&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, help=&apos;new foo help&apos;)Traceback (most recent call last): ..ArgumentError: argument --foo: conflicting option string(s): --foo&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;, conflict_handler=&apos;resolve&apos;)&gt;&gt;&gt; parser.add_argument(&apos;-f&apos;, &apos;--foo&apos;, help=&apos;old foo help&apos;)&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, help=&apos;new foo help&apos;)&gt;&gt;&gt; parser.print_help()usage: PROG [-h] [-f FOO] [--foo FOO]optional arguments: -h, --help show this help message and exit -f FOO old foo help --foo FOO new foo help add_help可以用这个参数来关闭默认的help信息，然后自定义。关闭了之后还是可以用parser.print_help() 来打印帮助信息的。 1parser = argparse.ArgumentParser(prog=&apos;PROG&apos;, add_help=False) add_argument方法参数解析这个才是本文的重点 12ArgumentParser.add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest])Define how a single command-line argument should be parsed. Each parameter has its own more detailed description below, but in short they are: name or flags - Either a name or a list of option strings, e.g. foo or -f, –foo. action - The basic type of action to be taken when this argument is encountered at the command line. nargs - The number of command-line arguments that should be consumed. const - A constant value required by some action and nargs selections. default - The value produced if the argument is absent from the command line. type - The type to which the command-line argument should be converted. choices - A container of the allowable values for the argument. required - Whether or not the command-line option may be omitted (optionals only). help - A brief description of what the argument does. metavar - A name for the argument in usage messages. dest - The name of the attribute to be added to the object returned by parse_args(). name or flags选项名、参数名、标签。分为两种 带前缀：-f -foo ，参数项 不带前缀：bar，位置变量 12parser.add_argument(&apos;-f&apos;, &apos;--foo&apos;)parser.add_argument(&apos;bar&apos;) actionaction用来定义添加参数项的属性类型。 store默认类型，只是用来存储一个值。 1234&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;)&gt;&gt;&gt; parser.parse_args(&apos;--foo 1&apos;.split())Namespace(foo=&apos;1&apos;) store_const和const参数一起使用，存储一个常量。 1234&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, action=&apos;store_const&apos;, const=42)&gt;&gt;&gt; parser.parse_args([&apos;--foo&apos;])Namespace(foo=42) store_true &amp; store_false存储常量的特殊类型，存储布尔值。 123456&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, action=&apos;store_true&apos;)&gt;&gt;&gt; parser.add_argument(&apos;--bar&apos;, action=&apos;store_false&apos;)&gt;&gt;&gt; parser.add_argument(&apos;--baz&apos;, action=&apos;store_false&apos;)&gt;&gt;&gt; parser.parse_args(&apos;--foo --bar&apos;.split())Namespace(foo=True, bar=False, baz=True) append追加属性，参数项可以多次出现，多次值会追加成一个list。 1234&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, action=&apos;append&apos;)&gt;&gt;&gt; parser.parse_args(&apos;--foo 1 --foo 2&apos;.split())Namespace(foo=[&apos;1&apos;, &apos;2&apos;]) append_const和const一起使用，用来追加常量。 12345&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--str&apos;, dest=&apos;types&apos;, action=&apos;append_const&apos;, const=str)&gt;&gt;&gt; parser.add_argument(&apos;--int&apos;, dest=&apos;types&apos;, action=&apos;append_const&apos;, const=int)&gt;&gt;&gt; parser.parse_args(&apos;--str --int&apos;.split())Namespace(types=[&lt;class &apos;str&apos;&gt;, &lt;class &apos;int&apos;&gt;]) count数量统计，参数项可以多次出现，记录出现次数为选项的值。 1234&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--verbose&apos;, &apos;-v&apos;, action=&apos;count&apos;)&gt;&gt;&gt; parser.parse_args([&apos;-vvv&apos;])Namespace(verbose=3) help自定义help选项的时候使用的。 version创建版本信息使用。 12345&gt;&gt;&gt; import argparse&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;--version&apos;, action=&apos;version&apos;, version=&apos;%(prog)s 2.0&apos;)&gt;&gt;&gt; parser.parse_args([&apos;--version&apos;])PROG 2.0 自定义action1234567891011121314151617&gt;&gt;&gt; class FooAction(argparse.Action):... def __init__(self, option_strings, dest, nargs=None, **kwargs):... if nargs is not None:... raise ValueError(&quot;nargs not allowed&quot;)... super(FooAction, self).__init__(option_strings, dest, **kwargs)... def __call__(self, parser, namespace, values, option_string=None):... print(&apos;%r %r %r&apos; % (namespace, values, option_string))... setattr(namespace, self.dest, values)...&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, action=FooAction)&gt;&gt;&gt; parser.add_argument(&apos;bar&apos;, action=FooAction)&gt;&gt;&gt; args = parser.parse_args(&apos;1 --foo 2&apos;.split())Namespace(bar=None, foo=None) &apos;1&apos; NoneNamespace(bar=&apos;1&apos;, foo=None) &apos;2&apos; &apos;--foo&apos;&gt;&gt;&gt; argsNamespace(bar=&apos;1&apos;, foo=&apos;2&apos;) nargs这个参数用来规定添加的参数项对应参数值的个数。 1、2、3… ‘?’ 0或1个 ‘+’ 至少一个 ‘*’ 0到多个 argparse.REMAINDER ？？？ 12345&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, nargs=2)&gt;&gt;&gt; parser.add_argument(&apos;bar&apos;, nargs=1)&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, nargs=&apos;?&apos;, const=&apos;c&apos;, default=&apos;d&apos;)&gt;&gt;&gt; parser.add_argument(&apos;--bar&apos;, nargs=&apos;*&apos;)&gt;&gt;&gt; parser.add_argument(&apos;foo&apos;, nargs=&apos;+&apos;) const设定一个常量的值，与store_const 和append_const 一起使用。 default设定一个默认值 type设定参数项的值的类型。默认str类型，可以int、float等。 12345678910&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;foo&apos;, type=int)&gt;&gt;&gt; parser.add_argument(&apos;bar&apos;, type=open)&gt;&gt;&gt; parser.parse_args(&apos;2 temp.txt&apos;.split())Namespace(bar=&lt;_io.TextIOWrapper name=&apos;temp.txt&apos; encoding=&apos;UTF-8&apos;&gt;, foo=2)&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;bar&apos;, type=argparse.FileType(&apos;w&apos;))&gt;&gt;&gt; parser.parse_args([&apos;out.txt&apos;])Namespace(bar=&lt;_io.TextIOWrapper name=&apos;out.txt&apos; encoding=&apos;UTF-8&apos;&gt;) 可以自定义类型 123456789101112131415&gt;&gt;&gt; def perfect_square(string):... value = int(string)... sqrt = math.sqrt(value)... if sqrt != int(sqrt):... msg = &quot;%r is not a perfect square&quot; % string... raise argparse.ArgumentTypeError(msg)... return value...&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;foo&apos;, type=perfect_square)&gt;&gt;&gt; parser.parse_args([&apos;9&apos;])Namespace(foo=9)&gt;&gt;&gt; parser.parse_args([&apos;7&apos;])usage: PROG [-h] fooPROG: error: argument foo: &apos;7&apos; is not a perfect square 可以配合使用choices参数来定义参数值范围 1234567&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;foo&apos;, type=int, choices=range(5, 10))&gt;&gt;&gt; parser.parse_args([&apos;7&apos;])Namespace(foo=7)&gt;&gt;&gt; parser.parse_args([&apos;11&apos;])usage: PROG [-h] &#123;5,6,7,8,9&#125;PROG: error: argument foo: invalid choice: 11 (choose from 5, 6, 7, 8, 9) choices用来限制参数值的选定范围 12parser.add_argument(&apos;move&apos;, choices=[&apos;rock&apos;, &apos;paper&apos;, &apos;scissors&apos;])parser.add_argument(&apos;door&apos;, type=int, choices=range(1, 4)) required设定这个属性可是让参数项变成必选项。 1234567&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, required=True)&gt;&gt;&gt; parser.parse_args([&apos;--foo&apos;, &apos;BAR&apos;])Namespace(foo=&apos;BAR&apos;)&gt;&gt;&gt; parser.parse_args([])usage: argparse.py [-h] [--foo FOO]argparse.py: error: option --foo is required help用来添加参数项的帮助信息 metavar打印帮助信息时，参数项的演示值。 1234567891011121314151617181920212223242526&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, metavar=&apos;YYY&apos;)&gt;&gt;&gt; parser.add_argument(&apos;bar&apos;, metavar=&apos;XXX&apos;)&gt;&gt;&gt; parser.parse_args(&apos;X --foo Y&apos;.split())Namespace(bar=&apos;X&apos;, foo=&apos;Y&apos;)&gt;&gt;&gt; parser.print_help()usage: [-h] [--foo YYY] XXXpositional arguments: XXXoptional arguments: -h, --help show this help message and exit --foo YYY &gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;-x&apos;, nargs=2)&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, nargs=2, metavar=(&apos;bar&apos;, &apos;baz&apos;))&gt;&gt;&gt; parser.print_help()usage: PROG [-h] [-x X X] [--foo bar baz]optional arguments: -h, --help show this help message and exit -x X X --foo bar baz dest可以用来指定参数项名字。 1234&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, dest=&apos;bar&apos;)&gt;&gt;&gt; parser.parse_args(&apos;--foo XXX&apos;.split())Namespace(bar=&apos;XXX&apos;) parse_args()方法参数说明用来解析参数项，这部分不是重点，看看就好。 Option value syntax参数项使用方式 1234567891011121314151617181920&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;-x&apos;)&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;)&gt;&gt;&gt; parser.parse_args([&apos;-x&apos;, &apos;X&apos;])Namespace(foo=None, x=&apos;X&apos;)&gt;&gt;&gt; parser.parse_args([&apos;--foo&apos;, &apos;FOO&apos;])Namespace(foo=&apos;FOO&apos;, x=None)&gt;&gt;&gt; parser.parse_args([&apos;--foo=FOO&apos;])Namespace(foo=&apos;FOO&apos;, x=None)&gt;&gt;&gt; parser.parse_args([&apos;-xX&apos;])Namespace(foo=None, x=&apos;X&apos;)&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;-x&apos;, action=&apos;store_true&apos;)&gt;&gt;&gt; parser.add_argument(&apos;-y&apos;, action=&apos;store_true&apos;)&gt;&gt;&gt; parser.add_argument(&apos;-z&apos;)&gt;&gt;&gt; parser.parse_args([&apos;-xyzZ&apos;])Namespace(x=True, y=True, z=&apos;Z&apos;) Invalid arguments错误输出，参数项或参数值不符合规定时会提示错误信息 123456789101112131415161718&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, type=int)&gt;&gt;&gt; parser.add_argument(&apos;bar&apos;, nargs=&apos;?&apos;)&gt;&gt;&gt; # invalid type&gt;&gt;&gt; parser.parse_args([&apos;--foo&apos;, &apos;spam&apos;])usage: PROG [-h] [--foo FOO] [bar]PROG: error: argument --foo: invalid int value: &apos;spam&apos;&gt;&gt;&gt; # invalid option&gt;&gt;&gt; parser.parse_args([&apos;--bar&apos;])usage: PROG [-h] [--foo FOO] [bar]PROG: error: no such option: --bar&gt;&gt;&gt; # wrong number of arguments&gt;&gt;&gt; parser.parse_args([&apos;spam&apos;, &apos;badger&apos;])usage: PROG [-h] [--foo FOO] [bar]PROG: error: extra arguments found: badger Arguments containing1234567891011121314151617181920212223242526272829&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;-x&apos;)&gt;&gt;&gt; parser.add_argument(&apos;foo&apos;, nargs=&apos;?&apos;)&gt;&gt;&gt; # no negative number options, so -1 is a positional argument&gt;&gt;&gt; parser.parse_args([&apos;-x&apos;, &apos;-1&apos;])Namespace(foo=None, x=&apos;-1&apos;)&gt;&gt;&gt; # no negative number options, so -1 and -5 are positional arguments&gt;&gt;&gt; parser.parse_args([&apos;-x&apos;, &apos;-1&apos;, &apos;-5&apos;])Namespace(foo=&apos;-5&apos;, x=&apos;-1&apos;)&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;-1&apos;, dest=&apos;one&apos;)&gt;&gt;&gt; parser.add_argument(&apos;foo&apos;, nargs=&apos;?&apos;)&gt;&gt;&gt; # negative number options present, so -1 is an option&gt;&gt;&gt; parser.parse_args([&apos;-1&apos;, &apos;X&apos;])Namespace(foo=None, one=&apos;X&apos;)&gt;&gt;&gt; # negative number options present, so -2 is an option&gt;&gt;&gt; parser.parse_args([&apos;-2&apos;])usage: PROG [-h] [-1 ONE] [foo]PROG: error: no such option: -2&gt;&gt;&gt; # negative number options present, so both -1s are options&gt;&gt;&gt; parser.parse_args([&apos;-1&apos;, &apos;-1&apos;])usage: PROG [-h] [-1 ONE] [foo]PROG: error: argument -1: expected one argument Argument abbreviations (prefix matching)12345678910&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;-bacon&apos;)&gt;&gt;&gt; parser.add_argument(&apos;-badger&apos;)&gt;&gt;&gt; parser.parse_args(&apos;-bac MMM&apos;.split())Namespace(bacon=&apos;MMM&apos;, badger=None)&gt;&gt;&gt; parser.parse_args(&apos;-bad WOOD&apos;.split())Namespace(bacon=None, badger=&apos;WOOD&apos;)&gt;&gt;&gt; parser.parse_args(&apos;-ba BA&apos;.split())usage: PROG [-h] [-bacon BACON] [-badger BADGER]PROG: error: ambiguous option: -ba could match -badger, -bacon Beyond sys.argv1234567891011&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(... &apos;integers&apos;, metavar=&apos;int&apos;, type=int, choices=range(10),... nargs=&apos;+&apos;, help=&apos;an integer in the range 0..9&apos;)&gt;&gt;&gt; parser.add_argument(... &apos;--sum&apos;, dest=&apos;accumulate&apos;, action=&apos;store_const&apos;, const=sum,... default=max, help=&apos;sum the integers (default: find the max)&apos;)&gt;&gt;&gt; parser.parse_args([&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;])Namespace(accumulate=&lt;built-in function max&gt;, integers=[1, 2, 3, 4])&gt;&gt;&gt; parser.parse_args([&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;4&apos;, &apos;--sum&apos;])Namespace(accumulate=&lt;built-in function sum&gt;, integers=[1, 2, 3, 4]) The Namespace object1234567891011121314151617&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;)&gt;&gt;&gt; args = parser.parse_args([&apos;--foo&apos;, &apos;BAR&apos;])&gt;&gt;&gt; vars(args)&#123;&apos;foo&apos;: &apos;BAR&apos;&#125;&gt;&gt;&gt; class C:... pass...&gt;&gt;&gt; c = C()&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;)&gt;&gt;&gt; parser.parse_args(args=[&apos;--foo&apos;, &apos;BAR&apos;], namespace=c)&gt;&gt;&gt; c.foo&apos;BAR&apos; 其他方法这部分内容有一些东西还是比较有用的 Sub-commands子参数项，可以设置子参数项的参数项。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&gt;&gt;&gt; # create the top-level parser&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, action=&apos;store_true&apos;, help=&apos;foo help&apos;)&gt;&gt;&gt; subparsers = parser.add_subparsers(help=&apos;sub-command help&apos;)&gt;&gt;&gt;&gt;&gt;&gt; # create the parser for the &quot;a&quot; command&gt;&gt;&gt; parser_a = subparsers.add_parser(&apos;a&apos;, help=&apos;a help&apos;)&gt;&gt;&gt; parser_a.add_argument(&apos;bar&apos;, type=int, help=&apos;bar help&apos;)&gt;&gt;&gt;&gt;&gt;&gt; # create the parser for the &quot;b&quot; command&gt;&gt;&gt; parser_b = subparsers.add_parser(&apos;b&apos;, help=&apos;b help&apos;)&gt;&gt;&gt; parser_b.add_argument(&apos;--baz&apos;, choices=&apos;XYZ&apos;, help=&apos;baz help&apos;)&gt;&gt;&gt;&gt;&gt;&gt; # parse some argument lists&gt;&gt;&gt; parser.parse_args([&apos;a&apos;, &apos;12&apos;])Namespace(bar=12, foo=False)&gt;&gt;&gt; parser.parse_args([&apos;--foo&apos;, &apos;b&apos;, &apos;--baz&apos;, &apos;Z&apos;])Namespace(baz=&apos;Z&apos;, foo=True)#####&gt;&gt;&gt; parser.parse_args([&apos;--help&apos;])usage: PROG [-h] [--foo] &#123;a,b&#125; ...positional arguments: &#123;a,b&#125; sub-command help a a help b b helpoptional arguments: -h, --help show this help message and exit --foo foo help&gt;&gt;&gt; parser.parse_args([&apos;a&apos;, &apos;--help&apos;])usage: PROG a [-h] barpositional arguments: bar bar helpoptional arguments: -h, --help show this help message and exit&gt;&gt;&gt; parser.parse_args([&apos;b&apos;, &apos;--help&apos;])usage: PROG b [-h] [--baz &#123;X,Y,Z&#125;]optional arguments: -h, --help show this help message and exit --baz &#123;X,Y,Z&#125; baz help 还可以设置单独的标题和描述信息 12345678910111213141516&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; subparsers = parser.add_subparsers(title=&apos;subcommands&apos;,... description=&apos;valid subcommands&apos;,... help=&apos;additional help&apos;)&gt;&gt;&gt; subparsers.add_parser(&apos;foo&apos;)&gt;&gt;&gt; subparsers.add_parser(&apos;bar&apos;)&gt;&gt;&gt; parser.parse_args([&apos;-h&apos;])usage: [-h] &#123;foo,bar&#125; ...optional arguments: -h, --help show this help message and exitsubcommands: valid subcommands &#123;foo,bar&#125; additional help 可以添加别名 123456&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; subparsers = parser.add_subparsers()&gt;&gt;&gt; checkout = subparsers.add_parser(&apos;checkout&apos;, aliases=[&apos;co&apos;])&gt;&gt;&gt; checkout.add_argument(&apos;foo&apos;)&gt;&gt;&gt; parser.parse_args([&apos;co&apos;, &apos;bar&apos;])Namespace(foo=&apos;bar&apos;) 12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; # sub-command functions&gt;&gt;&gt; def foo(args):... print(args.x * args.y)...&gt;&gt;&gt; def bar(args):... print(&apos;((%s))&apos; % args.z)...&gt;&gt;&gt; # create the top-level parser&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; subparsers = parser.add_subparsers()&gt;&gt;&gt;&gt;&gt;&gt; # create the parser for the &quot;foo&quot; command&gt;&gt;&gt; parser_foo = subparsers.add_parser(&apos;foo&apos;)&gt;&gt;&gt; parser_foo.add_argument(&apos;-x&apos;, type=int, default=1)&gt;&gt;&gt; parser_foo.add_argument(&apos;y&apos;, type=float)&gt;&gt;&gt; parser_foo.set_defaults(func=foo)&gt;&gt;&gt;&gt;&gt;&gt; # create the parser for the &quot;bar&quot; command&gt;&gt;&gt; parser_bar = subparsers.add_parser(&apos;bar&apos;)&gt;&gt;&gt; parser_bar.add_argument(&apos;z&apos;)&gt;&gt;&gt; parser_bar.set_defaults(func=bar)&gt;&gt;&gt;&gt;&gt;&gt; # parse the args and call whatever function was selected&gt;&gt;&gt; args = parser.parse_args(&apos;foo 1 -x 2&apos;.split())&gt;&gt;&gt; args.func(args)2.0&gt;&gt;&gt;&gt;&gt;&gt; # parse the args and call whatever function was selected&gt;&gt;&gt; args = parser.parse_args(&apos;bar XYZYX&apos;.split())&gt;&gt;&gt; args.func(args)((XYZYX)) 12345678&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; subparsers = parser.add_subparsers(dest=&apos;subparser_name&apos;)&gt;&gt;&gt; subparser1 = subparsers.add_parser(&apos;1&apos;)&gt;&gt;&gt; subparser1.add_argument(&apos;-x&apos;)&gt;&gt;&gt; subparser2 = subparsers.add_parser(&apos;2&apos;)&gt;&gt;&gt; subparser2.add_argument(&apos;y&apos;)&gt;&gt;&gt; parser.parse_args([&apos;2&apos;, &apos;frobble&apos;])Namespace(subparser_name=&apos;2&apos;, y=&apos;frobble&apos;) FileType objects12345678910&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--raw&apos;, type=argparse.FileType(&apos;wb&apos;, 0))&gt;&gt;&gt; parser.add_argument(&apos;out&apos;, type=argparse.FileType(&apos;w&apos;, encoding=&apos;UTF-8&apos;))&gt;&gt;&gt; parser.parse_args([&apos;--raw&apos;, &apos;raw.dat&apos;, &apos;file.txt&apos;])Namespace(out=&lt;_io.TextIOWrapper name=&apos;file.txt&apos; mode=&apos;w&apos; encoding=&apos;UTF-8&apos;&gt;, raw=&lt;_io.FileIO name=&apos;raw.dat&apos; mode=&apos;wb&apos;&gt;)&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;infile&apos;, type=argparse.FileType(&apos;r&apos;))&gt;&gt;&gt; parser.parse_args([&apos;-&apos;])Namespace(infile=&lt;_io.TextIOWrapper name=&apos;&lt;stdin&gt;&apos; encoding=&apos;UTF-8&apos;&gt;) Argument groups参数项组 1234567891011121314151617&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;, add_help=False)&gt;&gt;&gt; group1 = parser.add_argument_group(&apos;group1&apos;, &apos;group1 description&apos;)&gt;&gt;&gt; group1.add_argument(&apos;foo&apos;, help=&apos;foo help&apos;)&gt;&gt;&gt; group2 = parser.add_argument_group(&apos;group2&apos;, &apos;group2 description&apos;)&gt;&gt;&gt; group2.add_argument(&apos;--bar&apos;, help=&apos;bar help&apos;)&gt;&gt;&gt; parser.print_help()usage: PROG [--bar BAR] foogroup1: group1 description foo foo helpgroup2: group2 description --bar BAR bar help Mutual exclusion12345678910111213141516171819&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; group = parser.add_mutually_exclusive_group()&gt;&gt;&gt; group.add_argument(&apos;--foo&apos;, action=&apos;store_true&apos;)&gt;&gt;&gt; group.add_argument(&apos;--bar&apos;, action=&apos;store_false&apos;)&gt;&gt;&gt; parser.parse_args([&apos;--foo&apos;])Namespace(bar=True, foo=True)&gt;&gt;&gt; parser.parse_args([&apos;--bar&apos;])Namespace(bar=False, foo=False)&gt;&gt;&gt; parser.parse_args([&apos;--foo&apos;, &apos;--bar&apos;])usage: PROG [-h] [--foo | --bar]PROG: error: argument --bar: not allowed with argument --foo&gt;&gt;&gt; parser = argparse.ArgumentParser(prog=&apos;PROG&apos;)&gt;&gt;&gt; group = parser.add_mutually_exclusive_group(required=True)&gt;&gt;&gt; group.add_argument(&apos;--foo&apos;, action=&apos;store_true&apos;)&gt;&gt;&gt; group.add_argument(&apos;--bar&apos;, action=&apos;store_false&apos;)&gt;&gt;&gt; parser.parse_args([])usage: PROG [-h] (--foo | --bar)PROG: error: one of the arguments --foo --bar is required Parser defaults123456789101112131415161718&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;foo&apos;, type=int)&gt;&gt;&gt; parser.set_defaults(bar=42, baz=&apos;badger&apos;)&gt;&gt;&gt; parser.parse_args([&apos;736&apos;])Namespace(bar=42, baz=&apos;badger&apos;, foo=736)&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, default=&apos;bar&apos;)&gt;&gt;&gt; parser.set_defaults(foo=&apos;spam&apos;)&gt;&gt;&gt; parser.parse_args([])Namespace(foo=&apos;spam&apos;)&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, default=&apos;badger&apos;)&gt;&gt;&gt; parser.get_default(&apos;foo&apos;)&apos;badger&apos; Printing help12345678910111213ArgumentParser.print_usage(file=None)Print a brief description of how the ArgumentParser should be invoked on the command line. If file is None, sys.stdout is assumed.ArgumentParser.print_help(file=None)Print a help message, including the program usage and information about the arguments registered with the ArgumentParser. If file is None, sys.stdout is assumed.There are also variants of these methods that simply return a string instead of printing it:ArgumentParser.format_usage()Return a string containing a brief description of how the ArgumentParser should be invoked on the command line.ArgumentParser.format_help()Return a string containing a help message, including the program usage and information about the arguments registered with the ArgumentParser. Partial parsing12345&gt;&gt;&gt; parser = argparse.ArgumentParser()&gt;&gt;&gt; parser.add_argument(&apos;--foo&apos;, action=&apos;store_true&apos;)&gt;&gt;&gt; parser.add_argument(&apos;bar&apos;)&gt;&gt;&gt; parser.parse_known_args([&apos;--foo&apos;, &apos;--badger&apos;, &apos;BAR&apos;, &apos;spam&apos;])(Namespace(bar=&apos;BAR&apos;, foo=True), [&apos;--badger&apos;, &apos;spam&apos;]) Customizing file parsing123class MyArgumentParser(argparse.ArgumentParser): def convert_arg_line_to_args(self, arg_line): return arg_line.split() Exiting methods12345ArgumentParser.exit(status=0, message=None)This method terminates the program, exiting with the specified status and, if given, it prints a message before that.ArgumentParser.error(message)This method prints a usage message including the message to the standard error and terminates the program with a status code of 2.]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-hashlib加密]]></title>
    <url>%2Fposts%2F263d2024.html</url>
    <content type="text"><![CDATA[摘要本文记录hashlib的常用方法 hashlibhash：一种算法,用于加密相关的操作,3.x里代替了md5模块和sha模块，主要提供 SHA1, SHA224, SHA256, SHA384, SHA512 ，MD5 算法三个特点： 内容相同则hash运算结果相同，内容稍微改变则hash值则变 不可逆推 相同算法：无论校验多长的数据，得到的哈希值长度固定。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import hashlib# ######## md5 ######## hash = hashlib.md5()hash.update(&apos;hello&apos;.encode(&apos;utf8&apos;))print(hash.hexdigest())#5d41402abc4b2a76b9719d911017c592 hash.update(&apos;alvin&apos;.encode(&apos;utf8&apos;)) print(hash.hexdigest())#92a7e713c30abbb0319fa07da2a5c4af m2=hashlib.md5()m2.update(&apos;helloalvin&apos;.encode(&apos;utf8&apos;))print(m2.hexdigest())#92a7e713c30abbb0319fa07da2a5c4af# 注意：把一段很长的数据update多次，与一次update这段长数据，得到的结果一样# 但是update多次为校验大文件提供了可能。 # ######## sha1 ########hash = hashlib.sha1()hash.update(&apos;admin&apos;)print hash.hexdigest() # ######## sha256 ########hash = hashlib.sha256()hash.update(&apos;admin&apos;)print hash.hexdigest() hash = hashlib.sha256(&apos;898oaFs09f&apos;.encode(&apos;utf8&apos;))hash.update(&apos;alvin&apos;.encode(&apos;utf8&apos;))print (hash.hexdigest())#e79e68f070cdedcfe63eaf1a2e92c83b4cfb1b5c6bc452d214c1b7e77cdfd1c7 # ######## sha384 ######## hash = hashlib.sha384()hash.update(&apos;admin&apos;)print hash.hexdigest() # ######## sha512 ######## hash = hashlib.sha512()hash.update(&apos;admin&apos;) 以上加密算法虽然依然非常厉害，但时候存在缺陷，即：通过撞库可以反解。所以，有必要对加密算法中添加自定义key再来做加密。 1234567import hashlib # ######## md5 ######## hash = hashlib.md5(&apos;898oaFs09f&apos;)hash.update(&apos;admin&apos;)print hash.hexdigest() 模拟撞库破解密码 123456789101112131415161718192021222324import hashlibpasswds=[ &apos;alex3714&apos;, &apos;alex1313&apos;, &apos;alex94139413&apos;, &apos;alex123456&apos;, &apos;123456alex&apos;, &apos;a123lex&apos;, ]def make_passwd_dic(passwds): dic=&#123;&#125; for passwd in passwds: m=hashlib.md5() m.update(passwd.encode(&apos;utf-8&apos;)) dic[passwd]=m.hexdigest() return dicdef break_code(cryptograph,passwd_dic): for k,v in passwd_dic.items(): if v == cryptograph: print(&apos;密码是===&gt;\033[46m%s\033[0m&apos; %k)cryptograph=&apos;aee949757a2e698417463d47acac93df&apos;break_code(cryptograph,make_passwd_dic(passwds)) 还不够吊？python 还有一个 hmac 模块，它内部对我们创建 key 和 内容 再进行处理然后再加密 123456import hmach = hmac.new(&apos;alvin&apos;.encode(&apos;utf8&apos;))h.update(&apos;hello&apos;.encode(&apos;utf8&apos;))print(h.hexdigest())#320df9832eab4c038b6c1d7ed73a5940 注意！注意！注意 1234567891011121314151617181920212223#要想保证hmac最终结果一致，必须保证：#1:hmac.new括号内指定的初始key一样#2:无论update多少次，校验的内容累加到一起是一样的内容import hmach1=hmac.new(b&apos;egon&apos;)h1.update(b&apos;hello&apos;)h1.update(b&apos;world&apos;)print(h1.hexdigest())h2=hmac.new(b&apos;egon&apos;)h2.update(b&apos;helloworld&apos;)print(h2.hexdigest())h3=hmac.new(b&apos;egonhelloworld&apos;)print(h3.hexdigest())&apos;&apos;&apos;f1bf38d054691688f89dcd34ac3c27f2f1bf38d054691688f89dcd34ac3c27f2bcca84edd9eeb86f30539922b28f3981&apos;&apos;&apos;]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-time、datetime]]></title>
    <url>%2Fposts%2F5d959c43.html</url>
    <content type="text"><![CDATA[摘要在Python中，通常有这几种方式来表示时间： 时间戳 格式化的时间字符串 元组（struct_time）共九个元素。 由于Python的time模块实现主要调用C库，所以各个平台可能有所不同。 UTC（Coordinated Universal Time）即格林威治天文时间，为世界标准时间。中国北京为UTC+8。 DST（Daylight Saving Time）即夏令时。 时间戳（timestamp）的方式：通常来说，时间戳是指格林威治时间1970年01月01日00时00分00秒(北京时间1970年01月01日08时00分00秒)起至现在的总秒数。我们运行“type(time.time())”，返回的是float类型。返回时间戳方式的函数主要有time()，clock()等。 元组（struct_time）方式：struct_time元组共有9个元素，返回struct_time的函数主要有gmtime()，localtime()，strptime()。 datetime123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309import datetime1.返回当前时间&gt;&gt;&gt; datetime.datetime.now()datetime.datetime(2017, 5, 9, 17, 7, 0, 514481)2.时间戳转换成日期&gt;&gt;&gt; datetime.date.fromtimestamp(1178766678)datetime.date(2007, 5, 10)3.当前时间+3天&gt;&gt;&gt; datetime.datetime.now() + datetime.timedelta(+3)datetime.datetime(2017, 5, 12, 17, 12, 42, 124379)4.当前时间-3天&gt;&gt;&gt; datetime.datetime.now() + datetime.timedelta(-3)datetime.datetime(2017, 5, 6, 17, 13, 18, 474406)5.当前时间+3小时&gt;&gt;&gt; datetime.datetime.now() + datetime.timedelta(hours=3)datetime.datetime(2017, 5, 9, 20, 13, 55, 678310)6.当前时间+30分钟&gt;&gt;&gt; datetime.datetime.now() + datetime.timedelta(minutes=30)datetime.datetime(2017, 5, 9, 17, 44, 40, 392370)1. 日期输出格式化 datetime =&gt; stringimport datetimenow = datetime.datetime.now()now.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;) 输出&apos;2015-04-07 19:11:21&apos;strftime是datetime类的实例方法。 2. 日期输出格式化 string =&gt; datetimeimport datetimet_str = &apos;2015-04-07 19:11:21&apos;d = datetime.datetime.strptime(t_str, &apos;%Y-%m-%d %H:%M:%S&apos;)strptime是datetime类的静态方法。 3. 日期比较操作在datetime模块中有timedelta类，这个类的对象用于表示一个时间间隔，比如两个日期或者时间的差别。构造方法：import datetimedatetime.timedelta(days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0) 所有的参数都有默认值0，这些参数可以是int或float，正的或负的。可以通过 timedelta.days、tiemdelta.seconds 等获取相应的时间值。timedelta 类的实例，支持加、减、乘、除等操作，所得的结果也是 timedelta 类的实例。比如：import datetimeyear = datetime.timedelta(days=365)ten_years = year *10nine_years = ten_years - year 同时，date、time和datetime类也支持与timedelta的加、减运算。datetime1 = datetime2 + timedeltatimedelta = datetime1 - datetime2这样，可以很方便的实现一些功能。 4. 两个日期相差多少天。import datetimed1 = datetime.datetime.strptime(&apos;2015-03-05 17:41:20&apos;, &apos;%Y-%m-%d %H:%M:%S&apos;)d2 = datetime.datetime.strptime(&apos;2015-03-02 17:41:20&apos;, &apos;%Y-%m-%d %H:%M:%S&apos;)delta = d1 - d2print delta.days 输出：3 5. 今天的n天后的日期。import datetimenow = datetime.datetime.now()delta = datetime.timedelta(days=3)n_days = now + deltaprint n_days.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;) 输出：2015-04-10 19:16:34 #coding=utf-8import datetimenow=datetime.datetime.now()print now #将日期转化为字符串datetime =&gt; stringimport datetimenow=datetime.datetime.now()print now.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;) #将字符串转换为日期 string =&gt; datetimeimport datetimet_str = &apos;2015-03-05 16:26:23&apos;d=datetime.datetime.strptime(t_str,&apos;%Y-%m-%d %H:%M:%S&apos;)print d#在datetime模块中有timedelta类，这个类的对象用于表示一个时间间隔，比如两个日#期或者时间的差别。 #计算两个日期的间隔import datetimed1 = datetime.datetime.strptime(&apos;2012-03-05 17:41:20&apos;, &apos;%Y-%m-%d %H:%M:%S&apos;)d2 = datetime.datetime.strptime(&apos;2012-03-02 17:41:20&apos;, &apos;%Y-%m-%d %H:%M:%S&apos;)delta = d1 - d2print delta.days print delta #今天的n天后的日期import datetimenow=datetime.datetime.now()delta=datetime.timedelta(days=3)n_days=now+deltaprint n_days.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)datetime的好处是可以实现方便的时间运算,比如 endTime - starTime,这在时间duration计算时非常方便.##datetime 模块常用方法小记datetime模块常用的主要有下面这四个类：1. datetime.date: 是指年月日构成的日期(相当于日历)2. datetime.time: 是指时分秒微秒构成的一天24小时中的具体时间(相当于手表)3. datetime.datetime: 上面两个合在一起，既包含时间又包含日期4. datetime.timedelta: 时间间隔对象(timedelta)。一个时间点(datetime)加上一个时间间隔(timedelta)可以得到一个新的时间点(datetime)。比如今天的上午3点加上5个小时得到今天的上午8点。同理，两个时间点相减会得到一个时间间隔。1.datetime.date 类1.新建一个date对象，日期为今天，既可以直接调用datetime.date.today()，也可以直接向datetime.date()传值，如下：In [4]: today = datetime.date.today()In [5]: todayOut[5]: datetime.date(2014, 8, 15)In [6]: t = datetime.date(2014,8,15)In [7]: tOut[7]: datetime.date(2014, 8, 15)2.datetime.date.strftime(format) 格式化为需要的时间，如常用的 “年-月-日 小时：分钟：秒” 格式In [8]: today.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)Out[8]: &apos;2014-08-15 00:00:00’date对象中小时、分钟、秒默认都是0，纪元年的那个时间3.datetime.date.timple() 转成struct_time格式，这样传递给time.mktime(t) 后，直接转成时间戳格式In [9]: today.timetuple()Out[9]: time.struct_time(tm_year=2014, tm_mon=8, tm_mday=15, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=4, tm_yday=227, tm_isdst=-1)In [10]: time.mktime(today.timetuple())Out[10]: 1408032000.04.datetime.date.replace(year, month, day) 返回一个替换后的date对象In [11]: today.replace(year=2013)Out[11]: datetime.date(2013, 8, 15)5.datetime.date.fromtimestamp(timestamp) 将时间戳转化为date对象In [12]: datetime.date.fromtimestamp(1408058729)Out[12]: datetime.date(2014, 8, 15)2.datetime.time 类1.新建一个time对象In [15]: tOut[15]: datetime.time(8, 45, 20)2.datetime.time.(format)格式化输出In [16]: t.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)Out[16]: &apos;1900-01-01 08:45:20’time对应的年、月、日为1900、01、01，纪元年的那个时间3.datetime.time.replace([hour[, minute[, second[, microsecond[, tzinfo]]]]]) 返回一个替换后的time对象In [17]: t.replace(hour=9)Out[17]: datetime.time(9, 45, 20)3.datetime.datetime类其实和date的那些方法差不多了，大概看以下，简单说说1.新建一个datetime对象，日期为今天，既可以直接调用datetime.datetime.today()，也可以直接向datetime.datetime()传值，如下：In [21]: d1 = datetime.datetime.today()In [22]: d1Out[22]: datetime.datetime(2014, 8, 15, 8, 12, 34, 790945)In [23]: d2 = datetime.datetime(2014, 8, 15, 8, 12, 34, 790945)In [24]: d2Out[24]: datetime.datetime(2014, 8, 15, 8, 12, 34, 790945)2.datetime.datetime.now([tz]) 当不指定时区时，和datetime.datetime.today()是一样的结果，如下In [25]: datetime.datetime.now()Out[25]: datetime.datetime(2014, 8, 15, 8, 14, 50, 738672)3..datetime.datetime.strftime(format) 格式化为需要的时间，如常用的 “年-月-日 小时：分钟：秒” 格式In [27]: d1Out[27]: datetime.datetime(2014, 8, 15, 8, 12, 34, 790945)In [28]: d1.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)Out[28]: &apos;2014-08-15 08:12:34’4.datetime.datetime.timple() 转成struct_time格式，这样传递给time.mktime(t) 后，直接转成时间戳格式In [29]: d1Out[29]: datetime.datetime(2014, 8, 15, 8, 12, 34, 790945)In [30]: d1.timetuple()Out[30]: time.struct_time(tm_year=2014, tm_mon=8, tm_mday=15, tm_hour=8, tm_min=12, tm_sec=34, tm_wday=4, tm_yday=227, tm_isdst=-1)In [31]: time.mktime(d1.timetuple())Out[31]: 1408061554.05.datetime.datetime.replace(year, month, day) 返回一个替换后的date对象In [32]: d1Out[32]: datetime.datetime(2014, 8, 15, 8, 12, 34, 790945)In [33]: d1.replace(year=2000)Out[33]: datetime.datetime(2000, 8, 15, 8, 12, 34, 790945)6.datetime.datetime.fromtimestamp(timestamp) 将时间戳转化为datetime对象In [34]: time.time()Out[34]: 1408061894.081552In [35]: datetime.datetime.fromtimestamp(1408061894)Out[35]: datetime.datetime(2014, 8, 15, 8, 18, 14)4.datetime.timedelta类没啥好说的，主要做时间的加减法用，如下：In [78]: today = datetime.datetime.today()In [79]: yesterday = today - datetime.timedelta(days=1)In [80]: yesterdayOut[80]: datetime.datetime(2014, 8, 14, 15, 8, 25, 783471)In [81]: todayOut[81]: datetime.datetime(2014, 8, 15, 15, 8, 25, 783471)#!/usr/bin/python# -*- coding: UTF-8 -*-import datetimei = datetime.datetime.now()print (&quot;当前的日期和时间是 %s&quot; % i)print (&quot;ISO格式的日期和时间是 %s&quot; % i.isoformat() )print (&quot;当前的年份是 %s&quot; %i.year)print (&quot;当前的月份是 %s&quot; %i.month)print (&quot;当前的日期是 %s&quot; %i.day)print (&quot;dd/mm/yyyy 格式是 %s/%s/%s&quot; % (i.day, i.month, i.year) )print (&quot;当前小时是 %s&quot; %i.hour)print (&quot;当前分钟是 %s&quot; %i.minute)print (&quot;当前秒是 %s&quot; %i.second) time1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681.以元组方式返回本地当前时间&gt;&gt;&gt; time.localtime()time.struct_time(tm_year=2017, tm_mon=5, tm_mday=8, tm_hour=16, tm_min=13, tm_sec=34, tm_wday=0, tm_yday=128, tm_isdst=0)2.以元组方式返回格林威治时间&gt;&gt;&gt; time.gmtime() time.struct_time(tm_year=2017, tm_mon=5, tm_mday=8, tm_hour=8, tm_min=13, tm_sec=38, tm_wday=0, tm_yday=128, tm_isdst=0)3.将元组时间转换为时间戳&gt;&gt;&gt; x = time.localtime()&gt;&gt;&gt; time.mktime(x)1494232890.04.将元组时间转换为字符串格式时间&gt;&gt;&gt; x = time.localtime()&gt;&gt;&gt; time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,x)&apos;2017-05-08 16:57:38&apos;5.将字符串格式时间转换为元组格式时间&gt;&gt;&gt; time.strptime(&apos;2017-05-08 17:03:12&apos;,&apos;%Y-%m-%d %H:%M:%S&apos;)time.struct_time(tm_year=2017, tm_mon=5, tm_mday=8, tm_hour=17, tm_min=3, tm_sec=12, tm_wday=0, tm_yday=128, tm_isdst=-1)6.元组格式时间转换为字符串格式时间&gt;&gt;&gt; time.asctime()&apos;Tue May 9 15:23:21 2017&apos;&gt;&gt;&gt; x = time.localtime()&gt;&gt;&gt; time.asctime(x)&apos;Tue May 9 15:23:39 2017&apos;7.时间戳转换成字符串格式时间&gt;&gt;&gt; time.ctime()&apos;Tue May 9 16:07:24 2017&apos;&gt;&gt;&gt; time.ctime(987867475)&apos;Sat Apr 21 23:37:55 2001&apos;格式参照%a 本地（locale）简化星期名称 %A 本地完整星期名称 %b 本地简化月份名称 %B 本地完整月份名称 %c 本地相应的日期和时间表示 %d 一个月中的第几天（01 - 31） %H 一天中的第几个小时（24小时制，00 - 23） %I 第几个小时（12小时制，01 - 12） %j 一年中的第几天（001 - 366） %m 月份（01 - 12） %M 分钟数（00 - 59） %p 本地am或者pm的相应符 %S 秒（01 - 61） %U 一年中的星期数。（00 - 53星期天是一个星期的开始。）第一个星期天之前的所有天数都放在第0周。 %w 一个星期中的第几天（0 - 6，0是星期天）%W 和%U基本相同，不同的是%W以星期一为一个星期的开始。 %x 本地相应日期 %X 本地相应时间 %y 去掉世纪的年份（00 - 99） %Y 完整的年份 %Z 时区的名字（如果不存在为空字符） %% ‘%’字符import time # 当前时间print time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime()) t = time.localtime(time.time() - 300) # 5分钟前print time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, t)python datetime 获取 几分钟 小时 天之前的时间 print ((datetime.datetime.now()-datetime.timedelta(minutes=2)).strftime(&quot;%Y-%m-%d %H:%M&quot;))可以控制days、seconds、minutes、hours、weeks等------一：经常使用的时间方法1.得到当前时间使用time模块，首先得到当前的时间戳In [42]: time.time()Out[42]: 1408066927.208922将时间戳转换为时间元组 struct_timeIn [43]: time.localtime(time.time())Out[43]: time.struct_time(tm_year=2014, tm_mon=8, tm_mday=15, tm_hour=9, tm_min=42, tm_sec=20, tm_wday=4, tm_yday=227, tm_isdst=0)格式化输出想要的时间In [44]: time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;,time.localtime(time.time()))Out[44]: &apos;2014-08-15 09:43:04&apos;接上文，不加参数时，默认就是输出当前的时间In [48]: time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)Out[48]: &apos;2014-08-15 09:46:53’当然也可以透过datetime模块来实现，如下：In [68]: t = time.time()In [69]: datetime.datetime.fromtimestamp(t).strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)Out[69]: &apos;2014-08-15 10:04:51’同时，也可以只使用datetime模块In [46]: datetime.datetime.now().strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)Out[46]: &apos;2014-08-15 09:45:27’In [47]: datetime.datetime.today().strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)Out[47]: &apos;2014-08-15 09:46:10&apos;2.获取时间差，计算程序的执行时间等：使用time模块：In [75]: def t(): ....: start = time.time() ....: time.sleep(10) ....: end = time.time() ....: print end - start ....:In [76]: t()10.0014948845使用datetime模块：In [49]: starttime = datetime.datetime.now()In [50]: endtime = datetime.datetime.now()In [51]: print (endtime - starttime).seconds63.计算昨天的日期（发散思维，计算其他日期相加、相减等）：In [52]: d1 = datetime.datetime.now()In [53]: d2 = d1 - datetime.timedelta(days=1)In [54]: d1Out[54]: datetime.datetime(2014, 8, 15, 9, 54, 10, 68665)In [55]: d2Out[55]: datetime.datetime(2014, 8, 14, 9, 54, 10, 68665)4.时间元组 struct_time转化为时间戳In [56]: datetime.datetime.now()Out[56]: datetime.datetime(2014, 8, 15, 9, 57, 52, 779893)In [57]: datetime.datetime.now().timetuple()Out[57]: time.struct_time(tm_year=2014, tm_mon=8, tm_mday=15, tm_hour=9, tm_min=58, tm_sec=12, tm_wday=4, tm_yday=227, tm_isdst=-1)In [58]: time.mktime(datetime.datetime.now().timetuple())Out[58]: 1408067904.05.strptime也挺有用的，将时间字符串转换为时间元组struct_timeIn [73]: time.strftime(&apos;%Y-%m-%d %H:%M:%S&apos;)Out[73]: &apos;2014-08-15 10:27:36&apos;In [74]: time.strptime(&apos;2014-08-15 10:27:36&apos;,&apos;%Y-%m-%d %H:%M:%S&apos;)Out[74]: time.struct_time(tm_year=2014, tm_mon=8, tm_mday=15, tm_hour=10, tm_min=27, tm_sec=36, tm_wday=4, tm_yday=227, tm_isdst=-1)]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-shutl]]></title>
    <url>%2Fposts%2Fe1b5539d.html</url>
    <content type="text"><![CDATA[摘要本文记录shutil模块的常用方法 shutil高级的 文件、文件夹、压缩包 处理模块 shutil.copyfileobj(fsrc, fdst[, length])将文件内容拷贝到另一个文件中 123import shutil shutil.copyfileobj(open(&apos;old.xml&apos;,&apos;r&apos;), open(&apos;new.xml&apos;, &apos;w&apos;)) shutil.copyfile(src, dst)拷贝文件 1shutil.copyfile(&apos;f1.log&apos;, &apos;f2.log&apos;) #目标文件无需存在 shutil.copymode(src, dst)仅拷贝权限。内容、组、用户均不变 1shutil.copymode(&apos;f1.log&apos;, &apos;f2.log&apos;) #目标文件必须存在 shutil.copystat(src, dst)仅拷贝状态的信息，包括：mode bits, atime, mtime, flags 1shutil.copystat(&apos;f1.log&apos;, &apos;f2.log&apos;) #目标文件必须存在 shutil.copy(src, dst)拷贝文件和权限 123import shutil shutil.copy(&apos;f1.log&apos;, &apos;f2.log&apos;) shutil.copy2(src, dst)拷贝文件和状态信息 123import shutil shutil.copy2(&apos;f1.log&apos;, &apos;f2.log&apos;) shutil.ignore_patterns(*patterns)shutil.copytree(src, dst, symlinks=False, ignore=None)递归的去拷贝文件夹 123import shutil shutil.copytree(&apos;folder1&apos;, &apos;folder2&apos;, ignore=shutil.ignore_patterns(&apos;*.pyc&apos;, &apos;tmp*&apos;)) #目标目录不能存在，注意对folder2目录父级目录要有可写权限，ignore的意思是排除 1234567import shutilshutil.copytree(&apos;f1&apos;, &apos;f2&apos;, symlinks=True, ignore=shutil.ignore_patterns(&apos;*.pyc&apos;, &apos;tmp*&apos;))&apos;&apos;&apos;通常的拷贝都把软连接拷贝成硬链接，即对待软连接来说，创建新的文件&apos;&apos;&apos; shutil.rmtree(path[, ignore_errors[, onerror]])递归的去删除文件 123import shutil shutil.rmtree(&apos;folder1&apos;) shutil.move(src, dst)递归的去移动文件，它类似mv命令，其实就是重命名。 123import shutil shutil.move(&apos;folder1&apos;, &apos;folder3&apos;) shutil.make_archive(base_name, format,…)创建压缩包并返回文件路径，例如：zip、tar 12345678base_name： 压缩包的文件名，也可以是压缩包的路径。只是文件名时，则保存至当前目录，否则保存至指定路径，如 data_bak =&gt;保存至当前路径如：/tmp/data_bak =&gt;保存至/tmp/format： 压缩包种类，“zip”, “tar”, “bztar”，“gztar”root_dir： 要压缩的文件夹路径（默认当前目录）owner： 用户，默认当前用户group： 组，默认当前组logger： 用于记录日志，通常是logging.Logger对象 12345678#将 /data 下的文件打包放置当前程序目录import shutilret = shutil.make_archive(&quot;data_bak&quot;, &apos;gztar&apos;, root_dir=&apos;/data&apos;) #将 /data下的文件打包放置 /tmp/目录import shutilret = shutil.make_archive(&quot;/tmp/data_bak&quot;, &apos;gztar&apos;, root_dir=&apos;/data&apos;) shutil 对压缩包的处理是调用 ZipFile 和 TarFile 两个模块来进行的，详细： 1234567891011121314151617181920212223242526272829import zipfile# 压缩z = zipfile.ZipFile(&apos;laxi.zip&apos;, &apos;w&apos;)z.write(&apos;a.log&apos;)z.write(&apos;data.data&apos;)z.close()# 解压z = zipfile.ZipFile(&apos;laxi.zip&apos;, &apos;r&apos;)z.extractall(path=&apos;.&apos;)z.close()zipfile压缩解压缩import tarfile# 压缩&gt;&gt;&gt; t=tarfile.open(&apos;/tmp/egon.tar&apos;,&apos;w&apos;)&gt;&gt;&gt; t.add(&apos;/test1/a.py&apos;,arcname=&apos;a.bak&apos;)&gt;&gt;&gt; t.add(&apos;/test1/b.py&apos;,arcname=&apos;b.bak&apos;)&gt;&gt;&gt; t.close()# 解压&gt;&gt;&gt; t=tarfile.open(&apos;/tmp/egon.tar&apos;,&apos;r&apos;)&gt;&gt;&gt; t.extractall(&apos;/egon&apos;)&gt;&gt;&gt; t.close()tarfile压缩解压缩]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-re]]></title>
    <url>%2Fposts%2F4333f888.html</url>
    <content type="text"><![CDATA[摘要正则表达式是一个特殊的字符序列，它能帮助你方便的检查一个字符串是否与某种模式匹配。Python自1.5版本起增加了re模块，它提供Perl风格的正则表达式模式。 re模块使Python语言拥有全部的正则表达式功能。 compile函数根据一个模式字符串和可选的标志参数生成一个正则表达式对象。该对象拥有一系列方法用于正则表达式匹配和替换。 re模块也提供了与这些方法功能完全一致的函数，这些函数使用一个模式字符串做为它们的第一个参数。 语法: 1234567891011121314import re #导入模块名p = re.compile(&quot;^[0-9]&quot;)#生成要匹配的正则对象，^代表从开头匹配，[0-9]代表匹配0至9的任意一个数字，所以这里的意思是对传进来的字符串进行匹配，如果这个字符串的开头第一个字符是数字，就代表匹配上了m = p.match(&apos;14534Abc&apos;)#按上面生成的正则对象去匹配字符串，如果能匹配成功，这个m就会有值，否则m为Noneif m: #不为空代表匹配上了 print(m.group()) #m.group()返回匹配上的结果，此处为1，因为匹配上的是1这个字符else: print(&quot;doesn&apos;t match.&quot;) 上面的第2 和第3行也可以合并成一行来写 1m = p.match(&quot;^[0-9]&quot;,&apos;14534Abc&apos;) 效果是一样的，区别在于，第一种方式是提前对要匹配的格式进行了编译（对匹配公式进行解析），这样再去匹配的时候就不用在编译匹配的格式，第2种简写是每次匹配的时候都要进行一次匹配公式的编译，所以，如果你需要从一个5w行的文件中匹配出所有以数字开头的行，建议先把正则公式进行编译再匹配，这样速度会快点。 re.match函数re.match 尝试从字符串的起始位置匹配一个模式，如果不是起始位置匹配成功的话，match()就返回none。 函数语法：1re.match(pattern, string, flags=0) 函数参数说明： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串。 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。 匹配成功re.match方法返回一个匹配的对象，否则返回None。 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 匹配对象方法 描述 group(num=0) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。 groups() 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。 实例：12345678910111213#!/usr/bin/pythonimport re line = &quot;Cats are smarter than dogs&quot; matchObj = re.match( r&apos;(.*) are (.*?) .*&apos;, line, re.M|re.I) if matchObj: print &quot;matchObj.group() : &quot;, matchObj.group() print &quot;matchObj.group(1) : &quot;, matchObj.group(1) print &quot;matchObj.group(2) : &quot;, matchObj.group(2)else: print &quot;No match!!&quot; 以上实例执行结果如下： 123matchObj.group() : Cats are smarter than dogsmatchObj.group(1) : CatsmatchObj.group(2) : smarter re.search方法re.search 扫描整个字符串并返回第一个成功的匹配。 函数语法：1re.search(pattern, string, flags=0) 函数参数说明： 参数 描述 pattern 匹配的正则表达式 string 要匹配的字符串。 flags 标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。 匹配成功re.search方法方法返回一个匹配的对象，否则返回None。 我们可以使用group(num) 或 groups() 匹配对象函数来获取匹配表达式。 匹配对象方法 描述 group(num=0) 匹配的整个表达式的字符串，group() 可以一次输入多个组号，在这种情况下它将返回一个包含那些组所对应值的元组。 groups() 返回一个包含所有小组字符串的元组，从 1 到 所含的小组号。 实例：12345678910111213#!/usr/bin/pythonimport re line = &quot;Cats are smarter than dogs&quot;; matchObj = re.match( r&apos;(.*) are (.*?) .*&apos;, line, re.M|re.I) if matchObj: print &quot;matchObj.group() : &quot;, matchObj.group() print &quot;matchObj.group(1) : &quot;, matchObj.group(1) print &quot;matchObj.group(2) : &quot;, matchObj.group(2)else: print &quot;No match!!&quot; 以上实例执行结果如下： 123matchObj.group() : Cats are smarter than dogsmatchObj.group(1) : CatsmatchObj.group(2) : smarter re.match与re.search的区别re.match只匹配字符串的开始，如果字符串开始不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。 实例：12345678910111213141516#!/usr/bin/pythonimport re line = &quot;Cats are smarter than dogs&quot;; matchObj = re.match( r&apos;dogs&apos;, line, re.M|re.I)if matchObj: print &quot;match --&gt; matchObj.group() : &quot;, matchObj.group()else: print &quot;No match!!&quot; matchObj = re.search( r&apos;dogs&apos;, line, re.M|re.I)if matchObj: print &quot;search --&gt; matchObj.group() : &quot;, matchObj.group()else: print &quot;No match!!&quot; 以上实例运行结果如下： 12No match!!search --&gt; matchObj.group() : dogs 检索和替换 re.subPython 的re模块提供了re.sub用于替换字符串中的匹配项。 语法：1re.sub(pattern, repl, string, max=0) pattern : 正则中的模式字符串。repl : 替换的字符串，也可为一个函数。string : 要被查找替换的原始字符串。count : 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。 返回的字符串是在字符串中用 RE 最左边不重复的匹配来替换。如果模式没有发现，字符将被没有改变地返回。 可选参数 count 是模式匹配后替换的最大次数；count 必须是非负整数。缺省值是 0 表示替换所有的匹配。 实例：123456789101112#!/usr/bin/pythonimport re phone = &quot;2004-959-559 # This is Phone Number&quot; # Delete Python-style commentsnum = re.sub(r&apos;#.*$&apos;, &quot;&quot;, phone)print &quot;Phone Num : &quot;, num # Remove anything other than digitsnum = re.sub(r&apos;\D&apos;, &quot;&quot;, phone) print &quot;Phone Num : &quot;, num 以上实例执行结果如下： 12Phone Num : 2004-959-559Phone Num : 2004959559 re.findallPython 的re模块提供了re.findall用于找出字符串中所有的匹配项。 实例：12str1 = &quot;This is a apple! This is a pen!&quot;print(re.findall(r&quot;is&quot;, str1)) 执行输出结果为： 1[&apos;is&apos;, &apos;is&apos;, &apos;is&apos;, &apos;is&apos;] repl 参数是一个函数以下实例中将字符串中的匹配的数字乘于 2： 123456789101112#!/usr/bin/python# -*- coding: UTF-8 -*-import re# 将匹配的数字乘于 2def double(matched): value = int(matched.group(&apos;value&apos;)) return str(value * 2)s = &apos;A23G4HFD567&apos;print(re.sub(&apos;(?P&lt;value&gt;\d+)&apos;, double, s)) 执行输出结果为： 1A46G8HFD1134 一些正则实例正则表达式常用5种操作 re.match(pattern, string) # 从头匹配 re.search(pattern, string) # 匹配整个字符串，直到找到一个匹配 re.split() # 将匹配到的格式当做分割点对字符串分割成列表 123&gt;&gt;&gt;m = re.split(&quot;[0-9]&quot;, &quot;alex1rain2jack3helen rachel8&quot;)&gt;&gt;&gt;print(m)输出： [&apos;alex&apos;, &apos;rain&apos;, &apos;jack&apos;, &apos;helen rachel&apos;, &apos;&apos;] re.findall() # 找到所有要匹配的字符并返回列表格式 123&gt;&gt;&gt;m = re.findall(&quot;[0-9]&quot;, &quot;alex1rain2jack3helen rachel8&quot;)&gt;&gt;&gt;print(m)&lt;br&gt;输出：[&apos;1&apos;, &apos;2&apos;, &apos;3&apos;, &apos;8&apos;] re.sub(pattern, repl, string, count,flag) # 替换匹配到的字符 123m=re.sub(&quot;[0-9]&quot;,&quot;|&quot;, &quot;alex1rain2jack3helen rachel8&quot;,count=2 )print(m)输出：alex|rain|jack3helen rachel8 “abbbcccbba” -&gt; “abcba”12345&gt;&gt;&gt; import re&gt;&gt;&gt; p = re.compile(ur&quot;([a-zA-Z])(\1+)&quot;)&gt;&gt;&gt; s = &quot;abbbcccbba&quot;&gt;&gt;&gt; p.sub(ur&quot;\1&quot;,s)&apos;abcba&apos; “abbbcccbba” -&gt; “a1b3c3b2a1”12345&gt;&gt;&gt; import re&gt;&gt;&gt; p = re.compile(ur&quot;([a-zA-Z])(\1*)&quot;)&gt;&gt;&gt; s = &quot;abbbcccbba&quot;&gt;&gt;&gt; p.sub(lambda m: m.group(1)+str(1+len(m.group(2))), s)&apos;a1b3c3b2a1&apos; 手机号123456phone_str = &quot;hey my name is alex, and my phone number is 13651054607, please call me if you are pretty!&quot;phone_str2 = &quot;hey my name is alex, and my phone number is 18651054604, please call me if you are pretty!&quot; m = re.search(&quot;(1)([358]\d&#123;9&#125;)&quot;,phone_str2)if m: print(m.group()) 匹配IP V412345ip_addr = &quot;inet 192.168.60.223 netmask 0xffffff00 broadcast 192.168.60.255&quot; m = re.search(&quot;\d&#123;1,3&#125;\.\d&#123;1,3&#125;\.\d&#123;1,3&#125;\.\d&#123;1,3&#125;&quot;, ip_addr) print(m.group()) 分组匹配地址12345678910111213141516171819contactInfo = &apos;Oldboy School, Beijing Changping Shahe: 010-8343245&apos;match = re.search(r&apos;(\w+), (\w+): (\S+)&apos;, contactInfo) #分组&quot;&quot;&quot;&gt;&gt;&gt; match.group(1) &apos;Doe&apos; &gt;&gt;&gt; match.group(2) &apos;John&apos; &gt;&gt;&gt; match.group(3) &apos;555-1212&apos;&quot;&quot;&quot;match = re.search(r&apos;(?P&lt;last&gt;\w+), (?P&lt;first&gt;\w+): (?P&lt;phone&gt;\S+)&apos;, contactInfo)&quot;&quot;&quot; &gt;&gt;&gt; match.group(&apos;last&apos;) &apos;Doe&apos; &gt;&gt;&gt; match.group(&apos;first&apos;) &apos;John&apos; &gt;&gt;&gt; match.group(&apos;phone&apos;) &apos;555-1212&apos;&quot;&quot;&quot; 匹配email1234email = &quot;alex.li@126.com http://www.oldboyedu.com&quot; m = re.search(r&quot;[0-9.a-z]&#123;0,26&#125;@[0-9.a-z]&#123;0,20&#125;.[0-9a-z]&#123;0,8&#125;&quot;, email)print(m.group()) 正则表达式修饰符 - 可选标志正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。多个标志可以通过按位 OR(|) 它们来指定。如 re.I | re.M 被设置成 I 和 M 标志： 修饰符 描述 re.I 使匹配对大小写不敏感 re.L 做本地化识别（locale-aware）匹配 re.M 多行匹配，影响 ^ 和 $ re.S 使 . 匹配包括换行在内的所有字符 re.U 根据Unicode字符集解析字符。这个标志影响 \w, \W, \b, \B. re.X 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。 正则表达式模式模式字符串使用特殊的语法来表示一个正则表达式： 字母和数字表示他们自身。一个正则表达式模式中的字母和数字匹配同样的字符串。 多数字母和数字前加一个反斜杠时会拥有不同的含义。 标点符号只有被转义时才匹配自身，否则它们表示特殊的含义。 反斜杠本身需要使用反斜杠转义。 由于正则表达式通常都包含反斜杠，所以你最好使用原始字符串来表示它们。模式元素(如 r’/t’，等价于’//t’)匹配相应的特殊字符。 下表列出了正则表达式模式语法中的特殊元素。如果你使用模式的同时提供了可选的标志参数，某些模式元素的含义会改变。 模式 描述 ^ 匹配字符串的开头 $ 匹配字符串的末尾。 . 匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。 […] 用来表示一组字符,单独列出：[amk] 匹配 ‘a’，’m’或’k’ [^…] 不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符。 re* 匹配0个或多个的表达式。 re+ 匹配1个或多个的表达式。 re? 匹配0个或1个由前面的正则表达式定义的片段，贪婪方式 re{ n} re{ n,} 精确匹配n个前面表达式。 re{ n, m} 匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式 a丨b 匹配a或b (re) G匹配括号内的表达式，也表示一个组 (?imx) 正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域。 (?-imx) 正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域。 (?: re) 类似 (…), 但是不表示一个组 (?imx: re) 在括号中使用i, m, 或 x 可选标志 (?-imx: re) 在括号中不使用i, m, 或 x 可选标志 (?#…) 注释. (?= re) 前向肯定界定符。如果所含正则表达式，以 … 表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。 (?! re) 前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功 (?&gt; re) 匹配的独立模式，省去回溯。 \w 匹配字母数字 \W 匹配非字母数字 \s 匹配任意空白字符，等价于 [\t\n\r\f]. \S 匹配任意非空字符 \d 匹配任意数字，等价于 [0-9]. \D 匹配任意非数字 \A 匹配字符串开始 \Z 匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。c \z 匹配字符串结束 \G 匹配最后匹配完成的位置。 \b 匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。 \B 匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。 \n, \t, 等. 匹配一个换行符。匹配一个制表符。等 \1…\9 匹配第n个分组的子表达式。 \10 匹配第n个分组的子表达式，如果它经匹配。否则指的是八进制字符码的表达式。/ 正则表达式实例字符匹配 实例 描述 python 匹配 “python”. 字符类 实例 描述 [Pp]ython 匹配 “Python” 或 “python” rub[ye] 匹配 “ruby” 或 “rube” [aeiou] 匹配中括号内的任意一个字母 [0-9] 匹配任何数字。类似于 [0123456789] [a-z] 匹配任何小写字母 [A-Z] 匹配任何大写字母 [a-zA-Z0-9] 匹配任何字母及数字 [^aeiou] 除了aeiou字母以外的所有字符 [^0-9] 匹配除了数字外的字符 特殊字符类 实例 描述 . 匹配除 “\n” 之外的任何单个字符。要匹配包括 ‘\n’ 在内的任何字符，请使用象 ‘[.\n]’ 的模式。 \d 匹配一个数字字符。等价于 [0-9]。 \D 匹配一个非数字字符。等价于 [^0-9]。 \s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ \f\n\r\t\v]。 \S 匹配任何非空白字符。等价于 [^ \f\n\r\t\v]。 \w 匹配包括下划线的任何单词字符。等价于’[A-Za-z0-9_]’。 \W 匹配任何非单词字符。等价于 ‘[^A-Za-z0-9_]’。]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-random]]></title>
    <url>%2Fposts%2F1901393b.html</url>
    <content type="text"><![CDATA[摘要本文记录random常用方法 random123456789101112131415161718192021222324#!/usr/bin/env python#_*_encoding: utf-8_*_import randomprint (random.random()) #0.6445010863311293 #random.random()用于生成一个0到1的随机符点数: 0 &lt;= n &lt; 1.0print (random.randint(1,7)) #4#random.randint()的函数原型为：random.randint(a, b)，用于生成一个指定范围内的整数。# 其中参数a是下限，参数b是上限，生成的随机数n: a &lt;= n &lt;= bprint (random.randrange(1,10)) #5#random.randrange的函数原型为：random.randrange([start], stop[, step])，# 从指定范围内，按指定基数递增的集合中 获取一个随机数。如：random.randrange(10, 100, 2)，# 结果相当于从[10, 12, 14, 16, ... 96, 98]序列中获取一个随机数。# random.randrange(10, 100, 2)在结果上与 random.choice(range(10, 100, 2) 等效。print(random.choice(&apos;liukuni&apos;)) #i#random.choice从序列中获取一个随机元素。# 其函数原型为：random.choice(sequence)。参数sequence表示一个有序类型。# 这里要说明一下：sequence在python不是一种特定的类型，而是泛指一系列的类型。# list, tuple, 字符串都属于sequence。有关sequence可以查看python手册数据模型这一章。# 下面是使用choice的一些例子：print(random.choice(&quot;学习Python&quot;))#学print(random.choice([&quot;JGood&quot;,&quot;is&quot;,&quot;a&quot;,&quot;handsome&quot;,&quot;boy&quot;])) #Listprint(random.choice((&quot;Tuple&quot;,&quot;List&quot;,&quot;Dict&quot;))) #Listprint(random.sample([1,2,3,4,5],3)) #[1, 2, 5]#random.sample的函数原型为：random.sample(sequence, k)，从指定序列中随机获取指定长度的片断。sample函数不会修改原有序列。 实际使用123456789101112131415161718192021222324252627#!/usr/bin/env python# encoding: utf-8import randomimport string#随机整数：print( random.randint(0,99)) #70 #随机选取0到100间的偶数：print(random.randrange(0, 101, 2)) #4 #随机浮点数：print( random.random()) #0.2746445568079129print(random.uniform(1, 10)) #9.887001463194844 #随机字符：print(random.choice(&apos;abcdefg&amp;#%^*f&apos;)) #f #多个字符中选取特定数量的字符：print(random.sample(&apos;abcdefghij&apos;,3)) #[&apos;f&apos;, &apos;h&apos;, &apos;d&apos;] #随机选取字符串：print( random.choice ( [&apos;apple&apos;, &apos;pear&apos;, &apos;peach&apos;, &apos;orange&apos;, &apos;lemon&apos;] )) #apple#洗牌#items = [1,2,3,4,5,6,7]print(items) #[1, 2, 3, 4, 5, 6, 7]random.shuffle(items)print(items) #[1, 4, 7, 2, 5, 3, 6] 生成随机验证码12345678910import randomcheckcode = &apos;&apos;for i in range(4): current = random.randrange(0,4) if current != i: temp = chr(random.randint(65,90)) else: temp = random.randint(0,9) checkcode += str(temp)print (checkcode)]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-os、sys]]></title>
    <url>%2Fposts%2F5d02f982.html</url>
    <content type="text"><![CDATA[摘要本文记录一些python中os和sys模块的常用方法 os123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107## diros.getcwd()#获取当前工作目录，即当前python脚本工作的目录路径os.chdir(&quot;dirname&quot;)#改变当前脚本工作目录；相当于shell下cdos.curdir#返回当前目录: (&apos;.&apos;)os.pardir#获取当前目录的父目录字符串名：(&apos;..&apos;)os.makedirs(&apos;dirname1/dirname2&apos;)#可生成多层递归目录os.removedirs(&apos;dirname1&apos;)#若目录为空，则删除，并递归到上一级目录，如若也为空，则删除，依此类推os.mkdir(&apos;dirname&apos;)#生成单级目录；相当于shell中mkdir dirnameos.rmdir(&apos;dirname&apos;)#删除单级空目录，若目录不为空则无法删除，报错；相当于shell中rmdir dirnameos.listdir(&apos;dirname&apos;)#列出指定目录下的所有文件和子目录，包括隐藏文件，并以列表方式打印## fileos.remove()#删除一个文件os.rename(&quot;oldname&quot;,&quot;newname&quot;)#重命名文件/目录os.stat(&apos;path/filename&apos;)#获取文件/目录信息## otheros.sep#输出操作系统特定的路径分隔符，win下为&quot;\\&quot;,Linux下为&quot;/&quot;os.linesep#输出当前平台使用的行终止符，win下为&quot;\t\n&quot;,Linux下为&quot;\n&quot;os.pathsep#输出用于分割文件路径的字符串os.name#输出字符串指示当前使用平台。win-&gt;&apos;nt&apos;; Linux-&gt;&apos;posix&apos;os.system(&quot;bash command&quot;)#运行shell命令，直接显示os.environ#获取系统环境变量## os.pathos.path.isfile(path)#如果path是一个存在的文件，返回True。否则返回Falseos.path.isdir(path)#如果path是一个存在的目录，则返回True。否则返回Falseos.path.abspath(path)#返回绝对路径os.path.basename(path)# 返回path最后的文件名。如何path以／或\结尾，那么就会返回空值。即os.path.split(path)的第二个元素os.path.commonprefix(list)#返回list(多个路径)中，所有path共有的最长的路径。os.path.dirname(path)#返回path的目录。其实就是os.path.split(path)的第一个元素os.path.exists(path)#如果path存在，返回True；如果path不存在，返回Falseos.path.lexists#路径存在则返回True,路径损坏也返回Trueos.path.expanduser(path)#把path中包含的&quot;~&quot;和&quot;~user&quot;转换成用户目录os.path.expandvars(path)#根据环境变量的值替换path中包含的”$name”和”$&#123;name&#125;”os.path.getatime(path)#返回path所指向的文件或者目录的最后存取时间os.path.getmtime(path)#返回path所指向的文件或者目录的最后修改时间os.path.getctime(path)#返回path的大小os.path.getsize(path)#返回文件大小，如果文件不存在就返回错误os.path.isabs(path)#判断是否为绝对路径os.path.islink(path)#判断路径是否为链接os.path.ismount(path)#判断路径是否为挂载点（）os.path.join(path1[, path2[, ...]])#把目录和文件名合成一个路径os.path.normcase(path)#转换path的大小写和斜杠os.path.normpath(path)#规范path字符串形式os.path.realpath(path)#返回path的真实路径os.path.relpath(path[, start])#从start开始计算相对路径os.path.samefile(path1, path2)#判断目录或文件是否相同os.path.sameopenfile(fp1, fp2)#判断fp1和fp2是否指向同一文件os.path.samestat(stat1, stat2)#判断stat tuple stat1和stat2是否指向同一个文件os.path.split(path)#把路径分割成dirname和basename，返回一个元组os.path.splitdrive(path)#一般用在windows下，返回驱动器名和路径组成的元组os.path.splitext(path)#分割路径，返回路径名和文件扩展名的元组os.path.splitunc(path)#把路径分割为加载点与文件os.path.walk(path, visit, arg)#遍历path，进入每个目录都调用visit函数，visit函数必须有3个参数(arg, dirname, names)，dirname表示当前目录的目录名，names代表当前目录下的所有文件名，args则为walk的第三个参数os.path.supports_unicode_filenames#设置是否支持unicode路径名 123456789101112在Linux和Mac平台上，该函数会原样返回path，在windows平台上会将路径中所有字符转换为小写，并将所有斜杠转换为饭斜杠。&gt;&gt;&gt; os.path.normcase(&apos;c:/windows\\system32\\&apos;) &apos;c:\\windows\\system32\\&apos; 规范化路径，如..和/&gt;&gt;&gt; os.path.normpath(&apos;c://windows\\System32\\../Temp/&apos;) &apos;c:\\windows\\Temp&apos; &gt;&gt;&gt; a=&apos;/Users/jieli/test1/\\\a1/\\\\aa.py/../..&apos;&gt;&gt;&gt; print(os.path.normpath(a))/Users/jieli/test1 123456简单使用import oscur_path = os.path.dirname(os.path.realpath(__file__)) #当前路径print(cur_path)filename = cur_path + &quot;/scripts.log&quot; #带上文件路径print(filename) 12345678910111213141516os路径处理#方式一：推荐使用import os#具体应用import os,syspossible_topdir = os.path.normpath(os.path.join( os.path.abspath(__file__), os.pardir, #上一级 os.pardir, os.pardir))sys.path.insert(0,possible_topdir)#方式二：不推荐使用os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) sys12345678sys.argv 命令行参数List，第一个元素是程序本身路径sys.exit(n) 退出程序，正常退出时exit(0)sys.version 获取Python解释程序的版本信息sys.maxint 最大的Int值sys.path 返回模块的搜索路径，初始化时使用PYTHONPATH环境变量的值sys.platform 返回操作系统平台名称sys.stdout.write(&apos;please:&apos;)val = sys.stdin.readline()[:-1] 模拟打印进度条 12345678910111213141516171819202122232425262728293031323334353637383940414243#=========知识储备==========#进度条的效果[# ][## ][### ][#### ]#指定宽度print(&apos;[%-15s]&apos; %&apos;#&apos;)print(&apos;[%-15s]&apos; %&apos;##&apos;)print(&apos;[%-15s]&apos; %&apos;###&apos;)print(&apos;[%-15s]&apos; %&apos;####&apos;)#打印%print(&apos;%s%%&apos; %(100)) #第二个%号代表取消第一个%的特殊意义#可传参来控制宽度print(&apos;[%%-%ds]&apos; %50) #[%-50s]print((&apos;[%%-%ds]&apos; %50) %&apos;#&apos;)print((&apos;[%%-%ds]&apos; %50) %&apos;##&apos;)print((&apos;[%%-%ds]&apos; %50) %&apos;###&apos;)#=========实现打印进度条函数==========import sysimport timedef progress(percent,width=50): if percent &gt;= 1: percent=1 show_str=(&apos;[%%-%ds]&apos; %width) %(int(width*percent)*&apos;#&apos;) print(&apos;\r%s %d%%&apos; %(show_str,int(100*percent)),file=sys.stdout,flush=True,end=&apos;&apos;)#=========应用==========data_size=1025recv_size=0while recv_size &lt; data_size: time.sleep(0.1) #模拟数据的传输延迟 recv_size+=1024 #每次收1024 percent=recv_size/data_size #接收的比例 progress(percent,width=70) #进度条的宽度70]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-logging]]></title>
    <url>%2Fposts%2F8d289766.html</url>
    <content type="text"><![CDATA[摘要本文记录logging模块的使用方法 logging日志级别123456CRITICAL = 50 #FATAL = CRITICALERROR = 40WARNING = 30 #WARN = WARNINGINFO = 20DEBUG = 10NOTSET = 0 #不设置 默认级别为warning，默认打印到终端12345678910111213import logginglogging.debug(&apos;调试debug&apos;)logging.info(&apos;消息info&apos;)logging.warning(&apos;警告warn&apos;)logging.error(&apos;错误error&apos;)logging.critical(&apos;严重critical&apos;)&apos;&apos;&apos;WARNING:root:警告warnERROR:root:错误errorCRITICAL:root:严重critical&apos;&apos;&apos; 为logging模块指定全局配置，针对所有logger有效，控制打印到文件中12345678910111213141516171819202122232425262728可在logging.basicConfig()函数中通过具体参数来更改logging模块默认行为，可用参数有filename：用指定的文件名创建FiledHandler（后边会具体讲解handler的概念），这样日志会被存储在指定的文件中。filemode：文件打开方式，在指定了filename时使用这个参数，默认值为“a”还可指定为“w”。format：指定handler使用的日志显示格式。 datefmt：指定日期时间格式。 level：设置rootlogger（后边会讲解具体概念）的日志级别 stream：用指定的stream创建StreamHandler。可以指定输出到sys.stderr,sys.stdout或者文件，默认为sys.stderr。若同时列出了filename和stream两个参数，则stream参数会被忽略。#格式%(name)s：Logger的名字，并非用户名，详细查看%(levelno)s：数字形式的日志级别%(levelname)s：文本形式的日志级别%(pathname)s：调用日志输出函数的模块的完整路径名，可能没有%(filename)s：调用日志输出函数的模块的文件名%(module)s：调用日志输出函数的模块名%(funcName)s：调用日志输出函数的函数名%(lineno)d：调用日志输出函数的语句所在的代码行%(created)f：当前时间，用UNIX标准的表示时间的浮 点数表示%(relativeCreated)d：输出日志信息时的，自Logger创建以 来的毫秒数%(asctime)s：字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒%(thread)d：线程ID。可能没有%(threadName)s：线程名。可能没有%(process)d：进程ID。可能没有%(message)s：用户输出的消息logging.basicConfig() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#======介绍可在logging.basicConfig()函数中可通过具体参数来更改logging模块默认行为，可用参数有filename：用指定的文件名创建FiledHandler（后边会具体讲解handler的概念），这样日志会被存储在指定的文件中。filemode：文件打开方式，在指定了filename时使用这个参数，默认值为“a”还可指定为“w”。format：指定handler使用的日志显示格式。datefmt：指定日期时间格式。level：设置rootlogger（后边会讲解具体概念）的日志级别stream：用指定的stream创建StreamHandler。可以指定输出到sys.stderr,sys.stdout或者文件，默认为sys.stderr。若同时列出了filename和stream两个参数，则stream参数会被忽略。format参数中可能用到的格式化串：%(name)s Logger的名字%(levelno)s 数字形式的日志级别%(levelname)s 文本形式的日志级别%(pathname)s 调用日志输出函数的模块的完整路径名，可能没有%(filename)s 调用日志输出函数的模块的文件名%(module)s 调用日志输出函数的模块名%(funcName)s 调用日志输出函数的函数名%(lineno)d 调用日志输出函数的语句所在的代码行%(created)f 当前时间，用UNIX标准的表示时间的浮 点数表示%(relativeCreated)d 输出日志信息时的，自Logger创建以 来的毫秒数%(asctime)s 字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒%(thread)d 线程ID。可能没有%(threadName)s 线程名。可能没有%(process)d 进程ID。可能没有%(message)s用户输出的消息#========使用import logginglogging.basicConfig(filename=&apos;access.log&apos;, format=&apos;%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s&apos;, datefmt=&apos;%Y-%m-%d %H:%M:%S %p&apos;, level=10)logging.debug(&apos;调试debug&apos;)logging.info(&apos;消息info&apos;)logging.warning(&apos;警告warn&apos;)logging.error(&apos;错误error&apos;)logging.critical(&apos;严重critical&apos;)#========结果access.log内容:2017-07-28 20:32:17 PM - root - DEBUG -test: 调试debug2017-07-28 20:32:17 PM - root - INFO -test: 消息info2017-07-28 20:32:17 PM - root - WARNING -test: 警告warn2017-07-28 20:32:17 PM - root - ERROR -test: 错误error2017-07-28 20:32:17 PM - root - CRITICAL -test: 严重criticalpart2: 可以为logging模块指定模块级的配置,即所有logger的配置 logging模块的Formatter，Handler，Logger，Filter对象1234567#logger：产生日志的对象#Filter：过滤日志的对象#Handler：接收日志然后控制打印到不同的地方，FileHandler用来打印到文件中，StreamHandler用来打印到终端#Formatter对象：可以定制不同的日志格式对象，然后绑定给不同的Handler对象使用，以此来控制不同的Handler的日志格式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&apos;&apos;&apos;critical=50error =40warning =30info = 20debug =10&apos;&apos;&apos;import logging#1、logger对象：负责产生日志，然后交给Filter过滤，然后交给不同的Handler输出logger=logging.getLogger(__file__)#2、Filter对象：不常用，略#3、Handler对象：接收logger传来的日志，然后控制输出h1=logging.FileHandler(&apos;t1.log&apos;) #打印到文件h2=logging.FileHandler(&apos;t2.log&apos;) #打印到文件h3=logging.StreamHandler() #打印到终端#4、Formatter对象：日志格式formmater1=logging.Formatter(&apos;%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s&apos;, datefmt=&apos;%Y-%m-%d %H:%M:%S %p&apos;,)formmater2=logging.Formatter(&apos;%(asctime)s : %(message)s&apos;, datefmt=&apos;%Y-%m-%d %H:%M:%S %p&apos;,)formmater3=logging.Formatter(&apos;%(name)s %(message)s&apos;,)#5、为Handler对象绑定格式h1.setFormatter(formmater1)h2.setFormatter(formmater2)h3.setFormatter(formmater3)#6、将Handler添加给logger并设置日志级别logger.addHandler(h1)logger.addHandler(h2)logger.addHandler(h3)logger.setLevel(10)#7、测试logger.debug(&apos;debug&apos;)logger.info(&apos;info&apos;)logger.warning(&apos;warning&apos;)logger.error(&apos;error&apos;)logger.critical(&apos;critical&apos;) Logger与Handler的级别logger是第一级过滤，然后才能到handler，我们可以给logger和handler同时设置level，但是需要注意的是 1234567891011121314151617181920212223Logger is also the first to filter the message based on a level — if you set the logger to INFO, and all handlers to DEBUG, you still won&apos;t receive DEBUG messages on handlers — they&apos;ll be rejected by the logger itself. If you set logger to DEBUG, but all handlers to INFO, you won&apos;t receive any DEBUG messages either — because while the logger says &quot;ok, process this&quot;, the handlers reject it (DEBUG &lt; INFO).#验证import loggingform=logging.Formatter(&apos;%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s&apos;, datefmt=&apos;%Y-%m-%d %H:%M:%S %p&apos;,)ch=logging.StreamHandler()ch.setFormatter(form)# ch.setLevel(10)ch.setLevel(20)l1=logging.getLogger(&apos;root&apos;)# l1.setLevel(20)l1.setLevel(10)l1.addHandler(ch)l1.debug(&apos;l1 debug&apos;)重要，重要，重要！！！ Logger的继承（了解）123456789101112131415161718192021222324252627282930import loggingformatter=logging.Formatter(&apos;%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s&apos;, datefmt=&apos;%Y-%m-%d %H:%M:%S %p&apos;,)ch=logging.StreamHandler()ch.setFormatter(formatter)logger1=logging.getLogger(&apos;root&apos;)logger2=logging.getLogger(&apos;root.child1&apos;)logger3=logging.getLogger(&apos;root.child1.child2&apos;)logger1.addHandler(ch)logger2.addHandler(ch)logger3.addHandler(ch)logger1.setLevel(10)logger2.setLevel(10)logger3.setLevel(10)logger1.debug(&apos;log1 debug&apos;)logger2.debug(&apos;log2 debug&apos;)logger3.debug(&apos;log3 debug&apos;)&apos;&apos;&apos;2017-07-28 22:22:05 PM - root - DEBUG -test: log1 debug2017-07-28 22:22:05 PM - root.child1 - DEBUG -test: log2 debug2017-07-28 22:22:05 PM - root.child1 - DEBUG -test: log2 debug2017-07-28 22:22:05 PM - root.child1.child2 - DEBUG -test: log3 debug2017-07-28 22:22:05 PM - root.child1.child2 - DEBUG -test: log3 debug2017-07-28 22:22:05 PM - root.child1.child2 - DEBUG -test: log3 debug&apos;&apos;&apos; 应用logging配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&quot;&quot;&quot;logging配置&quot;&quot;&quot;import osimport logging.config# 定义三种日志输出格式 开始standard_format = &apos;[%(asctime)s][%(threadName)s:%(thread)d][task_id:%(name)s][%(filename)s:%(lineno)d]&apos; \ &apos;[%(levelname)s][%(message)s]&apos; #其中name为getlogger指定的名字simple_format = &apos;[%(levelname)s][%(asctime)s][%(filename)s:%(lineno)d]%(message)s&apos;id_simple_format = &apos;[%(levelname)s][%(asctime)s] %(message)s&apos;# 定义日志输出格式 结束logfile_dir = os.path.dirname(os.path.abspath(__file__)) # log文件的目录logfile_name = &apos;all2.log&apos; # log文件名# 如果不存在定义的日志目录就创建一个if not os.path.isdir(logfile_dir): os.mkdir(logfile_dir)# log文件的全路径logfile_path = os.path.join(logfile_dir, logfile_name)# log配置字典LOGGING_DIC = &#123; &apos;version&apos;: 1, &apos;disable_existing_loggers&apos;: False, &apos;formatters&apos;: &#123; &apos;standard&apos;: &#123; &apos;format&apos;: standard_format &#125;, &apos;simple&apos;: &#123; &apos;format&apos;: simple_format &#125;, &#125;, &apos;filters&apos;: &#123;&#125;, &apos;handlers&apos;: &#123; #打印到终端的日志 &apos;console&apos;: &#123; &apos;level&apos;: &apos;DEBUG&apos;, &apos;class&apos;: &apos;logging.StreamHandler&apos;, # 打印到屏幕 &apos;formatter&apos;: &apos;simple&apos; &#125;, #打印到文件的日志,收集info及以上的日志 &apos;default&apos;: &#123; &apos;level&apos;: &apos;DEBUG&apos;, &apos;class&apos;: &apos;logging.handlers.RotatingFileHandler&apos;, # 保存到文件 &apos;formatter&apos;: &apos;standard&apos;, &apos;filename&apos;: logfile_path, # 日志文件 &apos;maxBytes&apos;: 1024*1024*5, # 日志大小 5M &apos;backupCount&apos;: 5, &apos;encoding&apos;: &apos;utf-8&apos;, # 日志文件的编码，再也不用担心中文log乱码了 &#125;, &#125;, &apos;loggers&apos;: &#123; #logging.getLogger(__name__)拿到的logger配置 &apos;&apos;: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos;], # 这里把上面定义的两个handler都加上，即log数据既写入文件又打印到屏幕 &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: True, # 向上（更高level的logger）传递 &#125;, &#125;,&#125;def load_my_logging_cfg(): logging.config.dictConfig(LOGGING_DIC) # 导入上面定义的logging配置 logger = logging.getLogger(__name__) # 生成一个log实例 logger.info(&apos;It works!&apos;) # 记录该文件的运行状态if __name__ == &apos;__main__&apos;: load_my_logging_cfg() 使用123456789101112131415161718192021222324&quot;&quot;&quot;MyLogging Test&quot;&quot;&quot;import timeimport loggingimport my_logging # 导入自定义的logging配置logger = logging.getLogger(__name__) # 生成logger实例def demo(): logger.debug(&quot;start range... time:&#123;&#125;&quot;.format(time.time())) logger.info(&quot;中文测试开始。。。&quot;) for i in range(10): logger.debug(&quot;i:&#123;&#125;&quot;.format(i)) time.sleep(0.2) else: logger.debug(&quot;over range... time:&#123;&#125;&quot;.format(time.time())) logger.info(&quot;中文测试结束。。。&quot;)if __name__ == &quot;__main__&quot;: my_logging.load_my_logging_cfg() # 在你程序文件的入口加载自定义logging配置 demo() 关于如何拿到logger对象的详细解释12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849注意注意注意：#1、有了上述方式我们的好处是：所有与logging模块有关的配置都写到字典中就可以了，更加清晰，方便管理#2、我们需要解决的问题是： 1、从字典加载配置：logging.config.dictConfig(settings.LOGGING_DIC) 2、拿到logger对象来产生日志 logger对象都是配置到字典的loggers 键对应的子字典中的 按照我们对logging模块的理解，要想获取某个东西都是通过名字，也就是key来获取的 于是我们要获取不同的logger对象就是 logger=logging.getLogger(&apos;loggers子字典的key名&apos;) 但问题是：如果我们想要不同logger名的logger对象都共用一段配置，那么肯定不能在loggers子字典中定义n个key &apos;loggers&apos;: &#123; &apos;l1&apos;: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos;], # &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: True, # 向上（更高level的logger）传递 &#125;, &apos;l2: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos; ], &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: False, # 向上（更高level的logger）传递 &#125;, &apos;l3&apos;: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos;], # &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: True, # 向上（更高level的logger）传递 &#125;,&#125; #我们的解决方式是，定义一个空的key &apos;loggers&apos;: &#123; &apos;&apos;: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos;], &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: True, &#125;,&#125;这样我们再取logger对象时logging.getLogger(__name__)，不同的文件__name__不同，这保证了打印日志时标识信息不同，但是拿着该名字去loggers里找key名时却发现找不到，于是默认使用key=&apos;&apos;的配置 另外一个django的配置，瞄一眼就可以，跟上面的一样123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#logging_config.pyLOGGING = &#123; &apos;version&apos;: 1, &apos;disable_existing_loggers&apos;: False, &apos;formatters&apos;: &#123; &apos;standard&apos;: &#123; &apos;format&apos;: &apos;[%(asctime)s][%(threadName)s:%(thread)d][task_id:%(name)s][%(filename)s:%(lineno)d]&apos; &apos;[%(levelname)s][%(message)s]&apos; &#125;, &apos;simple&apos;: &#123; &apos;format&apos;: &apos;[%(levelname)s][%(asctime)s][%(filename)s:%(lineno)d]%(message)s&apos; &#125;, &apos;collect&apos;: &#123; &apos;format&apos;: &apos;%(message)s&apos; &#125; &#125;, &apos;filters&apos;: &#123; &apos;require_debug_true&apos;: &#123; &apos;()&apos;: &apos;django.utils.log.RequireDebugTrue&apos;, &#125;, &#125;, &apos;handlers&apos;: &#123; #打印到终端的日志 &apos;console&apos;: &#123; &apos;level&apos;: &apos;DEBUG&apos;, &apos;filters&apos;: [&apos;require_debug_true&apos;], &apos;class&apos;: &apos;logging.StreamHandler&apos;, &apos;formatter&apos;: &apos;simple&apos; &#125;, #打印到文件的日志,收集info及以上的日志 &apos;default&apos;: &#123; &apos;level&apos;: &apos;INFO&apos;, &apos;class&apos;: &apos;logging.handlers.RotatingFileHandler&apos;, # 保存到文件，自动切 &apos;filename&apos;: os.path.join(BASE_LOG_DIR, &quot;xxx_info.log&quot;), # 日志文件 &apos;maxBytes&apos;: 1024 * 1024 * 5, # 日志大小 5M &apos;backupCount&apos;: 3, &apos;formatter&apos;: &apos;standard&apos;, &apos;encoding&apos;: &apos;utf-8&apos;, &#125;, #打印到文件的日志:收集错误及以上的日志 &apos;error&apos;: &#123; &apos;level&apos;: &apos;ERROR&apos;, &apos;class&apos;: &apos;logging.handlers.RotatingFileHandler&apos;, # 保存到文件，自动切 &apos;filename&apos;: os.path.join(BASE_LOG_DIR, &quot;xxx_err.log&quot;), # 日志文件 &apos;maxBytes&apos;: 1024 * 1024 * 5, # 日志大小 5M &apos;backupCount&apos;: 5, &apos;formatter&apos;: &apos;standard&apos;, &apos;encoding&apos;: &apos;utf-8&apos;, &#125;, #打印到文件的日志 &apos;collect&apos;: &#123; &apos;level&apos;: &apos;INFO&apos;, &apos;class&apos;: &apos;logging.handlers.RotatingFileHandler&apos;, # 保存到文件，自动切 &apos;filename&apos;: os.path.join(BASE_LOG_DIR, &quot;xxx_collect.log&quot;), &apos;maxBytes&apos;: 1024 * 1024 * 5, # 日志大小 5M &apos;backupCount&apos;: 5, &apos;formatter&apos;: &apos;collect&apos;, &apos;encoding&apos;: &quot;utf-8&quot; &#125; &#125;, &apos;loggers&apos;: &#123; #logging.getLogger(__name__)拿到的logger配置 &apos;&apos;: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos;, &apos;error&apos;], &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: True, &#125;, #logging.getLogger(&apos;collect&apos;)拿到的logger配置 &apos;collect&apos;: &#123; &apos;handlers&apos;: [&apos;console&apos;, &apos;collect&apos;], &apos;level&apos;: &apos;INFO&apos;, &#125; &#125;,&#125;# -----------# 用法:拿到俩个loggerlogger = logging.getLogger(__name__) #线上正常的日志collect_logger = logging.getLogger(&quot;collect&quot;) #领导说,需要为领导们单独定制领导们看的日志 文件自动截断12345678910111213141516171819202122import loggingfrom logging import handlerslogger = logging.getLogger(__name__)log_file = &quot;timelog.log&quot;#fh = handlers.RotatingFileHandler(filename=log_file,maxBytes=10,backupCount=3)fh = handlers.TimedRotatingFileHandler(filename=log_file,when=&quot;S&quot;,interval=5,backupCount=3)formatter = logging.Formatter(&apos;%(asctime)s %(module)s:%(lineno)d %(message)s&apos;)fh.setFormatter(formatter)logger.addHandler(fh)logger.warning(&quot;test1&quot;)logger.warning(&quot;test12&quot;)logger.warning(&quot;test13&quot;)logger.warning(&quot;test14&quot;) other12345678910111213141516171819202122232425262728293031import logging #create loggerlogger = logging.getLogger(&apos;TEST-LOG&apos;)logger.setLevel(logging.DEBUG) # create console handler and set level to debugch = logging.StreamHandler()ch.setLevel(logging.DEBUG) # create file handler and set level to warningfh = logging.FileHandler(&quot;access.log&quot;)fh.setLevel(logging.WARNING)# create formatterformatter = logging.Formatter(&apos;%(asctime)s - %(name)s - %(levelname)s - %(message)s&apos;) # add formatter to ch and fhch.setFormatter(formatter)fh.setFormatter(formatter) # add ch and fh to loggerlogger.addHandler(ch)logger.addHandler(fh) # &apos;application&apos; codelogger.debug(&apos;debug message&apos;)logger.info(&apos;info message&apos;)logger.warn(&apos;warn message&apos;)logger.error(&apos;error message&apos;)logger.critical(&apos;critical message&apos;)]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[gitlab-ci自动部署（二）]]></title>
    <url>%2Fposts%2Fa98a4c62.html</url>
    <content type="text"><![CDATA[下面来说说CI/CD是怎么实现的： 登录gitlab-runner机器切换runner普通用户 [root@localhost ~]# su - gitlab-runner 编写.gitlab-ci.yml文件随便创建一个目录，叫什么无所谓 [gitlab-runner@localhost ~]$ mkdir git 将gitlab上想要发布的项目克隆到这个目录里 [gitlab-runner@localhost git]$ git clone git@gitlab.xxx.com:xxx/xxx.git 因为我们的项目是大工程，里面带着很多子工程，所以就需要通过yml文件，将项目分离出去 比方说我们想发布bb项目，但是bb项目属于aa这个大项目的子项目，所以就进aa大工程目录下 在项目目录里面创建一个.gitlab-ci.yml文件，如下 [gitlab-runner@localhost ~]$ cat .gitlab-ci.yml12345678job 1: stage: test script: - git subtree push -q --prefix=bb git@gitlab.xx.com:bb1/bb.git dev only: - dev tags: - shell 这样就可以将bb项目分离出去，相当于创建了一个新的项目 进入bb目录，也编写.gitlab-ci.yml文件，这个就是我们需要编译的脚本，中间可以穿插maven、node和shell的一系列命令 [gitlab-runner@localhost bb]$ cat .gitlab-ci.yml12345678910job 1: stage: build script: - rm -rf /opt/M2_REPO/com/bb/* - mvn clean package -P test -Dmaven.test.skip - bash -x /opt/bb/shell/bb.sh only: - dev tags: - shell 编写完yml文件后都需要提交下 git add . git commit -m “add gitlab-ci.yml” git push origin dev 另外包括传包，启动等命令都可以在yml文件里面体现，这样只要dev分支有commit的改变了，gitlab-ci就会自动创建job来自动发布]]></content>
      <categories>
        <category>gitlab</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[gitlab-ci自动部署（一）]]></title>
    <url>%2Fposts%2F999ca0ba.html</url>
    <content type="text"><![CDATA[目前GitLab已经有了CI功能，即持续集成的功能。可以实现代码提交后自动测试、编译、发布、部署等自动化工作 下面是我总结的实现内容： 安装runner在root下执行 下载gitlab-runner wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-runner/yum/el7/gitlab-runner-10.5.0-1.x86_64.rpm 安装 rpm -ivh gitlab-runner-10.5.0-1.x86_64.rpm 配置Runner默认情况，Runner是通过gitlab-runner的这个用户来执行一系列操作，其工作目录也是在gitlab-runner的用户目录下面。如果使用默认gitlab-runner用户操作一些文件时经常会遇到权限问题，就需要给gitlab-runner赋权。我们通过以下方式修改。 #在root下执行 #删除服务 gitlab-runner uninstall #添加服务 gitlab-runner install –working-directory /home/builds –user gitlab-ci #重启服务 gitlab-runner restart #查看状态 gitlab-runner status 输出：gitlab-runner: Service is running! #查看是否生效 ps -ef | grep gitlab-runner 注册Runner先打开GitLab上需要自动部署的项目界面，找到该项目的Settings –&gt; CI/CD –&gt; Runners settings 在gitlab上可以看到自己的token信息，用来注册runner #在root下执行gitlab-runner register (会出现注册信息，填url，token，runner的名字) 至此安装部分就完成了]]></content>
      <categories>
        <category>gitlab</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos7搭建shadowsock实现vpn翻墙]]></title>
    <url>%2Fposts%2F253bb9c3.html</url>
    <content type="text"><![CDATA[安装使用root用户登录，运行以下命令：123wget --no-check-certificate -O shadowsocks.sh https://cyh.abcdocker.com/vpn/shadowsocks.sh chmod +x shadowsocks.sh ./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 安装完成后，脚本提示如下： 1234567 Congratulations, Shadowsocks-python server install completed!Your Server IP :your_server_ipYour Server Port :your_server_portYour Password :your_passwordYour Encryption Method:your_encryption_methodWelcome to visit:https://teddysun.com/342.htmlEnjoy it! 卸载方法使用root用户登录，运行以下命令： ./shadowsocks.sh uninstall 配置文件路径：/etc/shadowsocks.json 12345678910&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:your_server_port, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;your_password&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;your_encryption_method&quot;, &quot;fast_open&quot;: false&#125; 多用户多端口配置文件配置文件路径：/etc/shadowsocks.json 123456789101112131415 &#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;port_password&quot;:&#123; &quot;8989&quot;:&quot;password0&quot;, &quot;9001&quot;:&quot;password1&quot;, &quot;9002&quot;:&quot;password2&quot;, &quot;9003&quot;:&quot;password3&quot;, &quot;9004&quot;:&quot;password4&quot; &#125;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;your_encryption_method&quot;, &quot;fast_open&quot;: false&#125; 1234启动：/etc/init.d/shadowsocks start停止：/etc/init.d/shadowsocks stop重启：/etc/init.d/shadowsocks restart状态：/etc/init.d/shadowsocks status]]></content>
      <categories>
        <category>vpn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pptp]]></title>
    <url>%2Fposts%2F5dcda1e.html</url>
    <content type="text"><![CDATA[安装pptpd$ yum install pptpd 配置本地及远程IP地址$ vi /etc/pptpd.conf localip 192.168.100.1 remoteip 192.168.100.2-245 配置DNS$ vi /etc/ppp/options.pptpd ms-dns 223.5.5.5 ms-dns 223.6.6.6 修改MTU$ vi / etc / ppp / ip-up /sbin/ifconfig $ 1 mtu 1500 用户和密码配置$ vi /etc/ppp/chap-secrets client server secret IP address 注释掉 testin_user1 pptpd testin_user1 * 配置防火墙$ vi /etc/sysctl.conf net.ipv4.ip_forward = 1＃开启IP转发 $ iptables -t nat -A POSTROUTING -s 192.168.100.0/24 -o eth1 -j MASQUERADE＃允许外网连接 开启服务$ service pptpd start $ service iptables start]]></content>
      <categories>
        <category>vpn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux下使用rsync最快删除海量文件的方法]]></title>
    <url>%2Fposts%2F39f911ed.html</url>
    <content type="text"><![CDATA[摘要在linux的web服务器维护过程当中，有些程序会使用本地file缓存或生成大量程序日志。当发布进行版本迭代时，由于上个版本的程序会保留一段时间，因此这些小文件会消耗大量的inode。这个时候，我们常用的删除命令rm -fr * 就不好用了，因为要等待的时间太长。所以必须要采取一些其他手段来删除这些大量的小文件。这个时候，我们可以使用rsync来实现快速删除大量文件。 安装rsync安装很简单，这里我们直接使用yum安装即可 1yum install -y rsync 创建一个空的文件夹1mkdir /tmp/null 用rsync删除目标目录1rsync --delete-before -a -H -v --progress --stats /tmp/null/ /data/web/app/xxx/cache/ 这样我们要删除的cache目录就会被清空了，删除的速度会非常快。 rsync实际上用的是替换原理，处理数十万个文件也是秒删。 参数123456–delete-before 接收者在传输之前进行删除操作–progress 在传输时显示传输过程-a 归档模式，表示以递归方式传输文件，并保持所有文件属性-H 保持硬连接的文件-v 详细输出模式–stats 给出某些文件的传输状态]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redmine]]></title>
    <url>%2Fposts%2F9afc5d97.html</url>
    <content type="text"><![CDATA[版本： redmine 3.1.1官方文档地址： http://www.redmine.org.cn/category/install Redmine实战下列描述均以操作系统 Centos7 为例部署。 环境介绍OS IP HostName RoleCentOS7 x64 192.168.10.10 node1 Redmine恢复机器CentOS7 x64 10.10.1.17 localhost Redmine机器 准备工作关闭Iptables和SELinux[root@node1 ~]# systemctl stop firewalld[root@node1 ~]# systemctl disable firewalld[root@node1 ~]# setenforce 0[root@node1 ~]# sed -i ‘/^SELINUX=/{ s/enforcing/disabled/ }’ /etc/selinux/config 调整服务器时间[root@node1 ~]# yum -y install ntp[root@node1 ~]# ntpdate -u 202.120.2.101 安装配置 Redmine安装依赖环境[root@node1 ~]# yum install -y zlib-devel openssl-devel ImageMagick-devel wget curl-devel rubygems mod_fcgid 安装RVM[root@node1 ~]# gpg –keyserver hkp://keys.gnupg.net –recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3[root@node1 ~]# curl -L https://get.rvm.io | bash -s stable 载入RVM环境并获取需要的支持安装包[root@node1 ~]# source /etc/profile.d/rvm.sh[root@node1 ~]# rvm requirements 利用rvm安装 Ruby 2.2.3 并设为默认[root@node1 ~]# sed -i -E ‘s!https?://cache.ruby-lang.org/pub/ruby!https://ruby.taobao.org/mirrors/ruby!&#39; /usr/local/rvm/config/db[root@node1 ~]# rvm gemset create[root@node1 ~]# rvm install 2.2.3[root@node1 ~]# rvm use 2.2.3 –default 添加淘宝镜像[root@node1 ~]# gem sources –add https://gems.ruby-china.org/ –remove https://rubygems.org/[root@node1 ~]# gem sources -l CURRENT SOURCES https://gems.ruby-china.org 安装rails[root@node1 ~]# gem install rails -v=4.2 安装mysql和httpd[root@node1 ~]# yum install httpd httpd-devel -y [root@node1 ~]# wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm[root@node1 ~]# rpm -ivh mysql-community-release-el7-5.noarch.rpm[root@node1 ~]# yum -y install mysql-community-server mysql-devel[root@node1 ~]# service mysqld restart[root@node1 ~]# mysql -u rootmysql&gt; create database redmine character set utf8;mysql&gt; create user ‘redmine‘@’localhost’ identified by ‘redmine’;mysql&gt; grant all privileges on redmine.* to ‘redmine‘@’localhost’; 安装redmine的apache支持，这样可以通过apache访问[root@node1 ~]# gem install passenger[root@node1 ~]# passenger-install-apache2-module [root@node1 ~]# vim /etc/httpd/conf.d/passenger.confLoadModule passenger_module /usr/local/rvm/gems/ruby-2.2.3/gems/passenger-5.0.30/buildout/apache2/mod_passenger.so PassengerRoot /usr/local/rvm/gems/ruby-2.2.3/gems/passenger-5.0.30 PassengerDefaultRuby /usr/local/rvm/gems/ruby-2.2.3/wrappers/ruby [root@node1 ~]# vim /etc/httpd/conf.d/redmine.conf ServerName www.a.com # !!! Be sure to point DocumentRoot to ‘public’! DocumentRoot /var/www/html/redmine/public ErrorLog logs/redmine_error_log &lt;Directory /var/www/html/redmine/public&gt; Options Indexes ExecCGI FollowSymLinks Order allow,deny Allow from all # This relaxes Apache security settings. AllowOverride all # MultiViews must be turned off. Options -MultiViews # Uncomment this if you’re on Apache &gt;= 2.4: #Require all granted 安装redmine[root@node1 ~]# cd /var/www/html[root@node1 ~]# wget http://www.redmine.org/releases/redmine-3.1.1.tar.gz[root@node1 ~]# tar -zxvf redmine-3.1.1.tar.gz[root@node1 ~]# mv redmine-3.1.1 redmine[root@node1 ~]# cd /var/www/html/redmine/ [root@node1 ~]# vim Gemfile # 修改sourcesource ‘https://rubygems.org&#39; （注释掉）source’https://ruby.taobao.org&#39; [root@node1 ~]# cp config/configuration.yml.example config/configuration.yml[root@node1 ~]# cp config/database.yml.example config/database.yml[root@node1 ~]# vim config/database.yml # 修改数据连接production: adapter: mysql2 database: redmine host: localhost username: redmine password: “redmine” encoding: utf8 [root@node1 redmine]# gem install bundler # 注意是在网站根目录下执行[root@node1 redmine]# gem install rack-cache -v ‘1.4.2’[root@node1 redmine]# bundle install 为Rails生成cookies密钥[root@node1 redmine]# rake generate_secret_token 初始化redmine数据库表名[root@node1 redmine]# RAILS_ENV=production rake db:migrate[root@node1 redmine]# RAILS_ENV=production rake redmine:load_default_data 启动[root@node1 ~]# cd /var/www/html/redmine[root@node1 redmine]# mkdir /var/www/html/logs[root@node1 redmine]# bundle exec rails server webrick -e production -b 0.0.0.0 &amp;&gt;&gt; /var/www/html/logs/redmine.log &amp;访问地址：http://IP:3000 备份[root@node1 ~]# mysqldump -u root redmine &gt; /root/redmine.sql 恢复停止redmine， 步骤：ps -ef | grep rails，找到redmine的进程号，然后kill掉 恢复数据库[root@node1 ~]# mysql -u redmine -p redmine &lt; /root/redmine.sql 把10.10.2.120上的/backup/redmine-back/redmine_file目录下的所有文件拷贝到192.168.100.10中的/var/www/html/redmine/files目录[root@node1 ~]# scp -r root@10.10.2.120:/backup/redmine-back/redmine_file/* /var/www/html/redmine/files[root@node1 ~]# chmod -R 755 /var/www/html/redmine/files 启动redmine[root@node1 redmine]# bundle exec rails server webrick -e production -b 0.0.0.0 &amp;&gt;&gt; /var/www/html/logs/redmine.log &amp; 调整配置配置邮件发送，空格缩进必须如下，不然redmine无法启动[root@node1 ~]# vim /var/www/html/redmine/config/configuration.ymldefault: email_delivery: delivery_method: :smtp smtp_settings: openssl_verify_mode: ‘none’ address: mail.testin.cn port: 587 domain: testin.cn authentication: :login user_name: “project@testin.cn“ password: “m12345678” 重启redmine即可]]></content>
      <categories>
        <category>版本管理工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos6.4安装kvm]]></title>
    <url>%2Fposts%2F2074d833.html</url>
    <content type="text"><![CDATA[首先检查您的CPU是否支持硬件虚拟化 egrep ‘(vmx|svm)’ –color=always /proc/cpuinfo 应该显示一些东西，例如： [root@server1 ~]# egrep ‘(vmx|svm)’ –color=always /proc/cpuinfoflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt rdtscp lm 3dnowext 3dnow pni cx16 lahf_lm cmp_legacy svm extapic cr8_legacy misalignsseflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt rdtscp lm 3dnowext 3dnow pni cx16 lahf_lm cmp_legacy svm extapic cr8_legacy misalignsse[root@server1 ~]# 现在我们导入软件包的GPG密钥： rpm–import /etc/pki/rpm-gpg/RPM-GPG-KEY* yum install kvm libvirt python-virtinst qemu-kvm /etc/init.d/libvirtd start virsh -c qemu:///system list 导入镜像文件 xterm-253-1.el6.x86_64.rpm 之后xmanager开始kvm就可以了]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
  </entry>
</search>
