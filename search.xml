<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Prometheus监控]]></title>
    <url>%2F2019%2F01%2F29%2Fk8s%2Fprometheus%2F</url>
    <content type="text"><![CDATA[在Kubernetes上快速部署Prometheus创建一个新的命名空间12345678[root@prometheus]# cat monitor_namespace.yaml apiVersion: v1kind: Namespacemetadata: name: monitor labels: name: monitor[root@prometheus]#kubectl create -f monitor_namespace.yaml rbac文件12345678910111213141516171819202122232425262728293031323334353637383940414243[root@prometheus]# cat rbac-setup.yaml apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: [&quot;&quot;] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- apiGroups: - extensions resources: - ingresses verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- nonResourceURLs: [&quot;/metrics&quot;] verbs: [&quot;get&quot;]---apiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: monitor---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: monitor [root@prometheus]#kubectl create -f rbac-setup.yaml prometheus-deploy文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373[root@prometheus]# cat configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: monitordata: #被引用到/etc/prometheus/prometheus.yml prometheus.yml: | global: #每15s采集一次数据和15s做一次告警检测 scrape_interval: 15s evaluation_interval: 15s #指定加载的告警规则文件 rule_files: - /etc/prometheus/rules.yml #将报警送至何地进行报警 alerting: alertmanagers: - static_configs: - targets: [&quot;192.168.50.60:9093&quot;] #指定prometheus要监控的目标 scrape_configs: - job_name: &apos;k8s-node&apos; scrape_interval: 10s static_configs: - targets: - &apos;192.168.50.61:31672&apos; #自定义获取监控数据,每个 job_name 都是独立的 - job_name: &apos;tomcat-pods&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_service_annotation_prometheus_io_jvm_scrape] regex: true;true action: keep - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_app_metrics_patn] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__meta_kubernetes_pod_ip, __meta_kubernetes_service_annotation_prometheus_io_app_metrics_port] action: replace target_label: __address__ regex: (.+);(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_pod_host_ip] action: replace target_label: kubernetes_host_ip - job_name: &apos;kubernetes-apiservers&apos; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: &apos;kubernetes-nodes&apos; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics - job_name: &apos;kubernetes-cadvisor&apos; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: &apos;kubernetes-service-endpoints&apos; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: &apos;kubernetes-services&apos; kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: &apos;kubernetes-ingresses&apos; kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path] regex: (.+);(.+);(.+) replacement: $&#123;1&#125;://$&#123;2&#125;$&#123;3&#125; target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: kubernetes_name - job_name: &apos;kubernetes-pods&apos; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # 监控规则文件,被引用到/etc/prometheus/rules.yml rules.yml: | groups: - name: test-rule rules: ############# Node监控 ############# - alert: k8s-node状态异常 expr: up&#123;job=&quot;k8s-node&quot;&#125; != 1 for: 3m labels: team: k8s-node annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点状态异常&quot; description: &quot;可能是重启了&quot; - alert: k8s-node节点CPU使用率 expr: (1 - avg(irate(node_cpu_seconds_total&#123;job=&quot;k8s-node&quot;,mode=&quot;idle&quot;&#125;[1m])) by (instance)) * 100 &gt; 95 for: 1m labels: team: k8s-node annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点CPU使用率超过95%&quot; description: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点当前CPU使用率为: &#123;&#123; $value &#125;&#125;&quot; - alert: k8s-node节点磁盘使用率 expr: (node_filesystem_size_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125; - node_filesystem_avail_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125;) / node_filesystem_size_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125; * 100 &gt; 85 for: 1m labels: team: k8s-node annotations: description: &quot;Node服务器[[ &#123;&#123;$labels.instance&#125;&#125; ]] 的 &#123;&#123;mountpoint&#125;&#125; 磁盘空间使用率超过85%&quot; summary: &quot;磁盘 &#123;&#123;$labels.device&#125;&#125; 当前使用率为: &#123;&#123; $value &#125;&#125;&quot; - alert: k8s-node节点内存使用率 expr: (node_memory_MemTotal_bytes&#123;job=&quot;k8s-node&quot;&#125; - (node_memory_Buffers_bytes&#123;job=&quot;k8s-node&quot;&#125; + node_memory_Cached_bytes&#123;job=&quot;k8s-node&quot;&#125; + node_memory_MemFree_bytes&#123;job=&quot;k8s-node&quot;&#125;)) / node_memory_MemTotal_bytes&#123;job=&quot;k8s-node&quot;&#125; * 100 for: 1m labels: team: k8s-node annotations: description: &quot;Node服务器[[ &#123;&#123;$labels.instance&#125;&#125; ]] 内存使用率超过95%&quot; summary: &quot;&#123;&#123;$labels.instance&#125;&#125; 当前内存使用率为: &#123;&#123; $value &#125;&#125;&quot; ############ Pod 监控 ############ - alert: 监控k8s的pod状态异常 expr: up&#123;kubernetes_namespace=&quot;monitor&quot;&#125; != 1 for: 3m labels: team: &quot;kube-state-metrics&quot; annotations: description: &quot;&#123;&#123;$labels.kubernetes_namespace&#125;&#125; 内的 pod 状态有变动&quot; summary: &quot;此 Pod 用于获取 k8s 监控数据, 绑定在一个节点上&quot; - alert: 应用的 pod 状态有变动 expr: kube_pod_container_status_ready&#123;namespace=&quot;product&quot;&#125; != 1 for: 3m labels: status: &quot;product 命名空间内的 pod &#123;&#123;$labels.pod&#125;&#125;有变动&quot; annotations: description: &quot;Deployment &#123;&#123;$labels.container&#125;&#125; 内的 pod 状态有变动&quot; summary: &quot;可能是重启或者在升级版本,如果频繁重启,请跟踪排查问题&quot; - alert: 以下应用的 pod 重启次数已经超过15,请查看原因 expr: kube_pod_container_status_restarts_total&#123;namespace=&quot;product&quot;&#125; &gt; 15 for: 3m labels: status: &quot;product 命名空间内的 pod &#123;&#123;$labels.pod&#125;&#125; 重启次数太多&quot; annotations: description: &quot;Deployment &#123;&#123;$labels.container&#125;&#125; 内的 pod 重启次数太多&quot; summary: &quot;重启次数太多,可能是因为 pod 内应用有问题&quot; ########### Java 监控 ############ - alert: jvm线程数过高 expr: jvm_threads_current&#123;job=&quot;tomcat-pods&quot;&#125;&gt;2000 for: 1m labels: status: &quot;空间内 jvm 的变动情况&quot; annotations: description: &quot;&#123;&#123;$labels.kubernetes_pod_name&#125;&#125;: Jvm线程数过高&quot; summary: &apos;&#123;&#123; $labels.kubernetes_pod_name &#125;&#125; : 当前你线程值为: &#123;&#123; $value &#125;&#125;&apos; [root@prometheus]# cat prometheus.deploy.yml ---apiVersion: apps/v1beta2kind: Deploymentmetadata: labels: name: prometheus-deployment name: prometheus namespace: monitorspec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: containers: - image: prom/prometheus:v2.6.0 name: prometheus command: - &quot;/bin/prometheus&quot; args: - &quot;--config.file=/etc/prometheus/prometheus.yml&quot; - &quot;--storage.tsdb.path=/home/prometheus&quot; - &quot;--storage.tsdb.retention=168h&quot; - &quot;--web.enable-lifecycle&quot; ports: - containerPort: 9090 protocol: TCP volumeMounts: - mountPath: &quot;/home/prometheus&quot; name: data - mountPath: &quot;/etc/prometheus&quot; name: config-volume - mountPath: &quot;/etc/localtime&quot; readOnly: false name: localtime resources: requests: cpu: 100m memory: 2048Mi limits: cpu: 500m memory: 3180Mi serviceAccountName: prometheus nodeSelector: nodetype: prometheus volumes: - name: data hostPath: path: &quot;/opt/prometheus/data&quot; - name: config-volume configMap: name: prometheus-config - name: localtime hostPath: path: &quot;/etc/localtime&quot; type: File [root@prometheus]# cat prometheus.svc.yml ---kind: ServiceapiVersion: v1metadata: labels: app: prometheus name: prometheus namespace: monitorspec: type: NodePort ports: - port: 9090 targetPort: 9090 nodePort: 30003 selector: app: prometheus [root@prometheus]#kubectl create -f configmap.yaml[root@prometheus]#kubectl create -f prometheus.deploy.yml[root@prometheus]#kubectl create -f prometheus.svc.yml注：需要在本地创建/opt/prometheus/data作为prometheus数据路径，另需要给data目录赋予777权限 热重读配置文件congfigmap有热重启功能，这样每次改完配置文件都不需要重启prometheus的pod来重读配置了 123456789- &quot;--web.enable-lifecycle&quot;在prometheus.deploy.yml的配置文件里面加上这段话就可以了[root@prometheus]# cat reload-prometheus.sh #!/bin/bashkubectl apply -f configmap.yamlsleep 60curl -XPOST http://192.168.50.60:30003/-/reload可以写个脚本，每次修改完配置文件的配置之后，执行一下脚本就可以同步生效了！ 安装kube-state-metrics123[root@prometheus]# git clone https://github.com/kubernetes/kube-state-metrics.git之后把默认的命名空间改成monitor，进入kube-state-metrics目录[root@prometheus]#kubectl create -f ./ 安装grafana12345678910111213141516创建grafana的数据目录mkdir /opt/grafana/data启动脚本[root@grafana]# cat start_grafana.sh #!/bin/bashdocker stop `docker ps -a |awk &apos;/grafana/&#123;print $1&#125;&apos;`docker rm `docker ps -a |awk &apos;/grafana/&#123;print $1&#125;&apos;`docker run -d \ --name=grafana \ --restart=always \ -p 3000:3000 \ -m 4096m \ -v /opt/grafana/data:/var/lib/grafana \ -v /opt/grafana/log:/var/log/grafana \ grafana/grafana:5.4.3 1、安装完之后，需要添加source，source直接点prometheus，链接就是http://192.168.50.60:30003之前创建的prometheus界面 2、添加模板dashboad（列出几个常用的） 点import导入，有俩种方式，直接填官网模板，或者导入json https://grafana.com/dashboards/9276 node的cpu、内存等 https://grafana.com/dashboards/3146 pod https://grafana.com/dashboards/8588 deployment 安装alertmanager创建配置文件、目录12345678910111213141516171819202122232425262728293031创建alert数据目录mkdir /opt/alert/data注意：需要alertmanager.yml配置，此配置钉钉和邮件可同时放松[root@docker60 alert]# cat alertmanager.yml global: resolve_timeout: 5mroute: group_by: [&apos;alertname&apos;] group_wait: 10s group_interval: 10s repeat_interval: 6m receiver: defaultreceivers:- name: &apos;default&apos; email_configs: - to: &quot;&quot; from: &quot;&quot; smarthost: &quot;smtp.xxx.com:25&quot; auth_username: &quot;&quot; auth_password: &quot;&quot; webhook_configs: - url: &apos;http://192.168.50.60:8060/dingtalk/ops_dingding/send&apos; send_resolved: trueinhibit_rules: - source_match: severity: &apos;critical&apos; target_match: severity: &apos;warning&apos; equal: [&apos;alertname&apos;] 启动脚本1234567891011121314[root@alert]# cat start_alert.sh#!/bin/bashdocker stop `docker ps -a |awk &apos;/alertmanager/&#123;print $1&#125;&apos;`docker rm `docker ps -a |awk &apos;/alertmanager/&#123;print $1&#125;&apos;`docker run -d \ --name alertmanager \ --restart=always \ -p 9093:9093 \ -v /etc/localtime:/etc/localtime:ro \ -v /opt/alert/alertmanager.yml:/etc/alertmanager/alertmanager.yml \ -v /opt/alert/data:/alertmanager \ prom/alertmanager:v0.15.3 安装dingding插件1234567891011121、安装go （这里就不叙述了）2、假设go的路径是/usr/local/gomkdir -pv /usr/local/go/src/github.com/timonwong3、下载dingding插件git clone https://github.com/timonwong/prometheus-webhook-dingtalk.git4、添加dingding机器人在dingding群里面添加即可5、启动dingding[root@alert]# cat start_dingding.sh cd /usr/local/go/src/github.com/timonwong/prometheus-webhook-dingtalkkill -9 `ps -ef | grep prometheus-webhook-dingtalk | grep -v grep | awk &apos;&#123;print $2&#125;&apos;`nohup ./prometheus-webhook-dingtalk --ding.profile=&quot;ops_dingding=https://oapi.dingtalk.com/robot/send?access_token=xxxx&quot; 2&gt;&amp;1 1&gt;dingding.log &amp;]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jira-安装及破解]]></title>
    <url>%2F2018%2F11%2F21%2F%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2Fjira-%E5%AE%89%E8%A3%85%E5%8F%8A%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[安装jira 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061wget https://product-downloads.atlassian.com/software/jira/downloads/atlassian-jira-software-7.13.0-x64.bin[root@YZSJHL82-204 ~]# chmod +x atlassian-jira-software-7.13.0-x64.bin[root@YZSJHL82-204 ~]# ./atlassian-jira-software-7.13.0-x64.binUnpacking JRE ...Starting Installer ...十月 23, 2018 4:38:25 下午 java.util.prefs.FileSystemPreferences$1 run信息: Created user preferences directory.十月 23, 2018 4:38:25 下午 java.util.prefs.FileSystemPreferences$2 run信息: Created system preferences directory in java.home.This will install JIRA Software 7.4.1 on your computer.OK [o, Enter], Cancel [c]o #按o安装Choose the appropriate installation or upgrade option.Please choose one of the following:Express Install (use default settings) [1], Custom Install (recommended for advanced users) [2, Enter], Upgrade an existing JIRA installation [3]2 #2为自定义安装Where should JIRA Software be installed?[/opt/atlassian/jira]/usr/local/atlassina/jira #自定义安装目录Default location for JIRA Software data[/var/atlassian/application-data/jira]/usr/local/atlassina/jira_data #自定义数据目录Configure which ports JIRA Software will use.JIRA requires two TCP ports that are not being used by any otherapplications on this machine. The HTTP port is where you will access JIRAthrough your browser. The Control port is used to startup and shutdown JIRA.Use default ports (HTTP: 8080, Control: 8005) - Recommended [1, Enter], Set custom value for HTTP and Control ports [2]2 #2为自定义端口HTTP Port Number[8080] #8080为默认端口8050 #http连接端口Control Port Number[8005]8040 #控制端口JIRA can be run in the background.You may choose to run JIRA as a service, which means it will startautomatically whenever the computer restarts.Install JIRA as Service?Yes [y, Enter], No [n]y #是否开机自启Details on where JIRA Software will be installed and the settings that will be used.Installation Directory: /usr/local/atlassina/jira Home Directory: /usr/local/atlassina/jira_data HTTP Port: 8050 RMI Port: 8040 Install as service: Yes Install [i, Enter], Exit [e]i #确认已选配置Extracting files ...Please wait a few moments while JIRA Software is configured.Installation of JIRA Software 7.4.1 is completeStart JIRA Software 7.4.1 now?Yes [y, Enter], No [n]y #启动Please wait a few moments while JIRA Software starts up.Launching JIRA Software ...Installation of JIRA Software 7.4.1 is completeYour installation of JIRA Software 7.4.1 is now ready and can be accessedvia your browser.JIRA Software 7.4.1 can be accessed at http://localhost:8050Finishing installation ... 浏览器访问jira，地址为：http://IP:8050 请自行修改IP和端口。如果可以访问，说明安装成功。 配置数据库及密码在mySQL上创建用户及库做授权123create database jira_new;grant all privileges on *.* to jira@&apos;10.4.82.204&apos; identified by &apos;jira&apos;;flush privileges; 在授权完用户我们不可以马上填写信息，需要添加MySQL的一个jra包，否则下一步会提示找不到mysql的驱动 wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.zip1234567停止jira[root@YZSJHL82-204 ~]# /etc/init.d/jira stop上传软件包[root@YZSJHL82-204 ~]# cp mysql-connector-java-5.1.46-bin.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/启动jira[root@YZSJHL82-204 ~]# /etc/init.d/jira start注意防火墙 安装完数据库插件即可下一步: 设置jira主题 因为第一次安装，我们需要去jira官网注册用户，获取授权码 (免费30天，安装后更换破解即可) 保存好服务器ID，进入atlassian官网获取试用许可证，下边附上注册地址： 注册官网：https://my.atlassian.com 或使用以下地址： https://id.atlassian.com/signup?application=mac&amp;continue=https://my.atlassian.com 登陆账号后，选择New Evaluation License 设置管理员用户:官网注册的账号只可以免费试用30天，所以当我们安装完需要尽快进行破解 破解jirahttps://download.csdn.net/download/lbwahoo/100308071234567停止jira[root@YZSJHL82-204 ~]# /etc/init.d/jira stop进入安装目录下的atlassian-jira/WEB-INF/lib/目录下，用破解包atlassian-extras-3.2.jar替换原来的包。并将mysql连接驱动复制到此目录下。[root@YZSJHL82-204 ~]# cp atlassian-extras-3.2.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/启动jira[root@YZSJHL82-204 ~]# /etc/init.d/jira start注意防火墙 配置数据库连接地址12/var/atlassian/application-data/jira/dbconfig.xml#此路径为默认路径]]></content>
      <categories>
        <category>版本管理工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s基本命令]]></title>
    <url>%2F2018%2F05%2F10%2Fk8s%2Fk8s%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[pods12345$ kubectl get pods -n pro$ kubectl get pods --all-namespaces -n pro$ kubectl get pod aa -o wide -n pro$ kubectl get pod aa -o yaml -n pro$ kubectl describe pod aa -n pro POD升级和历史列出部署历史记录1$ kubectl rollout history deployment/DEPLOYMENT_NAME 跳转到特定修订版1$ kubectl rollout undo deployment/DEPLOYMENT_NAME --to-revision=N service查看服务1$ kubectl get services 将POD作为服务公开（创建端点）1$ kubectl expose deployment/aa --port=2000 --type=NodePort login$ kubectl exec -ti $1 bash -n product log$ kubectl logs -f $1 -n product]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[gitlab-ci自动部署（二）]]></title>
    <url>%2F2017%2F05%2F21%2Fgitlab%2Fgitlab-ci(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[下面来说说CI/CD是怎么实现的： 登录gitlab-runner机器切换runner普通用户 [root@localhost ~]# su - gitlab-runner 编写.gitlab-ci.yml文件随便创建一个目录，叫什么无所谓 [gitlab-runner@localhost ~]$ mkdir git 将gitlab上想要发布的项目克隆到这个目录里 [gitlab-runner@localhost git]$ git clone git@gitlab.xxx.com:xxx/xxx.git 因为我们的项目是大工程，里面带着很多子工程，所以就需要通过yml文件，将项目分离出去 比方说我们想发布bb项目，但是bb项目属于aa这个大项目的子项目，所以就进aa大工程目录下 在项目目录里面创建一个.gitlab-ci.yml文件，如下 [gitlab-runner@localhost ~]$ cat .gitlab-ci.yml12345678job 1: stage: test script: - git subtree push -q --prefix=bb git@gitlab.xx.com:bb1/bb.git dev only: - dev tags: - shell 这样就可以将bb项目分离出去，相当于创建了一个新的项目 进入bb目录，也编写.gitlab-ci.yml文件，这个就是我们需要编译的脚本，中间可以穿插maven、node和shell的一系列命令 [gitlab-runner@localhost bb]$ cat .gitlab-ci.yml12345678910job 1: stage: build script: - rm -rf /opt/M2_REPO/com/bb/* - mvn clean package -P test -Dmaven.test.skip - bash -x /opt/bb/shell/bb.sh only: - dev tags: - shell 编写完yml文件后都需要提交下 git add . git commit -m “add gitlab-ci.yml” git push origin dev 另外包括传包，启动等命令都可以在yml文件里面体现，这样只要dev分支有commit的改变了，gitlab-ci就会自动创建job来自动发布]]></content>
      <categories>
        <category>gitlab</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[gitlab-ci自动部署（一）]]></title>
    <url>%2F2017%2F05%2F20%2Fgitlab%2Fgitlab-ci(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[目前GitLab已经有了CI功能，即持续集成的功能。可以实现代码提交后自动测试、编译、发布、部署等自动化工作 下面是我总结的实现内容： 安装runner在root下执行 下载gitlab-runner wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-runner/yum/el7/gitlab-runner-10.5.0-1.x86_64.rpm 安装 rpm -ivh gitlab-runner-10.5.0-1.x86_64.rpm 配置Runner默认情况，Runner是通过gitlab-runner的这个用户来执行一系列操作，其工作目录也是在gitlab-runner的用户目录下面。如果使用默认gitlab-runner用户操作一些文件时经常会遇到权限问题，就需要给gitlab-runner赋权。我们通过以下方式修改。 #在root下执行 #删除服务 gitlab-runner uninstall #添加服务 gitlab-runner install –working-directory /home/builds –user gitlab-ci #重启服务 gitlab-runner restart #查看状态 gitlab-runner status 输出：gitlab-runner: Service is running! #查看是否生效 ps -ef | grep gitlab-runner 注册Runner先打开GitLab上需要自动部署的项目界面，找到该项目的Settings –&gt; CI/CD –&gt; Runners settings 在gitlab上可以看到自己的token信息，用来注册runner #在root下执行gitlab-runner register (会出现注册信息，填url，token，runner的名字) 成功之后会如下图： 至此安装部分就完成了]]></content>
      <categories>
        <category>gitlab</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos7搭建shadowsock实现vpn翻墙]]></title>
    <url>%2F2017%2F04%2F22%2Fvpn%2Fcentos7%E6%90%AD%E5%BB%BAshadowsock%E5%AE%9E%E7%8E%B0vpn%E7%BF%BB%E5%A2%99%2F</url>
    <content type="text"><![CDATA[安装使用root用户登录，运行以下命令：123wget --no-check-certificate -O shadowsocks.sh https://cyh.abcdocker.com/vpn/shadowsocks.sh chmod +x shadowsocks.sh ./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 安装完成后，脚本提示如下： 1234567 Congratulations, Shadowsocks-python server install completed!Your Server IP :your_server_ipYour Server Port :your_server_portYour Password :your_passwordYour Encryption Method:your_encryption_methodWelcome to visit:https://teddysun.com/342.htmlEnjoy it! 卸载方法使用root用户登录，运行以下命令： ./shadowsocks.sh uninstall 配置文件路径：/etc/shadowsocks.json 12345678910&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:your_server_port, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;your_password&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;your_encryption_method&quot;, &quot;fast_open&quot;: false&#125; 多用户多端口配置文件配置文件路径：/etc/shadowsocks.json 123456789101112131415 &#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;port_password&quot;:&#123; &quot;8989&quot;:&quot;password0&quot;, &quot;9001&quot;:&quot;password1&quot;, &quot;9002&quot;:&quot;password2&quot;, &quot;9003&quot;:&quot;password3&quot;, &quot;9004&quot;:&quot;password4&quot; &#125;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;your_encryption_method&quot;, &quot;fast_open&quot;: false&#125; 1234启动：/etc/init.d/shadowsocks start停止：/etc/init.d/shadowsocks stop重启：/etc/init.d/shadowsocks restart状态：/etc/init.d/shadowsocks status]]></content>
      <categories>
        <category>vpn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pptp]]></title>
    <url>%2F2017%2F04%2F20%2Fvpn%2Fpptp%2F</url>
    <content type="text"><![CDATA[安装pptpd$ yum install pptpd 配置本地及远程IP地址$ vi /etc/pptpd.conf localip 192.168.100.1 remoteip 192.168.100.2-245 配置DNS$ vi /etc/ppp/options.pptpd ms-dns 223.5.5.5 ms-dns 223.6.6.6 修改MTU$ vi / etc / ppp / ip-up /sbin/ifconfig $ 1 mtu 1500 用户和密码配置$ vi /etc/ppp/chap-secrets client server secret IP address 注释掉 testin_user1 pptpd testin_user1 * 配置防火墙$ vi /etc/sysctl.conf net.ipv4.ip_forward = 1＃开启IP转发 $ iptables -t nat -A POSTROUTING -s 192.168.100.0/24 -o eth1 -j MASQUERADE＃允许外网连接 开启服务$ service pptpd start $ service iptables start]]></content>
      <categories>
        <category>vpn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redmine]]></title>
    <url>%2F2017%2F03%2F21%2F%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2Fredmine%2F</url>
    <content type="text"><![CDATA[版本： redmine 3.1.1官方文档地址： http://www.redmine.org.cn/category/install Redmine实战下列描述均以操作系统 Centos7 为例部署。 环境介绍OS IP HostName RoleCentOS7 x64 192.168.10.10 node1 Redmine恢复机器CentOS7 x64 10.10.1.17 localhost Redmine机器 准备工作关闭Iptables和SELinux[root@node1 ~]# systemctl stop firewalld[root@node1 ~]# systemctl disable firewalld[root@node1 ~]# setenforce 0[root@node1 ~]# sed -i ‘/^SELINUX=/{ s/enforcing/disabled/ }’ /etc/selinux/config 调整服务器时间[root@node1 ~]# yum -y install ntp[root@node1 ~]# ntpdate -u 202.120.2.101 安装配置 Redmine安装依赖环境[root@node1 ~]# yum install -y zlib-devel openssl-devel ImageMagick-devel wget curl-devel rubygems mod_fcgid 安装RVM[root@node1 ~]# gpg –keyserver hkp://keys.gnupg.net –recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3[root@node1 ~]# curl -L https://get.rvm.io | bash -s stable 载入RVM环境并获取需要的支持安装包[root@node1 ~]# source /etc/profile.d/rvm.sh[root@node1 ~]# rvm requirements 利用rvm安装 Ruby 2.2.3 并设为默认[root@node1 ~]# sed -i -E ‘s!https?://cache.ruby-lang.org/pub/ruby!https://ruby.taobao.org/mirrors/ruby!&#39; /usr/local/rvm/config/db[root@node1 ~]# rvm gemset create[root@node1 ~]# rvm install 2.2.3[root@node1 ~]# rvm use 2.2.3 –default 添加淘宝镜像[root@node1 ~]# gem sources –add https://gems.ruby-china.org/ –remove https://rubygems.org/[root@node1 ~]# gem sources -l CURRENT SOURCES https://gems.ruby-china.org 安装rails[root@node1 ~]# gem install rails -v=4.2 安装mysql和httpd[root@node1 ~]# yum install httpd httpd-devel -y [root@node1 ~]# wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm[root@node1 ~]# rpm -ivh mysql-community-release-el7-5.noarch.rpm[root@node1 ~]# yum -y install mysql-community-server mysql-devel[root@node1 ~]# service mysqld restart[root@node1 ~]# mysql -u rootmysql&gt; create database redmine character set utf8;mysql&gt; create user ‘redmine‘@’localhost’ identified by ‘redmine’;mysql&gt; grant all privileges on redmine.* to ‘redmine‘@’localhost’; 安装redmine的apache支持，这样可以通过apache访问[root@node1 ~]# gem install passenger[root@node1 ~]# passenger-install-apache2-module [root@node1 ~]# vim /etc/httpd/conf.d/passenger.confLoadModule passenger_module /usr/local/rvm/gems/ruby-2.2.3/gems/passenger-5.0.30/buildout/apache2/mod_passenger.so PassengerRoot /usr/local/rvm/gems/ruby-2.2.3/gems/passenger-5.0.30 PassengerDefaultRuby /usr/local/rvm/gems/ruby-2.2.3/wrappers/ruby [root@node1 ~]# vim /etc/httpd/conf.d/redmine.conf ServerName www.a.com # !!! Be sure to point DocumentRoot to ‘public’! DocumentRoot /var/www/html/redmine/public ErrorLog logs/redmine_error_log &lt;Directory /var/www/html/redmine/public&gt; Options Indexes ExecCGI FollowSymLinks Order allow,deny Allow from all # This relaxes Apache security settings. AllowOverride all # MultiViews must be turned off. Options -MultiViews # Uncomment this if you’re on Apache &gt;= 2.4: #Require all granted 安装redmine[root@node1 ~]# cd /var/www/html[root@node1 ~]# wget http://www.redmine.org/releases/redmine-3.1.1.tar.gz[root@node1 ~]# tar -zxvf redmine-3.1.1.tar.gz[root@node1 ~]# mv redmine-3.1.1 redmine[root@node1 ~]# cd /var/www/html/redmine/ [root@node1 ~]# vim Gemfile # 修改sourcesource ‘https://rubygems.org&#39; （注释掉）source’https://ruby.taobao.org&#39; [root@node1 ~]# cp config/configuration.yml.example config/configuration.yml[root@node1 ~]# cp config/database.yml.example config/database.yml[root@node1 ~]# vim config/database.yml # 修改数据连接production: adapter: mysql2 database: redmine host: localhost username: redmine password: “redmine” encoding: utf8 [root@node1 redmine]# gem install bundler # 注意是在网站根目录下执行[root@node1 redmine]# gem install rack-cache -v ‘1.4.2’[root@node1 redmine]# bundle install 为Rails生成cookies密钥[root@node1 redmine]# rake generate_secret_token 初始化redmine数据库表名[root@node1 redmine]# RAILS_ENV=production rake db:migrate[root@node1 redmine]# RAILS_ENV=production rake redmine:load_default_data 启动[root@node1 ~]# cd /var/www/html/redmine[root@node1 redmine]# mkdir /var/www/html/logs[root@node1 redmine]# bundle exec rails server webrick -e production -b 0.0.0.0 &amp;&gt;&gt; /var/www/html/logs/redmine.log &amp;访问地址：http://IP:3000 备份[root@node1 ~]# mysqldump -u root redmine &gt; /root/redmine.sql 恢复停止redmine， 步骤：ps -ef | grep rails，找到redmine的进程号，然后kill掉 恢复数据库[root@node1 ~]# mysql -u redmine -p redmine &lt; /root/redmine.sql 把10.10.2.120上的/backup/redmine-back/redmine_file目录下的所有文件拷贝到192.168.100.10中的/var/www/html/redmine/files目录[root@node1 ~]# scp -r root@10.10.2.120:/backup/redmine-back/redmine_file/* /var/www/html/redmine/files[root@node1 ~]# chmod -R 755 /var/www/html/redmine/files 启动redmine[root@node1 redmine]# bundle exec rails server webrick -e production -b 0.0.0.0 &amp;&gt;&gt; /var/www/html/logs/redmine.log &amp; 调整配置配置邮件发送，空格缩进必须如下，不然redmine无法启动[root@node1 ~]# vim /var/www/html/redmine/config/configuration.ymldefault: email_delivery: delivery_method: :smtp smtp_settings: openssl_verify_mode: ‘none’ address: mail.testin.cn port: 587 domain: testin.cn authentication: :login user_name: “project@testin.cn“ password: “m12345678” 重启redmine即可]]></content>
      <categories>
        <category>版本管理工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos6.4安装kvm]]></title>
    <url>%2F2017%2F03%2F20%2F%E8%99%9A%E6%8B%9F%E5%8C%96%2Fcentos6.4%E5%AE%89%E8%A3%85kvm%2F</url>
    <content type="text"><![CDATA[首先检查您的CPU是否支持硬件虚拟化 egrep ‘(vmx|svm)’ –color=always /proc/cpuinfo 应该显示一些东西，例如： [root@server1 ~]# egrep ‘(vmx|svm)’ –color=always /proc/cpuinfoflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt rdtscp lm 3dnowext 3dnow pni cx16 lahf_lm cmp_legacy svm extapic cr8_legacy misalignsseflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt rdtscp lm 3dnowext 3dnow pni cx16 lahf_lm cmp_legacy svm extapic cr8_legacy misalignsse[root@server1 ~]# 现在我们导入软件包的GPG密钥： rpm–import /etc/pki/rpm-gpg/RPM-GPG-KEY* yum install kvm libvirt python-virtinst qemu-kvm /etc/init.d/libvirtd start virsh -c qemu:///system list 导入镜像文件 xterm-253-1.el6.x86_64.rpm 之后xmanager开始kvm就可以了]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
  </entry>
</search>
