<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[tornado-hello]]></title>
    <url>%2F2019%2F03%2F19%2Fpython%2Ftornado-hello%2F</url>
    <content type="text"><![CDATA[概念Tornado是一个Python Web框架和异步网络库，最初是在FriendFeed上开发的。通过使用非阻塞网络I / O，Tornado可以扩展到数万个开放连接，使其成为长轮询， WebSockets和其他需要与每个用户建立长期连接的应用程序的理想选择 。 安装1pip3 install tornado 简单的web12345678910111213import tornado.ioloopimport tornado.webclass MainHandler(tornado.web.RequestHandler): def get(self): self.write(&quot;Hello, world&quot;)if __name__ == &quot;__main__&quot;: application = tornado.web.Application([ (r&quot;/index&quot;, MainHandler), ]) application.listen(8888) tornado.ioloop.IOLoop.current().start() 访问http://ip:8888/index]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[fabric-ca部署]]></title>
    <url>%2F2019%2F03%2F05%2F%E5%8C%BA%E5%9D%97%E9%93%BE%2Ffabric-ca%2F</url>
    <content type="text"><![CDATA[部署一个fabric-ca架构图 创建一个由两个组织org1.example.com和org2.example.com组成的的联盟1234567891011121314151617181920还有一个组织example.com用来部署orderer。组织example.com部署了一个solo模式的orderer。（多个orderer的部署方式，以后探讨）orderer.example.com组织org1.example.com部署了两个peer:peer0.org1.example.compeer1.org1.example.com组织org2.example.com部署了一个peer:peer0.org2.example.com每个组织都要有一个Admin用户，每个组件(peer/orderer)也需要一个账号，因此需要通过FabricCA创建7个用户：example.com: Admin@example.com orderer.example.comorg1.example.com: Admin@org1.example.com peer0.org1.example.com peer1.org1.example.com org2.example.com: Admin@org2.example.com peer0.org2.example.com这里只创建了Admin用户和每个组件的账号，普通用户的创建方式相同，只是普通用户的证书不需要添加到目标组件的admincerts目录中。或者说一个用户的证书如果被添加到了对应组织或组件的msp/admincerts目录中，那么这个用户就称为对应的管理员。 启动fabric-ca123456789101112131415161718192021fabirc-ca的编译：$ go get -u github.com/hyperledger/fabric-ca$ cd $GOPATH/src/github.com/hyperledger/fabric-ca$ make fabric-ca-server$ make fabric-ca-client$ ls bin/fabric-ca-client fabric-ca-server这里将fabric-ca部署在/opt/app/fabric-ca/server目录中：mkdir -p /opt/app/fabric-ca/servercp -rf $GOPATH/src/github.com/hyperledger/fabric-ca/bin/* /opt/app/fabric-ca/serverln -s /opt/app/fabric-ca/server/fabric-ca-client /usr/bin/fabric-ca-client直接启动ca，fabric-ca admin的名称为admin，密码为pass。(这里只是演示，生产中使用，你需要根据实际的情况配置)cd /opt/app/fabric-ca/server./fabric-ca-server start -b admin:pass &amp;如果有删除联盟和删除用户的需求，需要用下面的方式启动：cd /opt/app/fabric-ca/server./fabric-ca-server start -b admin:pass --cfg.affiliations.allowremove --cfg.identities.allowremove &amp; 生成fabric-ca admin的凭证123456789101112mkdir /root/fabric-deploycd ~/fabric-deploymkdir fabric-ca-files 生成fabric-ca admin的凭证，用-H参数指定client目录：mkdir -p `pwd`/fabric-ca-files/adminfabric-ca-client enroll -u http://admin:pass@localhost:7054 -H `pwd`/fabric-ca-files/admin也可以用环境变量FABRIC_CA_CLIENT_HOME指定了client的工作目录，生成的用户凭证将存放在这个目录中。export FABRIC_CA_CLIENT_HOME=`pwd`/fabric-ca-files/adminmkdir -p $FABRIC_CA_CLIENT_HOMEfabric-ca-client enroll -u http://admin:pass@localhost:7054 创建联盟123456789101112131415161718192021222324252627282930上面的启动方式默认会创建两个组织：$ fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation list2018/05/07 02:36:46 [INFO] [::1]:56148 GET /affiliations 200 0 &quot;OK&quot;affiliation: . affiliation: org2 affiliation: org2.department1 affiliation: org1 affiliation: org1.department1 affiliation: org1.department2为了查看信息的时候，看到的输出比较简洁，用下面的命令将其删除：fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation remove --force org1fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation remove --force org2执行下面命令创建联盟：fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com.examplefabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com.example.org1fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com.example.org2注意：联盟是有层级的。创建联盟如下：$ fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation list2018/04/28 15:19:34 [INFO] 127.0.0.1:38160 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org1 affiliation: com.example.org2 为每个组织准备msp12345678910111213141516171819202122232425262728293031323334353637就是从Fabric-CA中，读取出用来签署用户的根证书等。为example.com准备msp，将ca证书等存放example.com组织的目录中:mkdir -p ./fabric-ca-files/example.com/mspfabric-ca-client getcacert -M `pwd`/fabric-ca-files/example.com/msp //-M需要指定绝对路径命令执行结束后，会在fabric-ca-files/example.com/msp得到文件：$ tree fabric-ca-files/example.com/msp/example.com/msp/|-- cacerts| `-- localhost-7054.pem|-- intermediatecerts| `-- localhost-7054.pem|-- keystore`-- signcerts注意通过getcacert得到msp目录中只有CA证书，而且这里没有使用中间CA，fabric-ca-files/example.com/msp/intermediatecerts/localhost-7054.pem是一个空文件。同样的方式为org1.example.com获取msp:mkdir -p fabric-ca-files/org1.example.com/mspfabric-ca-client getcacert -M `pwd`/fabric-ca-files/org1.example.com/msp为org2.example.com准备msp:mkdir -p ./fabric-ca-files/org2.example.com/mspfabric-ca-client getcacert -M `pwd`/fabric-ca-files/org2.example.com/msp这里是用getcacert为每个组织准备需要的ca文件，在生成创始块的时候会用到。在1.1.0版本的fabric-ca中，只会生成用户在操作区块链的时候用到的证书和密钥，不会生成用来加密grpc通信的证书。这里复用之前用cryptogen生成的tls证书，需要将验证tls证书的ca添加到msp目录中，如下：cp -rf certs/ordererOrganizations/example.com/msp/tlscacerts fabric-ca-files/example.com/msp/cp -rf certs/peerOrganizations/org1.example.com/msp/tlscacerts/ fabric-ca-files/org1.example.com/msp/cp -rf certs/peerOrganizations/org2.example.com/msp/tlscacerts/ fabric-ca-files/org2.example.com/msp/如果在你的环境中，各个组件域名的证书，是由第三方CA签署的，就将第三方CA的根证书添加到msp/tlscacerts目录中。组织的msp目录中，包含都是CA根证书，分别是TLS加密的根证书，和用于身份验证的根证书。另外还需要admin用户的证书，后面的操作中会添加。 注册example.com的管理员Admin@example.com可以直接用命令行（命令比较长，这里用\\截断了）： 123fabric-ca-client register --id.name Admin@example.com --id.type client --id.affiliation &quot;com.example.org1&quot; \ --id.attrs &apos;&quot;hf.Registrar.Roles=client,orderer,peer,user&quot;,&quot;hf.Registrar.DelegateRoles=client,orderer,peer,user&quot;,\ hf.Registrar.Attributes=*,hf.GenCRL=true,hf.Revoker=true,hf.AffiliationMgr=true,hf.IntermediateCA=true,role=admin:ecert&apos; 也可以将命令行参数写在fabric-ca admin的配置文件fabric-ca-files/admin/fabric-ca-client-config.yaml中。 12$ ls fabric-ca-files/admin/admin/fabric-ca-client-config.yaml msp 为了演示清楚，这里使用修改配置文件的方式，将fabric-ca-files/admin/fabric-ca-client-config.yaml其中的id部分修改为： 1234567891011121314151617181920212223id: name: Admin@example.com type: client affiliation: com.example maxenrollments: 0 attributes: - name: hf.Registrar.Roles value: client,orderer,peer,user - name: hf.Registrar.DelegateRoles value: client,orderer,peer,user - name: hf.Registrar.Attributes value: &quot;*&quot; - name: hf.GenCRL value: true - name: hf.Revoker value: true - name: hf.AffiliationMgr value: true - name: hf.IntermediateCA value: true - name: role value: admin ecert: true 注意最后一行role属性，是我们自定义的属性，对于自定义的属性，要设置certs，在配置文件中需要单独设置ecert属性为true或者false。如果在命令行中，添加后缀:ecert表示true，例如: 1fabric-ca-client register --id.affiliation &quot;com.example.org1&quot; --id.attrs &quot;role=admin:ecert&quot; 直接执行下面的命令，即可完成用户`Admin@example.com`注册，注意这时候的注册使用fabricCA的admin账号完成的： 1fabric-ca-client register -H `pwd`/fabric-ca-files/admin --id.secret=password 如果不用--id.secret指定密码，会自动生成密码。 其它配置的含义是用户名为`Admin@example.com，类型是client，它能够管理com.example.*`下的用户，如下: 1234567891011--id.name Admin@example.com //用户名--id.type client //类型为client--id.affiliation &quot;com.example&quot; //权利访问hf.Registrar.Roles=client,orderer,peer,user //能够管理的用户类型hf.Registrar.DelegateRoles=client,orderer,peer,user //可以授权给子用户管理的用户类型hf.Registrar.Attributes=* //可以为子用户设置所有属性hf.GenCRL=true //可以生成撤销证书列表hf.Revoker=true //可以撤销用户hf.AffiliationMgr=true //能够管理联盟hf.IntermediateCA=true //可以作为中间CArole=admin:ecert //自定义属性 完成注册之后，还需生成Admin@example.com凭证： 1234$ mkdir -p ./fabric-ca-files/example.com/admin$ fabric-ca-client enroll -u http://Admin@example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/example.com/admin$ ls ./fabric-ca-files/example.com/adminfabric-ca-client-config.yaml msp/ 这时候可以用Admin@example.com的身份查看联盟： 123456$ fabric-ca-client affiliation list -H `pwd`/fabric-ca-files/example.com/admin2018/04/28 15:35:10 [INFO] 127.0.0.1:38172 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org1 affiliation: com.example.org2 最后将Admin@example.com的证书复制到example.com/msp/admincerts/中： 12mkdir fabric-ca-files/example.com/msp/admincerts/cp fabric-ca-files/example.com/admin/msp/signcerts/cert.pem fabric-ca-files/example.com/msp/admincerts/ 注册org1.example.com的管理员Admin@org1.example.com为org1.example.com的管理员Admin@org1.example.com准备一个目录: 12cd ~/fabric-deploymkdir -p ./fabric-ca-files/org1.example.com/admin 将fabric-ca-files/admin/fabric-ca-client-config.yaml其中的id部分修改为： 1234567891011121314151617181920212223id: name: Admin@org1.example.com type: client affiliation: com.example.org1 maxenrollments: 0 attributes: - name: hf.Registrar.Roles value: client,orderer,peer,user - name: hf.Registrar.DelegateRoles value: client,orderer,peer,user - name: hf.Registrar.Attributes value: &quot;*&quot; - name: hf.GenCRL value: true - name: hf.Revoker value: true - name: hf.AffiliationMgr value: true - name: hf.IntermediateCA value: true - name: role value: admin ecert: true 注册： 1fabric-ca-client register -H `pwd`/fabric-ca-files/admin --id.secret=password 生成凭证： 123$ fabric-ca-client enroll -u http://Admin@org1.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org1.example.com/admin$ ls ./fabric-ca-files/org1.example.com/adminfabric-ca-client-config.yaml msp/ 查看联盟： 12345$ fabric-ca-client affiliation list -H `pwd`/fabric-ca-files/org1.example.com/admin2018/05/04 15:42:53 [INFO] 127.0.0.1:51298 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org1 注意与`Admin@example.com`的区别，这里只能看到组织com.example.org1 将Admin@org1.example.com的证书复制到org1.example.com的msp/admincerts中： 12mkdir fabric-ca-files/org1.example.com/msp/admincerts/cp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/msp/admincerts/ 在`Admin@org1.example.com目录`中也需要创建msp/admincerts目录，通过peer命令操作fabric的时候会要求admincerts存在： 12mkdir fabric-ca-files/org1.example.com/admin/msp/admincerts/ # 注意是org1.example.com/admin目录cp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/admin/msp/admincerts/ 另外，这里没有使用中间CA，将intermediatecerts中的空文件删除，否则peer会提示Warning： 1rm fabric-ca-files/org1.example.com/admin/msp/intermediatecerts/* 注册org2.example.com的管理员Admin@org2.example.com为org2.example.com的管理员Admin@org2.example.com准备一个目录: 12cd ~/fabric-deploymkdir -p ./fabric-ca-files/org2.example.com/admin 将fabric-ca-files/admin/fabric-ca-client-config.yaml其中的id部分修改为： 1234567891011121314151617181920212223id: name: Admin@org2.example.com type: client affiliation: com.example.org2 maxenrollments: 0 attributes: - name: hf.Registrar.Roles value: client,orderer,peer,user - name: hf.Registrar.DelegateRoles value: client,orderer,peer,user - name: hf.Registrar.Attributes value: &quot;*&quot; - name: hf.GenCRL value: true - name: hf.Revoker value: true - name: hf.AffiliationMgr value: true - name: hf.IntermediateCA value: true - name: role value: admin ecert: true 注册： 1fabric-ca-client register -H `pwd`/fabric-ca-files/admin --id.secret=password 生成凭证： 123$ fabric-ca-client enroll -u http://Admin@org2.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org2.example.com/admin$ ls ./fabric-ca-files/org2.example.com/adminfabric-ca-client-config.yaml msp/ 查看联盟： 12345$ fabric-ca-client affiliation list -H `pwd`/fabric-ca-files/org2.example.com/admin2018/05/02 16:49:00 [INFO] 127.0.0.1:50828 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org2 Admin@org2.example.com只能看到组织com.example.org2。 将Admin@org2.example.com的证书复制到org2.example.com的msp/admincerts中： 12mkdir fabric-ca-files/org2.example.com/msp/admincerts/cp fabric-ca-files/org2.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org2.example.com/msp/admincerts/ 在Admin@org2.example.com中也需要创建msp/admincerts目录，通过peer命令操作fabric的时候会要求admincerts存在： 12mkdir fabric-ca-files/org2.example.com/admin/msp/admincerts/cp fabric-ca-files/org2.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org2.example.com/admin/msp/admincerts/ 另外，这里没有使用中间CA，将intermediatecerts中的空文件删除，否则peer会提示Warning： 1rm fabric-ca-files/org2.example.com/admin/msp/intermediatecerts/* 各个组织分别使用自己的Admin账户创建其它账号example.com、org1.example.com、org2.example.com三个组织这时候可以分别使用自己的Admin账号创建子账号。 orderer.example.com使用`Admin@example.com注册账号orderer.example.com。注意这时候指定的目录是fabric-ca-files/example.com`/admin/。 修改fabric-ca-files/example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: orderer.example.com type: orderer affiliation: com.example maxenrollments: 0 attributes: - name: role value: orderer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/example.com/ordererfabric-ca-client enroll -u http://orderer.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/example.com/orderer 将`Admin@example.com`的证书复制到fabric-ca-files/example.com/orderer/msp/admincerts： 12mkdir fabric-ca-files/example.com/orderer/msp/admincertscp fabric-ca-files/example.com/admin/msp/signcerts/cert.pem fabric-ca-files/example.com/orderer/msp/admincerts/ peer0.org1.example.com使用`Admin@org1.example.com注册账号peer0.org1.example.com。这时候指定的目录是fabric-ca-files/org1.example.com`/admin/。 修改fabric-ca-files/org1.example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: peer0.org1.example.com type: peer affiliation: com.example.org1 maxenrollments: 0 attributes: - name: role value: peer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/org1.example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/org1.example.com/peer0fabric-ca-client enroll -u http://peer0.org1.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org1.example.com/peer0 将`Admin@org1.example.com`的证书复制到fabric-ca-files/org1.example.com/peer0/msp/admincerts： 12mkdir fabric-ca-files/org1.example.com/peer0/msp/admincertscp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/peer0/msp/admincerts/ peer1.org1.example.com使用`Admin@org1.example.com注册账号peer1.org1.example.com。这时候指定的目录是fabric-ca-files/org1.example.com`/admin/。 修改fabric-ca-files/org1.example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: peer1.org1.example.com type: peer affiliation: com.example.org1 maxenrollments: 0 attributes: - name: role value: peer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/org1.example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/org1.example.com/peer1fabric-ca-client enroll -u http://peer1.org1.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org1.example.com/peer1 将`Admin@org1.example.com`的证书复制到fabric-ca-files/org1.example.com/peer1/msp/admincerts： 12mkdir fabric-ca-files/org1.example.com/peer1/msp/admincertscp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/peer1/msp/admincerts/ peer0.org2.example.com使用`Admin@org2.example.com注册账号peer0.org2.example.com。这时候指定的目录是fabric-ca-files/org2.example.com`/admin/。 修改fabric-ca-files/org2.example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: peer0.org2.example.com type: peer affiliation: com.example.org2 maxenrollments: 0 attributes: - name: role value: peer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/org2.example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/org2.example.com/peer0fabric-ca-client enroll -u http://peer0.org2.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org2.example.com/peer0 将`Admin@org2.example.com`的证书复制到fabric-ca-files/org2.example.com/peer0/msp/admincerts： 12mkdir fabric-ca-files/org2.example.com/peer0/msp/admincertscp fabric-ca-files/org2.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org2.example.com/peer0/msp/admincerts/]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Prometheus监控]]></title>
    <url>%2F2019%2F01%2F29%2Fk8s%2Fprometheus%2F</url>
    <content type="text"><![CDATA[在Kubernetes上快速部署Prometheus创建一个新的命名空间12345678[root@prometheus]# cat monitor_namespace.yaml apiVersion: v1kind: Namespacemetadata: name: monitor labels: name: monitor[root@prometheus]#kubectl create -f monitor_namespace.yaml rbac文件12345678910111213141516171819202122232425262728293031323334353637383940414243[root@prometheus]# cat rbac-setup.yaml apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: [&quot;&quot;] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- apiGroups: - extensions resources: - ingresses verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- nonResourceURLs: [&quot;/metrics&quot;] verbs: [&quot;get&quot;]---apiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: monitor---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: monitor [root@prometheus]#kubectl create -f rbac-setup.yaml prometheus-deploy文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373[root@prometheus]# cat configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: monitordata: #被引用到/etc/prometheus/prometheus.yml prometheus.yml: | global: #每15s采集一次数据和15s做一次告警检测 scrape_interval: 15s evaluation_interval: 15s #指定加载的告警规则文件 rule_files: - /etc/prometheus/rules.yml #将报警送至何地进行报警 alerting: alertmanagers: - static_configs: - targets: [&quot;192.168.50.60:9093&quot;] #指定prometheus要监控的目标 scrape_configs: - job_name: &apos;k8s-node&apos; scrape_interval: 10s static_configs: - targets: - &apos;192.168.50.61:31672&apos; #自定义获取监控数据,每个 job_name 都是独立的 - job_name: &apos;tomcat-pods&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_service_annotation_prometheus_io_jvm_scrape] regex: true;true action: keep - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_app_metrics_patn] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__meta_kubernetes_pod_ip, __meta_kubernetes_service_annotation_prometheus_io_app_metrics_port] action: replace target_label: __address__ regex: (.+);(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_pod_host_ip] action: replace target_label: kubernetes_host_ip - job_name: &apos;kubernetes-apiservers&apos; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: &apos;kubernetes-nodes&apos; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics - job_name: &apos;kubernetes-cadvisor&apos; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: &apos;kubernetes-service-endpoints&apos; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: &apos;kubernetes-services&apos; kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: &apos;kubernetes-ingresses&apos; kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path] regex: (.+);(.+);(.+) replacement: $&#123;1&#125;://$&#123;2&#125;$&#123;3&#125; target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: kubernetes_name - job_name: &apos;kubernetes-pods&apos; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # 监控规则文件,被引用到/etc/prometheus/rules.yml rules.yml: | groups: - name: test-rule rules: ############# Node监控 ############# - alert: k8s-node状态异常 expr: up&#123;job=&quot;k8s-node&quot;&#125; != 1 for: 3m labels: team: k8s-node annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点状态异常&quot; description: &quot;可能是重启了&quot; - alert: k8s-node节点CPU使用率 expr: (1 - avg(irate(node_cpu_seconds_total&#123;job=&quot;k8s-node&quot;,mode=&quot;idle&quot;&#125;[1m])) by (instance)) * 100 &gt; 95 for: 1m labels: team: k8s-node annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点CPU使用率超过95%&quot; description: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点当前CPU使用率为: &#123;&#123; $value &#125;&#125;&quot; - alert: k8s-node节点磁盘使用率 expr: (node_filesystem_size_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125; - node_filesystem_avail_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125;) / node_filesystem_size_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125; * 100 &gt; 85 for: 1m labels: team: k8s-node annotations: description: &quot;Node服务器[[ &#123;&#123;$labels.instance&#125;&#125; ]] 的 &#123;&#123;mountpoint&#125;&#125; 磁盘空间使用率超过85%&quot; summary: &quot;磁盘 &#123;&#123;$labels.device&#125;&#125; 当前使用率为: &#123;&#123; $value &#125;&#125;&quot; - alert: k8s-node节点内存使用率 expr: (node_memory_MemTotal_bytes&#123;job=&quot;k8s-node&quot;&#125; - (node_memory_Buffers_bytes&#123;job=&quot;k8s-node&quot;&#125; + node_memory_Cached_bytes&#123;job=&quot;k8s-node&quot;&#125; + node_memory_MemFree_bytes&#123;job=&quot;k8s-node&quot;&#125;)) / node_memory_MemTotal_bytes&#123;job=&quot;k8s-node&quot;&#125; * 100 for: 1m labels: team: k8s-node annotations: description: &quot;Node服务器[[ &#123;&#123;$labels.instance&#125;&#125; ]] 内存使用率超过95%&quot; summary: &quot;&#123;&#123;$labels.instance&#125;&#125; 当前内存使用率为: &#123;&#123; $value &#125;&#125;&quot; ############ Pod 监控 ############ - alert: 监控k8s的pod状态异常 expr: up&#123;kubernetes_namespace=&quot;monitor&quot;&#125; != 1 for: 3m labels: team: &quot;kube-state-metrics&quot; annotations: description: &quot;&#123;&#123;$labels.kubernetes_namespace&#125;&#125; 内的 pod 状态有变动&quot; summary: &quot;此 Pod 用于获取 k8s 监控数据, 绑定在一个节点上&quot; - alert: 应用的 pod 状态有变动 expr: kube_pod_container_status_ready&#123;namespace=&quot;product&quot;&#125; != 1 for: 3m labels: status: &quot;product 命名空间内的 pod &#123;&#123;$labels.pod&#125;&#125;有变动&quot; annotations: description: &quot;Deployment &#123;&#123;$labels.container&#125;&#125; 内的 pod 状态有变动&quot; summary: &quot;可能是重启或者在升级版本,如果频繁重启,请跟踪排查问题&quot; - alert: 以下应用的 pod 重启次数已经超过15,请查看原因 expr: kube_pod_container_status_restarts_total&#123;namespace=&quot;product&quot;&#125; &gt; 15 for: 3m labels: status: &quot;product 命名空间内的 pod &#123;&#123;$labels.pod&#125;&#125; 重启次数太多&quot; annotations: description: &quot;Deployment &#123;&#123;$labels.container&#125;&#125; 内的 pod 重启次数太多&quot; summary: &quot;重启次数太多,可能是因为 pod 内应用有问题&quot; ########### Java 监控 ############ - alert: jvm线程数过高 expr: jvm_threads_current&#123;job=&quot;tomcat-pods&quot;&#125;&gt;2000 for: 1m labels: status: &quot;空间内 jvm 的变动情况&quot; annotations: description: &quot;&#123;&#123;$labels.kubernetes_pod_name&#125;&#125;: Jvm线程数过高&quot; summary: &apos;&#123;&#123; $labels.kubernetes_pod_name &#125;&#125; : 当前你线程值为: &#123;&#123; $value &#125;&#125;&apos; [root@prometheus]# cat prometheus.deploy.yml ---apiVersion: apps/v1beta2kind: Deploymentmetadata: labels: name: prometheus-deployment name: prometheus namespace: monitorspec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: containers: - image: prom/prometheus:v2.6.0 name: prometheus command: - &quot;/bin/prometheus&quot; args: - &quot;--config.file=/etc/prometheus/prometheus.yml&quot; - &quot;--storage.tsdb.path=/home/prometheus&quot; - &quot;--storage.tsdb.retention=168h&quot; - &quot;--web.enable-lifecycle&quot; ports: - containerPort: 9090 protocol: TCP volumeMounts: - mountPath: &quot;/home/prometheus&quot; name: data - mountPath: &quot;/etc/prometheus&quot; name: config-volume - mountPath: &quot;/etc/localtime&quot; readOnly: false name: localtime resources: requests: cpu: 100m memory: 2048Mi limits: cpu: 500m memory: 3180Mi serviceAccountName: prometheus nodeSelector: nodetype: prometheus volumes: - name: data hostPath: path: &quot;/opt/prometheus/data&quot; - name: config-volume configMap: name: prometheus-config - name: localtime hostPath: path: &quot;/etc/localtime&quot; type: File [root@prometheus]# cat prometheus.svc.yml ---kind: ServiceapiVersion: v1metadata: labels: app: prometheus name: prometheus namespace: monitorspec: type: NodePort ports: - port: 9090 targetPort: 9090 nodePort: 30003 selector: app: prometheus [root@prometheus]#kubectl create -f configmap.yaml[root@prometheus]#kubectl create -f prometheus.deploy.yml[root@prometheus]#kubectl create -f prometheus.svc.yml注：需要在本地创建/opt/prometheus/data作为prometheus数据路径，另需要给data目录赋予777权限 热重读配置文件congfigmap有热重启功能，这样每次改完配置文件都不需要重启prometheus的pod来重读配置了 123456789- &quot;--web.enable-lifecycle&quot;在prometheus.deploy.yml的配置文件里面加上这段话就可以了[root@prometheus]# cat reload-prometheus.sh #!/bin/bashkubectl apply -f configmap.yamlsleep 60curl -XPOST http://192.168.50.60:30003/-/reload可以写个脚本，每次修改完配置文件的配置之后，执行一下脚本就可以同步生效了！ 安装kube-state-metrics123[root@prometheus]# git clone https://github.com/kubernetes/kube-state-metrics.git之后把默认的命名空间改成monitor，进入kube-state-metrics目录[root@prometheus]#kubectl create -f ./ 安装grafana12345678910111213141516创建grafana的数据目录mkdir /opt/grafana/data启动脚本[root@grafana]# cat start_grafana.sh #!/bin/bashdocker stop `docker ps -a |awk &apos;/grafana/&#123;print $1&#125;&apos;`docker rm `docker ps -a |awk &apos;/grafana/&#123;print $1&#125;&apos;`docker run -d \ --name=grafana \ --restart=always \ -p 3000:3000 \ -m 4096m \ -v /opt/grafana/data:/var/lib/grafana \ -v /opt/grafana/log:/var/log/grafana \ grafana/grafana:5.4.3 1、安装完之后，需要添加source，source直接点prometheus，链接就是http://192.168.50.60:30003之前创建的prometheus界面 2、添加模板dashboad（列出几个常用的） 点import导入，有俩种方式，直接填官网模板，或者导入json https://grafana.com/dashboards/9276 node的cpu、内存等 https://grafana.com/dashboards/3146 pod https://grafana.com/dashboards/8588 deployment 安装alertmanager创建配置文件、目录12345678910111213141516171819202122232425262728293031创建alert数据目录mkdir /opt/alert/data注意：需要alertmanager.yml配置，此配置钉钉和邮件可同时放松[root@docker60 alert]# cat alertmanager.yml global: resolve_timeout: 5mroute: group_by: [&apos;alertname&apos;] group_wait: 10s group_interval: 10s repeat_interval: 6m receiver: defaultreceivers:- name: &apos;default&apos; email_configs: - to: &quot;&quot; from: &quot;&quot; smarthost: &quot;smtp.xxx.com:25&quot; auth_username: &quot;&quot; auth_password: &quot;&quot; webhook_configs: - url: &apos;http://192.168.50.60:8060/dingtalk/ops_dingding/send&apos; send_resolved: trueinhibit_rules: - source_match: severity: &apos;critical&apos; target_match: severity: &apos;warning&apos; equal: [&apos;alertname&apos;] 启动脚本1234567891011121314[root@alert]# cat start_alert.sh#!/bin/bashdocker stop `docker ps -a |awk &apos;/alertmanager/&#123;print $1&#125;&apos;`docker rm `docker ps -a |awk &apos;/alertmanager/&#123;print $1&#125;&apos;`docker run -d \ --name alertmanager \ --restart=always \ -p 9093:9093 \ -v /etc/localtime:/etc/localtime:ro \ -v /opt/alert/alertmanager.yml:/etc/alertmanager/alertmanager.yml \ -v /opt/alert/data:/alertmanager \ prom/alertmanager:v0.15.3 安装dingding插件1234567891011121、安装go （这里就不叙述了）2、假设go的路径是/usr/local/gomkdir -pv /usr/local/go/src/github.com/timonwong3、下载dingding插件git clone https://github.com/timonwong/prometheus-webhook-dingtalk.git4、添加dingding机器人在dingding群里面添加即可5、启动dingding[root@alert]# cat start_dingding.sh cd /usr/local/go/src/github.com/timonwong/prometheus-webhook-dingtalkkill -9 `ps -ef | grep prometheus-webhook-dingtalk | grep -v grep | awk &apos;&#123;print $2&#125;&apos;`nohup ./prometheus-webhook-dingtalk --ding.profile=&quot;ops_dingding=https://oapi.dingtalk.com/robot/send?access_token=xxxx&quot; 2&gt;&amp;1 1&gt;dingding.log &amp;]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jira-安装及破解]]></title>
    <url>%2F2018%2F11%2F21%2F%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2Fjira-%E5%AE%89%E8%A3%85%E5%8F%8A%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[安装jira 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061wget https://product-downloads.atlassian.com/software/jira/downloads/atlassian-jira-software-7.13.0-x64.bin[root@YZSJHL82-204 ~]# chmod +x atlassian-jira-software-7.13.0-x64.bin[root@YZSJHL82-204 ~]# ./atlassian-jira-software-7.13.0-x64.binUnpacking JRE ...Starting Installer ...十月 23, 2018 4:38:25 下午 java.util.prefs.FileSystemPreferences$1 run信息: Created user preferences directory.十月 23, 2018 4:38:25 下午 java.util.prefs.FileSystemPreferences$2 run信息: Created system preferences directory in java.home.This will install JIRA Software 7.4.1 on your computer.OK [o, Enter], Cancel [c]o #按o安装Choose the appropriate installation or upgrade option.Please choose one of the following:Express Install (use default settings) [1], Custom Install (recommended for advanced users) [2, Enter], Upgrade an existing JIRA installation [3]2 #2为自定义安装Where should JIRA Software be installed?[/opt/atlassian/jira]/usr/local/atlassina/jira #自定义安装目录Default location for JIRA Software data[/var/atlassian/application-data/jira]/usr/local/atlassina/jira_data #自定义数据目录Configure which ports JIRA Software will use.JIRA requires two TCP ports that are not being used by any otherapplications on this machine. The HTTP port is where you will access JIRAthrough your browser. The Control port is used to startup and shutdown JIRA.Use default ports (HTTP: 8080, Control: 8005) - Recommended [1, Enter], Set custom value for HTTP and Control ports [2]2 #2为自定义端口HTTP Port Number[8080] #8080为默认端口8050 #http连接端口Control Port Number[8005]8040 #控制端口JIRA can be run in the background.You may choose to run JIRA as a service, which means it will startautomatically whenever the computer restarts.Install JIRA as Service?Yes [y, Enter], No [n]y #是否开机自启Details on where JIRA Software will be installed and the settings that will be used.Installation Directory: /usr/local/atlassina/jira Home Directory: /usr/local/atlassina/jira_data HTTP Port: 8050 RMI Port: 8040 Install as service: Yes Install [i, Enter], Exit [e]i #确认已选配置Extracting files ...Please wait a few moments while JIRA Software is configured.Installation of JIRA Software 7.4.1 is completeStart JIRA Software 7.4.1 now?Yes [y, Enter], No [n]y #启动Please wait a few moments while JIRA Software starts up.Launching JIRA Software ...Installation of JIRA Software 7.4.1 is completeYour installation of JIRA Software 7.4.1 is now ready and can be accessedvia your browser.JIRA Software 7.4.1 can be accessed at http://localhost:8050Finishing installation ... 浏览器访问jira，地址为：http://IP:8050 请自行修改IP和端口。如果可以访问，说明安装成功。 配置数据库及密码在mySQL上创建用户及库做授权123create database jira_new;grant all privileges on *.* to jira@&apos;10.4.82.204&apos; identified by &apos;jira&apos;;flush privileges; 在授权完用户我们不可以马上填写信息，需要添加MySQL的一个jra包，否则下一步会提示找不到mysql的驱动 wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.zip1234567停止jira[root@YZSJHL82-204 ~]# /etc/init.d/jira stop上传软件包[root@YZSJHL82-204 ~]# cp mysql-connector-java-5.1.46-bin.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/启动jira[root@YZSJHL82-204 ~]# /etc/init.d/jira start注意防火墙 安装完数据库插件即可下一步: 设置jira主题 因为第一次安装，我们需要去jira官网注册用户，获取授权码 (免费30天，安装后更换破解即可) 保存好服务器ID，进入atlassian官网获取试用许可证，下边附上注册地址： 注册官网：https://my.atlassian.com 或使用以下地址： https://id.atlassian.com/signup?application=mac&amp;continue=https://my.atlassian.com 登陆账号后，选择New Evaluation License 设置管理员用户:官网注册的账号只可以免费试用30天，所以当我们安装完需要尽快进行破解 破解jirahttps://download.csdn.net/download/lbwahoo/100308071234567停止jira[root@YZSJHL82-204 ~]# /etc/init.d/jira stop进入安装目录下的atlassian-jira/WEB-INF/lib/目录下，用破解包atlassian-extras-3.2.jar替换原来的包。并将mysql连接驱动复制到此目录下。[root@YZSJHL82-204 ~]# cp atlassian-extras-3.2.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/启动jira[root@YZSJHL82-204 ~]# /etc/init.d/jira start注意防火墙 配置数据库连接地址12/var/atlassian/application-data/jira/dbconfig.xml#此路径为默认路径]]></content>
      <categories>
        <category>版本管理工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s基本命令]]></title>
    <url>%2F2018%2F05%2F10%2Fk8s%2Fk8s%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[pods12345$ kubectl get pods -n pro$ kubectl get pods --all-namespaces -n pro$ kubectl get pod aa -o wide -n pro$ kubectl get pod aa -o yaml -n pro$ kubectl describe pod aa -n pro POD升级和历史列出部署历史记录1$ kubectl rollout history deployment/DEPLOYMENT_NAME 跳转到特定修订版1$ kubectl rollout undo deployment/DEPLOYMENT_NAME --to-revision=N service查看服务1$ kubectl get services 将POD作为服务公开（创建端点）1$ kubectl expose deployment/aa --port=2000 --type=NodePort login$ kubectl exec -ti $1 bash -n product log$ kubectl logs -f $1 -n product]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[gitlab-ci自动部署（二）]]></title>
    <url>%2F2017%2F05%2F21%2Fgitlab%2Fgitlab-ci(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[下面来说说CI/CD是怎么实现的： 登录gitlab-runner机器切换runner普通用户 [root@localhost ~]# su - gitlab-runner 编写.gitlab-ci.yml文件随便创建一个目录，叫什么无所谓 [gitlab-runner@localhost ~]$ mkdir git 将gitlab上想要发布的项目克隆到这个目录里 [gitlab-runner@localhost git]$ git clone git@gitlab.xxx.com:xxx/xxx.git 因为我们的项目是大工程，里面带着很多子工程，所以就需要通过yml文件，将项目分离出去 比方说我们想发布bb项目，但是bb项目属于aa这个大项目的子项目，所以就进aa大工程目录下 在项目目录里面创建一个.gitlab-ci.yml文件，如下 [gitlab-runner@localhost ~]$ cat .gitlab-ci.yml12345678job 1: stage: test script: - git subtree push -q --prefix=bb git@gitlab.xx.com:bb1/bb.git dev only: - dev tags: - shell 这样就可以将bb项目分离出去，相当于创建了一个新的项目 进入bb目录，也编写.gitlab-ci.yml文件，这个就是我们需要编译的脚本，中间可以穿插maven、node和shell的一系列命令 [gitlab-runner@localhost bb]$ cat .gitlab-ci.yml12345678910job 1: stage: build script: - rm -rf /opt/M2_REPO/com/bb/* - mvn clean package -P test -Dmaven.test.skip - bash -x /opt/bb/shell/bb.sh only: - dev tags: - shell 编写完yml文件后都需要提交下 git add . git commit -m “add gitlab-ci.yml” git push origin dev 另外包括传包，启动等命令都可以在yml文件里面体现，这样只要dev分支有commit的改变了，gitlab-ci就会自动创建job来自动发布]]></content>
      <categories>
        <category>gitlab</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[gitlab-ci自动部署（一）]]></title>
    <url>%2F2017%2F05%2F20%2Fgitlab%2Fgitlab-ci(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[目前GitLab已经有了CI功能，即持续集成的功能。可以实现代码提交后自动测试、编译、发布、部署等自动化工作 下面是我总结的实现内容： 安装runner在root下执行 下载gitlab-runner wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-runner/yum/el7/gitlab-runner-10.5.0-1.x86_64.rpm 安装 rpm -ivh gitlab-runner-10.5.0-1.x86_64.rpm 配置Runner默认情况，Runner是通过gitlab-runner的这个用户来执行一系列操作，其工作目录也是在gitlab-runner的用户目录下面。如果使用默认gitlab-runner用户操作一些文件时经常会遇到权限问题，就需要给gitlab-runner赋权。我们通过以下方式修改。 #在root下执行 #删除服务 gitlab-runner uninstall #添加服务 gitlab-runner install –working-directory /home/builds –user gitlab-ci #重启服务 gitlab-runner restart #查看状态 gitlab-runner status 输出：gitlab-runner: Service is running! #查看是否生效 ps -ef | grep gitlab-runner 注册Runner先打开GitLab上需要自动部署的项目界面，找到该项目的Settings –&gt; CI/CD –&gt; Runners settings 在gitlab上可以看到自己的token信息，用来注册runner #在root下执行gitlab-runner register (会出现注册信息，填url，token，runner的名字) 成功之后会如下图： 至此安装部分就完成了]]></content>
      <categories>
        <category>gitlab</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos7搭建shadowsock实现vpn翻墙]]></title>
    <url>%2F2017%2F04%2F22%2Fvpn%2Fcentos7%E6%90%AD%E5%BB%BAshadowsock%E5%AE%9E%E7%8E%B0vpn%E7%BF%BB%E5%A2%99%2F</url>
    <content type="text"><![CDATA[安装使用root用户登录，运行以下命令：123wget --no-check-certificate -O shadowsocks.sh https://cyh.abcdocker.com/vpn/shadowsocks.sh chmod +x shadowsocks.sh ./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 安装完成后，脚本提示如下： 1234567 Congratulations, Shadowsocks-python server install completed!Your Server IP :your_server_ipYour Server Port :your_server_portYour Password :your_passwordYour Encryption Method:your_encryption_methodWelcome to visit:https://teddysun.com/342.htmlEnjoy it! 卸载方法使用root用户登录，运行以下命令： ./shadowsocks.sh uninstall 配置文件路径：/etc/shadowsocks.json 12345678910&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:your_server_port, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;your_password&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;your_encryption_method&quot;, &quot;fast_open&quot;: false&#125; 多用户多端口配置文件配置文件路径：/etc/shadowsocks.json 123456789101112131415 &#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;port_password&quot;:&#123; &quot;8989&quot;:&quot;password0&quot;, &quot;9001&quot;:&quot;password1&quot;, &quot;9002&quot;:&quot;password2&quot;, &quot;9003&quot;:&quot;password3&quot;, &quot;9004&quot;:&quot;password4&quot; &#125;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;your_encryption_method&quot;, &quot;fast_open&quot;: false&#125; 1234启动：/etc/init.d/shadowsocks start停止：/etc/init.d/shadowsocks stop重启：/etc/init.d/shadowsocks restart状态：/etc/init.d/shadowsocks status]]></content>
      <categories>
        <category>vpn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pptp]]></title>
    <url>%2F2017%2F04%2F20%2Fvpn%2Fpptp%2F</url>
    <content type="text"><![CDATA[安装pptpd$ yum install pptpd 配置本地及远程IP地址$ vi /etc/pptpd.conf localip 192.168.100.1 remoteip 192.168.100.2-245 配置DNS$ vi /etc/ppp/options.pptpd ms-dns 223.5.5.5 ms-dns 223.6.6.6 修改MTU$ vi / etc / ppp / ip-up /sbin/ifconfig $ 1 mtu 1500 用户和密码配置$ vi /etc/ppp/chap-secrets client server secret IP address 注释掉 testin_user1 pptpd testin_user1 * 配置防火墙$ vi /etc/sysctl.conf net.ipv4.ip_forward = 1＃开启IP转发 $ iptables -t nat -A POSTROUTING -s 192.168.100.0/24 -o eth1 -j MASQUERADE＃允许外网连接 开启服务$ service pptpd start $ service iptables start]]></content>
      <categories>
        <category>vpn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redmine]]></title>
    <url>%2F2017%2F03%2F21%2F%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2Fredmine%2F</url>
    <content type="text"><![CDATA[版本： redmine 3.1.1官方文档地址： http://www.redmine.org.cn/category/install Redmine实战下列描述均以操作系统 Centos7 为例部署。 环境介绍OS IP HostName RoleCentOS7 x64 192.168.10.10 node1 Redmine恢复机器CentOS7 x64 10.10.1.17 localhost Redmine机器 准备工作关闭Iptables和SELinux[root@node1 ~]# systemctl stop firewalld[root@node1 ~]# systemctl disable firewalld[root@node1 ~]# setenforce 0[root@node1 ~]# sed -i ‘/^SELINUX=/{ s/enforcing/disabled/ }’ /etc/selinux/config 调整服务器时间[root@node1 ~]# yum -y install ntp[root@node1 ~]# ntpdate -u 202.120.2.101 安装配置 Redmine安装依赖环境[root@node1 ~]# yum install -y zlib-devel openssl-devel ImageMagick-devel wget curl-devel rubygems mod_fcgid 安装RVM[root@node1 ~]# gpg –keyserver hkp://keys.gnupg.net –recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3[root@node1 ~]# curl -L https://get.rvm.io | bash -s stable 载入RVM环境并获取需要的支持安装包[root@node1 ~]# source /etc/profile.d/rvm.sh[root@node1 ~]# rvm requirements 利用rvm安装 Ruby 2.2.3 并设为默认[root@node1 ~]# sed -i -E ‘s!https?://cache.ruby-lang.org/pub/ruby!https://ruby.taobao.org/mirrors/ruby!&#39; /usr/local/rvm/config/db[root@node1 ~]# rvm gemset create[root@node1 ~]# rvm install 2.2.3[root@node1 ~]# rvm use 2.2.3 –default 添加淘宝镜像[root@node1 ~]# gem sources –add https://gems.ruby-china.org/ –remove https://rubygems.org/[root@node1 ~]# gem sources -l CURRENT SOURCES https://gems.ruby-china.org 安装rails[root@node1 ~]# gem install rails -v=4.2 安装mysql和httpd[root@node1 ~]# yum install httpd httpd-devel -y [root@node1 ~]# wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm[root@node1 ~]# rpm -ivh mysql-community-release-el7-5.noarch.rpm[root@node1 ~]# yum -y install mysql-community-server mysql-devel[root@node1 ~]# service mysqld restart[root@node1 ~]# mysql -u rootmysql&gt; create database redmine character set utf8;mysql&gt; create user ‘redmine‘@’localhost’ identified by ‘redmine’;mysql&gt; grant all privileges on redmine.* to ‘redmine‘@’localhost’; 安装redmine的apache支持，这样可以通过apache访问[root@node1 ~]# gem install passenger[root@node1 ~]# passenger-install-apache2-module [root@node1 ~]# vim /etc/httpd/conf.d/passenger.confLoadModule passenger_module /usr/local/rvm/gems/ruby-2.2.3/gems/passenger-5.0.30/buildout/apache2/mod_passenger.so PassengerRoot /usr/local/rvm/gems/ruby-2.2.3/gems/passenger-5.0.30 PassengerDefaultRuby /usr/local/rvm/gems/ruby-2.2.3/wrappers/ruby [root@node1 ~]# vim /etc/httpd/conf.d/redmine.conf ServerName www.a.com # !!! Be sure to point DocumentRoot to ‘public’! DocumentRoot /var/www/html/redmine/public ErrorLog logs/redmine_error_log &lt;Directory /var/www/html/redmine/public&gt; Options Indexes ExecCGI FollowSymLinks Order allow,deny Allow from all # This relaxes Apache security settings. AllowOverride all # MultiViews must be turned off. Options -MultiViews # Uncomment this if you’re on Apache &gt;= 2.4: #Require all granted 安装redmine[root@node1 ~]# cd /var/www/html[root@node1 ~]# wget http://www.redmine.org/releases/redmine-3.1.1.tar.gz[root@node1 ~]# tar -zxvf redmine-3.1.1.tar.gz[root@node1 ~]# mv redmine-3.1.1 redmine[root@node1 ~]# cd /var/www/html/redmine/ [root@node1 ~]# vim Gemfile # 修改sourcesource ‘https://rubygems.org&#39; （注释掉）source’https://ruby.taobao.org&#39; [root@node1 ~]# cp config/configuration.yml.example config/configuration.yml[root@node1 ~]# cp config/database.yml.example config/database.yml[root@node1 ~]# vim config/database.yml # 修改数据连接production: adapter: mysql2 database: redmine host: localhost username: redmine password: “redmine” encoding: utf8 [root@node1 redmine]# gem install bundler # 注意是在网站根目录下执行[root@node1 redmine]# gem install rack-cache -v ‘1.4.2’[root@node1 redmine]# bundle install 为Rails生成cookies密钥[root@node1 redmine]# rake generate_secret_token 初始化redmine数据库表名[root@node1 redmine]# RAILS_ENV=production rake db:migrate[root@node1 redmine]# RAILS_ENV=production rake redmine:load_default_data 启动[root@node1 ~]# cd /var/www/html/redmine[root@node1 redmine]# mkdir /var/www/html/logs[root@node1 redmine]# bundle exec rails server webrick -e production -b 0.0.0.0 &amp;&gt;&gt; /var/www/html/logs/redmine.log &amp;访问地址：http://IP:3000 备份[root@node1 ~]# mysqldump -u root redmine &gt; /root/redmine.sql 恢复停止redmine， 步骤：ps -ef | grep rails，找到redmine的进程号，然后kill掉 恢复数据库[root@node1 ~]# mysql -u redmine -p redmine &lt; /root/redmine.sql 把10.10.2.120上的/backup/redmine-back/redmine_file目录下的所有文件拷贝到192.168.100.10中的/var/www/html/redmine/files目录[root@node1 ~]# scp -r root@10.10.2.120:/backup/redmine-back/redmine_file/* /var/www/html/redmine/files[root@node1 ~]# chmod -R 755 /var/www/html/redmine/files 启动redmine[root@node1 redmine]# bundle exec rails server webrick -e production -b 0.0.0.0 &amp;&gt;&gt; /var/www/html/logs/redmine.log &amp; 调整配置配置邮件发送，空格缩进必须如下，不然redmine无法启动[root@node1 ~]# vim /var/www/html/redmine/config/configuration.ymldefault: email_delivery: delivery_method: :smtp smtp_settings: openssl_verify_mode: ‘none’ address: mail.testin.cn port: 587 domain: testin.cn authentication: :login user_name: “project@testin.cn“ password: “m12345678” 重启redmine即可]]></content>
      <categories>
        <category>版本管理工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos6.4安装kvm]]></title>
    <url>%2F2017%2F03%2F20%2F%E8%99%9A%E6%8B%9F%E5%8C%96%2Fcentos6.4%E5%AE%89%E8%A3%85kvm%2F</url>
    <content type="text"><![CDATA[首先检查您的CPU是否支持硬件虚拟化 egrep ‘(vmx|svm)’ –color=always /proc/cpuinfo 应该显示一些东西，例如： [root@server1 ~]# egrep ‘(vmx|svm)’ –color=always /proc/cpuinfoflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt rdtscp lm 3dnowext 3dnow pni cx16 lahf_lm cmp_legacy svm extapic cr8_legacy misalignsseflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt rdtscp lm 3dnowext 3dnow pni cx16 lahf_lm cmp_legacy svm extapic cr8_legacy misalignsse[root@server1 ~]# 现在我们导入软件包的GPG密钥： rpm–import /etc/pki/rpm-gpg/RPM-GPG-KEY* yum install kvm libvirt python-virtinst qemu-kvm /etc/init.d/libvirtd start virsh -c qemu:///system list 导入镜像文件 xterm-253-1.el6.x86_64.rpm 之后xmanager开始kvm就可以了]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
  </entry>
</search>
