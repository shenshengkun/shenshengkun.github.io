<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[钉钉定时发送值班人员]]></title>
    <url>%2F2019%2F04%2F19%2Flinux%2F%E9%92%89%E9%92%89%E5%AE%9A%E6%97%B6%E5%8F%91%E9%80%81%E5%80%BC%E7%8F%AD%E4%BA%BA%E5%91%98%2F</url>
    <content type="text"><![CDATA[需求每天运维人员都需要去做些基础服务，就需要值班人员去轮班解决，现在需要写一个定时发送值班人员的脚本 前提需要自己在钉钉群，申请个机器人，申请过程这里不赘述了，下面是脚本 脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[root@Ops-script dingding]# mkdir /home/monitor/dingding[root@Ops-script dingding]# touch groupkey helpkey[root@Ops-script dingding]# cat send_dingding.sh #!/bin/bashgroupfiles=&quot;/home/monitor/dingding/groupkey&quot;helpfiles=&quot;/home/monitor/dingding/helpkey&quot;Date=`date +%Y-%m-%d\ %H:%M:%S`url=&quot;https://oapi.dingtalk.com/robot/send?access_token=c2123f81820fccfadfc47bbd629d26e7613ae49f1a053edc6e81f5864c550e30&quot;group=(&quot;a:xx;&quot; &quot;b:xx;&quot; &quot;c:xx;&quot;)opshelp=(&quot;a:xx;&quot; &quot;b:xx;&quot; &quot;c:xx;&quot;)groupkey=`sed -n &quot;1p&quot; $groupfiles`helpkeys=`awk &apos;NR==1&#123;print $1&#125;&apos; $helpfiles`helpkey=`awk &apos;NR==1&#123;print $2&#125;&apos; $helpfiles`# 每日值班人for crew in $&#123;group[@]&#125;;do if echo $crew | grep -q $groupkey ;then values=`echo $crew | awk -F&apos;:&apos; &apos;&#123;print $2&#125;&apos;` onduty_mess=&quot;今日运维值班人: [ $values ]&quot; fidone# 修改缓存文件内的运维值班人员 key , 使得下次人员自动更换if [ $groupkey == &quot;a&quot; ];then echo &quot;b&quot; &gt; $groupfileselif [ $groupkey == &quot;b&quot; ];then echo &quot;c&quot; &gt; $groupfileselif [ $groupkey == &quot;c&quot; ];then echo &quot;a&quot; &gt; $groupfilesfi# 修改缓存文件内的运维上线人员 key , 使得下次人员自动更换if [ $helpkeys == 7 ];then helpsum=1else helpsum=$(($helpkeys+1))fiif [ $helpsum == 1 ];then if [ $helpkey == &quot;a&quot; ];then echo &quot;$helpsum b&quot; &gt; $helpfiles elif [ $helpkey == &quot;b&quot; ];then echo &quot;$helpsum c&quot; &gt; $helpfiles elif [ $helpkey == &quot;c&quot; ];then echo &quot;$helpsum a&quot; &gt; $helpfiles fielse echo &quot;$helpsum $helpkey&quot; &gt; $helpfilesfi# 每周支持上线人for opsdit in $&#123;opshelp[@]&#125;;do if [ $helpsum == 1 ];then helpkey=`awk &apos;NR==1&#123;print $2&#125;&apos; $helpfiles` fi if echo $opsdit | grep -q $helpkey ;then helpvalues=`echo $opsdit | awk -F&apos;:&apos; &apos;&#123;print $2&#125;&apos;` help_mess=&quot;本周版本上线运维支持: [ $helpvalues ]&quot; fidonecurl -XPOST -s -L -H &quot;Content-Type:application/json&quot; -H &quot;charset:utf-8&quot; $url -d &quot; &#123; \&quot;msgtype\&quot;: \&quot;text\&quot;, \&quot;text\&quot;: &#123; \&quot;content\&quot;: \&quot;大家好~\n$onduty_mess\n$help_mess\&quot; &#125; &#125;&quot;]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker存储驱动]]></title>
    <url>%2F2019%2F04%2F18%2F%E8%99%9A%E6%8B%9F%E5%8C%96%2Fdocker%E5%AD%98%E5%82%A8%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[摘要Docker最开始采用AUFS作为文件系统，也得益于AUFS分层的概念，实现了多个Container可以共享同一个image。但由于AUFS未并入Linux内核，且只支持Ubuntu，考虑到兼容性问题，在Docker 0.7版本中引入了存储驱动。 目前，Docker支持的存储驱动：aufs，devicemapper，btrfs，zfs，overlay和overlay2。 就如Docker官网上说的，没有单一的驱动适合所有的应用场景，要根据不同的场景选择合适的存储驱动，才能有效的提高Docker的性能。如何选择适合的存储驱动，要先了解存储驱动原理才能更好的判断 写时复制（CoW）所有驱动都用到的技术——写时复制（CoW）。CoW就是copy-on-write，表示只在需要写时才去复制，这个是针对已有文件的修改场景。比如基于一个image启动多个Container，如果为每个Container都去分配一个image一样的文件系统，那么将会占用大量的磁盘空间。而CoW技术可以让所有的容器共享image的文件系统，所有数据都从image中读取，只有当要对文件进行写操作时，才从image里把要写的文件复制到自己的文件系统进行修改。所以无论有多少个容器共享同一个image，所做的写操作都是对从image中复制到自己的文件系统中的复本上进行，并不会修改image的源文件，且多个容器操作同一个文件，会在每个容器的文件系统里生成一个复本，每个容器修改的都是自己的复本，相互隔离，相互不影响。使用CoW可以有效的提高磁盘的利用率。 用时分配（allocate-on-demand）而写时分配是用在原本没有这个文件的场景，只有在要新写入一个文件时才分配空间，这样可以提高存储资源的利用率。比如启动一个容器，并不会为这个容器预分配一些磁盘空间，而是当有新文件写入时，才按需分配新空间。 AUFSAUFS（AnotherUnionFS）是一种Union FS，是文件级的存储驱动。AUFS能透明覆盖一或多个现有文件系统的层状文件系统，把多层合并成文件系统的单层表示。简单来说就是支持将不同目录挂载到同一个虚拟文件系统下的文件系统。这种文件系统可以一层一层地叠加修改文件。无论底下有多少层都是只读的，只有最上层的文件系统是可写的。当需要修改一个文件时，AUFS创建该文件的一个副本，使用CoW将文件从只读层复制到可写层进行修改，结果也保存在可写层。在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示： OverlayOverlay是Linux内核3.18后支持的，也是一种Union FS，和AUFS的多层不同的是Overlay只有两层：一个upper文件系统和一个lower文件系统，分别代表Docker的镜像层和容器层。当需要修改一个文件时，使用CoW将文件从只读的lower复制到可写的upper进行修改，结果也保存在upper层。在Docker中，底下的只读层就是image，可写层就是Container。结构如下图所示： Device mapperDevice mapper是Linux内核2.6.9后支持的，提供的一种从逻辑设备到物理设备的映射框架机制，在该机制下，用户可以很方便的根据自己的需要制定实现存储资源的管理策略。前面讲的AUFS和OverlayFS都是文件级存储，而Device mapper是块级存储，所有的操作都是直接对块进行操作，而不是文件。Device mapper驱动会先在块设备上创建一个资源池，然后在资源池上创建一个带有文件系统的基本设备，所有镜像都是这个基本设备的快照，而容器则是镜像的快照。所以在容器里看到文件系统是资源池上基本设备的文件系统的快照，并不有为容器分配空间。当要写入一个新文件时，在容器的镜像内为其分配新的块并写入数据，这个叫用时分配。当要修改已有文件时，再使用CoW为容器快照分配块空间，将要修改的数据复制到在容器快照中新的块里再进行修改。Device mapper 驱动默认会创建一个100G的文件包含镜像和容器。每一个容器被限制在10G大小的卷内，可以自己配置调整。结构如下图所示： BtrfsBtrfs被称为下一代写时复制文件系统，并入Linux内核，也是文件级级存储，但可以像Device mapper一直接操作底层设备。Btrfs把文件系统的一部分配置为一个完整的子文件系统，称之为subvolume 。那么采用 subvolume，一个大的文件系统可以被划分为多个子文件系统，这些子文件系统共享底层的设备空间，在需要磁盘空间时便从底层设备中分配，类似应用程序调用 malloc()分配内存一样。为了灵活利用设备空间，Btrfs 将磁盘空间划分为多个chunk 。每个chunk可以使用不同的磁盘空间分配策略。比如某些chunk只存放metadata，某些chunk只存放数据。这种模型有很多优点，比如Btrfs支持动态添加设备。用户在系统中增加新的磁盘之后，可以使用Btrfs的命令将该设备添加到文件系统中。Btrfs把一个大的文件系统当成一个资源池，配置成多个完整的子文件系统，还可以往资源池里加新的子文件系统，而基础镜像则是子文件系统的快照，每个子镜像和容器都有自己的快照，这些快照则都是subvolume的快照。 当写入一个新文件时，为在容器的快照里为其分配一个新的数据块，文件写在这个空间里，这个叫用时分配。而当要修改已有文件时，使用CoW复制分配一个新的原始数据和快照，在这个新分配的空间变更数据，变结束再更新相关的数据结构指向新子文件系统和快照，原来的原始数据和快照没有指针指向，被覆盖。 ZFSZFS 文件系统是一个革命性的全新的文件系统，它从根本上改变了文件系统的管理方式，ZFS 完全抛弃了“卷管理”，不再创建虚拟的卷，而是把所有设备集中到一个存储池中来进行管理，用“存储池”的概念来管理物理存储空间。过去，文件系统都是构建在物理设备之上的。为了管理这些物理设备，并为数据提供冗余，“卷管理”的概念提供了一个单设备的映像。而ZFS创建在虚拟的，被称为“zpools”的存储池之上。每个存储池由若干虚拟设备（virtual devices，vdevs）组成。这些虚拟设备可以是原始磁盘，也可能是一个RAID1镜像设备，或是非标准RAID等级的多磁盘组。于是zpool上的文件系统可以使用这些虚拟设备的总存储容量。 下面看一下在Docker里ZFS的使用。首先从zpool里分配一个ZFS文件系统给镜像的基础层，而其他镜像层则是这个ZFS文件系统快照的克隆，快照是只读的，而克隆是可写的，当容器启动时则在镜像的最顶层生成一个可写层。如下图所示： 当要写一个新文件时，使用按需分配，一个新的数据快从zpool里生成，新的数据写入这个块，而这个新空间存于容器（ZFS的克隆）里。当要修改一个已存在的文件时，使用写时复制，分配一个新空间并把原始数据复制到新空间完成修改。 overlay2OverlayFS将Linux主机上的两个单独目录分层，并将它们显示为一个目录。这些目录称为层，统一过程称为联合安装。OverlayFS指向一个upper文件系统和一个lower文件系统，分别代表Docker的镜像层和容器层。用统一视图将整合的目录公开。 该overlay2驱动程序原生支持多达128个较低的OverlayFS层。此功能为与层相关的Docker命令（如docker build和docker commit）提供了更好的性能，并且大量减少了inode的消耗。 对比 存储驱动 简介 优点 缺点 存储级别 场景 aufs 最古老的联合文件系统，没有被内核收录，只支持ubuntu 允许容器共享可执行文件和共享内存，历史悠久，使用广泛 会导致一些严重的内核崩溃，多层，在CoW时如果文件大且在低层会慢一些 文件级存储 大并发少IO devicemapper 自动创建的稀疏文件的loop挂载后，自动创建块设备 精简配置和写时复制（CoW）快照技术，只复制修改的块 不支持共享存储，多个容器读同一个文件复制多份，容器启停可能会有磁盘溢出 块级存储 IO密集场景 btrfs 和devicemapper一样操作底层设备 非常快，支持动态添加设备 设备之间不共享可执行内存 文件级块存储 不适合高密度容器的paas平台 zfs 支持多个容器共享一个缓存块，适合大内存场景 CoW使碎片化问题更严重，文件在磁盘上物理地址不连续，顺序读性能差 所有设备集中到一个共享池里面进行管理 Paas平台和高密度场景 overlay 联合文件系统，内核版本3.18.0开始合并到内核中，只有两层 非常快速的联合文件系统。还支持页面缓存共享，这意味着访问同一文件的多个容器可以共享单个页面缓存条目（或条目），如aufs一样高效 会导致过多的inode消耗，不管修改内容大小都会复制整个文件，修改大文件消耗时间长 文件级存储 大并发少IO overlay2 内核版本4.0有附加功能，避免过多的inode消耗 文件级存储 大并发少IO AUFS VS OverlayAUFS和Overlay都是联合文件系统，但AUFS有多层，而Overlay只有两层，所以在做写时复制操作时，如果文件比较大且存在比较低的层，则AUSF可能会慢一些。而且Overlay并入了linux kernel mainline，AUFS没有，所以可能会比AUFS快。但Overlay还太年轻，要谨慎在生产使用。而AUFS做为docker的第一个存储驱动，已经有很长的历史，比较的稳定，且在大量的生产中实践过，有较强的社区支持。目前开源的DC/OS指定使用Overlay。 Overlay VS Device mapperOverlay是文件级存储，Device mapper是块级存储，当文件特别大而修改的内容很小，Overlay不管修改的内容大小都会复制整个文件，对大文件进行修改显示要比小文件要消耗更多的时间，而块级无论是大文件还是小文件都只复制需要修改的块，并不是整个文件，在这种场景下，显然device mapper要快一些。因为块级的是直接访问逻辑盘，适合IO密集的场景。而对于程序内部复杂，大并发但少IO的场景，Overlay的性能相对要强一些。 Device mapper VS Btrfs Driver VS ZFSDevice mapper和Btrfs都是直接对块操作，都不支持共享存储，表示当有多个容器读同一个文件时，需要生活多个复本，所以这种存储驱动不适合在高密度容器的PaaS平台上使用。而且在很多容器启停的情况下可能会导致磁盘溢出，造成主机不能工作。Device mapper不建议在生产使用。Btrfs在docker build可以很高效。 ZFS最初是为拥有大量内存的Salaris服务器设计的，所在在使用时对内存会有影响，适合内存大的环境。ZFS的COW使碎片化问题更加严重，对于顺序写生成的大文件，如果以后随机的对其中的一部分进行了更改，那么这个文件在硬盘上的物理地址就变得不再连续，未来的顺序读会变得性能比较差。ZFS支持多个容器共享一个缓存块，适合PaaS和高密度的用户场景。 IO性能对比 测试工具：IOzone（是一个文件系统的benchmark工具，可以测试不同的操作系统中文件系统的读写性能） 测试场景：从4K到1G文件的顺序和随机IO性能 测试方法：基于不同的存储驱动启动容器，在容器内安装IOzone，执行命令： 1./iozone -a -n 4k -g 1g -i 0 -i 1 -i 2 -f /root/test.rar -Rb ./iozone.xls 测试项的定义和解释 Write：测试向一个新文件写入的性能。 Re-write：测试向一个已存在的文件写入的性能。 Read：测试读一个已存在的文件的性能。 Re-Read：测试读一个最近读过的文件的性能。 Random Read：测试读一个文件中的随机偏移量的性能。 Random Write：测试写一个文件中的随机偏移量的性能。 通过以上的性能数据可以看到：AUFS在读的方面性能相比Overlay要差一些，但在写的方面性能比Overlay要好。device mapper在512M以上文件的读写性能都非常的差，但在512M以下的文件读写性能都比较好。btrfs在512M以上的文件读写性能都非常好，但在512M以下的文件读写性能相比其他的存储驱动都比较差。ZFS整体的读写性能相比其他的存储驱动都要差一些。 简单的测试了一些数据，对测试出来的数据原理还需要进一步的解析。 参数devicemapper12345678&#123; &quot;storage-driver&quot;: &quot;devicemapper&quot;, &quot;storage-opts&quot;: [ &quot;dm.thinpooldev=/dev/mapper/thin-pool&quot;, &quot;dm.use_deferred_deletion=true&quot;, &quot;dm.use_deferred_removal=true&quot; ]&#125; overlay2overlay2需要使用4.0以上版本的内核，如果使用的是RHEL或CentOS，需要3.10.0-514以上版本的内核 12345# 查看是否开启overlaylsmod |grep over# 开启overlay支持modprobe overlay 配置 1234567&#123; &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;storage-opts&quot;: [ &quot;overlay2.override_kernel_check=true&quot; #&quot;overlay2.size=1G&quot;, # xfs文件系统 ]&#125;]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes节点资源耗尽状态的处理]]></title>
    <url>%2F2019%2F04%2F18%2Fk8s%2FKubernetes%E8%8A%82%E7%82%B9%E8%B5%84%E6%BA%90%E8%80%97%E5%B0%BD%E7%8A%B6%E6%80%81%E7%9A%84%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[最近发现测试环境的k8s集群，总有node利用不上，pod漂移过去之后，启动不了，故仔细排查了一下缘由！ 问题现象12345678[root@master35 scripts]# ./list_pod.sh | grep imisimis-866d46c464-nvz4b 0/1 ContainerCreating 0 3m &lt;none&gt; node149发现有的pod无法启动，刚开始describe查了下原因，看到，一直在拉镜像状态中，但是3分钟了，也不至于镜像拉不下来啊！查看了下node149的状态，发现Warning: “EvictionThresholdMet Attempting to reclaim nodefs”发现大概应该是由于磁盘原因造成的，也可以看下kubelet日志，也会报这个类似的错误 原因分析12345678[root@node149 ~]# df -hFilesystem Size Used Avail Use% Mounted on/dev/mapper/cl-root 36G 30G 6G 86% /devtmpfs 7.8G 0 7.8G 0% /devtmpfs 7.8G 0 7.8G 0% /dev/shmtmpfs 7.8G 9.3M 7.8G 1% /runtmpfs 7.8G 0 7.8G 0% /sys/fs/cgroup/dev/sda1 1014M 186M 829M 19% /boot 1由于这是测试环境，所以docker的目录，默认在/var/lib/docker，没有单独挂载别的目录，这样的话，也没加定时任务清理磁盘，/ 磁盘就会越来越满，现在看是用了86% 由于某些原因，我们的那个portal pod必须运行于该node上（通过nodeSelector选定node的方式）。在无法扩充根分区size的情况下，为了临时恢复pod运行，我们只能进一步“压榨”node了。于是我们的思路是：通过调整node的eviction threshold值来让node恢复healthy。 解决方案每个node上的kubelet都负责定期采集资源占用数据，并与预设的 threshold值进行比对，如果超过 threshold值，kubelet就会尝试杀掉一些Pod以回收相关资源，对Node进行保护。kubelet关注的资源指标threshold大约有如下几种： 12345- memory.available- nodefs.available- nodefs.inodesFree- imagefs.available- imagefs.inodesFree 每种threshold又分为eviction-soft和eviction-hard两组值。soft和hard的区别在于前者在到达threshold值时会给pod一段时间优雅退出，而后者则崇尚“暴力”，直接杀掉pod，没有任何优雅退出的机会。这里还要提一下nodefs和imagefs的区别： 12nodefs: 指node自身的存储，存储daemon的运行日志等，一般指root分区/；imagefs: 指docker daemon用于存储image和容器可写层(writable layer)的磁盘； 解决步骤我们需要为kubelet重新设定nodefs.available的threshold值。怎么做呢？ kubelet是运行于每个kubernetes node上的daemon，它在system boot时由systemd拉起: 123root@master35 ~# ps -ef|grep kubeletroot 5718 5695 0 16:38 pts/3 00:00:00 grep --color=auto kubeletroot 13640 1 4 10:25 ? 00:17:25 /usr/bin/kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --require-kubeconfig=true --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --cluster-dns=10.96.0.10 --cluster-domain=cluster.local --authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt --cadvisor-port=0 查看一下kubelet service的状态： 123456789101112131415[root@master35 scripts]# systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Thu 2018-07-19 21:04:35 CST; 8 months 29 days ago Docs: http://kubernetes.io/docs/ Main PID: 1921 (kubelet) Tasks: 19 Memory: 54.9M CGroup: /system.slice/kubelet.service └─1921 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --pod-manifest-path=...Apr 14 09:26:16 master35 kubelet[1921]: W0414 09:26:16.673359 1921 reflector.go:341] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: watch o...(56737582)Apr 15 06:36:48 master35 kubelet[1921]: W0415 06:36:48.938194 1921 reflector.go:341] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: watch o...(56940044) 我们定义一个新的Environment var，比如就叫：KUBELET_EVICTION_POLICY_ARGS 在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf 123Environment=&quot;KUBELET_EVICTION_POLICY_ARGS=--eviction-hard=nodefs.available&lt;5%&quot;ExecStart=ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_SYSTEM_PODS_ARGS $KUBELET_NETWORK_ARGS $KUBELET_DNS_ARGS $KUBELET_AUTHZ_ARGS $KUBELET_CADVISOR_ARGS $KUBELET_EXTRA_ARGS $KUBELET_EVICTION_POLICY_ARGS 这样控制，node的磁盘策略为&lt;5%的硬盘就可以用，不像之前默认的15%就用不了了！]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[wordpress快速安装]]></title>
    <url>%2F2019%2F04%2F16%2F%E4%B8%AD%E9%97%B4%E4%BB%B6%2Fwordpress%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[yum安装lnmp环境安装前准备123456789# 配置阿里云 yum 仓库$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo$ wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo$ yum clean all$ yum makecache# 配置时间同步$ vim /etc/crontab00 00 * * * root /sbin/ntpdate ntp.aliyun.com &amp;&gt;/dev/null 配置 nginx repo1234567891011121314$ vim /etc/yum.repos.d/nginx.repo[nginx-stable]name=nginx stable repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=1enabled=1gpgkey=https://nginx.org/keys/nginx_signing.key[nginx-mainline]name=nginx mainline repobaseurl=http://nginx.org/packages/mainline/centos/$releasever/$basearch/gpgcheck=1enabled=0gpgkey=https://nginx.org/keys/nginx_signing.key yum安装lnmp1$ yum -y install nginx mariadb-server php php-bcmath php-fpm php-gd php-json php-mbstring php-mcrypt php-mysqlnd php-opcache php-pdo php-pdo_dblib php-pgsql php-recode php-snmp php-soap php-xml php-pecl-zip 启动php和mariadb123456# 启动 PHP-FPM$ systemctl enable php-fpm$ systemctl start php-fpm# 启动 mariadb$ systemctl enable mariadb.service$ systemctl start mariadb.service 创建 wordpress 数据库12345678# 连接数据库，默认密码为空mysql -uroot -p# 创建wordpress数据库名为 wpcreate database wp;# 创建数据库用户，用户名: blog 密码：123456grant all privileges on wp.* to &apos;blog&apos;@&apos;127.0.0.1&apos; identified by &apos;123456&apos;;# 刷新授权flush privileges; 配置nginx虚拟主机123456789101112131415161718192021$ vim /etc/nginx/conf.d/blog.confserver &#123; listen 80; server_name 10.100.4.169; index index.html index.php; # 访问日志目录 access_log /var/log/nginx/blog_access.log main; # 网站根目录 root /data/www; location / &#123; root /data/www; &#125; location ~ \.php$ &#123; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; &#125;&#125; 配置wordpress下载最新版wordpress1$ wget https://cn.wordpress.org/latest-zh_CN.tar.gz 配置wordpress连接数据库1234567891011121314151617181920212223$ tar xf latest-zh_CN.tar.gz$ mv wordpress/ /data/www$ cd /data/www/$ cp wp-config-sample.php wp-config.php$ vim wp-config.php// ** MySQL 设置 - 具体信息来自您正在使用的主机 ** ///** WordPress数据库的名称 */define(&apos;DB_NAME&apos;, &apos;wp&apos;);/** MySQL数据库用户名 */define(&apos;DB_USER&apos;, &apos;blog&apos;);/** MySQL数据库密码 */define(&apos;DB_PASSWORD&apos;, &apos;123456&apos;);/** MySQL主机 */define(&apos;DB_HOST&apos;, &apos;127.0.0.1&apos;);/** 创建数据表时默认的文字编码 */define(&apos;DB_CHARSET&apos;, &apos;utf8&apos;);/** 数据库整理类型。如不确定请勿更改 */define(&apos;DB_COLLATE&apos;, &apos;&apos;); 启动 nginx123$ systemctl enable nginx$ systemctl start nginx$ ps -ef|grep nginx 访问wordpressnginx 启动后我们就可以在浏览器通过 IP 地址访问 WordPress 了，首先会让我们给博客起个名字，名设置管理员的账号密码，点击安装 WordPress 就完成了。]]></content>
      <categories>
        <category>中间件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[多级缓存]]></title>
    <url>%2F2019%2F04%2F15%2F%E4%B8%AD%E9%97%B4%E4%BB%B6%2F%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[每一级缓存的意义时效性高的数据：采取DB和redis缓存双写方案 时效性不高的数据：采取nginx本地缓存+redis分布式缓存+tomcat堆缓存的多级缓存架构 a: nginx本地缓存，抗的是热数据的高并发访问。利用nginx本地缓存，将热数据锁定在nginx的本地缓存内，那么对这些热数据的大量访问，就直接走nginx就可以，不需要走后续的各种网络开销了。 b: redis分布式大规模缓存，抗的是很高的离散访问，支撑海量的数据，高并发的访问，高可用的服务。最完整的数据和缓存。 c: tomcat jvm堆内存缓存，主要是抗redis大规模灾难的，如果redis出现了大规模的宕机，导致nginx大量流量直接涌入数据生产服务，那么最后的tomcat堆内存缓存至少可以再抗一下，不至于让数据库直接裸奔， 同时tomcat jvm堆内存缓存，也可以抗住redis没有cache住的最后那少量的部分缓存。 RedisRedis Cluster通过master的水平扩容，来横向扩展读写吞吐量，还有支撑更多的海量数据 redis cluster 高可用性：redis cluster 提供主备切换。slave做master的热备，一旦master故障。slave提升为master，对外提供服务，保证集群的高可用性。并且，当master恢复后，会作为 slave加入到集群中。 redis cluster水平扩容master的水平扩容，来横向扩展读写吞吐量，还有支撑更多的海量数据 slave 自动迁移为redis cluster 添加冗余slave redis性能（需根据机器配置测试） redis单机，读吞吐是5w/s，写吞吐2w/s 扩展redis更多master，那么如果有5台master，不就读吞吐可以达到总量25w/s QPS，写可以达到10w/s QPS redis单机，内存，6G-8G，内存不易过大fork类操作的时候很耗时，会导致请求延时的问题。扩容到5台master，能支撑的总的缓存数据量就是30G Cache Aside模式12（1）读的时候，先读缓存，缓存没有的话，那么就读数据库，然后取出数据后放入缓存，同时返回响应（2）更新的时候，先删除缓存，然后再更新数据库 DB和缓存双写不一致问题以及解决方案 缓存不一致场景一： 1234 解决思路：先删除缓存，再修改数据库，如果删除缓存成功了，如果修改数据库失败了，那么数据库中是旧数据，缓存中是空的，那么数据不会不一致因为读的时候缓存没有，则读数据库中旧数据，然后更新到缓存中 缓存不一致场景二： 12345678910111213141516 解决思路： a：更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的队列中 b:读取数据的时候，如果发现数据不在缓存中，那么将重新读取数据+更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中 c:一个队列对应一个工作线程 d:每个工作线程串行拿到对应的操作，然后一条一条的执行这样的话，一个数据变更的操作，先执行，删除缓存，然后再去更新数据库，但是还没完成更新此时如果一个读请求过来，读到了空的缓存，那么可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成 e: 多个更新缓存请求处理：这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可待那个队列对应的工作线程完成了上一个操作的数据库的修改之后，才会去执行下一个操作，也就是缓存更新的操作，此时会从数据库中读取最新的值，然后写入缓存中如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回; 如果请求等待的时间超过一定时长，那么这一次直接从数据库中读取当前的旧值 f:多个读请求，进行读请求过滤：对一个商品的库存的数据库更新操作已经在内存队列中了然后对这个商品的库存的读取操作，要求读取数据库的库存数据，然后更新到缓存中，多个读这多个读，其实只要有一个读请求操作压到队列里就可以了其他的读操作，全部都wait那个读请求的操作，刷新缓存，就可以读到缓存中的最新数据了如果读请求发现redis缓存中没有数据，就会发送读请求给库存服务，但是此时缓存中为空，可能是因为写请求先删除了缓存，也可能是数据库里压根儿没这条数据如果是数据库中压根儿没这条数据的场景，那么就不应该将读请求操作给压入队列中，而是直接返回空就可以了 大value缓存的全量更新效率低下问题缓存数据的维度化拆分 缓存数据生产服务工作流程 （1）监听多个kafka topic，每个kafka topic对应一个服务（简化一下，监听一个kafka topic） （2）如果一个服务发生了数据变更，那么就发送一个消息到kafka topic中 （3）缓存数据生产服务监听到了消息以后，就发送请求到对应的服务中调用接口以及拉取数据，此时是从mysql中查询的 （4）缓存数据生产服务拉取到了数据之后，会将数据在本地缓存中写入一份，就是ehcache中 ​ 同时会将数据在redis中写入一份 缓存并发重建冲突解决方案 重建缓存：比如数据在所有的缓存中都不存在了（LRU算法弄掉了），就需要重新查询数据写入缓存，重建缓存 123456缓存重建存在的问题一：缓存数据生产服务在多个机器节点上部署了多个实例 若没有缓存数据。12:00的时候发来一个读请求 12:01发来一个读请求（此时12:00的读请求由于网络延迟还未执行完）。12:01请求比12:00的请求执行速度快。更新了生产服务的数据并将数据写入缓存。写完后。12:00的请求将数据写入了缓存。那么此时生产服务的最新数据是12：01的，但是缓存中是服务数据是12:00的。数据不一致。解决思路：对请求的数据ID 进行hash，让对同一个数据的请求落在同一个服务实例上 123456789缓存重建存在的问题二：生产服务发送的变更消息到kafka。由于问题一解决方案中的hash算法与kafka分区策略不一致。数据变更的消息所到的缓存服务实例，跟请求分发到的那个缓存服务实例也许就不在一台机器上了 （1）变更缓存重建以及空缓存请求重建，更新redis之前，都需要先获取对应商品id的分布式锁（2）拿到分布式锁之后，需要根据时间版本去比较一下，如果自己的版本新于redis中的版本，那么就更新，否则就不更新（3）如果拿不到分布式锁，那么就等待，不断轮询等待，直到自己获取到分布式的锁 缓存雪崩问题12345678910111213缓存雪崩产生场景：1、redis集群彻底崩溃2、缓存服务大量对redis的请求hang住，占用资源3、缓存服务大量的请求打到源头服务去查询mysql，直接打死mysql4、源头服务因为mysql被打死也崩溃，对源服务的请求也hang住，占用资源5、缓存服务大量的资源全部耗费在访问redis和源服务无果，最后自己被拖死，无法提供服务6、nginx无法访问缓存服务，redis和源服务，只能基于本地缓存提供服务，但是缓存过期后，没有数据提供解决思路 1、对redis访问做资源隔离 2、若redis集群崩溃，对redis进行熔断 3、对源服务的访问做限流 4、限流失败后采用stubbed fallback降级机制 缓存穿透问题每次如果从生产查询到的数据是空，就说明这个数据根本就不存在 那么如果这个数据不存在的话，我们不要不往redis和ehcache等缓存中写入数据，我们呢，给写入一个空的数据，比如说空的productInfo的json串 因为我们有一个异步监听数据变更的机制在里面，也就是说，如果数据变更的话，某个数据本来是没有的，可能会导致缓存穿透，所以我们给了个空数据 但是现在这个数据有了，我们接收到这个变更的消息过后，就可以将数据再次从生产服务中查询出来 然后设置到各级缓存中去了 缓存失效问题设置随机的缓存失效时间]]></content>
      <categories>
        <category>中间件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[fabric分布式部署]]></title>
    <url>%2F2019%2F04%2F08%2F%E5%8C%BA%E5%9D%97%E9%93%BE%2Ffabric%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Kafka模式简介Hyperledger Fabric采用kafka方式实现排序（orderer）服务的集群，kafka模块被认为是半中心化结构。顺便提一下，去中心化的BFT（拜占庭容错）排序（orderer）服务集群方式目前还在开发，还没有规定发布时间，将在1.x周期内发布，可以关注跟踪FAB-33的更新。 Kafka模式由排序（orderer）服务、kafka集群和zookeeper集群组成。每个排序(orderer)服务相互之间不通信，只与kafka集群通信，kafka集群与zookeeper相互连接。 Fabric网络中的各节点（Peer）收到客户端发送的交易请求时，把交易信息发送给与其连接的排序（orderer）服务，交由排序（orderer）服务集群进行排序处理。 配置 orderer1.example.com,kafka1,zookeeper1 192.168.3.98 orderer1.example.com,kafka1,zookeeper1 192.168.3.97 orderer1.example.com,kafka1,zookeeper1 192.168.3.94 peer0.org1.example.com 192.168.10.174 peer1.org1.example.com 192.168.10.173 peer0.org2.example.com 192.168.3.93 安装在六台机器上安装依赖工具docker、go、fabric源码 docker就不多说了，17,03以上就可以，go可以yum安装 123456789101112131415161718192021222324252627安装fabric源码:git clone https://github.com/hyperledger/fabric.gitcd fabricgit checkout v1.4.0拉镜像：# mkdir -p /etc/docker# tee /etc/docker/daemon.json &lt;&lt;-&apos;EOF&apos;&#123;&quot;registry-mirrors&quot;: [&quot;https://8w1wqmsz.mirror.aliyuncs.com&quot;]&#125;EOF# systemctl daemon-reload# systemctl restart docker# docker pull hyperledger/fabric-peer:latest# docker pull hyperledger/fabric-orderer:latest# docker pull hyperledger/fabric-tools:latest# docker pull hyperledger/fabric-ccenv:latest# docker pull hyperledger/fabric-baseos:latest# docker pull hyperledger/fabric-kafka:latest# docker pull hyperledger/fabric-zookeeper:latest# docker pull hyperledger/fabric-couchdb:latest# docker pull hyperledger/fabric-ca:latest 部署创建创世快123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339# cd $GOPATH/src/github.com/hyperledger/fabric# mkdir kafkapeer# cd kafkapeer# cat configtx.yaml# Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#---################################################################################## Section: Organizations## - This section defines the different organizational identities which will# be referenced later in the configuration.#################################################################################Organizations: # SampleOrg defines an MSP using the sampleconfig. It should never be used # in production but may be used as a template for other definitions - &amp;OrdererOrg # DefaultOrg defines the organization which is used in the sampleconfig # of the fabric.git development environment Name: OrdererOrg # ID to load the MSP definition as ID: OrdererMSP # MSPDir is the filesystem path which contains the MSP configuration MSPDir: crypto-config/ordererOrganizations/example.com/msp # Policies defines the set of policies at this level of the config tree # For organization policies, their canonical path is usually # /Channel/&lt;Application|Orderer&gt;/&lt;OrgName&gt;/&lt;PolicyName&gt; Policies: Readers: Type: Signature Rule: &quot;OR(&apos;OrdererMSP.member&apos;)&quot; Writers: Type: Signature Rule: &quot;OR(&apos;OrdererMSP.member&apos;)&quot; Admins: Type: Signature Rule: &quot;OR(&apos;OrdererMSP.admin&apos;)&quot; - &amp;Org1 # DefaultOrg defines the organization which is used in the sampleconfig # of the fabric.git development environment Name: Org1MSP # ID to load the MSP definition as ID: Org1MSP MSPDir: crypto-config/peerOrganizations/org1.example.com/msp # Policies defines the set of policies at this level of the config tree # For organization policies, their canonical path is usually # /Channel/&lt;Application|Orderer&gt;/&lt;OrgName&gt;/&lt;PolicyName&gt; Policies: Readers: Type: Signature Rule: &quot;OR(&apos;Org1MSP.admin&apos;, &apos;Org1MSP.peer&apos;, &apos;Org1MSP.client&apos;)&quot; Writers: Type: Signature Rule: &quot;OR(&apos;Org1MSP.admin&apos;, &apos;Org1MSP.client&apos;)&quot; Admins: Type: Signature Rule: &quot;OR(&apos;Org1MSP.admin&apos;)&quot; AnchorPeers: # AnchorPeers defines the location of peers which can be used # for cross org gossip communication. Note, this value is only # encoded in the genesis block in the Application section context - Host: peer0.org1.example.com Port: 7051 - &amp;Org2 # DefaultOrg defines the organization which is used in the sampleconfig # of the fabric.git development environment Name: Org2MSP # ID to load the MSP definition as ID: Org2MSP MSPDir: crypto-config/peerOrganizations/org2.example.com/msp # Policies defines the set of policies at this level of the config tree # For organization policies, their canonical path is usually # /Channel/&lt;Application|Orderer&gt;/&lt;OrgName&gt;/&lt;PolicyName&gt; Policies: Readers: Type: Signature Rule: &quot;OR(&apos;Org2MSP.admin&apos;, &apos;Org2MSP.peer&apos;, &apos;Org2MSP.client&apos;)&quot; Writers: Type: Signature Rule: &quot;OR(&apos;Org2MSP.admin&apos;, &apos;Org2MSP.client&apos;)&quot; Admins: Type: Signature Rule: &quot;OR(&apos;Org2MSP.admin&apos;)&quot; AnchorPeers: # AnchorPeers defines the location of peers which can be used # for cross org gossip communication. Note, this value is only # encoded in the genesis block in the Application section context - Host: peer0.org2.example.com Port: 7051################################################################################## SECTION: Capabilities## - This section defines the capabilities of fabric network. This is a new# concept as of v1.1.0 and should not be utilized in mixed networks with# v1.0.x peers and orderers. Capabilities define features which must be# present in a fabric binary for that binary to safely participate in the# fabric network. For instance, if a new MSP type is added, newer binaries# might recognize and validate the signatures from this type, while older# binaries without this support would be unable to validate those# transactions. This could lead to different versions of the fabric binaries# having different world states. Instead, defining a capability for a channel# informs those binaries without this capability that they must cease# processing transactions until they have been upgraded. For v1.0.x if any# capabilities are defined (including a map with all capabilities turned off)# then the v1.0.x peer will deliberately crash.#################################################################################Capabilities: # Channel capabilities apply to both the orderers and the peers and must be # supported by both. Set the value of the capability to true to require it. Global: &amp;ChannelCapabilities # V1.1 for Global is a catchall flag for behavior which has been # determined to be desired for all orderers and peers running v1.0.x, # but the modification of which would cause incompatibilities. Users # should leave this flag set to true. V1_1: true # Orderer capabilities apply only to the orderers, and may be safely # manipulated without concern for upgrading peers. Set the value of the # capability to true to require it. Orderer: &amp;OrdererCapabilities # V1.1 for Order is a catchall flag for behavior which has been # determined to be desired for all orderers running v1.0.x, but the # modification of which would cause incompatibilities. Users should # leave this flag set to true. V1_1: true # Application capabilities apply only to the peer network, and may be safely # manipulated without concern for upgrading orderers. Set the value of the # capability to true to require it. Application: &amp;ApplicationCapabilities # V1.1 for Application is a catchall flag for behavior which has been # determined to be desired for all peers running v1.0.x, but the # modification of which would cause incompatibilities. Users should # leave this flag set to true. V1_2: true################################################################################## SECTION: Application## - This section defines the values to encode into a config transaction or# genesis block for application related parameters#################################################################################Application: &amp;ApplicationDefaults # Organizations is the list of orgs which are defined as participants on # the application side of the network Organizations: # Policies defines the set of policies at this level of the config tree # For Application policies, their canonical path is # /Channel/Application/&lt;PolicyName&gt; Policies: Readers: Type: ImplicitMeta Rule: &quot;ANY Readers&quot; Writers: Type: ImplicitMeta Rule: &quot;ANY Writers&quot; Admins: Type: ImplicitMeta Rule: &quot;MAJORITY Admins&quot; # Capabilities describes the application level capabilities, see the # dedicated Capabilities section elsewhere in this file for a full # description Capabilities: &lt;&lt;: *ApplicationCapabilities################################################################################## SECTION: Orderer## - This section defines the values to encode into a config transaction or# genesis block for orderer related parameters#################################################################################Orderer: &amp;OrdererDefaults # Orderer Type: The orderer implementation to start # Available types are &quot;solo&quot; and &quot;kafka&quot; OrdererType: kafka Addresses: - orderer0.example.com:7050 - orderer1.example.com:7050 - orderer2.example.com:7050 # Batch Timeout: The amount of time to wait before creating a batch BatchTimeout: 2s # Batch Size: Controls the number of messages batched into a block BatchSize: # Max Message Count: The maximum number of messages to permit in a batch MaxMessageCount: 10 # Absolute Max Bytes: The absolute maximum number of bytes allowed for # the serialized messages in a batch. AbsoluteMaxBytes: 98 MB # Preferred Max Bytes: The preferred maximum number of bytes allowed for # the serialized messages in a batch. A message larger than the preferred # max bytes will result in a batch larger than preferred max bytes. PreferredMaxBytes: 512 KB Kafka: # Brokers: A list of Kafka brokers to which the orderer connects. Edit # this list to identify the brokers of the ordering service. # NOTE: Use IP:port notation. Brokers: - kafka0:9092 - kafka1:9092 - kafka2:9092 - kafka3:9092 # Organizations is the list of orgs which are defined as participants on # the orderer side of the network Organizations: # Policies defines the set of policies at this level of the config tree # For Orderer policies, their canonical path is # /Channel/Orderer/&lt;PolicyName&gt; Policies: Readers: Type: ImplicitMeta Rule: &quot;ANY Readers&quot; Writers: Type: ImplicitMeta Rule: &quot;ANY Writers&quot; Admins: Type: ImplicitMeta Rule: &quot;MAJORITY Admins&quot; # BlockValidation specifies what signatures must be included in the block # from the orderer for the peer to validate it. BlockValidation: Type: ImplicitMeta Rule: &quot;ANY Writers&quot; # Capabilities describes the orderer level capabilities, see the # dedicated Capabilities section elsewhere in this file for a full # description Capabilities: &lt;&lt;: *OrdererCapabilities################################################################################## CHANNEL## This section defines the values to encode into a config transaction or# genesis block for channel related parameters.#################################################################################Channel: &amp;ChannelDefaults # Policies defines the set of policies at this level of the config tree # For Channel policies, their canonical path is # /Channel/&lt;PolicyName&gt; Policies: # Who may invoke the &apos;Deliver&apos; API Readers: Type: ImplicitMeta Rule: &quot;ANY Readers&quot; # Who may invoke the &apos;Broadcast&apos; API Writers: Type: ImplicitMeta Rule: &quot;ANY Writers&quot; # By default, who may modify elements at this config level Admins: Type: ImplicitMeta Rule: &quot;MAJORITY Admins&quot; # Capabilities describes the channel level capabilities, see the # dedicated Capabilities section elsewhere in this file for a full # description Capabilities: &lt;&lt;: *ChannelCapabilities################################################################################## Profile## - Different configuration profiles may be encoded here to be specified# as parameters to the configtxgen tool#################################################################################Profiles: TwoOrgsOrdererGenesis: &lt;&lt;: *ChannelDefaults Orderer: &lt;&lt;: *OrdererDefaults Organizations: - *OrdererOrg Consortiums: SampleConsortium: Organizations: - *Org1 - *Org2 TwoOrgsChannel: Consortium: SampleConsortium Application: &lt;&lt;: *ApplicationDefaults Organizations: - *Org1 - *Org2 1234bin目录从fabric源码里面拷过来，这样方便生成块# mkdir channel-artifacts# ./bin/configtxgen -profile TwoOrgsOrdererGenesis -outputBlock ./channel-artifacts/genesis.block# ./bin/configtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/mychannel.tx -channelID mychannel 完事之后，把kafka目录，拷到所有机器上 部署kafka、zk1234567891011121314151617181920212223242526zk的yaml文件：# cat docker-compose-zookeeper.yaml# Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#version: &apos;2&apos;services: zookeeper0: container_name: zookeeper0 hostname: zookeeper0 image: hyperledger/fabric-zookeeper restart: always environment: - ZOO_MY_ID=1 - ZOO_SERVERS=server.1=zookeeper0:2888:3888 server.2=zookeeper1:2888:3888 server.3=zookeeper2:2888:3888 ports: - 2181:2181 - 2888:2888 - 3888:3888 dns: - &quot;192.168.3.94&quot; 12345678910111213141516171819202122232425262728293031323334kafka的yaml文件：# cat docker-compose-kafka.yaml# Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#version: &apos;2&apos;services: kafka0: container_name: kafka0 hostname: kafka0 image: hyperledger/fabric-kafka restart: always environment: - KAFKA_MESSAGE_MAX_BYTES=103809024 # 99 * 1024 * 1024 B - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024 # 99 * 1024 * 1024 B - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false environment: - KAFKA_BROKER_ID=1 - KAFKA_MIN_INSYNC_REPLICAS=2 - KAFKA_DEFAULT_REPLICATION_FACTOR=3 - KAFKA_ZOOKEEPER_CONNECT=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181 ports: - 9092:9092 dns: - &quot;192.168.3.94&quot; 部署orderer1234567891011121314151617181920212223242526272829303132333435363738394041424344因为咱们的fabric证书，没生成tls，所以下面的配置文件需要把tls去掉，zk，kafka都各自按照上述步骤配置在三台不同机器上，orderer也一样cat docker-compose-orderer.yaml# Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#version: &apos;2&apos;services: orderer0.example.com: container_name: orderer0.example.com image: hyperledger/fabric-orderer environment: - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] - ORDERER_KAFKA_RETRY_LONGINTERVAL=10s - ORDERER_KAFKA_RETRY_LONGTOTAL=100s - ORDERER_KAFKA_RETRY_SHORTINTERVAL=1s - ORDERER_KAFKA_RETRY_SHORTTOTAL=30s - ORDERER_KAFKA_VERBOSE=true working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer volumes: - ./channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - ./crypto-config/ordererOrganizations/example.com/orderers/orderer0.example.com/msp:/var/hyperledger/orderer/msp - ./crypto-config/ordererOrganizations/example.com/orderers/orderer0.example.com/tls/:/var/hyperledger/orderer/tls ports: - 7050:7050 dns: - &quot;192.168.3.94&quot; 部署peer1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# cat docker-compose-peer.yaml# All elements in this file should depend on the docker-compose-base.yaml# Provided fabric peer nodeversion: &apos;2&apos;services: peer1.org1.example.com: container_name: peer1.org1.example.com hostname: peer1.org1.example.com image: hyperledger/fabric-peer environment: - CORE_PEER_ID=peer1.org1.example.com - CORE_PEER_ADDRESS=peer1.org1.example.com:7051 - CORE_PEER_CHAINCODELISTENADDRESS=peer1.org1.example.com:7052 - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer1.org1.example.com:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock # the following setting starts chaincode containers on the same # bridge network as the peers # https://docs.docker.com/compose/networking/ #- CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_PEER_GOSSIP_USELEADERELECTION=true - CORE_PEER_GOSSIP_ORGLEADER=false - CORE_PEER_PROFILE_ENABLED=true - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: peer node start volumes: - /var/run/:/host/var/run/ - ./crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/msp:/etc/hyperledger/fabric/msp - ./crypto-config/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls:/etc/hyperledger/fabric/tls ports: - 7051:7051 - 7052:7052 - 7053:7053 dns: - &quot;192.168.3.94&quot; cli: container_name: cli image: hyperledger/fabric-tools tty: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock # - CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_PEER_ID=cli - CORE_PEER_ADDRESS=peer1.org1.example.com:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/peers/peer1.org1.example.com/tls/ca.crt - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer volumes: - /var/run/:/host/var/run/ - ./chaincode/go/:/opt/gopath/src/github.com/hyperledger/fabric/kafkapeer/chaincode/go - ./crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ - ./channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts dns: - &quot;192.168.3.94&quot; 启动1234docker-compose -f docker-compose-zookeeper.yaml up -ddocker-compose -f docker-compose-kafka.yaml up -ddocker-compose -f docker-compose-orderer.yaml up -ddocker-compose -f docker-compose-peer.yaml up -d 创建channel123进入cli：peer channel create -o orderer0.example.com:7050 -c mychannel -f ./channel-artifacts/mychannel.txpeer channel join -b mychannel.block 之后切换变量，批量加就可以了]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tornado-电话报警]]></title>
    <url>%2F2019%2F03%2F19%2Fpython%2Ftornado-%E7%94%B5%E8%AF%9D%E6%8A%A5%E8%AD%A6%2F</url>
    <content type="text"><![CDATA[需求某些时候邮件，钉钉的报警我们在家里，或者周末是很少去观看的，这时候如果服务器出了问题，运维人员是没法第一时间排查到，所以短信和电话报警就很有必要去做。 已有阿里云的语音短信报警接口，故做了个端口电话报警。 电话报警脚本[root@aa phone_send]# cat send_model.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445# -*- coding:utf-8 -*-import requestsimport tornado.ioloopimport tornado.webphonenumber = &quot;xxxxxxx,xxxxxxx&quot;portdic = &#123; &quot;9876&quot;:&quot;服务类型MQ,端口9876&quot;, &quot;2181&quot;:&quot;服务类型ZK,端口2181&quot;, &quot;3306&quot;:&quot;服务类型数据库,端口3306&quot;, &quot;27017&quot;:&quot;服务类型数据库,端口27017&quot;, &quot;1908&quot;:&quot;服务类型spada,薛亮应用&quot;, &quot;53&quot;:&quot;服务类型dns,端口53&quot;, &quot;9200&quot;:&quot;服务类型es,端口9200&quot;, &quot;6379&quot;:&quot;服务类型redis,端口6379&quot;, &quot;80&quot;:&quot;服务类型nginx,端口80&quot;&#125;statusdic = &#123; &quot;PROBLEM&quot;:&quot;服务发生故障&quot;, &quot;OK&quot;:&quot;故障恢复&quot;&#125;class MainHandler(tornado.web.RequestHandler): def get(self): status = self.get_argument(&apos;status&apos;) endpoint = self.get_argument(&apos;endpoint&apos;) metric = self.get_argument(&apos;metric&apos;) tags = self.get_argument(&apos;tags&apos;) statusok = statusdic.get(status) port = tags.split(&quot;:&quot;)[1] p_endpoint = endpoint.split(&quot;.&quot;) del(p_endpoint[0]) portmes = portdic.get(port) if portmes == None: portmes = &quot;端口&quot; + port # 短信 requests.get(&quot;http://域名/send_sms/%s,%s,%s,%s/%s&quot;%(statusok,endpoint,metric,portmes,phonenumber)) # 电话 requests.get(&quot;http://域名/send_phone/%s%s%s%s/%s&quot;%(statusok,p_endpoint,metric,portmes,phonenumber)) message = status + endpoint + metric + tags print(status,endpoint,metric,tags,port)application = tornado.web.Application([(r&quot;/message&quot;, MainHandler), ])if __name__ == &quot;__main__&quot;: application.listen(8868) tornado.ioloop.IOLoop.instance().start() 执行1python3 send_model.py callback接口http://ip:8868/message openfalcon监控做模板]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[tornado-hello]]></title>
    <url>%2F2019%2F03%2F19%2Fpython%2Ftornado-hello%2F</url>
    <content type="text"><![CDATA[概念Tornado是一个Python Web框架和异步网络库，最初是在FriendFeed上开发的。通过使用非阻塞网络I / O，Tornado可以扩展到数万个开放连接，使其成为长轮询， WebSockets和其他需要与每个用户建立长期连接的应用程序的理想选择 。 安装1pip3 install tornado 简单的web12345678910111213import tornado.ioloopimport tornado.webclass MainHandler(tornado.web.RequestHandler): def get(self): self.write(&quot;Hello, world&quot;)if __name__ == &quot;__main__&quot;: application = tornado.web.Application([ (r&quot;/index&quot;, MainHandler), ]) application.listen(8888) tornado.ioloop.IOLoop.current().start() 访问http://ip:8888/index]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[fabric-ca部署]]></title>
    <url>%2F2019%2F03%2F05%2F%E5%8C%BA%E5%9D%97%E9%93%BE%2Ffabric-ca%2F</url>
    <content type="text"><![CDATA[部署一个fabric-ca架构图 创建一个由两个组织org1.example.com和org2.example.com组成的的联盟1234567891011121314151617181920还有一个组织example.com用来部署orderer。组织example.com部署了一个solo模式的orderer。（多个orderer的部署方式，以后探讨）orderer.example.com组织org1.example.com部署了两个peer:peer0.org1.example.compeer1.org1.example.com组织org2.example.com部署了一个peer:peer0.org2.example.com每个组织都要有一个Admin用户，每个组件(peer/orderer)也需要一个账号，因此需要通过FabricCA创建7个用户：example.com: Admin@example.com orderer.example.comorg1.example.com: Admin@org1.example.com peer0.org1.example.com peer1.org1.example.com org2.example.com: Admin@org2.example.com peer0.org2.example.com这里只创建了Admin用户和每个组件的账号，普通用户的创建方式相同，只是普通用户的证书不需要添加到目标组件的admincerts目录中。或者说一个用户的证书如果被添加到了对应组织或组件的msp/admincerts目录中，那么这个用户就称为对应的管理员。 启动fabric-ca123456789101112131415161718192021fabirc-ca的编译：$ go get -u github.com/hyperledger/fabric-ca$ cd $GOPATH/src/github.com/hyperledger/fabric-ca$ make fabric-ca-server$ make fabric-ca-client$ ls bin/fabric-ca-client fabric-ca-server这里将fabric-ca部署在/opt/app/fabric-ca/server目录中：mkdir -p /opt/app/fabric-ca/servercp -rf $GOPATH/src/github.com/hyperledger/fabric-ca/bin/* /opt/app/fabric-ca/serverln -s /opt/app/fabric-ca/server/fabric-ca-client /usr/bin/fabric-ca-client直接启动ca，fabric-ca admin的名称为admin，密码为pass。(这里只是演示，生产中使用，你需要根据实际的情况配置)cd /opt/app/fabric-ca/server./fabric-ca-server start -b admin:pass &amp;如果有删除联盟和删除用户的需求，需要用下面的方式启动：cd /opt/app/fabric-ca/server./fabric-ca-server start -b admin:pass --cfg.affiliations.allowremove --cfg.identities.allowremove &amp; 生成fabric-ca admin的凭证123456789101112mkdir /root/fabric-deploycd ~/fabric-deploymkdir fabric-ca-files 生成fabric-ca admin的凭证，用-H参数指定client目录：mkdir -p `pwd`/fabric-ca-files/adminfabric-ca-client enroll -u http://admin:pass@localhost:7054 -H `pwd`/fabric-ca-files/admin也可以用环境变量FABRIC_CA_CLIENT_HOME指定了client的工作目录，生成的用户凭证将存放在这个目录中。export FABRIC_CA_CLIENT_HOME=`pwd`/fabric-ca-files/adminmkdir -p $FABRIC_CA_CLIENT_HOMEfabric-ca-client enroll -u http://admin:pass@localhost:7054 创建联盟123456789101112131415161718192021222324252627282930上面的启动方式默认会创建两个组织：$ fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation list2018/05/07 02:36:46 [INFO] [::1]:56148 GET /affiliations 200 0 &quot;OK&quot;affiliation: . affiliation: org2 affiliation: org2.department1 affiliation: org1 affiliation: org1.department1 affiliation: org1.department2为了查看信息的时候，看到的输出比较简洁，用下面的命令将其删除：fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation remove --force org1fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation remove --force org2执行下面命令创建联盟：fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com.examplefabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com.example.org1fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation add com.example.org2注意：联盟是有层级的。创建联盟如下：$ fabric-ca-client -H `pwd`/fabric-ca-files/admin affiliation list2018/04/28 15:19:34 [INFO] 127.0.0.1:38160 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org1 affiliation: com.example.org2 为每个组织准备msp12345678910111213141516171819202122232425262728293031323334353637就是从Fabric-CA中，读取出用来签署用户的根证书等。为example.com准备msp，将ca证书等存放example.com组织的目录中:mkdir -p ./fabric-ca-files/example.com/mspfabric-ca-client getcacert -M `pwd`/fabric-ca-files/example.com/msp //-M需要指定绝对路径命令执行结束后，会在fabric-ca-files/example.com/msp得到文件：$ tree fabric-ca-files/example.com/msp/example.com/msp/|-- cacerts| `-- localhost-7054.pem|-- intermediatecerts| `-- localhost-7054.pem|-- keystore`-- signcerts注意通过getcacert得到msp目录中只有CA证书，而且这里没有使用中间CA，fabric-ca-files/example.com/msp/intermediatecerts/localhost-7054.pem是一个空文件。同样的方式为org1.example.com获取msp:mkdir -p fabric-ca-files/org1.example.com/mspfabric-ca-client getcacert -M `pwd`/fabric-ca-files/org1.example.com/msp为org2.example.com准备msp:mkdir -p ./fabric-ca-files/org2.example.com/mspfabric-ca-client getcacert -M `pwd`/fabric-ca-files/org2.example.com/msp这里是用getcacert为每个组织准备需要的ca文件，在生成创始块的时候会用到。在1.1.0版本的fabric-ca中，只会生成用户在操作区块链的时候用到的证书和密钥，不会生成用来加密grpc通信的证书。这里复用之前用cryptogen生成的tls证书，需要将验证tls证书的ca添加到msp目录中，如下：cp -rf certs/ordererOrganizations/example.com/msp/tlscacerts fabric-ca-files/example.com/msp/cp -rf certs/peerOrganizations/org1.example.com/msp/tlscacerts/ fabric-ca-files/org1.example.com/msp/cp -rf certs/peerOrganizations/org2.example.com/msp/tlscacerts/ fabric-ca-files/org2.example.com/msp/如果在你的环境中，各个组件域名的证书，是由第三方CA签署的，就将第三方CA的根证书添加到msp/tlscacerts目录中。组织的msp目录中，包含都是CA根证书，分别是TLS加密的根证书，和用于身份验证的根证书。另外还需要admin用户的证书，后面的操作中会添加。 注册example.com的管理员Admin@example.com可以直接用命令行（命令比较长，这里用\\截断了）： 123fabric-ca-client register --id.name Admin@example.com --id.type client --id.affiliation &quot;com.example.org1&quot; \ --id.attrs &apos;&quot;hf.Registrar.Roles=client,orderer,peer,user&quot;,&quot;hf.Registrar.DelegateRoles=client,orderer,peer,user&quot;,\ hf.Registrar.Attributes=*,hf.GenCRL=true,hf.Revoker=true,hf.AffiliationMgr=true,hf.IntermediateCA=true,role=admin:ecert&apos; 也可以将命令行参数写在fabric-ca admin的配置文件fabric-ca-files/admin/fabric-ca-client-config.yaml中。 12$ ls fabric-ca-files/admin/admin/fabric-ca-client-config.yaml msp 为了演示清楚，这里使用修改配置文件的方式，将fabric-ca-files/admin/fabric-ca-client-config.yaml其中的id部分修改为： 1234567891011121314151617181920212223id: name: Admin@example.com type: client affiliation: com.example maxenrollments: 0 attributes: - name: hf.Registrar.Roles value: client,orderer,peer,user - name: hf.Registrar.DelegateRoles value: client,orderer,peer,user - name: hf.Registrar.Attributes value: &quot;*&quot; - name: hf.GenCRL value: true - name: hf.Revoker value: true - name: hf.AffiliationMgr value: true - name: hf.IntermediateCA value: true - name: role value: admin ecert: true 注意最后一行role属性，是我们自定义的属性，对于自定义的属性，要设置certs，在配置文件中需要单独设置ecert属性为true或者false。如果在命令行中，添加后缀:ecert表示true，例如: 1fabric-ca-client register --id.affiliation &quot;com.example.org1&quot; --id.attrs &quot;role=admin:ecert&quot; 直接执行下面的命令，即可完成用户`Admin@example.com`注册，注意这时候的注册使用fabricCA的admin账号完成的： 1fabric-ca-client register -H `pwd`/fabric-ca-files/admin --id.secret=password 如果不用--id.secret指定密码，会自动生成密码。 其它配置的含义是用户名为`Admin@example.com，类型是client，它能够管理com.example.*`下的用户，如下: 1234567891011--id.name Admin@example.com //用户名--id.type client //类型为client--id.affiliation &quot;com.example&quot; //权利访问hf.Registrar.Roles=client,orderer,peer,user //能够管理的用户类型hf.Registrar.DelegateRoles=client,orderer,peer,user //可以授权给子用户管理的用户类型hf.Registrar.Attributes=* //可以为子用户设置所有属性hf.GenCRL=true //可以生成撤销证书列表hf.Revoker=true //可以撤销用户hf.AffiliationMgr=true //能够管理联盟hf.IntermediateCA=true //可以作为中间CArole=admin:ecert //自定义属性 完成注册之后，还需生成Admin@example.com凭证： 1234$ mkdir -p ./fabric-ca-files/example.com/admin$ fabric-ca-client enroll -u http://Admin@example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/example.com/admin$ ls ./fabric-ca-files/example.com/adminfabric-ca-client-config.yaml msp/ 这时候可以用Admin@example.com的身份查看联盟： 123456$ fabric-ca-client affiliation list -H `pwd`/fabric-ca-files/example.com/admin2018/04/28 15:35:10 [INFO] 127.0.0.1:38172 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org1 affiliation: com.example.org2 最后将Admin@example.com的证书复制到example.com/msp/admincerts/中： 12mkdir fabric-ca-files/example.com/msp/admincerts/cp fabric-ca-files/example.com/admin/msp/signcerts/cert.pem fabric-ca-files/example.com/msp/admincerts/ 注册org1.example.com的管理员Admin@org1.example.com为org1.example.com的管理员Admin@org1.example.com准备一个目录: 12cd ~/fabric-deploymkdir -p ./fabric-ca-files/org1.example.com/admin 将fabric-ca-files/admin/fabric-ca-client-config.yaml其中的id部分修改为： 1234567891011121314151617181920212223id: name: Admin@org1.example.com type: client affiliation: com.example.org1 maxenrollments: 0 attributes: - name: hf.Registrar.Roles value: client,orderer,peer,user - name: hf.Registrar.DelegateRoles value: client,orderer,peer,user - name: hf.Registrar.Attributes value: &quot;*&quot; - name: hf.GenCRL value: true - name: hf.Revoker value: true - name: hf.AffiliationMgr value: true - name: hf.IntermediateCA value: true - name: role value: admin ecert: true 注册： 1fabric-ca-client register -H `pwd`/fabric-ca-files/admin --id.secret=password 生成凭证： 123$ fabric-ca-client enroll -u http://Admin@org1.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org1.example.com/admin$ ls ./fabric-ca-files/org1.example.com/adminfabric-ca-client-config.yaml msp/ 查看联盟： 12345$ fabric-ca-client affiliation list -H `pwd`/fabric-ca-files/org1.example.com/admin2018/05/04 15:42:53 [INFO] 127.0.0.1:51298 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org1 注意与`Admin@example.com`的区别，这里只能看到组织com.example.org1 将Admin@org1.example.com的证书复制到org1.example.com的msp/admincerts中： 12mkdir fabric-ca-files/org1.example.com/msp/admincerts/cp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/msp/admincerts/ 在`Admin@org1.example.com目录`中也需要创建msp/admincerts目录，通过peer命令操作fabric的时候会要求admincerts存在： 12mkdir fabric-ca-files/org1.example.com/admin/msp/admincerts/ # 注意是org1.example.com/admin目录cp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/admin/msp/admincerts/ 另外，这里没有使用中间CA，将intermediatecerts中的空文件删除，否则peer会提示Warning： 1rm fabric-ca-files/org1.example.com/admin/msp/intermediatecerts/* 注册org2.example.com的管理员Admin@org2.example.com为org2.example.com的管理员Admin@org2.example.com准备一个目录: 12cd ~/fabric-deploymkdir -p ./fabric-ca-files/org2.example.com/admin 将fabric-ca-files/admin/fabric-ca-client-config.yaml其中的id部分修改为： 1234567891011121314151617181920212223id: name: Admin@org2.example.com type: client affiliation: com.example.org2 maxenrollments: 0 attributes: - name: hf.Registrar.Roles value: client,orderer,peer,user - name: hf.Registrar.DelegateRoles value: client,orderer,peer,user - name: hf.Registrar.Attributes value: &quot;*&quot; - name: hf.GenCRL value: true - name: hf.Revoker value: true - name: hf.AffiliationMgr value: true - name: hf.IntermediateCA value: true - name: role value: admin ecert: true 注册： 1fabric-ca-client register -H `pwd`/fabric-ca-files/admin --id.secret=password 生成凭证： 123$ fabric-ca-client enroll -u http://Admin@org2.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org2.example.com/admin$ ls ./fabric-ca-files/org2.example.com/adminfabric-ca-client-config.yaml msp/ 查看联盟： 12345$ fabric-ca-client affiliation list -H `pwd`/fabric-ca-files/org2.example.com/admin2018/05/02 16:49:00 [INFO] 127.0.0.1:50828 GET /affiliations 201 0 &quot;OK&quot;affiliation: com affiliation: com.example affiliation: com.example.org2 Admin@org2.example.com只能看到组织com.example.org2。 将Admin@org2.example.com的证书复制到org2.example.com的msp/admincerts中： 12mkdir fabric-ca-files/org2.example.com/msp/admincerts/cp fabric-ca-files/org2.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org2.example.com/msp/admincerts/ 在Admin@org2.example.com中也需要创建msp/admincerts目录，通过peer命令操作fabric的时候会要求admincerts存在： 12mkdir fabric-ca-files/org2.example.com/admin/msp/admincerts/cp fabric-ca-files/org2.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org2.example.com/admin/msp/admincerts/ 另外，这里没有使用中间CA，将intermediatecerts中的空文件删除，否则peer会提示Warning： 1rm fabric-ca-files/org2.example.com/admin/msp/intermediatecerts/* 各个组织分别使用自己的Admin账户创建其它账号example.com、org1.example.com、org2.example.com三个组织这时候可以分别使用自己的Admin账号创建子账号。 orderer.example.com使用`Admin@example.com注册账号orderer.example.com。注意这时候指定的目录是fabric-ca-files/example.com`/admin/。 修改fabric-ca-files/example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: orderer.example.com type: orderer affiliation: com.example maxenrollments: 0 attributes: - name: role value: orderer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/example.com/ordererfabric-ca-client enroll -u http://orderer.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/example.com/orderer 将`Admin@example.com`的证书复制到fabric-ca-files/example.com/orderer/msp/admincerts： 12mkdir fabric-ca-files/example.com/orderer/msp/admincertscp fabric-ca-files/example.com/admin/msp/signcerts/cert.pem fabric-ca-files/example.com/orderer/msp/admincerts/ peer0.org1.example.com使用`Admin@org1.example.com注册账号peer0.org1.example.com。这时候指定的目录是fabric-ca-files/org1.example.com`/admin/。 修改fabric-ca-files/org1.example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: peer0.org1.example.com type: peer affiliation: com.example.org1 maxenrollments: 0 attributes: - name: role value: peer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/org1.example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/org1.example.com/peer0fabric-ca-client enroll -u http://peer0.org1.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org1.example.com/peer0 将`Admin@org1.example.com`的证书复制到fabric-ca-files/org1.example.com/peer0/msp/admincerts： 12mkdir fabric-ca-files/org1.example.com/peer0/msp/admincertscp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/peer0/msp/admincerts/ peer1.org1.example.com使用`Admin@org1.example.com注册账号peer1.org1.example.com。这时候指定的目录是fabric-ca-files/org1.example.com`/admin/。 修改fabric-ca-files/org1.example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: peer1.org1.example.com type: peer affiliation: com.example.org1 maxenrollments: 0 attributes: - name: role value: peer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/org1.example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/org1.example.com/peer1fabric-ca-client enroll -u http://peer1.org1.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org1.example.com/peer1 将`Admin@org1.example.com`的证书复制到fabric-ca-files/org1.example.com/peer1/msp/admincerts： 12mkdir fabric-ca-files/org1.example.com/peer1/msp/admincertscp fabric-ca-files/org1.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org1.example.com/peer1/msp/admincerts/ peer0.org2.example.com使用`Admin@org2.example.com注册账号peer0.org2.example.com。这时候指定的目录是fabric-ca-files/org2.example.com`/admin/。 修改fabric-ca-files/org2.example.com/admin/fabric-ca-client-config.yaml: 123456789id: name: peer0.org2.example.com type: peer affiliation: com.example.org2 maxenrollments: 0 attributes: - name: role value: peer ecert: true 注册以及生成凭证： 123fabric-ca-client register -H `pwd`/fabric-ca-files/org2.example.com/admin --id.secret=passwordmkdir ./fabric-ca-files/org2.example.com/peer0fabric-ca-client enroll -u http://peer0.org2.example.com:password@localhost:7054 -H `pwd`/fabric-ca-files/org2.example.com/peer0 将`Admin@org2.example.com`的证书复制到fabric-ca-files/org2.example.com/peer0/msp/admincerts： 12mkdir fabric-ca-files/org2.example.com/peer0/msp/admincertscp fabric-ca-files/org2.example.com/admin/msp/signcerts/cert.pem fabric-ca-files/org2.example.com/peer0/msp/admincerts/ 注意： 之前发现直接这么生成的证书，会少东西，需要在每个组织的msp目录下面配置下config.yaml 1234567891011[root@localhost msp]# pwd/data/fabric/fabric-ca-files/gzyb.vaccine.com/msp[root@localhost msp]# cat config.yaml NodeOUs: Enable: true ClientOUIdentifier: Certificate: cacerts/localhost-7054.pem OrganizationalUnitIdentifier: client PeerOUIdentifier: Certificate: cacerts/localhost-7054.pem OrganizationalUnitIdentifier: peer]]></content>
      <categories>
        <category>区块链</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Prometheus监控]]></title>
    <url>%2F2019%2F01%2F29%2Fk8s%2Fprometheus%2F</url>
    <content type="text"><![CDATA[在Kubernetes上快速部署Prometheus创建一个新的命名空间12345678[root@prometheus]# cat monitor_namespace.yaml apiVersion: v1kind: Namespacemetadata: name: monitor labels: name: monitor[root@prometheus]#kubectl create -f monitor_namespace.yaml rbac文件12345678910111213141516171819202122232425262728293031323334353637383940414243[root@prometheus]# cat rbac-setup.yaml apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: [&quot;&quot;] resources: - nodes - nodes/proxy - services - endpoints - pods verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- apiGroups: - extensions resources: - ingresses verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]- nonResourceURLs: [&quot;/metrics&quot;] verbs: [&quot;get&quot;]---apiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: monitor---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: monitor [root@prometheus]#kubectl create -f rbac-setup.yaml prometheus-deploy文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373[root@prometheus]# cat configmap.yamlapiVersion: v1kind: ConfigMapmetadata: name: prometheus-config namespace: monitordata: #被引用到/etc/prometheus/prometheus.yml prometheus.yml: | global: #每15s采集一次数据和15s做一次告警检测 scrape_interval: 15s evaluation_interval: 15s #指定加载的告警规则文件 rule_files: - /etc/prometheus/rules.yml #将报警送至何地进行报警 alerting: alertmanagers: - static_configs: - targets: [&quot;192.168.50.60:9093&quot;] #指定prometheus要监控的目标 scrape_configs: - job_name: &apos;k8s-node&apos; scrape_interval: 10s static_configs: - targets: - &apos;192.168.50.61:31672&apos; #自定义获取监控数据,每个 job_name 都是独立的 - job_name: &apos;tomcat-pods&apos; tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape, __meta_kubernetes_service_annotation_prometheus_io_jvm_scrape] regex: true;true action: keep - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_app_metrics_patn] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__meta_kubernetes_pod_ip, __meta_kubernetes_service_annotation_prometheus_io_app_metrics_port] action: replace target_label: __address__ regex: (.+);(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_pod_host_ip] action: replace target_label: kubernetes_host_ip - job_name: &apos;kubernetes-apiservers&apos; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: &apos;kubernetes-nodes&apos; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics - job_name: &apos;kubernetes-cadvisor&apos; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor - job_name: &apos;kubernetes-service-endpoints&apos; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name - job_name: &apos;kubernetes-services&apos; kubernetes_sd_configs: - role: service metrics_path: /probe params: module: [http_2xx] relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__address__] target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] target_label: kubernetes_name - job_name: &apos;kubernetes-ingresses&apos; kubernetes_sd_configs: - role: ingress relabel_configs: - source_labels: [__meta_kubernetes_ingress_annotation_prometheus_io_probe] action: keep regex: true - source_labels: [__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path] regex: (.+);(.+);(.+) replacement: $&#123;1&#125;://$&#123;2&#125;$&#123;3&#125; target_label: __param_target - target_label: __address__ replacement: blackbox-exporter.example.com:9115 - source_labels: [__param_target] target_label: instance - action: labelmap regex: __meta_kubernetes_ingress_label_(.+) - source_labels: [__meta_kubernetes_namespace] target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_ingress_name] target_label: kubernetes_name - job_name: &apos;kubernetes-pods&apos; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # 监控规则文件,被引用到/etc/prometheus/rules.yml rules.yml: | groups: - name: test-rule rules: ############# Node监控 ############# - alert: k8s-node状态异常 expr: up&#123;job=&quot;k8s-node&quot;&#125; != 1 for: 3m labels: team: k8s-node annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点状态异常&quot; description: &quot;可能是重启了&quot; - alert: k8s-node节点CPU使用率 expr: (1 - avg(irate(node_cpu_seconds_total&#123;job=&quot;k8s-node&quot;,mode=&quot;idle&quot;&#125;[1m])) by (instance)) * 100 &gt; 95 for: 1m labels: team: k8s-node annotations: summary: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点CPU使用率超过95%&quot; description: &quot;&#123;&#123;$labels.instance&#125;&#125;: Node节点当前CPU使用率为: &#123;&#123; $value &#125;&#125;&quot; - alert: k8s-node节点磁盘使用率 expr: (node_filesystem_size_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125; - node_filesystem_avail_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125;) / node_filesystem_size_bytes&#123;mountpoint=&quot;/&quot;,job=&quot;k8s-node&quot;&#125; * 100 &gt; 85 for: 1m labels: team: k8s-node annotations: description: &quot;Node服务器[[ &#123;&#123;$labels.instance&#125;&#125; ]] 的 &#123;&#123;mountpoint&#125;&#125; 磁盘空间使用率超过85%&quot; summary: &quot;磁盘 &#123;&#123;$labels.device&#125;&#125; 当前使用率为: &#123;&#123; $value &#125;&#125;&quot; - alert: k8s-node节点内存使用率 expr: (node_memory_MemTotal_bytes&#123;job=&quot;k8s-node&quot;&#125; - (node_memory_Buffers_bytes&#123;job=&quot;k8s-node&quot;&#125; + node_memory_Cached_bytes&#123;job=&quot;k8s-node&quot;&#125; + node_memory_MemFree_bytes&#123;job=&quot;k8s-node&quot;&#125;)) / node_memory_MemTotal_bytes&#123;job=&quot;k8s-node&quot;&#125; * 100 for: 1m labels: team: k8s-node annotations: description: &quot;Node服务器[[ &#123;&#123;$labels.instance&#125;&#125; ]] 内存使用率超过95%&quot; summary: &quot;&#123;&#123;$labels.instance&#125;&#125; 当前内存使用率为: &#123;&#123; $value &#125;&#125;&quot; ############ Pod 监控 ############ - alert: 监控k8s的pod状态异常 expr: up&#123;kubernetes_namespace=&quot;monitor&quot;&#125; != 1 for: 3m labels: team: &quot;kube-state-metrics&quot; annotations: description: &quot;&#123;&#123;$labels.kubernetes_namespace&#125;&#125; 内的 pod 状态有变动&quot; summary: &quot;此 Pod 用于获取 k8s 监控数据, 绑定在一个节点上&quot; - alert: 应用的 pod 状态有变动 expr: kube_pod_container_status_ready&#123;namespace=&quot;product&quot;&#125; != 1 for: 3m labels: status: &quot;product 命名空间内的 pod &#123;&#123;$labels.pod&#125;&#125;有变动&quot; annotations: description: &quot;Deployment &#123;&#123;$labels.container&#125;&#125; 内的 pod 状态有变动&quot; summary: &quot;可能是重启或者在升级版本,如果频繁重启,请跟踪排查问题&quot; - alert: 以下应用的 pod 重启次数已经超过15,请查看原因 expr: kube_pod_container_status_restarts_total&#123;namespace=&quot;product&quot;&#125; &gt; 15 for: 3m labels: status: &quot;product 命名空间内的 pod &#123;&#123;$labels.pod&#125;&#125; 重启次数太多&quot; annotations: description: &quot;Deployment &#123;&#123;$labels.container&#125;&#125; 内的 pod 重启次数太多&quot; summary: &quot;重启次数太多,可能是因为 pod 内应用有问题&quot; ########### Java 监控 ############ - alert: jvm线程数过高 expr: jvm_threads_current&#123;job=&quot;tomcat-pods&quot;&#125;&gt;2000 for: 1m labels: status: &quot;空间内 jvm 的变动情况&quot; annotations: description: &quot;&#123;&#123;$labels.kubernetes_pod_name&#125;&#125;: Jvm线程数过高&quot; summary: &apos;&#123;&#123; $labels.kubernetes_pod_name &#125;&#125; : 当前你线程值为: &#123;&#123; $value &#125;&#125;&apos; [root@prometheus]# cat prometheus.deploy.yml ---apiVersion: apps/v1beta2kind: Deploymentmetadata: labels: name: prometheus-deployment name: prometheus namespace: monitorspec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: containers: - image: prom/prometheus:v2.6.0 name: prometheus command: - &quot;/bin/prometheus&quot; args: - &quot;--config.file=/etc/prometheus/prometheus.yml&quot; - &quot;--storage.tsdb.path=/home/prometheus&quot; - &quot;--storage.tsdb.retention=168h&quot; - &quot;--web.enable-lifecycle&quot; ports: - containerPort: 9090 protocol: TCP volumeMounts: - mountPath: &quot;/home/prometheus&quot; name: data - mountPath: &quot;/etc/prometheus&quot; name: config-volume - mountPath: &quot;/etc/localtime&quot; readOnly: false name: localtime resources: requests: cpu: 100m memory: 2048Mi limits: cpu: 500m memory: 3180Mi serviceAccountName: prometheus nodeSelector: nodetype: prometheus volumes: - name: data hostPath: path: &quot;/opt/prometheus/data&quot; - name: config-volume configMap: name: prometheus-config - name: localtime hostPath: path: &quot;/etc/localtime&quot; type: File [root@prometheus]# cat prometheus.svc.yml ---kind: ServiceapiVersion: v1metadata: labels: app: prometheus name: prometheus namespace: monitorspec: type: NodePort ports: - port: 9090 targetPort: 9090 nodePort: 30003 selector: app: prometheus [root@prometheus]#kubectl create -f configmap.yaml[root@prometheus]#kubectl create -f prometheus.deploy.yml[root@prometheus]#kubectl create -f prometheus.svc.yml注：需要在本地创建/opt/prometheus/data作为prometheus数据路径，另需要给data目录赋予777权限 热重读配置文件congfigmap有热重启功能，这样每次改完配置文件都不需要重启prometheus的pod来重读配置了 123456789- &quot;--web.enable-lifecycle&quot;在prometheus.deploy.yml的配置文件里面加上这段话就可以了[root@prometheus]# cat reload-prometheus.sh #!/bin/bashkubectl apply -f configmap.yamlsleep 60curl -XPOST http://192.168.50.60:30003/-/reload可以写个脚本，每次修改完配置文件的配置之后，执行一下脚本就可以同步生效了！ 安装kube-state-metrics123[root@prometheus]# git clone https://github.com/kubernetes/kube-state-metrics.git之后把默认的命名空间改成monitor，进入kube-state-metrics目录[root@prometheus]#kubectl create -f ./ 安装grafana12345678910111213141516创建grafana的数据目录mkdir /opt/grafana/data启动脚本[root@grafana]# cat start_grafana.sh #!/bin/bashdocker stop `docker ps -a |awk &apos;/grafana/&#123;print $1&#125;&apos;`docker rm `docker ps -a |awk &apos;/grafana/&#123;print $1&#125;&apos;`docker run -d \ --name=grafana \ --restart=always \ -p 3000:3000 \ -m 4096m \ -v /opt/grafana/data:/var/lib/grafana \ -v /opt/grafana/log:/var/log/grafana \ grafana/grafana:5.4.3 1、安装完之后，需要添加source，source直接点prometheus，链接就是http://192.168.50.60:30003之前创建的prometheus界面 2、添加模板dashboad（列出几个常用的） 点import导入，有俩种方式，直接填官网模板，或者导入json https://grafana.com/dashboards/9276 node的cpu、内存等 https://grafana.com/dashboards/3146 pod https://grafana.com/dashboards/8588 deployment 安装alertmanager创建配置文件、目录1234567891011121314151617181920212223242526272829303132创建alert数据目录mkdir /opt/alert/data注意：需要alertmanager.yml配置，此配置钉钉和邮件可同时放松[root@docker60 alert]# cat alertmanager.yml global: resolve_timeout: 5mroute: group_by: [&apos;alertname&apos;] group_wait: 10s group_interval: 10s repeat_interval: 6m receiver: defaultreceivers:- name: &apos;default&apos; email_configs: - to: &quot;&quot; send_resolved: true from: &quot;&quot; smarthost: &quot;smtp.xxx.com:25&quot; auth_username: &quot;&quot; auth_password: &quot;&quot; webhook_configs: - url: &apos;http://192.168.50.60:8060/dingtalk/ops_dingding/send&apos; send_resolved: trueinhibit_rules: - source_match: severity: &apos;critical&apos; target_match: severity: &apos;warning&apos; equal: [&apos;alertname&apos;] 启动脚本1234567891011121314[root@alert]# cat start_alert.sh#!/bin/bashdocker stop `docker ps -a |awk &apos;/alertmanager/&#123;print $1&#125;&apos;`docker rm `docker ps -a |awk &apos;/alertmanager/&#123;print $1&#125;&apos;`docker run -d \ --name alertmanager \ --restart=always \ -p 9093:9093 \ -v /etc/localtime:/etc/localtime:ro \ -v /opt/alert/alertmanager.yml:/etc/alertmanager/alertmanager.yml \ -v /opt/alert/data:/alertmanager \ prom/alertmanager:v0.15.3 安装dingding插件1234567891011121、安装go （这里就不叙述了）2、假设go的路径是/usr/local/gomkdir -pv /usr/local/go/src/github.com/timonwong3、下载dingding插件git clone https://github.com/timonwong/prometheus-webhook-dingtalk.git4、添加dingding机器人在dingding群里面添加即可5、启动dingding[root@alert]# cat start_dingding.sh cd /usr/local/go/src/github.com/timonwong/prometheus-webhook-dingtalkkill -9 `ps -ef | grep prometheus-webhook-dingtalk | grep -v grep | awk &apos;&#123;print $2&#125;&apos;`nohup ./prometheus-webhook-dingtalk --ding.profile=&quot;ops_dingding=https://oapi.dingtalk.com/robot/send?access_token=xxxx&quot; 2&gt;&amp;1 1&gt;dingding.log &amp;]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[jira-安装及破解]]></title>
    <url>%2F2018%2F11%2F21%2F%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2Fjira-%E5%AE%89%E8%A3%85%E5%8F%8A%E7%A0%B4%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[安装jiraJIRA是Atlassian公司出品的项目与事务跟踪工具，被广泛应用于缺陷跟踪、客户服务、需求收集、流程审批、任务跟踪、项目跟踪和敏捷管理等工作领域。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263下载包：wget https://product-downloads.atlassian.com/software/jira/downloads/atlassian-jira-software-7.13.0-x64.bin[root@YZSJHL82-204 ~]# chmod +x atlassian-jira-software-7.13.0-x64.bin[root@YZSJHL82-204 ~]# ./atlassian-jira-software-7.13.0-x64.binUnpacking JRE ...Starting Installer ...十月 23, 2018 4:38:25 下午 java.util.prefs.FileSystemPreferences$1 run信息: Created user preferences directory.十月 23, 2018 4:38:25 下午 java.util.prefs.FileSystemPreferences$2 run信息: Created system preferences directory in java.home.This will install JIRA Software 7.4.1 on your computer.OK [o, Enter], Cancel [c]o #按o安装Choose the appropriate installation or upgrade option.Please choose one of the following:Express Install (use default settings) [1], Custom Install (recommended for advanced users) [2, Enter], Upgrade an existing JIRA installation [3]2 #2为自定义安装Where should JIRA Software be installed?[/opt/atlassian/jira]/usr/local/atlassina/jira #自定义安装目录Default location for JIRA Software data[/var/atlassian/application-data/jira]/usr/local/atlassina/jira_data #自定义数据目录Configure which ports JIRA Software will use.JIRA requires two TCP ports that are not being used by any otherapplications on this machine. The HTTP port is where you will access JIRAthrough your browser. The Control port is used to startup and shutdown JIRA.Use default ports (HTTP: 8080, Control: 8005) - Recommended [1, Enter], Set custom value for HTTP and Control ports [2]2 #2为自定义端口HTTP Port Number[8080] #8080为默认端口8050 #http连接端口Control Port Number[8005]8040 #控制端口JIRA can be run in the background.You may choose to run JIRA as a service, which means it will startautomatically whenever the computer restarts.Install JIRA as Service?Yes [y, Enter], No [n]y #是否开机自启Details on where JIRA Software will be installed and the settings that will be used.Installation Directory: /usr/local/atlassina/jira Home Directory: /usr/local/atlassina/jira_data HTTP Port: 8050 RMI Port: 8040 Install as service: Yes Install [i, Enter], Exit [e]i #确认已选配置Extracting files ...Please wait a few moments while JIRA Software is configured.Installation of JIRA Software 7.4.1 is completeStart JIRA Software 7.4.1 now?Yes [y, Enter], No [n]y #启动Please wait a few moments while JIRA Software starts up.Launching JIRA Software ...Installation of JIRA Software 7.4.1 is completeYour installation of JIRA Software 7.4.1 is now ready and can be accessedvia your browser.JIRA Software 7.4.1 can be accessed at http://localhost:8050Finishing installation ... 浏览器访问jira，地址为：http://IP:8050 请自行修改IP和端口。如果可以访问，说明安装成功。 配置数据库及密码在mySQL上创建用户及库做授权123create database jira_new;grant all privileges on *.* to jira@&apos;10.4.82.204&apos; identified by &apos;jira&apos;;flush privileges; 在授权完用户我们不可以马上填写信息，需要添加MySQL的一个jra包，否则下一步会提示找不到mysql的驱动 wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-5.1.46.zip1234567停止jira[root@YZSJHL82-204 ~]# /etc/init.d/jira stop上传软件包[root@YZSJHL82-204 ~]# cp mysql-connector-java-5.1.46-bin.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/启动jira[root@YZSJHL82-204 ~]# /etc/init.d/jira start注意防火墙 安装完数据库插件即可下一步: 设置jira主题 因为第一次安装，我们需要去jira官网注册用户，获取授权码 (免费30天，安装后更换破解即可) 保存好服务器ID，进入atlassian官网获取试用许可证，下边附上注册地址： 注册官网：https://my.atlassian.com 或使用以下地址： https://id.atlassian.com/signup?application=mac&amp;continue=https://my.atlassian.com 登陆账号后，选择New Evaluation License 设置管理员用户:官网注册的账号只可以免费试用30天，所以当我们安装完需要尽快进行破解 破解jirahttps://download.csdn.net/download/lbwahoo/100308071234567停止jira[root@YZSJHL82-204 ~]# /etc/init.d/jira stop进入安装目录下的atlassian-jira/WEB-INF/lib/目录下，用破解包atlassian-extras-3.2.jar替换原来的包。并将mysql连接驱动复制到此目录下。[root@YZSJHL82-204 ~]# cp atlassian-extras-3.2.jar /opt/atlassian/jira/atlassian-jira/WEB-INF/lib/启动jira[root@YZSJHL82-204 ~]# /etc/init.d/jira start注意防火墙 配置数据库连接地址12/var/atlassian/application-data/jira/dbconfig.xml#此路径为默认路径]]></content>
      <categories>
        <category>版本管理工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python删除mongo表]]></title>
    <url>%2F2018%2F10%2F29%2Fpython%2Fpython%E5%88%A0%E9%99%A4mongo%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[PyMongoPython 要连接 MongoDB 需要 MongoDB 驱动，这里我们使用 PyMongo 驱动来连接。 pip 安装pip 是一个通用的 Python 包管理工具，提供了对 Python 包的查找、下载、安装、卸载的功能。 安装 pymongo: 1$ python3 -m pip3 install pymongo 也可以指定安装的版本: 1$ python3 -m pip3 install pymongo==3.5.1 更新 pymongo 命令： 1$ python3 -m pip3 install --upgrade pymongo easy_install 安装旧版的 Python 可以使用 easy_install 来安装，easy_install 也是 Python 包管理工具。 1$ python -m easy_install pymongo 更新 pymongo 命令： 1$ python -m easy_install -U pymongo 创建数据库创建数据库需要使用 MongoClient 对象，并且指定连接的 URL 地址和要创建的数据库名。 如下实例中，我们创建的数据库 aa : 123456#!/usr/bin/python3 import pymongo myclient = pymongo.MongoClient(&quot;mongodb://localhost:27017/&quot;)mydb = myclient[&quot;aa&quot;] 删除表12345678910111213141516171819202122232425262728293031323334#!/usr/bin/env python#-*- coding: utf-8 -*-from pymongo import MongoClientfrom datetime import datetimedef delete(year,month,day): try: client = MongoClient(&apos;mongodb://192.168.50.223:27017,192.168.50.224:27017,192.168.50.225:27017&apos;) db_auth = client.admin db_auth.authenticate(&quot;root&quot;, &quot;passwd&quot;) db = client.gag_bill old_count = db.billInfo.count() print (&quot;old_count = %d&quot; % (old_count)) db.billInfo.remove(&#123;&quot;cTimeStamp&quot;:&#123;&quot;$lte&quot;:datetime(year,month,day,0,0,0,000)&#125;&#125;) new_count = db.billInfo.count() client.close() print (&quot;del_data = %d&quot; %(old_count-new_count)) print (&quot;new_count = %d&quot; % (new_count)) except Exception as e: print (e)if __name__ == &apos;__main__&apos;: starttime = datetime.now() print (&quot;start_time = %s&quot; % (starttime)) year = starttime.year month = starttime.month day = starttime.day-4 delete(year,month,day) endtime = datetime.now() print (&quot;end_time = %s&quot; % (endtime)) runtime = (endtime - starttime).seconds print (&quot;run_time = %d seconds&quot; % (runtime))]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mongo副本集配置及一些常用命令]]></title>
    <url>%2F2018%2F09%2F19%2F%E4%B8%AD%E9%97%B4%E4%BB%B6%2Fmongo%E5%89%AF%E6%9C%AC%E9%9B%86%E9%85%8D%E7%BD%AE%E5%8F%8A%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[在每台机器都配置下mongo配置文件123456789101112[root@mangodb1 conf]# cat mongod.conf dbpath= /data/mongodb/data/logpath=/data/mongodb/logs/mongod.loglogappend=truefork=truemaxConns=2000bind_ip=127.0.0.1,10.92.160.5（IP或者主机名）directoryperdb=true#auth=truepidfilepath=/data/mongodb/logs/mongod.pidreplSet = rep#keyFile = /data/mongodb/conf/mongokey 配置mongo副本集1234config = &#123; _id:&quot;rep&quot;,members:[&#123;_id:0,host:&quot;10.92.160.5:27017&quot;&#125;,&#123;_id:1,host:&quot;10.92.160.6:27017&quot;&#125;,&#123;_id:2,host:&quot;10.92.160.7:27017&quot;&#125;]&#125;rs.initiate(config);rs.status(); （查看集群状态的） 常用命令基本命令1234show dbs 看库use 库db.setSlaveOk()show tables 看表 创建admin用户12use admindb.addUser(&quot;root&quot;,&quot;123456&quot;) 导出表加个-c1/data/mongodb/bin/mongoexport -uroot -p123456 --authenticationDatabase admin -d gag_shop -c organizationManagerAuthorities -o organizationManagerAuthorities.json 导入1/data/mongodb/bin/mongoimport -uroot -p123456 --authenticationDatabase admin -d open /root/userInterfaceAuthority.json 查表某个字段信息12db.表名字.findOne(&#123;&#125;)db.terminalMonitorInfo.findOne(&#123;&quot;_id&quot; : &quot;086273F59379&quot;&#125;) 清除日志123db.runCommand( &#123; dropDatabase: 1 &#125; ) 清楚日志，需谨慎，必须得进指定的库里面或者echo &quot;db.runCommand(&#123;dropDatabase:1&#125;)&quot; | /home/mongodb/bin/mongo -uprivate -pPrivate 127.0.0.1:27017/gag_log 导出命令1/data/server/mongodb/bin/mongoexport -uroot -pNTA3NAa579 --authenticationDatabase admin -d gag_base -c sysAuthority -q &quot;&#123;&quot;_id&quot; : /new_pro/&#125;&quot; -o new_pro.txt 导出命令]]></content>
      <categories>
        <category>中间件</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[kubectl在shell中的自动补全]]></title>
    <url>%2F2018%2F05%2F11%2Fk8s%2Fkubectl%E5%9C%A8shell%E4%B8%AD%E7%9A%84%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8%2F</url>
    <content type="text"><![CDATA[在zsh上设置1source &lt;(kubectl completion zsh) 可以导入到.zshrc中实现自动加载： 1kubectl completion zsh &gt;&gt; ~/.zshrc 键入-n以后，按tab，自动弹出可用的ns： 12➜ admin kubectl -ndefault demo-echo demo-webshell kong kube-public kube-system 在linux上设置1234yum install bash-completionsource /usr/share/bash-completion/bash_completionecho &apos;source &lt;(kubectl completion bash)&apos; &gt;&gt;~/.bashrckubectl completion bash &gt;/etc/bash_completion.d/kubectl 在mas上设置-bash12345brew install bash-completion@2export BASH_COMPLETION_COMPAT_DIR=/usr/local/etc/bash_completion.d[[ -r /usr/local/etc/profile.d/bash_completion.sh ]] &amp;&amp; . /usr/local/etc/profile.d/bash_completion.shecho &apos;source &lt;(kubectl completion bash)&apos; &gt;&gt;~/.bashrckubectl completion bash &gt;/usr/local/etc/bash_completion.d/kubectl]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[k8s基本命令]]></title>
    <url>%2F2018%2F05%2F10%2Fk8s%2Fk8s%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[pods12345$ kubectl get pods -n pro$ kubectl get pods --all-namespaces -n pro$ kubectl get pod aa -o wide -n pro$ kubectl get pod aa -o yaml -n pro$ kubectl describe pod aa -n pro POD升级和历史列出部署历史记录1$ kubectl rollout history deployment/DEPLOYMENT_NAME 跳转到特定修订版1$ kubectl rollout undo deployment/DEPLOYMENT_NAME --to-revision=N service查看服务1$ kubectl get services 将POD作为服务公开（创建端点）1$ kubectl expose deployment/aa --port=2000 --type=NodePort login$ kubectl exec -ti $1 bash -n product log$ kubectl logs -f $1 -n product]]></content>
      <categories>
        <category>k8s</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[rancher升级]]></title>
    <url>%2F2018%2F03%2F10%2F%E8%99%9A%E6%8B%9F%E5%8C%96%2Francher%E5%8D%87%E7%BA%A7%2F</url>
    <content type="text"><![CDATA[停掉 rancher先把之前的rancher-server停掉，然后在/etc/hosts上加上ip还有主机名的解析，否则升级完主机不识别 下载1.6.8镜像123docker pull privatecloud.docker.domain/privatecloud3.0/rancher_server:1.6.8在官网下载就有，我这个是自己的私有仓库 运行rancherserver1docker run -d -v /data/rancherdata:/var/lib/mysql --restart=always -p 48080:8080 privatecloud.docker.domain/privatecloud3.0/rancher_server:1.6.8 获取rancher的代理镜像1234567891011121314151617181920docker login privatecloud.docker.domain 从镜像库拉取镜像docker pull privatecloud.docker.domain/privatecloud3.0/rancher_agent:v1.2.6docker pull privatecloud.docker.domain/privatecloud3.0/rancher_network-manager:v0.7.8docker pull privatecloud.docker.domain/privatecloud3.0/rancher_net:v0.11.9docker pull privatecloud.docker.domain/privatecloud3.0/rancher_dns:v0.15.3docker pull privatecloud.docker.domain/privatecloud3.0/rancher_metadata:v0.9.4docker pull privatecloud.docker.domain/privatecloud3.0/rancher_healthcheck:v0.3.3docker pull privatecloud.docker.domain/privatecloud3.0/rancher_scheduler:v0.8.2docker pull privatecloud.docker.domain/privatecloud3.0/rancher_net:holder重命名镜像：docker tag privatecloud.docker.domain/privatecloud3.0/rancher_agent:v1.2.6 rancher/agent:v1.2.6docker tag privatecloud.docker.domain/privatecloud3.0/rancher_network-manager:v0.7.8 rancher/network-manager:v0.7.8docker tag privatecloud.docker.domain/privatecloud3.0/rancher_net:v0.11.9 rancher/net:v0.11.9docker tag privatecloud.docker.domain/privatecloud3.0/rancher_dns:v0.15.3 rancher/dns:v0.15.3docker tag privatecloud.docker.domain/privatecloud3.0/rancher_metadata:v0.9.4 rancher/metadata:v0.9.4docker tag privatecloud.docker.domain/privatecloud3.0/rancher_healthcheck:v0.3.3 rancher/healthcheck:v0.3.3docker tag privatecloud.docker.domain/privatecloud3.0/rancher_scheduler:v0.8.2 rancher/scheduler:v0.8.2docker tag privatecloud.docker.domain/privatecloud3.0/rancher_net:holder rancher/net:holder 升级12进入http://rancherserverip:48080/然后点升级即可]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python模块-logging]]></title>
    <url>%2F2018%2F02%2F15%2Fpython%2Fpython%E6%A8%A1%E5%9D%97-logging%2F</url>
    <content type="text"><![CDATA[摘要本文记录logging模块的使用方法 logging日志级别123456CRITICAL = 50 #FATAL = CRITICALERROR = 40WARNING = 30 #WARN = WARNINGINFO = 20DEBUG = 10NOTSET = 0 #不设置 默认级别为warning，默认打印到终端12345678910111213import logginglogging.debug(&apos;调试debug&apos;)logging.info(&apos;消息info&apos;)logging.warning(&apos;警告warn&apos;)logging.error(&apos;错误error&apos;)logging.critical(&apos;严重critical&apos;)&apos;&apos;&apos;WARNING:root:警告warnERROR:root:错误errorCRITICAL:root:严重critical&apos;&apos;&apos; 为logging模块指定全局配置，针对所有logger有效，控制打印到文件中12345678910111213141516171819202122232425262728可在logging.basicConfig()函数中通过具体参数来更改logging模块默认行为，可用参数有filename：用指定的文件名创建FiledHandler（后边会具体讲解handler的概念），这样日志会被存储在指定的文件中。filemode：文件打开方式，在指定了filename时使用这个参数，默认值为“a”还可指定为“w”。format：指定handler使用的日志显示格式。 datefmt：指定日期时间格式。 level：设置rootlogger（后边会讲解具体概念）的日志级别 stream：用指定的stream创建StreamHandler。可以指定输出到sys.stderr,sys.stdout或者文件，默认为sys.stderr。若同时列出了filename和stream两个参数，则stream参数会被忽略。#格式%(name)s：Logger的名字，并非用户名，详细查看%(levelno)s：数字形式的日志级别%(levelname)s：文本形式的日志级别%(pathname)s：调用日志输出函数的模块的完整路径名，可能没有%(filename)s：调用日志输出函数的模块的文件名%(module)s：调用日志输出函数的模块名%(funcName)s：调用日志输出函数的函数名%(lineno)d：调用日志输出函数的语句所在的代码行%(created)f：当前时间，用UNIX标准的表示时间的浮 点数表示%(relativeCreated)d：输出日志信息时的，自Logger创建以 来的毫秒数%(asctime)s：字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒%(thread)d：线程ID。可能没有%(threadName)s：线程名。可能没有%(process)d：进程ID。可能没有%(message)s：用户输出的消息logging.basicConfig() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#======介绍可在logging.basicConfig()函数中可通过具体参数来更改logging模块默认行为，可用参数有filename：用指定的文件名创建FiledHandler（后边会具体讲解handler的概念），这样日志会被存储在指定的文件中。filemode：文件打开方式，在指定了filename时使用这个参数，默认值为“a”还可指定为“w”。format：指定handler使用的日志显示格式。datefmt：指定日期时间格式。level：设置rootlogger（后边会讲解具体概念）的日志级别stream：用指定的stream创建StreamHandler。可以指定输出到sys.stderr,sys.stdout或者文件，默认为sys.stderr。若同时列出了filename和stream两个参数，则stream参数会被忽略。format参数中可能用到的格式化串：%(name)s Logger的名字%(levelno)s 数字形式的日志级别%(levelname)s 文本形式的日志级别%(pathname)s 调用日志输出函数的模块的完整路径名，可能没有%(filename)s 调用日志输出函数的模块的文件名%(module)s 调用日志输出函数的模块名%(funcName)s 调用日志输出函数的函数名%(lineno)d 调用日志输出函数的语句所在的代码行%(created)f 当前时间，用UNIX标准的表示时间的浮 点数表示%(relativeCreated)d 输出日志信息时的，自Logger创建以 来的毫秒数%(asctime)s 字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒%(thread)d 线程ID。可能没有%(threadName)s 线程名。可能没有%(process)d 进程ID。可能没有%(message)s用户输出的消息#========使用import logginglogging.basicConfig(filename=&apos;access.log&apos;, format=&apos;%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s&apos;, datefmt=&apos;%Y-%m-%d %H:%M:%S %p&apos;, level=10)logging.debug(&apos;调试debug&apos;)logging.info(&apos;消息info&apos;)logging.warning(&apos;警告warn&apos;)logging.error(&apos;错误error&apos;)logging.critical(&apos;严重critical&apos;)#========结果access.log内容:2017-07-28 20:32:17 PM - root - DEBUG -test: 调试debug2017-07-28 20:32:17 PM - root - INFO -test: 消息info2017-07-28 20:32:17 PM - root - WARNING -test: 警告warn2017-07-28 20:32:17 PM - root - ERROR -test: 错误error2017-07-28 20:32:17 PM - root - CRITICAL -test: 严重criticalpart2: 可以为logging模块指定模块级的配置,即所有logger的配置 logging模块的Formatter，Handler，Logger，Filter对象1234567#logger：产生日志的对象#Filter：过滤日志的对象#Handler：接收日志然后控制打印到不同的地方，FileHandler用来打印到文件中，StreamHandler用来打印到终端#Formatter对象：可以定制不同的日志格式对象，然后绑定给不同的Handler对象使用，以此来控制不同的Handler的日志格式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&apos;&apos;&apos;critical=50error =40warning =30info = 20debug =10&apos;&apos;&apos;import logging#1、logger对象：负责产生日志，然后交给Filter过滤，然后交给不同的Handler输出logger=logging.getLogger(__file__)#2、Filter对象：不常用，略#3、Handler对象：接收logger传来的日志，然后控制输出h1=logging.FileHandler(&apos;t1.log&apos;) #打印到文件h2=logging.FileHandler(&apos;t2.log&apos;) #打印到文件h3=logging.StreamHandler() #打印到终端#4、Formatter对象：日志格式formmater1=logging.Formatter(&apos;%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s&apos;, datefmt=&apos;%Y-%m-%d %H:%M:%S %p&apos;,)formmater2=logging.Formatter(&apos;%(asctime)s : %(message)s&apos;, datefmt=&apos;%Y-%m-%d %H:%M:%S %p&apos;,)formmater3=logging.Formatter(&apos;%(name)s %(message)s&apos;,)#5、为Handler对象绑定格式h1.setFormatter(formmater1)h2.setFormatter(formmater2)h3.setFormatter(formmater3)#6、将Handler添加给logger并设置日志级别logger.addHandler(h1)logger.addHandler(h2)logger.addHandler(h3)logger.setLevel(10)#7、测试logger.debug(&apos;debug&apos;)logger.info(&apos;info&apos;)logger.warning(&apos;warning&apos;)logger.error(&apos;error&apos;)logger.critical(&apos;critical&apos;) Logger与Handler的级别logger是第一级过滤，然后才能到handler，我们可以给logger和handler同时设置level，但是需要注意的是 1234567891011121314151617181920212223Logger is also the first to filter the message based on a level — if you set the logger to INFO, and all handlers to DEBUG, you still won&apos;t receive DEBUG messages on handlers — they&apos;ll be rejected by the logger itself. If you set logger to DEBUG, but all handlers to INFO, you won&apos;t receive any DEBUG messages either — because while the logger says &quot;ok, process this&quot;, the handlers reject it (DEBUG &lt; INFO).#验证import loggingform=logging.Formatter(&apos;%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s&apos;, datefmt=&apos;%Y-%m-%d %H:%M:%S %p&apos;,)ch=logging.StreamHandler()ch.setFormatter(form)# ch.setLevel(10)ch.setLevel(20)l1=logging.getLogger(&apos;root&apos;)# l1.setLevel(20)l1.setLevel(10)l1.addHandler(ch)l1.debug(&apos;l1 debug&apos;)重要，重要，重要！！！ Logger的继承（了解）123456789101112131415161718192021222324252627282930import loggingformatter=logging.Formatter(&apos;%(asctime)s - %(name)s - %(levelname)s -%(module)s: %(message)s&apos;, datefmt=&apos;%Y-%m-%d %H:%M:%S %p&apos;,)ch=logging.StreamHandler()ch.setFormatter(formatter)logger1=logging.getLogger(&apos;root&apos;)logger2=logging.getLogger(&apos;root.child1&apos;)logger3=logging.getLogger(&apos;root.child1.child2&apos;)logger1.addHandler(ch)logger2.addHandler(ch)logger3.addHandler(ch)logger1.setLevel(10)logger2.setLevel(10)logger3.setLevel(10)logger1.debug(&apos;log1 debug&apos;)logger2.debug(&apos;log2 debug&apos;)logger3.debug(&apos;log3 debug&apos;)&apos;&apos;&apos;2017-07-28 22:22:05 PM - root - DEBUG -test: log1 debug2017-07-28 22:22:05 PM - root.child1 - DEBUG -test: log2 debug2017-07-28 22:22:05 PM - root.child1 - DEBUG -test: log2 debug2017-07-28 22:22:05 PM - root.child1.child2 - DEBUG -test: log3 debug2017-07-28 22:22:05 PM - root.child1.child2 - DEBUG -test: log3 debug2017-07-28 22:22:05 PM - root.child1.child2 - DEBUG -test: log3 debug&apos;&apos;&apos; 应用logging配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&quot;&quot;&quot;logging配置&quot;&quot;&quot;import osimport logging.config# 定义三种日志输出格式 开始standard_format = &apos;[%(asctime)s][%(threadName)s:%(thread)d][task_id:%(name)s][%(filename)s:%(lineno)d]&apos; \ &apos;[%(levelname)s][%(message)s]&apos; #其中name为getlogger指定的名字simple_format = &apos;[%(levelname)s][%(asctime)s][%(filename)s:%(lineno)d]%(message)s&apos;id_simple_format = &apos;[%(levelname)s][%(asctime)s] %(message)s&apos;# 定义日志输出格式 结束logfile_dir = os.path.dirname(os.path.abspath(__file__)) # log文件的目录logfile_name = &apos;all2.log&apos; # log文件名# 如果不存在定义的日志目录就创建一个if not os.path.isdir(logfile_dir): os.mkdir(logfile_dir)# log文件的全路径logfile_path = os.path.join(logfile_dir, logfile_name)# log配置字典LOGGING_DIC = &#123; &apos;version&apos;: 1, &apos;disable_existing_loggers&apos;: False, &apos;formatters&apos;: &#123; &apos;standard&apos;: &#123; &apos;format&apos;: standard_format &#125;, &apos;simple&apos;: &#123; &apos;format&apos;: simple_format &#125;, &#125;, &apos;filters&apos;: &#123;&#125;, &apos;handlers&apos;: &#123; #打印到终端的日志 &apos;console&apos;: &#123; &apos;level&apos;: &apos;DEBUG&apos;, &apos;class&apos;: &apos;logging.StreamHandler&apos;, # 打印到屏幕 &apos;formatter&apos;: &apos;simple&apos; &#125;, #打印到文件的日志,收集info及以上的日志 &apos;default&apos;: &#123; &apos;level&apos;: &apos;DEBUG&apos;, &apos;class&apos;: &apos;logging.handlers.RotatingFileHandler&apos;, # 保存到文件 &apos;formatter&apos;: &apos;standard&apos;, &apos;filename&apos;: logfile_path, # 日志文件 &apos;maxBytes&apos;: 1024*1024*5, # 日志大小 5M &apos;backupCount&apos;: 5, &apos;encoding&apos;: &apos;utf-8&apos;, # 日志文件的编码，再也不用担心中文log乱码了 &#125;, &#125;, &apos;loggers&apos;: &#123; #logging.getLogger(__name__)拿到的logger配置 &apos;&apos;: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos;], # 这里把上面定义的两个handler都加上，即log数据既写入文件又打印到屏幕 &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: True, # 向上（更高level的logger）传递 &#125;, &#125;,&#125;def load_my_logging_cfg(): logging.config.dictConfig(LOGGING_DIC) # 导入上面定义的logging配置 logger = logging.getLogger(__name__) # 生成一个log实例 logger.info(&apos;It works!&apos;) # 记录该文件的运行状态if __name__ == &apos;__main__&apos;: load_my_logging_cfg() 使用123456789101112131415161718192021222324&quot;&quot;&quot;MyLogging Test&quot;&quot;&quot;import timeimport loggingimport my_logging # 导入自定义的logging配置logger = logging.getLogger(__name__) # 生成logger实例def demo(): logger.debug(&quot;start range... time:&#123;&#125;&quot;.format(time.time())) logger.info(&quot;中文测试开始。。。&quot;) for i in range(10): logger.debug(&quot;i:&#123;&#125;&quot;.format(i)) time.sleep(0.2) else: logger.debug(&quot;over range... time:&#123;&#125;&quot;.format(time.time())) logger.info(&quot;中文测试结束。。。&quot;)if __name__ == &quot;__main__&quot;: my_logging.load_my_logging_cfg() # 在你程序文件的入口加载自定义logging配置 demo() 关于如何拿到logger对象的详细解释12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849注意注意注意：#1、有了上述方式我们的好处是：所有与logging模块有关的配置都写到字典中就可以了，更加清晰，方便管理#2、我们需要解决的问题是： 1、从字典加载配置：logging.config.dictConfig(settings.LOGGING_DIC) 2、拿到logger对象来产生日志 logger对象都是配置到字典的loggers 键对应的子字典中的 按照我们对logging模块的理解，要想获取某个东西都是通过名字，也就是key来获取的 于是我们要获取不同的logger对象就是 logger=logging.getLogger(&apos;loggers子字典的key名&apos;) 但问题是：如果我们想要不同logger名的logger对象都共用一段配置，那么肯定不能在loggers子字典中定义n个key &apos;loggers&apos;: &#123; &apos;l1&apos;: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos;], # &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: True, # 向上（更高level的logger）传递 &#125;, &apos;l2: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos; ], &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: False, # 向上（更高level的logger）传递 &#125;, &apos;l3&apos;: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos;], # &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: True, # 向上（更高level的logger）传递 &#125;,&#125; #我们的解决方式是，定义一个空的key &apos;loggers&apos;: &#123; &apos;&apos;: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos;], &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: True, &#125;,&#125;这样我们再取logger对象时logging.getLogger(__name__)，不同的文件__name__不同，这保证了打印日志时标识信息不同，但是拿着该名字去loggers里找key名时却发现找不到，于是默认使用key=&apos;&apos;的配置 另外一个django的配置，瞄一眼就可以，跟上面的一样123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#logging_config.pyLOGGING = &#123; &apos;version&apos;: 1, &apos;disable_existing_loggers&apos;: False, &apos;formatters&apos;: &#123; &apos;standard&apos;: &#123; &apos;format&apos;: &apos;[%(asctime)s][%(threadName)s:%(thread)d][task_id:%(name)s][%(filename)s:%(lineno)d]&apos; &apos;[%(levelname)s][%(message)s]&apos; &#125;, &apos;simple&apos;: &#123; &apos;format&apos;: &apos;[%(levelname)s][%(asctime)s][%(filename)s:%(lineno)d]%(message)s&apos; &#125;, &apos;collect&apos;: &#123; &apos;format&apos;: &apos;%(message)s&apos; &#125; &#125;, &apos;filters&apos;: &#123; &apos;require_debug_true&apos;: &#123; &apos;()&apos;: &apos;django.utils.log.RequireDebugTrue&apos;, &#125;, &#125;, &apos;handlers&apos;: &#123; #打印到终端的日志 &apos;console&apos;: &#123; &apos;level&apos;: &apos;DEBUG&apos;, &apos;filters&apos;: [&apos;require_debug_true&apos;], &apos;class&apos;: &apos;logging.StreamHandler&apos;, &apos;formatter&apos;: &apos;simple&apos; &#125;, #打印到文件的日志,收集info及以上的日志 &apos;default&apos;: &#123; &apos;level&apos;: &apos;INFO&apos;, &apos;class&apos;: &apos;logging.handlers.RotatingFileHandler&apos;, # 保存到文件，自动切 &apos;filename&apos;: os.path.join(BASE_LOG_DIR, &quot;xxx_info.log&quot;), # 日志文件 &apos;maxBytes&apos;: 1024 * 1024 * 5, # 日志大小 5M &apos;backupCount&apos;: 3, &apos;formatter&apos;: &apos;standard&apos;, &apos;encoding&apos;: &apos;utf-8&apos;, &#125;, #打印到文件的日志:收集错误及以上的日志 &apos;error&apos;: &#123; &apos;level&apos;: &apos;ERROR&apos;, &apos;class&apos;: &apos;logging.handlers.RotatingFileHandler&apos;, # 保存到文件，自动切 &apos;filename&apos;: os.path.join(BASE_LOG_DIR, &quot;xxx_err.log&quot;), # 日志文件 &apos;maxBytes&apos;: 1024 * 1024 * 5, # 日志大小 5M &apos;backupCount&apos;: 5, &apos;formatter&apos;: &apos;standard&apos;, &apos;encoding&apos;: &apos;utf-8&apos;, &#125;, #打印到文件的日志 &apos;collect&apos;: &#123; &apos;level&apos;: &apos;INFO&apos;, &apos;class&apos;: &apos;logging.handlers.RotatingFileHandler&apos;, # 保存到文件，自动切 &apos;filename&apos;: os.path.join(BASE_LOG_DIR, &quot;xxx_collect.log&quot;), &apos;maxBytes&apos;: 1024 * 1024 * 5, # 日志大小 5M &apos;backupCount&apos;: 5, &apos;formatter&apos;: &apos;collect&apos;, &apos;encoding&apos;: &quot;utf-8&quot; &#125; &#125;, &apos;loggers&apos;: &#123; #logging.getLogger(__name__)拿到的logger配置 &apos;&apos;: &#123; &apos;handlers&apos;: [&apos;default&apos;, &apos;console&apos;, &apos;error&apos;], &apos;level&apos;: &apos;DEBUG&apos;, &apos;propagate&apos;: True, &#125;, #logging.getLogger(&apos;collect&apos;)拿到的logger配置 &apos;collect&apos;: &#123; &apos;handlers&apos;: [&apos;console&apos;, &apos;collect&apos;], &apos;level&apos;: &apos;INFO&apos;, &#125; &#125;,&#125;# -----------# 用法:拿到俩个loggerlogger = logging.getLogger(__name__) #线上正常的日志collect_logger = logging.getLogger(&quot;collect&quot;) #领导说,需要为领导们单独定制领导们看的日志 文件自动截断12345678910111213141516171819202122import loggingfrom logging import handlerslogger = logging.getLogger(__name__)log_file = &quot;timelog.log&quot;#fh = handlers.RotatingFileHandler(filename=log_file,maxBytes=10,backupCount=3)fh = handlers.TimedRotatingFileHandler(filename=log_file,when=&quot;S&quot;,interval=5,backupCount=3)formatter = logging.Formatter(&apos;%(asctime)s %(module)s:%(lineno)d %(message)s&apos;)fh.setFormatter(formatter)logger.addHandler(fh)logger.warning(&quot;test1&quot;)logger.warning(&quot;test12&quot;)logger.warning(&quot;test13&quot;)logger.warning(&quot;test14&quot;) other12345678910111213141516171819202122232425262728293031import logging #create loggerlogger = logging.getLogger(&apos;TEST-LOG&apos;)logger.setLevel(logging.DEBUG) # create console handler and set level to debugch = logging.StreamHandler()ch.setLevel(logging.DEBUG) # create file handler and set level to warningfh = logging.FileHandler(&quot;access.log&quot;)fh.setLevel(logging.WARNING)# create formatterformatter = logging.Formatter(&apos;%(asctime)s - %(name)s - %(levelname)s - %(message)s&apos;) # add formatter to ch and fhch.setFormatter(formatter)fh.setFormatter(formatter) # add ch and fh to loggerlogger.addHandler(ch)logger.addHandler(fh) # &apos;application&apos; codelogger.debug(&apos;debug message&apos;)logger.info(&apos;info message&apos;)logger.warn(&apos;warn message&apos;)logger.error(&apos;error message&apos;)logger.critical(&apos;critical message&apos;)]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[gitlab-ci自动部署（二）]]></title>
    <url>%2F2017%2F05%2F21%2Fgitlab%2Fgitlab-ci(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[下面来说说CI/CD是怎么实现的： 登录gitlab-runner机器切换runner普通用户 [root@localhost ~]# su - gitlab-runner 编写.gitlab-ci.yml文件随便创建一个目录，叫什么无所谓 [gitlab-runner@localhost ~]$ mkdir git 将gitlab上想要发布的项目克隆到这个目录里 [gitlab-runner@localhost git]$ git clone git@gitlab.xxx.com:xxx/xxx.git 因为我们的项目是大工程，里面带着很多子工程，所以就需要通过yml文件，将项目分离出去 比方说我们想发布bb项目，但是bb项目属于aa这个大项目的子项目，所以就进aa大工程目录下 在项目目录里面创建一个.gitlab-ci.yml文件，如下 [gitlab-runner@localhost ~]$ cat .gitlab-ci.yml12345678job 1: stage: test script: - git subtree push -q --prefix=bb git@gitlab.xx.com:bb1/bb.git dev only: - dev tags: - shell 这样就可以将bb项目分离出去，相当于创建了一个新的项目 进入bb目录，也编写.gitlab-ci.yml文件，这个就是我们需要编译的脚本，中间可以穿插maven、node和shell的一系列命令 [gitlab-runner@localhost bb]$ cat .gitlab-ci.yml12345678910job 1: stage: build script: - rm -rf /opt/M2_REPO/com/bb/* - mvn clean package -P test -Dmaven.test.skip - bash -x /opt/bb/shell/bb.sh only: - dev tags: - shell 编写完yml文件后都需要提交下 git add . git commit -m “add gitlab-ci.yml” git push origin dev 另外包括传包，启动等命令都可以在yml文件里面体现，这样只要dev分支有commit的改变了，gitlab-ci就会自动创建job来自动发布]]></content>
      <categories>
        <category>gitlab</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[gitlab-ci自动部署（一）]]></title>
    <url>%2F2017%2F05%2F20%2Fgitlab%2Fgitlab-ci(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[目前GitLab已经有了CI功能，即持续集成的功能。可以实现代码提交后自动测试、编译、发布、部署等自动化工作 下面是我总结的实现内容： 安装runner在root下执行 下载gitlab-runner wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-runner/yum/el7/gitlab-runner-10.5.0-1.x86_64.rpm 安装 rpm -ivh gitlab-runner-10.5.0-1.x86_64.rpm 配置Runner默认情况，Runner是通过gitlab-runner的这个用户来执行一系列操作，其工作目录也是在gitlab-runner的用户目录下面。如果使用默认gitlab-runner用户操作一些文件时经常会遇到权限问题，就需要给gitlab-runner赋权。我们通过以下方式修改。 #在root下执行 #删除服务 gitlab-runner uninstall #添加服务 gitlab-runner install –working-directory /home/builds –user gitlab-ci #重启服务 gitlab-runner restart #查看状态 gitlab-runner status 输出：gitlab-runner: Service is running! #查看是否生效 ps -ef | grep gitlab-runner 注册Runner先打开GitLab上需要自动部署的项目界面，找到该项目的Settings –&gt; CI/CD –&gt; Runners settings 在gitlab上可以看到自己的token信息，用来注册runner #在root下执行gitlab-runner register (会出现注册信息，填url，token，runner的名字) 成功之后会如下图： 至此安装部分就完成了]]></content>
      <categories>
        <category>gitlab</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos7搭建shadowsock实现vpn翻墙]]></title>
    <url>%2F2017%2F04%2F22%2Fvpn%2Fcentos7%E6%90%AD%E5%BB%BAshadowsock%E5%AE%9E%E7%8E%B0vpn%E7%BF%BB%E5%A2%99%2F</url>
    <content type="text"><![CDATA[安装使用root用户登录，运行以下命令：123wget --no-check-certificate -O shadowsocks.sh https://cyh.abcdocker.com/vpn/shadowsocks.sh chmod +x shadowsocks.sh ./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 安装完成后，脚本提示如下： 1234567 Congratulations, Shadowsocks-python server install completed!Your Server IP :your_server_ipYour Server Port :your_server_portYour Password :your_passwordYour Encryption Method:your_encryption_methodWelcome to visit:https://teddysun.com/342.htmlEnjoy it! 卸载方法使用root用户登录，运行以下命令： ./shadowsocks.sh uninstall 配置文件路径：/etc/shadowsocks.json 12345678910&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:your_server_port, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;your_password&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;your_encryption_method&quot;, &quot;fast_open&quot;: false&#125; 多用户多端口配置文件配置文件路径：/etc/shadowsocks.json 123456789101112131415 &#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;local_address&quot;:&quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;port_password&quot;:&#123; &quot;8989&quot;:&quot;password0&quot;, &quot;9001&quot;:&quot;password1&quot;, &quot;9002&quot;:&quot;password2&quot;, &quot;9003&quot;:&quot;password3&quot;, &quot;9004&quot;:&quot;password4&quot; &#125;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;your_encryption_method&quot;, &quot;fast_open&quot;: false&#125; 1234启动：/etc/init.d/shadowsocks start停止：/etc/init.d/shadowsocks stop重启：/etc/init.d/shadowsocks restart状态：/etc/init.d/shadowsocks status]]></content>
      <categories>
        <category>vpn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pptp]]></title>
    <url>%2F2017%2F04%2F20%2Fvpn%2Fpptp%2F</url>
    <content type="text"><![CDATA[安装pptpd$ yum install pptpd 配置本地及远程IP地址$ vi /etc/pptpd.conf localip 192.168.100.1 remoteip 192.168.100.2-245 配置DNS$ vi /etc/ppp/options.pptpd ms-dns 223.5.5.5 ms-dns 223.6.6.6 修改MTU$ vi / etc / ppp / ip-up /sbin/ifconfig $ 1 mtu 1500 用户和密码配置$ vi /etc/ppp/chap-secrets client server secret IP address 注释掉 testin_user1 pptpd testin_user1 * 配置防火墙$ vi /etc/sysctl.conf net.ipv4.ip_forward = 1＃开启IP转发 $ iptables -t nat -A POSTROUTING -s 192.168.100.0/24 -o eth1 -j MASQUERADE＃允许外网连接 开启服务$ service pptpd start $ service iptables start]]></content>
      <categories>
        <category>vpn</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux下使用rsync最快删除海量文件的方法]]></title>
    <url>%2F2017%2F04%2F17%2Flinux%2FLinux%E4%B8%8B%E4%BD%BF%E7%94%A8rsync%E6%9C%80%E5%BF%AB%E5%88%A0%E9%99%A4%E6%B5%B7%E9%87%8F%E6%96%87%E4%BB%B6%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[摘要在linux的web服务器维护过程当中，有些程序会使用本地file缓存或生成大量程序日志。当发布进行版本迭代时，由于上个版本的程序会保留一段时间，因此这些小文件会消耗大量的inode。这个时候，我们常用的删除命令rm -fr * 就不好用了，因为要等待的时间太长。所以必须要采取一些其他手段来删除这些大量的小文件。这个时候，我们可以使用rsync来实现快速删除大量文件。 安装rsync安装很简单，这里我们直接使用yum安装即可 1yum install -y rsync 创建一个空的文件夹1mkdir /tmp/null 用rsync删除目标目录1rsync --delete-before -a -H -v --progress --stats /tmp/null/ /data/web/app/xxx/cache/ 这样我们要删除的cache目录就会被清空了，删除的速度会非常快。 rsync实际上用的是替换原理，处理数十万个文件也是秒删。 参数123456–delete-before 接收者在传输之前进行删除操作–progress 在传输时显示传输过程-a 归档模式，表示以递归方式传输文件，并保持所有文件属性-H 保持硬连接的文件-v 详细输出模式–stats 给出某些文件的传输状态]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[redmine]]></title>
    <url>%2F2017%2F03%2F21%2F%E7%89%88%E6%9C%AC%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7%2Fredmine%2F</url>
    <content type="text"><![CDATA[版本： redmine 3.1.1官方文档地址： http://www.redmine.org.cn/category/install Redmine实战下列描述均以操作系统 Centos7 为例部署。 环境介绍OS IP HostName RoleCentOS7 x64 192.168.10.10 node1 Redmine恢复机器CentOS7 x64 10.10.1.17 localhost Redmine机器 准备工作关闭Iptables和SELinux[root@node1 ~]# systemctl stop firewalld[root@node1 ~]# systemctl disable firewalld[root@node1 ~]# setenforce 0[root@node1 ~]# sed -i ‘/^SELINUX=/{ s/enforcing/disabled/ }’ /etc/selinux/config 调整服务器时间[root@node1 ~]# yum -y install ntp[root@node1 ~]# ntpdate -u 202.120.2.101 安装配置 Redmine安装依赖环境[root@node1 ~]# yum install -y zlib-devel openssl-devel ImageMagick-devel wget curl-devel rubygems mod_fcgid 安装RVM[root@node1 ~]# gpg –keyserver hkp://keys.gnupg.net –recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3[root@node1 ~]# curl -L https://get.rvm.io | bash -s stable 载入RVM环境并获取需要的支持安装包[root@node1 ~]# source /etc/profile.d/rvm.sh[root@node1 ~]# rvm requirements 利用rvm安装 Ruby 2.2.3 并设为默认[root@node1 ~]# sed -i -E ‘s!https?://cache.ruby-lang.org/pub/ruby!https://ruby.taobao.org/mirrors/ruby!&#39; /usr/local/rvm/config/db[root@node1 ~]# rvm gemset create[root@node1 ~]# rvm install 2.2.3[root@node1 ~]# rvm use 2.2.3 –default 添加淘宝镜像[root@node1 ~]# gem sources –add https://gems.ruby-china.org/ –remove https://rubygems.org/[root@node1 ~]# gem sources -l CURRENT SOURCES https://gems.ruby-china.org 安装rails[root@node1 ~]# gem install rails -v=4.2 安装mysql和httpd[root@node1 ~]# yum install httpd httpd-devel -y [root@node1 ~]# wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm[root@node1 ~]# rpm -ivh mysql-community-release-el7-5.noarch.rpm[root@node1 ~]# yum -y install mysql-community-server mysql-devel[root@node1 ~]# service mysqld restart[root@node1 ~]# mysql -u rootmysql&gt; create database redmine character set utf8;mysql&gt; create user ‘redmine‘@’localhost’ identified by ‘redmine’;mysql&gt; grant all privileges on redmine.* to ‘redmine‘@’localhost’; 安装redmine的apache支持，这样可以通过apache访问[root@node1 ~]# gem install passenger[root@node1 ~]# passenger-install-apache2-module [root@node1 ~]# vim /etc/httpd/conf.d/passenger.confLoadModule passenger_module /usr/local/rvm/gems/ruby-2.2.3/gems/passenger-5.0.30/buildout/apache2/mod_passenger.so PassengerRoot /usr/local/rvm/gems/ruby-2.2.3/gems/passenger-5.0.30 PassengerDefaultRuby /usr/local/rvm/gems/ruby-2.2.3/wrappers/ruby [root@node1 ~]# vim /etc/httpd/conf.d/redmine.conf ServerName www.a.com # !!! Be sure to point DocumentRoot to ‘public’! DocumentRoot /var/www/html/redmine/public ErrorLog logs/redmine_error_log &lt;Directory /var/www/html/redmine/public&gt; Options Indexes ExecCGI FollowSymLinks Order allow,deny Allow from all # This relaxes Apache security settings. AllowOverride all # MultiViews must be turned off. Options -MultiViews # Uncomment this if you’re on Apache &gt;= 2.4: #Require all granted 安装redmine[root@node1 ~]# cd /var/www/html[root@node1 ~]# wget http://www.redmine.org/releases/redmine-3.1.1.tar.gz[root@node1 ~]# tar -zxvf redmine-3.1.1.tar.gz[root@node1 ~]# mv redmine-3.1.1 redmine[root@node1 ~]# cd /var/www/html/redmine/ [root@node1 ~]# vim Gemfile # 修改sourcesource ‘https://rubygems.org&#39; （注释掉）source’https://ruby.taobao.org&#39; [root@node1 ~]# cp config/configuration.yml.example config/configuration.yml[root@node1 ~]# cp config/database.yml.example config/database.yml[root@node1 ~]# vim config/database.yml # 修改数据连接production: adapter: mysql2 database: redmine host: localhost username: redmine password: “redmine” encoding: utf8 [root@node1 redmine]# gem install bundler # 注意是在网站根目录下执行[root@node1 redmine]# gem install rack-cache -v ‘1.4.2’[root@node1 redmine]# bundle install 为Rails生成cookies密钥[root@node1 redmine]# rake generate_secret_token 初始化redmine数据库表名[root@node1 redmine]# RAILS_ENV=production rake db:migrate[root@node1 redmine]# RAILS_ENV=production rake redmine:load_default_data 启动[root@node1 ~]# cd /var/www/html/redmine[root@node1 redmine]# mkdir /var/www/html/logs[root@node1 redmine]# bundle exec rails server webrick -e production -b 0.0.0.0 &amp;&gt;&gt; /var/www/html/logs/redmine.log &amp;访问地址：http://IP:3000 备份[root@node1 ~]# mysqldump -u root redmine &gt; /root/redmine.sql 恢复停止redmine， 步骤：ps -ef | grep rails，找到redmine的进程号，然后kill掉 恢复数据库[root@node1 ~]# mysql -u redmine -p redmine &lt; /root/redmine.sql 把10.10.2.120上的/backup/redmine-back/redmine_file目录下的所有文件拷贝到192.168.100.10中的/var/www/html/redmine/files目录[root@node1 ~]# scp -r root@10.10.2.120:/backup/redmine-back/redmine_file/* /var/www/html/redmine/files[root@node1 ~]# chmod -R 755 /var/www/html/redmine/files 启动redmine[root@node1 redmine]# bundle exec rails server webrick -e production -b 0.0.0.0 &amp;&gt;&gt; /var/www/html/logs/redmine.log &amp; 调整配置配置邮件发送，空格缩进必须如下，不然redmine无法启动[root@node1 ~]# vim /var/www/html/redmine/config/configuration.ymldefault: email_delivery: delivery_method: :smtp smtp_settings: openssl_verify_mode: ‘none’ address: mail.testin.cn port: 587 domain: testin.cn authentication: :login user_name: “project@testin.cn“ password: “m12345678” 重启redmine即可]]></content>
      <categories>
        <category>版本管理工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[centos6.4安装kvm]]></title>
    <url>%2F2017%2F03%2F20%2F%E8%99%9A%E6%8B%9F%E5%8C%96%2Fcentos6.4%E5%AE%89%E8%A3%85kvm%2F</url>
    <content type="text"><![CDATA[首先检查您的CPU是否支持硬件虚拟化 egrep ‘(vmx|svm)’ –color=always /proc/cpuinfo 应该显示一些东西，例如： [root@server1 ~]# egrep ‘(vmx|svm)’ –color=always /proc/cpuinfoflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt rdtscp lm 3dnowext 3dnow pni cx16 lahf_lm cmp_legacy svm extapic cr8_legacy misalignsseflags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt rdtscp lm 3dnowext 3dnow pni cx16 lahf_lm cmp_legacy svm extapic cr8_legacy misalignsse[root@server1 ~]# 现在我们导入软件包的GPG密钥： rpm–import /etc/pki/rpm-gpg/RPM-GPG-KEY* yum install kvm libvirt python-virtinst qemu-kvm /etc/init.d/libvirtd start virsh -c qemu:///system list 导入镜像文件 xterm-253-1.el6.x86_64.rpm 之后xmanager开始kvm就可以了]]></content>
      <categories>
        <category>虚拟化</category>
      </categories>
  </entry>
</search>
